[2024-10-10 02:14:20,703][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:16724', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-10 02:14:23,394][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-10 02:14:23,396][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-10 02:14:23,403][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-10 02:14:23,403][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-10 02:14:23,404][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-10 02:14:23,405][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-10 02:14:23,831][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-10 02:14:30,311][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:10473', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-10 02:14:33,281][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-10 02:14:33,283][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-10 02:14:33,283][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-10 02:14:33,283][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-10 02:14:33,284][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-10 02:14:33,285][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-10 02:14:33,530][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-10 02:14:33,615][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:13710', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-10 02:14:35,382][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:15238', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-10 02:14:35,448][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:19800', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-10 02:14:35,655][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:10531', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-10 02:14:37,005][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-10 02:14:37,011][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-10 02:14:37,011][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-10 02:14:37,011][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-10 02:14:37,012][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-10 02:14:37,013][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-10 02:14:37,387][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-10 02:14:37,647][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:13101', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-10 02:14:40,644][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:16485', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-10 02:14:41,572][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-10 02:14:41,583][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-10 02:14:41,583][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-10 02:14:41,583][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-10 02:14:41,584][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-10 02:14:41,585][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-10 02:14:42,069][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-10 02:14:43,353][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-10 02:14:43,364][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-10 02:14:43,364][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-10 02:14:43,364][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-10 02:14:43,365][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-10 02:14:43,365][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-10 02:14:44,774][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-10 02:14:47,717][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-10 02:14:47,731][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-10 02:14:47,731][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-10 02:14:47,731][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-10 02:14:47,732][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-10 02:14:47,733][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-10 02:14:48,710][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-10 02:14:50,561][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-10 02:14:50,564][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-10 02:14:50,595][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-10 02:14:50,595][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-10 02:14:50,597][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-10 02:14:50,607][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-10 02:14:51,290][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-10 02:15:18,710][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-10 02:15:19,375][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-10 02:15:19,375][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-10 02:15:19,375][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-10 02:15:19,377][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-10 02:15:19,377][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-10 02:15:28,606][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-10 02:16:14,548][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-10 02:16:14,570][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-10 02:16:14,570][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-10 02:16:14,570][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-10 02:16:14,570][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-10 02:16:14,570][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-10 02:16:14,570][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-10 02:16:14,570][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-10 02:16:14,570][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-10 02:16:14,570][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-10 02:16:14,570][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-10 02:16:14,570][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-10 02:16:14,574][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-10 02:16:20,552][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-10 02:16:20,552][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-10 02:16:20,553][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-10 02:16:20,553][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-10 02:16:20,553][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-10 02:16:20,553][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-10 02:16:20,553][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-10 02:16:20,553][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-10 02:16:20,553][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-10 02:16:20,553][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-10 02:16:20,553][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-10 02:16:20,553][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-10 02:16:20,554][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-10 02:16:50,223][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-10 02:16:50,321][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-10 02:16:50,322][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-10 02:16:50,322][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-10 02:16:50,322][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-10 02:16:50,322][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-10 02:16:50,322][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-10 02:16:50,322][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-10 02:16:50,322][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-10 02:16:50,322][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-10 02:16:50,322][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-10 02:16:50,323][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-10 02:16:50,324][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-10 02:16:57,555][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 1501 @ 71967 updates)
[2024-10-10 02:16:57,557][fairseq.trainer][INFO] - loading train data for epoch 1501
[2024-10-10 02:17:02,129][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-10 02:17:10,418][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-10 02:17:10,419][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-10 02:17:10,419][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-10 02:17:10,419][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-10 02:17:10,419][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-10 02:17:10,420][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-10 02:17:10,420][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-10 02:17:10,420][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-10 02:17:10,420][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-10 02:17:10,420][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-10 02:17:10,420][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-10 02:17:10,421][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-10 02:17:10,422][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-10 02:17:11,267][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 02:17:11,273][fairseq.trainer][INFO] - begin training epoch 1501
[2024-10-10 02:17:11,280][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 02:17:31,829][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 1501 @ 71967 updates)
[2024-10-10 02:17:31,839][fairseq.trainer][INFO] - loading train data for epoch 1501
[2024-10-10 02:17:32,399][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-10 02:17:38,761][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 02:17:38,776][fairseq.trainer][INFO] - begin training epoch 1501
[2024-10-10 02:17:38,777][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 02:17:52,260][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-10 02:17:52,261][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-10 02:17:52,261][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-10 02:17:52,261][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-10 02:17:52,261][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-10 02:17:52,261][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-10 02:17:52,261][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-10 02:17:52,261][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-10 02:17:52,261][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-10 02:17:52,261][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-10 02:17:52,262][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-10 02:17:52,262][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-10 02:17:52,264][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-10 02:17:56,775][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 1501 @ 71967 updates)
[2024-10-10 02:17:56,777][fairseq.trainer][INFO] - loading train data for epoch 1501
[2024-10-10 02:18:01,589][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-10 02:18:15,836][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-10 02:18:15,839][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-10 02:18:15,839][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-10 02:18:15,839][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-10 02:18:15,839][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-10 02:18:15,839][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-10 02:18:15,839][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-10 02:18:15,839][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-10 02:18:15,839][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-10 02:18:15,840][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-10 02:18:15,840][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-10 02:18:15,840][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-10 02:18:15,842][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-10 02:18:34,618][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 1501 @ 71967 updates)
[2024-10-10 02:18:34,628][fairseq.trainer][INFO] - loading train data for epoch 1501
[2024-10-10 02:18:35,174][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-10 02:18:42,093][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 02:18:42,100][fairseq.trainer][INFO] - begin training epoch 1501
[2024-10-10 02:18:42,101][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 02:18:46,701][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 1501 @ 71967 updates)
[2024-10-10 02:18:46,708][fairseq.trainer][INFO] - loading train data for epoch 1501
[2024-10-10 02:18:46,959][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-10 02:18:57,716][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 02:18:57,720][fairseq.trainer][INFO] - begin training epoch 1501
[2024-10-10 02:18:57,721][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 02:19:00,668][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-10 02:19:00,683][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-10 02:19:00,683][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-10 02:19:00,683][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-10 02:19:00,683][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-10 02:19:00,683][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-10 02:19:00,683][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-10 02:19:00,683][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-10 02:19:00,684][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-10 02:19:00,684][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-10 02:19:00,691][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-10 02:19:00,691][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-10 02:19:00,692][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-10 02:19:00,915][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 02:19:00,948][fairseq.trainer][INFO] - begin training epoch 1501
[2024-10-10 02:19:00,968][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-10 02:19:00,959][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 02:19:00,968][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-10 02:19:00,975][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-10 02:19:00,975][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-10 02:19:00,975][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-10 02:19:00,975][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-10 02:19:00,975][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-10 02:19:00,975][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-10 02:19:00,975][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-10 02:19:00,975][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-10 02:19:00,976][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-10 02:19:00,976][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-10 02:19:00,977][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-10 02:19:04,502][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 1501 @ 71967 updates)
[2024-10-10 02:19:04,508][fairseq.trainer][INFO] - loading train data for epoch 1501
[2024-10-10 02:19:04,886][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-10 02:19:13,863][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 02:19:13,871][fairseq.trainer][INFO] - begin training epoch 1501
[2024-10-10 02:19:13,871][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 02:19:43,738][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 1501 @ 71967 updates)
[2024-10-10 02:19:43,751][fairseq.trainer][INFO] - loading train data for epoch 1501
[2024-10-10 02:19:44,657][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-10 02:19:45,040][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 1501 @ 71967 updates)
[2024-10-10 02:19:45,042][fairseq.trainer][INFO] - loading train data for epoch 1501
[2024-10-10 02:19:45,396][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-10 02:19:53,144][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 02:19:53,156][fairseq.trainer][INFO] - begin training epoch 1501
[2024-10-10 02:19:53,157][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 02:19:57,822][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 02:19:57,827][fairseq.trainer][INFO] - begin training epoch 1501
[2024-10-10 02:19:57,827][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 02:26:50,282][fairseq.trainer][WARNING] - OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 88.00 MiB. GPU 
[2024-10-10 02:26:50,292][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 1            |        cudaMalloc retries: 1         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   6847 MiB |   6890 MiB |  10474 MiB |   3626 MiB |
|       from large pool |   6800 MiB |   6843 MiB |  10294 MiB |   3494 MiB |
|       from small pool |     47 MiB |    101 MiB |    179 MiB |    132 MiB |
|---------------------------------------------------------------------------|
| Active memory         |   6847 MiB |   6890 MiB |  10474 MiB |   3626 MiB |
|       from large pool |   6800 MiB |   6843 MiB |  10294 MiB |   3494 MiB |
|       from small pool |     47 MiB |    101 MiB |    179 MiB |    132 MiB |
|---------------------------------------------------------------------------|
| Requested memory      |   6836 MiB |   6878 MiB |  10461 MiB |   3624 MiB |
|       from large pool |   6789 MiB |   6832 MiB |  10281 MiB |   3492 MiB |
|       from small pool |     47 MiB |    101 MiB |    179 MiB |    132 MiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   7042 MiB |   7042 MiB |   7042 MiB |      0 B   |
|       from large pool |   6940 MiB |   6940 MiB |   6940 MiB |      0 B   |
|       from small pool |    102 MiB |    102 MiB |    102 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory | 198896 KiB | 321469 KiB |   3512 MiB |   3318 MiB |
|       from large pool | 143130 KiB | 264900 KiB |   3281 MiB |   3141 MiB |
|       from small pool |  55766 KiB |  67665 KiB |    230 MiB |    176 MiB |
|---------------------------------------------------------------------------|
| Allocations           |    1777    |    2201    |    2573    |     796    |
|       from large pool |     239    |     239    |     298    |      59    |
|       from small pool |    1538    |    2123    |    2275    |     737    |
|---------------------------------------------------------------------------|
| Active allocs         |    1777    |    2201    |    2573    |     796    |
|       from large pool |     239    |     239    |     298    |      59    |
|       from small pool |    1538    |    2123    |    2275    |     737    |
|---------------------------------------------------------------------------|
| GPU reserved segments |     126    |     126    |     126    |       0    |
|       from large pool |      75    |      75    |      75    |       0    |
|       from small pool |      51    |      51    |      51    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     241    |     242    |     450    |     209    |
|       from large pool |      55    |      56    |     103    |      48    |
|       from small pool |     186    |     188    |     347    |     161    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-10 02:26:50,293][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-10 02:26:50,302][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-10 02:26:50,306][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-10 02:26:50,307][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 4                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-10 02:26:50,325][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 5                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-10 02:26:50,325][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 6                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-10 02:26:50,326][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 7                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-10 02:26:50,329][fairseq.trainer][WARNING] - attempting to recover from OOM in forward/backward pass
[2024-10-10 02:27:01,984][train_inner][INFO] - {"epoch": 1501, "update": 1500.688, "loss": "0.454", "ntokens": "260343", "nsentences": "1780.24", "wps": "45965.2", "ups": "0.18", "wpb": "260343", "bsz": "1780.2", "num_updates": "72000", "lr": "0.000445652", "gnorm": "0.345", "loss_scale": "2", "train_wall": "224", "gb_free": "39.4", "wall": "647"}
[2024-10-10 02:28:11,756][fairseq.trainer][WARNING] - OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 174.00 MiB. GPU 
[2024-10-10 02:28:11,763][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 1            |        cudaMalloc retries: 1         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3121 MiB |   3122 MiB |   4106 MiB |    984 MiB |
|       from large pool |   3081 MiB |   3081 MiB |   3980 MiB |    899 MiB |
|       from small pool |     40 MiB |    101 MiB |    125 MiB |     85 MiB |
|---------------------------------------------------------------------------|
| Active memory         |   3121 MiB |   3122 MiB |   4106 MiB |    984 MiB |
|       from large pool |   3081 MiB |   3081 MiB |   3980 MiB |    899 MiB |
|       from small pool |     40 MiB |    101 MiB |    125 MiB |     85 MiB |
|---------------------------------------------------------------------------|
| Requested memory      |   3110 MiB |   3111 MiB |   4093 MiB |    983 MiB |
|       from large pool |   3070 MiB |   3070 MiB |   3968 MiB |    897 MiB |
|       from small pool |     40 MiB |    101 MiB |    125 MiB |     85 MiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   3290 MiB |   3290 MiB |   3290 MiB |      0 B   |
|       from large pool |   3188 MiB |   3188 MiB |   3188 MiB |      0 B   |
|       from small pool |    102 MiB |    102 MiB |    102 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory | 172320 KiB | 293341 KiB |   1082 MiB |    914 MiB |
|       from large pool | 109162 KiB | 229380 KiB |    898 MiB |    792 MiB |
|       from small pool |  63158 KiB |  67665 KiB |    183 MiB |    122 MiB |
|---------------------------------------------------------------------------|
| Allocations           |    1634    |    2201    |    2335    |     701    |
|       from large pool |     136    |     136    |     155    |      19    |
|       from small pool |    1498    |    2123    |    2180    |     682    |
|---------------------------------------------------------------------------|
| Active allocs         |    1634    |    2201    |    2335    |     701    |
|       from large pool |     136    |     136    |     155    |      19    |
|       from small pool |    1498    |    2123    |    2180    |     682    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      99    |      99    |      99    |       0    |
|       from large pool |      48    |      48    |      48    |       0    |
|       from small pool |      51    |      51    |      51    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     214    |     214    |     395    |     181    |
|       from large pool |      28    |      28    |      48    |      20    |
|       from small pool |     186    |     188    |     347    |     161    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-10 02:28:11,763][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-10 02:28:11,763][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-10 02:28:11,764][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-10 02:28:11,764][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 4                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-10 02:28:11,764][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 5                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-10 02:28:11,764][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 6                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-10 02:28:11,765][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 7                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-10 02:28:11,765][fairseq.trainer][WARNING] - attempting to recover from OOM in forward/backward pass
[2024-10-10 02:28:11,851][fairseq.trainer][WARNING] - OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 
[2024-10-10 02:28:11,851][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 1            |        cudaMalloc retries: 1         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   1714 MiB |   1743 MiB |   1812 MiB | 100856 KiB |
|       from large pool |   1676 MiB |   1705 MiB |   1705 MiB |  29520 KiB |
|       from small pool |     37 MiB |    101 MiB |    107 MiB |  71336 KiB |
|---------------------------------------------------------------------------|
| Active memory         |   1714 MiB |   1743 MiB |   1812 MiB | 100856 KiB |
|       from large pool |   1676 MiB |   1705 MiB |   1705 MiB |  29520 KiB |
|       from small pool |     37 MiB |    101 MiB |    107 MiB |  71336 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |   1707 MiB |   1736 MiB |   1805 MiB | 100843 KiB |
|       from large pool |   1669 MiB |   1698 MiB |   1698 MiB |  29520 KiB |
|       from small pool |     37 MiB |    101 MiB |    107 MiB |  71323 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   1836 MiB |   1836 MiB |   1836 MiB |      0 B   |
|       from large pool |   1734 MiB |   1734 MiB |   1734 MiB |      0 B   |
|       from small pool |    102 MiB |    102 MiB |    102 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  93711 KiB |  98749 KiB | 375776 KiB | 282065 KiB |
|       from large pool |  27838 KiB |  32818 KiB | 203542 KiB | 175704 KiB |
|       from small pool |  65873 KiB |  67665 KiB | 172234 KiB | 106361 KiB |
|---------------------------------------------------------------------------|
| Allocations           |    1573    |    2201    |    2235    |     662    |
|       from large pool |      91    |      92    |      92    |       1    |
|       from small pool |    1482    |    2123    |    2143    |     661    |
|---------------------------------------------------------------------------|
| Active allocs         |    1573    |    2201    |    2235    |     662    |
|       from large pool |      91    |      92    |      92    |       1    |
|       from small pool |    1482    |    2123    |    2143    |     661    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      75    |      75    |      75    |       0    |
|       from large pool |      24    |      24    |      24    |       0    |
|       from small pool |      51    |      51    |      51    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     202    |     203    |     370    |     168    |
|       from large pool |      14    |      15    |      24    |      10    |
|       from small pool |     188    |     188    |     346    |     158    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-10 02:28:11,852][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-10 02:28:11,852][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-10 02:28:11,852][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-10 02:28:11,852][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 4                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-10 02:28:11,923][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 5                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-10 02:28:11,923][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 6                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-10 02:28:11,923][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 7                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-10 02:28:11,923][fairseq.trainer][WARNING] - attempting to recover from OOM in forward/backward pass
[2024-10-10 02:28:33,793][fairseq_cli.train][INFO] - end of epoch 1501 (average epoch stats below)
[2024-10-10 02:28:33,897][train][INFO] - {"epoch": 1501, "train_loss": "0.457", "train_ntokens": "260942", "train_nsentences": "1750.04", "train_wps": "44829.2", "train_ups": "0.17", "train_wpb": "260942", "train_bsz": "1750", "train_num_updates": "72015", "train_lr": "0.000445632", "train_gnorm": "0.351", "train_loss_scale": "2", "train_train_wall": "314", "train_gb_free": "40", "train_wall": "739"}
[2024-10-10 02:28:34,349][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 02:28:34,356][fairseq.trainer][INFO] - begin training epoch 1502
[2024-10-10 02:28:34,363][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 02:34:51,982][fairseq_cli.train][INFO] - end of epoch 1502 (average epoch stats below)
[2024-10-10 02:34:52,008][train][INFO] - {"epoch": 1502, "train_loss": "0.45", "train_ntokens": "260514", "train_nsentences": "1750.04", "train_wps": "33072.6", "train_ups": "0.13", "train_wpb": "260514", "train_bsz": "1750", "train_num_updates": "72063", "train_lr": "0.000445567", "train_gnorm": "0.364", "train_loss_scale": "2", "train_train_wall": "81", "train_gb_free": "39.7", "train_wall": "1117"}
[2024-10-10 02:34:52,096][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 02:34:52,116][fairseq.trainer][INFO] - begin training epoch 1503
[2024-10-10 02:34:52,116][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 02:37:16,479][fairseq_cli.train][INFO] - end of epoch 1503 (average epoch stats below)
[2024-10-10 02:37:16,483][train][INFO] - {"epoch": 1503, "train_loss": "0.455", "train_ntokens": "260349", "train_nsentences": "1750.04", "train_wps": "86499.4", "train_ups": "0.33", "train_wpb": "260349", "train_bsz": "1750", "train_num_updates": "72111", "train_lr": "0.000445501", "train_gnorm": "0.387", "train_loss_scale": "2", "train_train_wall": "53", "train_gb_free": "39.2", "train_wall": "1262"}
[2024-10-10 02:37:16,546][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 02:37:16,551][fairseq.trainer][INFO] - begin training epoch 1504
[2024-10-10 02:37:16,552][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 02:39:27,468][fairseq_cli.train][INFO] - end of epoch 1504 (average epoch stats below)
[2024-10-10 02:39:27,472][train][INFO] - {"epoch": 1504, "train_loss": "0.456", "train_ntokens": "260884", "train_nsentences": "1750.04", "train_wps": "95600.7", "train_ups": "0.37", "train_wpb": "260884", "train_bsz": "1750", "train_num_updates": "72159", "train_lr": "0.000445436", "train_gnorm": "0.364", "train_loss_scale": "2", "train_train_wall": "47", "train_gb_free": "39.3", "train_wall": "1393"}
[2024-10-10 02:39:27,572][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 02:39:27,587][fairseq.trainer][INFO] - begin training epoch 1505
[2024-10-10 02:39:27,587][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 02:41:33,891][train_inner][INFO] - {"epoch": 1505, "update": 1504.854, "loss": "0.452", "ntokens": "260565", "nsentences": "1755.5", "wps": "59769", "ups": "0.23", "wpb": "260564", "bsz": "1755.5", "num_updates": "72200", "lr": "0.00044538", "gnorm": "0.366", "loss_scale": "2", "train_wall": "320", "gb_free": "40", "wall": "1519"}
[2024-10-10 02:41:36,346][fairseq_cli.train][INFO] - end of epoch 1505 (average epoch stats below)
[2024-10-10 02:41:36,348][train][INFO] - {"epoch": 1505, "train_loss": "0.442", "train_ntokens": "260391", "train_nsentences": "1750.04", "train_wps": "96984.8", "train_ups": "0.37", "train_wpb": "260390", "train_bsz": "1750", "train_num_updates": "72207", "train_lr": "0.000445371", "train_gnorm": "0.347", "train_loss_scale": "2", "train_train_wall": "51", "train_gb_free": "40.1", "train_wall": "1522"}
[2024-10-10 02:41:36,407][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 02:41:36,413][fairseq.trainer][INFO] - begin training epoch 1506
[2024-10-10 02:41:36,413][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 02:43:47,563][fairseq_cli.train][INFO] - end of epoch 1506 (average epoch stats below)
[2024-10-10 02:43:47,568][train][INFO] - {"epoch": 1506, "train_loss": "0.452", "train_ntokens": "260542", "train_nsentences": "1750.04", "train_wps": "95307.7", "train_ups": "0.37", "train_wpb": "260542", "train_bsz": "1750", "train_num_updates": "72255", "train_lr": "0.000445306", "train_gnorm": "0.361", "train_loss_scale": "2", "train_train_wall": "53", "train_gb_free": "39.8", "train_wall": "1653"}
[2024-10-10 02:43:47,621][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 02:43:47,625][fairseq.trainer][INFO] - begin training epoch 1507
[2024-10-10 02:43:47,625][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 02:45:53,914][fairseq_cli.train][INFO] - end of epoch 1507 (average epoch stats below)
[2024-10-10 02:45:53,922][train][INFO] - {"epoch": 1507, "train_loss": "0.449", "train_ntokens": "260600", "train_nsentences": "1750.04", "train_wps": "99000.1", "train_ups": "0.38", "train_wpb": "260600", "train_bsz": "1750", "train_num_updates": "72303", "train_lr": "0.00044524", "train_gnorm": "0.379", "train_loss_scale": "2", "train_train_wall": "48", "train_gb_free": "40.1", "train_wall": "1779"}
[2024-10-10 02:45:54,069][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 02:45:54,082][fairseq.trainer][INFO] - begin training epoch 1508
[2024-10-10 02:45:54,084][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 02:48:01,631][fairseq_cli.train][INFO] - end of epoch 1508 (average epoch stats below)
[2024-10-10 02:48:01,635][train][INFO] - {"epoch": 1508, "train_loss": "0.447", "train_ntokens": "260867", "train_nsentences": "1750.04", "train_wps": "98047.1", "train_ups": "0.38", "train_wpb": "260867", "train_bsz": "1750", "train_num_updates": "72351", "train_lr": "0.000445175", "train_gnorm": "0.371", "train_loss_scale": "2", "train_train_wall": "59", "train_gb_free": "39.7", "train_wall": "1907"}
[2024-10-10 02:48:01,701][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 02:48:01,705][fairseq.trainer][INFO] - begin training epoch 1509
[2024-10-10 02:48:01,706][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 02:50:09,691][fairseq_cli.train][INFO] - end of epoch 1509 (average epoch stats below)
[2024-10-10 02:50:09,694][train][INFO] - {"epoch": 1509, "train_loss": "0.447", "train_ntokens": "260836", "train_nsentences": "1750.04", "train_wps": "97769.8", "train_ups": "0.37", "train_wpb": "260836", "train_bsz": "1750", "train_num_updates": "72399", "train_lr": "0.00044511", "train_gnorm": "0.371", "train_loss_scale": "2", "train_train_wall": "57", "train_gb_free": "40.3", "train_wall": "2035"}
[2024-10-10 02:50:09,790][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 02:50:09,796][fairseq.trainer][INFO] - begin training epoch 1510
[2024-10-10 02:50:09,796][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 02:51:37,588][train_inner][INFO] - {"epoch": 1510, "update": 1509.021, "loss": "0.449", "ntokens": "260777", "nsentences": "1743.01", "wps": "86394.7", "ups": "0.33", "wpb": "260777", "bsz": "1743", "num_updates": "72400", "lr": "0.000445109", "gnorm": "0.369", "loss_scale": "2", "train_wall": "228", "gb_free": "40.2", "wall": "2123"}
[2024-10-10 02:52:17,244][fairseq_cli.train][INFO] - end of epoch 1510 (average epoch stats below)
[2024-10-10 02:52:17,246][train][INFO] - {"epoch": 1510, "train_loss": "0.454", "train_ntokens": "260960", "train_nsentences": "1750.04", "train_wps": "98205.4", "train_ups": "0.38", "train_wpb": "260960", "train_bsz": "1750", "train_num_updates": "72447", "train_lr": "0.000445045", "train_gnorm": "0.36", "train_loss_scale": "2", "train_train_wall": "47", "train_gb_free": "39.3", "train_wall": "2163"}
[2024-10-10 02:52:17,307][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 02:52:17,310][fairseq.trainer][INFO] - begin training epoch 1511
[2024-10-10 02:52:17,311][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 02:54:25,323][fairseq_cli.train][INFO] - end of epoch 1511 (average epoch stats below)
[2024-10-10 02:54:25,328][train][INFO] - {"epoch": 1511, "train_loss": "0.449", "train_ntokens": "260350", "train_nsentences": "1750.04", "train_wps": "97571.1", "train_ups": "0.37", "train_wpb": "260350", "train_bsz": "1750", "train_num_updates": "72495", "train_lr": "0.00044498", "train_gnorm": "0.372", "train_loss_scale": "2", "train_train_wall": "51", "train_gb_free": "39.3", "train_wall": "2291"}
[2024-10-10 02:54:25,385][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 02:54:25,389][fairseq.trainer][INFO] - begin training epoch 1512
[2024-10-10 02:54:25,389][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 02:56:34,094][fairseq_cli.train][INFO] - end of epoch 1512 (average epoch stats below)
[2024-10-10 02:56:34,097][train][INFO] - {"epoch": 1512, "train_loss": "0.455", "train_ntokens": "260820", "train_nsentences": "1750.04", "train_wps": "97224.9", "train_ups": "0.37", "train_wpb": "260820", "train_bsz": "1750", "train_num_updates": "72543", "train_lr": "0.000444914", "train_gnorm": "0.36", "train_loss_scale": "2", "train_train_wall": "65", "train_gb_free": "39.5", "train_wall": "2420"}
[2024-10-10 02:56:34,208][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 02:56:34,233][fairseq.trainer][INFO] - begin training epoch 1513
[2024-10-10 02:56:34,234][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 02:58:44,383][fairseq_cli.train][INFO] - end of epoch 1513 (average epoch stats below)
[2024-10-10 02:58:44,390][train][INFO] - {"epoch": 1513, "train_loss": "0.447", "train_ntokens": "260606", "train_nsentences": "1750.04", "train_wps": "96010.8", "train_ups": "0.37", "train_wpb": "260606", "train_bsz": "1750", "train_num_updates": "72591", "train_lr": "0.000444849", "train_gnorm": "0.351", "train_loss_scale": "2", "train_train_wall": "67", "train_gb_free": "39.8", "train_wall": "2550"}
[2024-10-10 02:58:44,490][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 02:58:44,498][fairseq.trainer][INFO] - begin training epoch 1514
[2024-10-10 02:58:44,498][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 03:00:18,855][train_inner][INFO] - {"epoch": 1514, "update": 1513.188, "loss": "0.451", "ntokens": "260646", "nsentences": "1756.23", "wps": "100006", "ups": "0.38", "wpb": "260646", "bsz": "1756.2", "num_updates": "72600", "lr": "0.000444837", "gnorm": "0.361", "loss_scale": "2", "train_wall": "241", "gb_free": "39.8", "wall": "2644"}
[2024-10-10 03:00:55,187][fairseq_cli.train][INFO] - end of epoch 1514 (average epoch stats below)
[2024-10-10 03:00:55,194][train][INFO] - {"epoch": 1514, "train_loss": "0.452", "train_ntokens": "260682", "train_nsentences": "1750.04", "train_wps": "95665", "train_ups": "0.37", "train_wpb": "260682", "train_bsz": "1750", "train_num_updates": "72639", "train_lr": "0.000444784", "train_gnorm": "0.39", "train_loss_scale": "2", "train_train_wall": "54", "train_gb_free": "39.3", "train_wall": "2681"}
[2024-10-10 03:00:55,289][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 03:00:55,310][fairseq.trainer][INFO] - begin training epoch 1515
[2024-10-10 03:00:55,311][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 03:03:03,306][fairseq_cli.train][INFO] - end of epoch 1515 (average epoch stats below)
[2024-10-10 03:03:03,310][train][INFO] - {"epoch": 1515, "train_loss": "0.457", "train_ntokens": "260839", "train_nsentences": "1750.04", "train_wps": "97728.6", "train_ups": "0.37", "train_wpb": "260839", "train_bsz": "1750", "train_num_updates": "72687", "train_lr": "0.000444719", "train_gnorm": "0.365", "train_loss_scale": "2", "train_train_wall": "65", "train_gb_free": "40.1", "train_wall": "2809"}
[2024-10-10 03:03:03,384][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 03:03:03,400][fairseq.trainer][INFO] - begin training epoch 1516
[2024-10-10 03:03:03,401][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 03:05:10,911][fairseq_cli.train][INFO] - end of epoch 1516 (average epoch stats below)
[2024-10-10 03:05:10,914][train][INFO] - {"epoch": 1516, "train_loss": "0.449", "train_ntokens": "260681", "train_nsentences": "1750.04", "train_wps": "98060", "train_ups": "0.38", "train_wpb": "260681", "train_bsz": "1750", "train_num_updates": "72735", "train_lr": "0.000444654", "train_gnorm": "0.374", "train_loss_scale": "2", "train_train_wall": "49", "train_gb_free": "39.6", "train_wall": "2936"}
[2024-10-10 03:05:11,029][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 03:05:11,040][fairseq.trainer][INFO] - begin training epoch 1517
[2024-10-10 03:05:11,041][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 03:07:21,891][fairseq_cli.train][INFO] - end of epoch 1517 (average epoch stats below)
[2024-10-10 03:07:21,907][train][INFO] - {"epoch": 1517, "train_loss": "0.447", "train_ntokens": "260623", "train_nsentences": "1750.04", "train_wps": "95510.2", "train_ups": "0.37", "train_wpb": "260623", "train_bsz": "1750", "train_num_updates": "72783", "train_lr": "0.000444588", "train_gnorm": "0.359", "train_loss_scale": "2", "train_train_wall": "60", "train_gb_free": "39.3", "train_wall": "3067"}
[2024-10-10 03:07:22,044][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 03:07:22,061][fairseq.trainer][INFO] - begin training epoch 1518
[2024-10-10 03:07:22,062][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 03:09:03,917][train_inner][INFO] - {"epoch": 1518, "update": 1517.354, "loss": "0.452", "ntokens": "260891", "nsentences": "1734.85", "wps": "99376.1", "ups": "0.38", "wpb": "260891", "bsz": "1734.9", "num_updates": "72800", "lr": "0.000444565", "gnorm": "0.37", "loss_scale": "2", "train_wall": "230", "gb_free": "40.7", "wall": "3169"}
[2024-10-10 03:09:30,562][fairseq_cli.train][INFO] - end of epoch 1518 (average epoch stats below)
[2024-10-10 03:09:30,566][train][INFO] - {"epoch": 1518, "train_loss": "0.454", "train_ntokens": "260743", "train_nsentences": "1750.04", "train_wps": "97280.5", "train_ups": "0.37", "train_wpb": "260744", "train_bsz": "1750", "train_num_updates": "72831", "train_lr": "0.000444523", "train_gnorm": "0.368", "train_loss_scale": "2", "train_train_wall": "46", "train_gb_free": "39.3", "train_wall": "3196"}
[2024-10-10 03:09:30,671][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 03:09:30,681][fairseq.trainer][INFO] - begin training epoch 1519
[2024-10-10 03:09:30,682][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 03:11:37,907][fairseq_cli.train][INFO] - end of epoch 1519 (average epoch stats below)
[2024-10-10 03:11:37,911][train][INFO] - {"epoch": 1519, "train_loss": "0.454", "train_ntokens": "260832", "train_nsentences": "1750.04", "train_wps": "98317.6", "train_ups": "0.38", "train_wpb": "260832", "train_bsz": "1750", "train_num_updates": "72879", "train_lr": "0.000444458", "train_gnorm": "0.38", "train_loss_scale": "2", "train_train_wall": "46", "train_gb_free": "39.7", "train_wall": "3323"}
[2024-10-10 03:11:38,003][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 03:11:38,007][fairseq.trainer][INFO] - begin training epoch 1520
[2024-10-10 03:11:38,008][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 03:13:48,688][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 1520 @ 72927 updates
[2024-10-10 03:13:48,689][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-10 03:13:52,192][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-10 03:13:52,207][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 1520 @ 72927 updates, score None) (writing took 3.5195656744763255 seconds)
[2024-10-10 03:13:52,208][fairseq_cli.train][INFO] - end of epoch 1520 (average epoch stats below)
[2024-10-10 03:13:52,210][train][INFO] - {"epoch": 1520, "train_loss": "0.447", "train_ntokens": "260488", "train_nsentences": "1750.04", "train_wps": "93103.4", "train_ups": "0.36", "train_wpb": "260488", "train_bsz": "1750", "train_num_updates": "72927", "train_lr": "0.000444393", "train_gnorm": "0.362", "train_loss_scale": "2", "train_train_wall": "55", "train_gb_free": "40.1", "train_wall": "3458"}
[2024-10-10 03:13:52,276][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 03:13:52,316][fairseq.trainer][INFO] - begin training epoch 1521
[2024-10-10 03:13:52,316][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 03:15:56,968][fairseq_cli.train][INFO] - end of epoch 1521 (average epoch stats below)
[2024-10-10 03:15:56,973][train][INFO] - {"epoch": 1521, "train_loss": "0.453", "train_ntokens": "260995", "train_nsentences": "1750.04", "train_wps": "100414", "train_ups": "0.38", "train_wpb": "260995", "train_bsz": "1750", "train_num_updates": "72975", "train_lr": "0.000444327", "train_gnorm": "0.38", "train_loss_scale": "2", "train_train_wall": "36", "train_gb_free": "39.6", "train_wall": "3582"}
[2024-10-10 03:15:57,083][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 03:15:57,100][fairseq.trainer][INFO] - begin training epoch 1522
[2024-10-10 03:15:57,101][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 03:17:44,773][train_inner][INFO] - {"epoch": 1522, "update": 1521.521, "loss": "0.453", "ntokens": "260670", "nsentences": "1758.53", "wps": "100094", "ups": "0.38", "wpb": "260670", "bsz": "1758.5", "num_updates": "73000", "lr": "0.000444293", "gnorm": "0.37", "loss_scale": "2", "train_wall": "199", "gb_free": "39.2", "wall": "3690"}
[2024-10-10 03:18:05,953][fairseq_cli.train][INFO] - end of epoch 1522 (average epoch stats below)
[2024-10-10 03:18:05,956][train][INFO] - {"epoch": 1522, "train_loss": "0.461", "train_ntokens": "260639", "train_nsentences": "1750.04", "train_wps": "96997", "train_ups": "0.37", "train_wpb": "260639", "train_bsz": "1750", "train_num_updates": "73023", "train_lr": "0.000444262", "train_gnorm": "0.36", "train_loss_scale": "2", "train_train_wall": "57", "train_gb_free": "40.2", "train_wall": "3711"}
[2024-10-10 03:18:06,009][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 03:18:06,015][fairseq.trainer][INFO] - begin training epoch 1523
[2024-10-10 03:18:06,015][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 03:20:12,179][fairseq_cli.train][INFO] - end of epoch 1523 (average epoch stats below)
[2024-10-10 03:20:12,190][train][INFO] - {"epoch": 1523, "train_loss": "0.448", "train_ntokens": "260361", "train_nsentences": "1750.04", "train_wps": "99004.4", "train_ups": "0.38", "train_wpb": "260361", "train_bsz": "1750", "train_num_updates": "73071", "train_lr": "0.000444197", "train_gnorm": "0.368", "train_loss_scale": "2", "train_train_wall": "53", "train_gb_free": "40.3", "train_wall": "3838"}
[2024-10-10 03:20:12,289][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 03:20:12,295][fairseq.trainer][INFO] - begin training epoch 1524
[2024-10-10 03:20:12,296][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 03:22:21,916][fairseq_cli.train][INFO] - end of epoch 1524 (average epoch stats below)
[2024-10-10 03:22:21,919][train][INFO] - {"epoch": 1524, "train_loss": "0.452", "train_ntokens": "260229", "train_nsentences": "1750.04", "train_wps": "96286.7", "train_ups": "0.37", "train_wpb": "260229", "train_bsz": "1750", "train_num_updates": "73119", "train_lr": "0.000444132", "train_gnorm": "0.364", "train_loss_scale": "2", "train_train_wall": "64", "train_gb_free": "40", "train_wall": "3967"}
[2024-10-10 03:22:22,006][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 03:22:22,011][fairseq.trainer][INFO] - begin training epoch 1525
[2024-10-10 03:22:22,011][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 03:24:31,969][fairseq_cli.train][INFO] - end of epoch 1525 (average epoch stats below)
[2024-10-10 03:24:31,980][train][INFO] - {"epoch": 1525, "train_loss": "0.451", "train_ntokens": "260802", "train_nsentences": "1750.04", "train_wps": "96253.6", "train_ups": "0.37", "train_wpb": "260802", "train_bsz": "1750", "train_num_updates": "73167", "train_lr": "0.000444067", "train_gnorm": "0.358", "train_loss_scale": "2", "train_train_wall": "62", "train_gb_free": "39.2", "train_wall": "4097"}
[2024-10-10 03:24:32,124][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 03:24:32,130][fairseq.trainer][INFO] - begin training epoch 1526
[2024-10-10 03:24:32,131][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 03:26:22,584][train_inner][INFO] - {"epoch": 1526, "update": 1525.688, "loss": "0.452", "ntokens": "260528", "nsentences": "1746.47", "wps": "100628", "ups": "0.39", "wpb": "260528", "bsz": "1746.5", "num_updates": "73200", "lr": "0.000444022", "gnorm": "0.361", "loss_scale": "2", "train_wall": "233", "gb_free": "39.6", "wall": "4208"}
[2024-10-10 03:26:38,160][fairseq_cli.train][INFO] - end of epoch 1526 (average epoch stats below)
[2024-10-10 03:26:38,162][train][INFO] - {"epoch": 1526, "train_loss": "0.451", "train_ntokens": "260857", "train_nsentences": "1750.04", "train_wps": "99232.7", "train_ups": "0.38", "train_wpb": "260857", "train_bsz": "1750", "train_num_updates": "73215", "train_lr": "0.000444001", "train_gnorm": "0.346", "train_loss_scale": "2", "train_train_wall": "48", "train_gb_free": "39.3", "train_wall": "4224"}
[2024-10-10 03:26:38,217][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 03:26:38,221][fairseq.trainer][INFO] - begin training epoch 1527
[2024-10-10 03:26:38,221][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 03:28:45,161][fairseq_cli.train][INFO] - end of epoch 1527 (average epoch stats below)
[2024-10-10 03:28:45,164][train][INFO] - {"epoch": 1527, "train_loss": "0.452", "train_ntokens": "260824", "train_nsentences": "1750.04", "train_wps": "98579.3", "train_ups": "0.38", "train_wpb": "260824", "train_bsz": "1750", "train_num_updates": "73263", "train_lr": "0.000443936", "train_gnorm": "0.393", "train_loss_scale": "2", "train_train_wall": "59", "train_gb_free": "39.2", "train_wall": "4351"}
[2024-10-10 03:28:45,279][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 03:28:45,293][fairseq.trainer][INFO] - begin training epoch 1528
[2024-10-10 03:28:45,293][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 03:30:51,748][fairseq_cli.train][INFO] - end of epoch 1528 (average epoch stats below)
[2024-10-10 03:30:51,751][train][INFO] - {"epoch": 1528, "train_loss": "0.445", "train_ntokens": "260807", "train_nsentences": "1750.04", "train_wps": "98896.5", "train_ups": "0.38", "train_wpb": "260807", "train_bsz": "1750", "train_num_updates": "73311", "train_lr": "0.000443871", "train_gnorm": "0.375", "train_loss_scale": "2", "train_train_wall": "39", "train_gb_free": "39.6", "train_wall": "4477"}
[2024-10-10 03:30:51,805][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 03:30:51,809][fairseq.trainer][INFO] - begin training epoch 1529
[2024-10-10 03:30:51,809][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 03:33:00,368][fairseq_cli.train][INFO] - end of epoch 1529 (average epoch stats below)
[2024-10-10 03:33:00,372][train][INFO] - {"epoch": 1529, "train_loss": "0.457", "train_ntokens": "260296", "train_nsentences": "1750.04", "train_wps": "97142.1", "train_ups": "0.37", "train_wpb": "260296", "train_bsz": "1750", "train_num_updates": "73359", "train_lr": "0.000443806", "train_gnorm": "0.382", "train_loss_scale": "2", "train_train_wall": "58", "train_gb_free": "40.8", "train_wall": "4606"}
[2024-10-10 03:33:00,436][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 03:33:00,439][fairseq.trainer][INFO] - begin training epoch 1530
[2024-10-10 03:33:00,440][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 03:35:09,497][train_inner][INFO] - {"epoch": 1530, "update": 1529.854, "loss": "0.452", "ntokens": "260746", "nsentences": "1746.09", "wps": "98972.1", "ups": "0.38", "wpb": "260746", "bsz": "1746.1", "num_updates": "73400", "lr": "0.00044375", "gnorm": "0.376", "loss_scale": "2", "train_wall": "201", "gb_free": "39.3", "wall": "4735"}
[2024-10-10 03:35:11,209][fairseq_cli.train][INFO] - end of epoch 1530 (average epoch stats below)
[2024-10-10 03:35:11,211][train][INFO] - {"epoch": 1530, "train_loss": "0.453", "train_ntokens": "260636", "train_nsentences": "1750.04", "train_wps": "95618.7", "train_ups": "0.37", "train_wpb": "260636", "train_bsz": "1750", "train_num_updates": "73407", "train_lr": "0.00044374", "train_gnorm": "0.352", "train_loss_scale": "2", "train_train_wall": "31", "train_gb_free": "39.8", "train_wall": "4737"}
[2024-10-10 03:35:11,272][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 03:35:11,275][fairseq.trainer][INFO] - begin training epoch 1531
[2024-10-10 03:35:11,276][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 03:37:17,845][fairseq_cli.train][INFO] - end of epoch 1531 (average epoch stats below)
[2024-10-10 03:37:17,849][train][INFO] - {"epoch": 1531, "train_loss": "0.452", "train_ntokens": "260458", "train_nsentences": "1750.04", "train_wps": "98724.7", "train_ups": "0.38", "train_wpb": "260458", "train_bsz": "1750", "train_num_updates": "73455", "train_lr": "0.000443675", "train_gnorm": "0.352", "train_loss_scale": "2", "train_train_wall": "54", "train_gb_free": "39.3", "train_wall": "4863"}
[2024-10-10 03:37:17,959][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 03:37:17,972][fairseq.trainer][INFO] - begin training epoch 1532
[2024-10-10 03:37:17,973][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 03:39:26,447][fairseq_cli.train][INFO] - end of epoch 1532 (average epoch stats below)
[2024-10-10 03:39:26,450][train][INFO] - {"epoch": 1532, "train_loss": "0.45", "train_ntokens": "260652", "train_nsentences": "1750.04", "train_wps": "97289.5", "train_ups": "0.37", "train_wpb": "260652", "train_bsz": "1750", "train_num_updates": "73503", "train_lr": "0.00044361", "train_gnorm": "0.37", "train_loss_scale": "2", "train_train_wall": "52", "train_gb_free": "39.6", "train_wall": "4992"}
[2024-10-10 03:39:26,555][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 03:39:26,569][fairseq.trainer][INFO] - begin training epoch 1533
[2024-10-10 03:39:26,570][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 03:41:34,282][fairseq_cli.train][INFO] - end of epoch 1533 (average epoch stats below)
[2024-10-10 03:41:34,285][train][INFO] - {"epoch": 1533, "train_loss": "0.451", "train_ntokens": "260669", "train_nsentences": "1750.04", "train_wps": "97879.3", "train_ups": "0.38", "train_wpb": "260669", "train_bsz": "1750", "train_num_updates": "73551", "train_lr": "0.000443545", "train_gnorm": "0.381", "train_loss_scale": "2", "train_train_wall": "60", "train_gb_free": "39.2", "train_wall": "5120"}
[2024-10-10 03:41:34,413][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 03:41:34,416][fairseq.trainer][INFO] - begin training epoch 1534
[2024-10-10 03:41:34,417][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 03:43:42,974][fairseq_cli.train][INFO] - end of epoch 1534 (average epoch stats below)
[2024-10-10 03:43:42,978][train][INFO] - {"epoch": 1534, "train_loss": "0.456", "train_ntokens": "260925", "train_nsentences": "1750.04", "train_wps": "97321.6", "train_ups": "0.37", "train_wpb": "260924", "train_bsz": "1750", "train_num_updates": "73599", "train_lr": "0.00044348", "train_gnorm": "0.376", "train_loss_scale": "2", "train_train_wall": "57", "train_gb_free": "39.8", "train_wall": "5248"}
[2024-10-10 03:43:43,038][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 03:43:43,066][fairseq.trainer][INFO] - begin training epoch 1535
[2024-10-10 03:43:43,066][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 03:45:11,851][train_inner][INFO] - {"epoch": 1535, "update": 1534.021, "loss": "0.452", "ntokens": "260535", "nsentences": "1759", "wps": "86506.7", "ups": "0.33", "wpb": "260535", "bsz": "1759", "num_updates": "73600", "lr": "0.000443478", "gnorm": "0.37", "loss_scale": "2", "train_wall": "236", "gb_free": "40.2", "wall": "5337"}
[2024-10-10 03:45:51,072][fairseq_cli.train][INFO] - end of epoch 1535 (average epoch stats below)
[2024-10-10 03:45:51,076][train][INFO] - {"epoch": 1535, "train_loss": "0.444", "train_ntokens": "260758", "train_nsentences": "1750.04", "train_wps": "97712.9", "train_ups": "0.37", "train_wpb": "260758", "train_bsz": "1750", "train_num_updates": "73647", "train_lr": "0.000443414", "train_gnorm": "0.364", "train_loss_scale": "2", "train_train_wall": "48", "train_gb_free": "40.4", "train_wall": "5377"}
[2024-10-10 03:45:51,190][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 03:45:51,217][fairseq.trainer][INFO] - begin training epoch 1536
[2024-10-10 03:45:51,217][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 03:47:58,227][fairseq_cli.train][INFO] - end of epoch 1536 (average epoch stats below)
[2024-10-10 03:47:58,230][train][INFO] - {"epoch": 1536, "train_loss": "0.448", "train_ntokens": "260710", "train_nsentences": "1750.04", "train_wps": "98418.5", "train_ups": "0.38", "train_wpb": "260710", "train_bsz": "1750", "train_num_updates": "73695", "train_lr": "0.000443349", "train_gnorm": "0.394", "train_loss_scale": "2", "train_train_wall": "51", "train_gb_free": "39.3", "train_wall": "5504"}
[2024-10-10 03:47:58,334][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 03:47:58,340][fairseq.trainer][INFO] - begin training epoch 1537
[2024-10-10 03:47:58,340][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 03:50:08,741][fairseq_cli.train][INFO] - end of epoch 1537 (average epoch stats below)
[2024-10-10 03:50:08,744][train][INFO] - {"epoch": 1537, "train_loss": "0.45", "train_ntokens": "261146", "train_nsentences": "1750.04", "train_wps": "96053.5", "train_ups": "0.37", "train_wpb": "261146", "train_bsz": "1750", "train_num_updates": "73743", "train_lr": "0.000443284", "train_gnorm": "0.369", "train_loss_scale": "2", "train_train_wall": "46", "train_gb_free": "39.8", "train_wall": "5634"}
[2024-10-10 03:50:08,811][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 03:50:08,816][fairseq.trainer][INFO] - begin training epoch 1538
[2024-10-10 03:50:08,817][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 03:52:16,586][fairseq_cli.train][INFO] - end of epoch 1538 (average epoch stats below)
[2024-10-10 03:52:16,603][train][INFO] - {"epoch": 1538, "train_loss": "0.447", "train_ntokens": "261084", "train_nsentences": "1750.04", "train_wps": "98023.6", "train_ups": "0.38", "train_wpb": "261084", "train_bsz": "1750", "train_num_updates": "73791", "train_lr": "0.000443219", "train_gnorm": "0.366", "train_loss_scale": "2", "train_train_wall": "47", "train_gb_free": "40.2", "train_wall": "5762"}
[2024-10-10 03:52:16,744][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 03:52:16,761][fairseq.trainer][INFO] - begin training epoch 1539
[2024-10-10 03:52:16,762][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 03:53:54,985][train_inner][INFO] - {"epoch": 1539, "update": 1538.188, "loss": "0.447", "ntokens": "260792", "nsentences": "1763.8", "wps": "99704.8", "ups": "0.38", "wpb": "260792", "bsz": "1763.8", "num_updates": "73800", "lr": "0.000443207", "gnorm": "0.373", "loss_scale": "2", "train_wall": "204", "gb_free": "40.5", "wall": "5860"}
[2024-10-10 03:54:23,298][fairseq_cli.train][INFO] - end of epoch 1539 (average epoch stats below)
[2024-10-10 03:54:23,301][train][INFO] - {"epoch": 1539, "train_loss": "0.451", "train_ntokens": "260613", "train_nsentences": "1750.04", "train_wps": "98735.9", "train_ups": "0.38", "train_wpb": "260613", "train_bsz": "1750", "train_num_updates": "73839", "train_lr": "0.000443154", "train_gnorm": "0.355", "train_loss_scale": "2", "train_train_wall": "51", "train_gb_free": "40", "train_wall": "5889"}
[2024-10-10 03:54:23,361][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 03:54:23,365][fairseq.trainer][INFO] - begin training epoch 1540
[2024-10-10 03:54:23,365][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 03:56:31,460][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 1540 @ 73887 updates
[2024-10-10 03:56:31,461][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-10 03:56:35,060][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-10 03:56:35,063][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 1540 @ 73887 updates, score None) (writing took 3.60320492554456 seconds)
[2024-10-10 03:56:35,063][fairseq_cli.train][INFO] - end of epoch 1540 (average epoch stats below)
[2024-10-10 03:56:35,065][train][INFO] - {"epoch": 1540, "train_loss": "0.455", "train_ntokens": "260689", "train_nsentences": "1750.04", "train_wps": "94967.5", "train_ups": "0.36", "train_wpb": "260689", "train_bsz": "1750", "train_num_updates": "73887", "train_lr": "0.000443088", "train_gnorm": "0.358", "train_loss_scale": "2", "train_train_wall": "41", "train_gb_free": "40.3", "train_wall": "6020"}
[2024-10-10 03:56:35,140][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 03:56:35,185][fairseq.trainer][INFO] - begin training epoch 1541
[2024-10-10 03:56:35,185][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 03:58:39,414][fairseq_cli.train][INFO] - end of epoch 1541 (average epoch stats below)
[2024-10-10 03:58:39,437][train][INFO] - {"epoch": 1541, "train_loss": "0.453", "train_ntokens": "260547", "train_nsentences": "1750.04", "train_wps": "100565", "train_ups": "0.39", "train_wpb": "260547", "train_bsz": "1750", "train_num_updates": "73935", "train_lr": "0.000443023", "train_gnorm": "0.366", "train_loss_scale": "2", "train_train_wall": "43", "train_gb_free": "39.7", "train_wall": "6145"}
[2024-10-10 03:58:39,588][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 03:58:39,605][fairseq.trainer][INFO] - begin training epoch 1542
[2024-10-10 03:58:39,605][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 04:00:44,722][fairseq_cli.train][INFO] - end of epoch 1542 (average epoch stats below)
[2024-10-10 04:00:44,744][train][INFO] - {"epoch": 1542, "train_loss": "0.447", "train_ntokens": "260896", "train_nsentences": "1750.04", "train_wps": "99947.7", "train_ups": "0.38", "train_wpb": "260896", "train_bsz": "1750", "train_num_updates": "73983", "train_lr": "0.000442958", "train_gnorm": "0.38", "train_loss_scale": "2", "train_train_wall": "52", "train_gb_free": "39.6", "train_wall": "6270"}
[2024-10-10 04:00:44,915][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 04:00:44,948][fairseq.trainer][INFO] - begin training epoch 1543
[2024-10-10 04:00:44,948][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 04:02:25,754][train_inner][INFO] - {"epoch": 1543, "update": 1542.354, "loss": "0.451", "ntokens": "260749", "nsentences": "1737.83", "wps": "102101", "ups": "0.39", "wpb": "260749", "bsz": "1737.8", "num_updates": "74000", "lr": "0.000442935", "gnorm": "0.364", "loss_scale": "2", "train_wall": "190", "gb_free": "39.3", "wall": "6371"}
[2024-10-10 04:02:50,386][fairseq_cli.train][INFO] - end of epoch 1543 (average epoch stats below)
[2024-10-10 04:02:50,389][train][INFO] - {"epoch": 1543, "train_loss": "0.446", "train_ntokens": "261028", "train_nsentences": "1750.04", "train_wps": "99730.9", "train_ups": "0.38", "train_wpb": "261028", "train_bsz": "1750", "train_num_updates": "74031", "train_lr": "0.000442893", "train_gnorm": "0.377", "train_loss_scale": "4", "train_train_wall": "50", "train_gb_free": "40.8", "train_wall": "6396"}
[2024-10-10 04:02:50,485][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 04:02:50,492][fairseq.trainer][INFO] - begin training epoch 1544
[2024-10-10 04:02:50,493][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 04:04:55,900][fairseq_cli.train][INFO] - end of epoch 1544 (average epoch stats below)
[2024-10-10 04:04:55,905][train][INFO] - {"epoch": 1544, "train_loss": "0.447", "train_ntokens": "260659", "train_nsentences": "1750.04", "train_wps": "99684.9", "train_ups": "0.38", "train_wpb": "260659", "train_bsz": "1750", "train_num_updates": "74079", "train_lr": "0.000442827", "train_gnorm": "0.359", "train_loss_scale": "4", "train_train_wall": "52", "train_gb_free": "39.6", "train_wall": "6521"}
[2024-10-10 04:04:56,003][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 04:04:56,010][fairseq.trainer][INFO] - begin training epoch 1545
[2024-10-10 04:04:56,010][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 04:07:04,022][fairseq_cli.train][INFO] - end of epoch 1545 (average epoch stats below)
[2024-10-10 04:07:04,026][train][INFO] - {"epoch": 1545, "train_loss": "0.454", "train_ntokens": "260906", "train_nsentences": "1750.04", "train_wps": "97751", "train_ups": "0.37", "train_wpb": "260906", "train_bsz": "1750", "train_num_updates": "74127", "train_lr": "0.000442762", "train_gnorm": "0.36", "train_loss_scale": "4", "train_train_wall": "51", "train_gb_free": "39.6", "train_wall": "6649"}
[2024-10-10 04:07:04,159][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 04:07:04,169][fairseq.trainer][INFO] - begin training epoch 1546
[2024-10-10 04:07:04,170][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 04:09:15,361][fairseq_cli.train][INFO] - end of epoch 1546 (average epoch stats below)
[2024-10-10 04:09:15,368][train][INFO] - {"epoch": 1546, "train_loss": "0.447", "train_ntokens": "261381", "train_nsentences": "1750.04", "train_wps": "95529.4", "train_ups": "0.37", "train_wpb": "261381", "train_bsz": "1750", "train_num_updates": "74175", "train_lr": "0.000442697", "train_gnorm": "0.362", "train_loss_scale": "4", "train_train_wall": "61", "train_gb_free": "39.3", "train_wall": "6781"}
[2024-10-10 04:09:15,482][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 04:09:15,499][fairseq.trainer][INFO] - begin training epoch 1547
[2024-10-10 04:09:15,499][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 04:11:10,715][train_inner][INFO] - {"epoch": 1547, "update": 1546.521, "loss": "0.448", "ntokens": "261140", "nsentences": "1744.27", "wps": "99489.8", "ups": "0.38", "wpb": "261140", "bsz": "1744.3", "num_updates": "74200", "lr": "0.000442663", "gnorm": "0.364", "loss_scale": "4", "train_wall": "233", "gb_free": "40", "wall": "6896"}
[2024-10-10 04:11:28,048][fairseq_cli.train][INFO] - end of epoch 1547 (average epoch stats below)
[2024-10-10 04:11:28,050][train][INFO] - {"epoch": 1547, "train_loss": "0.447", "train_ntokens": "260735", "train_nsentences": "1750.04", "train_wps": "94327.1", "train_ups": "0.36", "train_wpb": "260735", "train_bsz": "1750", "train_num_updates": "74223", "train_lr": "0.000442632", "train_gnorm": "0.377", "train_loss_scale": "4", "train_train_wall": "62", "train_gb_free": "39.6", "train_wall": "6913"}
[2024-10-10 04:11:28,140][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 04:11:28,160][fairseq.trainer][INFO] - begin training epoch 1548
[2024-10-10 04:11:28,162][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 04:13:36,664][fairseq_cli.train][INFO] - end of epoch 1548 (average epoch stats below)
[2024-10-10 04:13:36,668][train][INFO] - {"epoch": 1548, "train_loss": "0.454", "train_ntokens": "260556", "train_nsentences": "1750.04", "train_wps": "97241.9", "train_ups": "0.37", "train_wpb": "260556", "train_bsz": "1750", "train_num_updates": "74271", "train_lr": "0.000442567", "train_gnorm": "0.355", "train_loss_scale": "4", "train_train_wall": "51", "train_gb_free": "39.7", "train_wall": "7042"}
[2024-10-10 04:13:36,753][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 04:13:36,764][fairseq.trainer][INFO] - begin training epoch 1549
[2024-10-10 04:13:36,765][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 04:15:44,888][fairseq_cli.train][INFO] - end of epoch 1549 (average epoch stats below)
[2024-10-10 04:15:44,892][train][INFO] - {"epoch": 1549, "train_loss": "0.444", "train_ntokens": "260659", "train_nsentences": "1750.04", "train_wps": "97579", "train_ups": "0.37", "train_wpb": "260660", "train_bsz": "1750", "train_num_updates": "74319", "train_lr": "0.000442501", "train_gnorm": "0.386", "train_loss_scale": "4", "train_train_wall": "33", "train_gb_free": "39.3", "train_wall": "7170"}
[2024-10-10 04:15:45,028][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 04:15:45,045][fairseq.trainer][INFO] - begin training epoch 1550
[2024-10-10 04:15:45,046][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 04:17:52,711][fairseq_cli.train][INFO] - end of epoch 1550 (average epoch stats below)
[2024-10-10 04:17:52,714][train][INFO] - {"epoch": 1550, "train_loss": "0.446", "train_ntokens": "260415", "train_nsentences": "1750.04", "train_wps": "97793", "train_ups": "0.38", "train_wpb": "260415", "train_bsz": "1750", "train_num_updates": "74367", "train_lr": "0.000442436", "train_gnorm": "0.368", "train_loss_scale": "4", "train_train_wall": "48", "train_gb_free": "39.7", "train_wall": "7298"}
[2024-10-10 04:17:52,819][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 04:17:52,829][fairseq.trainer][INFO] - begin training epoch 1551
[2024-10-10 04:17:52,830][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 04:19:46,039][train_inner][INFO] - {"epoch": 1551, "update": 1550.688, "loss": "0.448", "ntokens": "260566", "nsentences": "1752.74", "wps": "101128", "ups": "0.39", "wpb": "260566", "bsz": "1752.7", "num_updates": "74400", "lr": "0.000442391", "gnorm": "0.371", "loss_scale": "4", "train_wall": "194", "gb_free": "40.1", "wall": "7411"}
[2024-10-10 04:19:59,521][fairseq_cli.train][INFO] - end of epoch 1551 (average epoch stats below)
[2024-10-10 04:19:59,533][train][INFO] - {"epoch": 1551, "train_loss": "0.446", "train_ntokens": "260836", "train_nsentences": "1750.04", "train_wps": "98733.9", "train_ups": "0.38", "train_wpb": "260836", "train_bsz": "1750", "train_num_updates": "74415", "train_lr": "0.000442371", "train_gnorm": "0.371", "train_loss_scale": "4", "train_train_wall": "58", "train_gb_free": "39.2", "train_wall": "7425"}
[2024-10-10 04:19:59,672][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 04:19:59,681][fairseq.trainer][INFO] - begin training epoch 1552
[2024-10-10 04:19:59,682][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 04:22:06,194][fairseq_cli.train][INFO] - end of epoch 1552 (average epoch stats below)
[2024-10-10 04:22:06,200][train][INFO] - {"epoch": 1552, "train_loss": "0.448", "train_ntokens": "260407", "train_nsentences": "1750.04", "train_wps": "98685.1", "train_ups": "0.38", "train_wpb": "260406", "train_bsz": "1750", "train_num_updates": "74463", "train_lr": "0.000442306", "train_gnorm": "0.387", "train_loss_scale": "4", "train_train_wall": "49", "train_gb_free": "40.1", "train_wall": "7552"}
[2024-10-10 04:22:06,331][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 04:22:06,335][fairseq.trainer][INFO] - begin training epoch 1553
[2024-10-10 04:22:06,335][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 04:24:14,045][fairseq_cli.train][INFO] - end of epoch 1553 (average epoch stats below)
[2024-10-10 04:24:14,048][train][INFO] - {"epoch": 1553, "train_loss": "0.449", "train_ntokens": "260562", "train_nsentences": "1750.04", "train_wps": "97828.5", "train_ups": "0.38", "train_wpb": "260562", "train_bsz": "1750", "train_num_updates": "74511", "train_lr": "0.00044224", "train_gnorm": "0.36", "train_loss_scale": "4", "train_train_wall": "47", "train_gb_free": "39.7", "train_wall": "7679"}
[2024-10-10 04:24:14,142][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 04:24:14,148][fairseq.trainer][INFO] - begin training epoch 1554
[2024-10-10 04:24:14,148][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 04:26:20,050][fairseq_cli.train][INFO] - end of epoch 1554 (average epoch stats below)
[2024-10-10 04:26:20,053][train][INFO] - {"epoch": 1554, "train_loss": "0.448", "train_ntokens": "260666", "train_nsentences": "1750.04", "train_wps": "99299.4", "train_ups": "0.38", "train_wpb": "260666", "train_bsz": "1750", "train_num_updates": "74559", "train_lr": "0.000442175", "train_gnorm": "0.364", "train_loss_scale": "4", "train_train_wall": "35", "train_gb_free": "40.3", "train_wall": "7805"}
[2024-10-10 04:26:20,160][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 04:26:20,170][fairseq.trainer][INFO] - begin training epoch 1555
[2024-10-10 04:26:20,170][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 04:28:30,729][train_inner][INFO] - {"epoch": 1555, "update": 1554.854, "loss": "0.447", "ntokens": "260505", "nsentences": "1751.6", "wps": "99300.3", "ups": "0.38", "wpb": "260505", "bsz": "1751.6", "num_updates": "74600", "lr": "0.00044212", "gnorm": "0.371", "loss_scale": "4", "train_wall": "205", "gb_free": "40.5", "wall": "7936"}
[2024-10-10 04:28:32,441][fairseq_cli.train][INFO] - end of epoch 1555 (average epoch stats below)
[2024-10-10 04:28:32,444][train][INFO] - {"epoch": 1555, "train_loss": "0.441", "train_ntokens": "260579", "train_nsentences": "1750.04", "train_wps": "94480.4", "train_ups": "0.36", "train_wpb": "260579", "train_bsz": "1750", "train_num_updates": "74607", "train_lr": "0.00044211", "train_gnorm": "0.36", "train_loss_scale": "4", "train_train_wall": "62", "train_gb_free": "40.5", "train_wall": "7938"}
[2024-10-10 04:28:32,499][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 04:28:32,503][fairseq.trainer][INFO] - begin training epoch 1556
[2024-10-10 04:28:32,503][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 04:30:41,325][fairseq_cli.train][INFO] - end of epoch 1556 (average epoch stats below)
[2024-10-10 04:30:41,328][train][INFO] - {"epoch": 1556, "train_loss": "0.446", "train_ntokens": "260564", "train_nsentences": "1750.04", "train_wps": "97043.2", "train_ups": "0.37", "train_wpb": "260564", "train_bsz": "1750", "train_num_updates": "74655", "train_lr": "0.000442045", "train_gnorm": "0.358", "train_loss_scale": "4", "train_train_wall": "53", "train_gb_free": "39.6", "train_wall": "8067"}
[2024-10-10 04:30:41,385][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 04:30:41,388][fairseq.trainer][INFO] - begin training epoch 1557
[2024-10-10 04:30:41,389][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 04:32:47,619][fairseq_cli.train][INFO] - end of epoch 1557 (average epoch stats below)
[2024-10-10 04:32:47,625][train][INFO] - {"epoch": 1557, "train_loss": "0.448", "train_ntokens": "260820", "train_nsentences": "1750.04", "train_wps": "99130.8", "train_ups": "0.38", "train_wpb": "260820", "train_bsz": "1750", "train_num_updates": "74703", "train_lr": "0.00044198", "train_gnorm": "0.383", "train_loss_scale": "4", "train_train_wall": "31", "train_gb_free": "40.1", "train_wall": "8193"}
[2024-10-10 04:32:47,706][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 04:32:47,718][fairseq.trainer][INFO] - begin training epoch 1558
[2024-10-10 04:32:47,719][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 04:34:52,392][fairseq_cli.train][INFO] - end of epoch 1558 (average epoch stats below)
[2024-10-10 04:34:52,399][train][INFO] - {"epoch": 1558, "train_loss": "0.45", "train_ntokens": "260706", "train_nsentences": "1750.04", "train_wps": "100297", "train_ups": "0.38", "train_wpb": "260706", "train_bsz": "1750", "train_num_updates": "74751", "train_lr": "0.000441914", "train_gnorm": "0.376", "train_loss_scale": "4", "train_train_wall": "44", "train_gb_free": "39.6", "train_wall": "8318"}
[2024-10-10 04:34:52,520][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 04:34:52,527][fairseq.trainer][INFO] - begin training epoch 1559
[2024-10-10 04:34:52,527][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 04:36:59,404][fairseq_cli.train][INFO] - end of epoch 1559 (average epoch stats below)
[2024-10-10 04:36:59,433][train][INFO] - {"epoch": 1559, "train_loss": "0.435", "train_ntokens": "260506", "train_nsentences": "1750.04", "train_wps": "98440.9", "train_ups": "0.38", "train_wpb": "260506", "train_bsz": "1750", "train_num_updates": "74799", "train_lr": "0.000441849", "train_gnorm": "0.367", "train_loss_scale": "4", "train_train_wall": "36", "train_gb_free": "40.1", "train_wall": "8445"}
[2024-10-10 04:36:59,538][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 04:36:59,553][fairseq.trainer][INFO] - begin training epoch 1560
[2024-10-10 04:36:59,554][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 04:38:26,917][train_inner][INFO] - {"epoch": 1560, "update": 1559.021, "loss": "0.445", "ntokens": "260702", "nsentences": "1747.76", "wps": "87458.2", "ups": "0.34", "wpb": "260702", "bsz": "1747.8", "num_updates": "74800", "lr": "0.000441848", "gnorm": "0.369", "loss_scale": "4", "train_wall": "182", "gb_free": "40", "wall": "8532"}
[2024-10-10 04:39:08,992][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 1560 @ 74847 updates
[2024-10-10 04:39:08,993][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-10 04:39:12,624][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-10 04:39:12,626][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 1560 @ 74847 updates, score None) (writing took 3.6346939289942384 seconds)
[2024-10-10 04:39:12,627][fairseq_cli.train][INFO] - end of epoch 1560 (average epoch stats below)
[2024-10-10 04:39:12,629][train][INFO] - {"epoch": 1560, "train_loss": "0.436", "train_ntokens": "260438", "train_nsentences": "1750.04", "train_wps": "93857.3", "train_ups": "0.36", "train_wpb": "260438", "train_bsz": "1750", "train_num_updates": "74847", "train_lr": "0.000441784", "train_gnorm": "0.365", "train_loss_scale": "4", "train_train_wall": "59", "train_gb_free": "39.6", "train_wall": "8578"}
[2024-10-10 04:39:12,691][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 04:39:12,695][fairseq.trainer][INFO] - begin training epoch 1561
[2024-10-10 04:39:12,696][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 04:41:17,734][fairseq_cli.train][INFO] - end of epoch 1561 (average epoch stats below)
[2024-10-10 04:41:17,738][train][INFO] - {"epoch": 1561, "train_loss": "0.439", "train_ntokens": "260900", "train_nsentences": "1750.04", "train_wps": "100101", "train_ups": "0.38", "train_wpb": "260900", "train_bsz": "1750", "train_num_updates": "74895", "train_lr": "0.000441719", "train_gnorm": "0.371", "train_loss_scale": "4", "train_train_wall": "45", "train_gb_free": "40", "train_wall": "8703"}
[2024-10-10 04:41:17,814][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 04:41:17,820][fairseq.trainer][INFO] - begin training epoch 1562
[2024-10-10 04:41:17,820][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 04:43:21,554][fairseq_cli.train][INFO] - end of epoch 1562 (average epoch stats below)
[2024-10-10 04:43:21,559][train][INFO] - {"epoch": 1562, "train_loss": "0.448", "train_ntokens": "260583", "train_nsentences": "1750.04", "train_wps": "101021", "train_ups": "0.39", "train_wpb": "260582", "train_bsz": "1750", "train_num_updates": "74943", "train_lr": "0.000441654", "train_gnorm": "0.372", "train_loss_scale": "4", "train_train_wall": "50", "train_gb_free": "39.2", "train_wall": "8827"}
[2024-10-10 04:43:21,664][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 04:43:21,673][fairseq.trainer][INFO] - begin training epoch 1563
[2024-10-10 04:43:21,674][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 04:45:30,142][fairseq_cli.train][INFO] - end of epoch 1563 (average epoch stats below)
[2024-10-10 04:45:30,159][train][INFO] - {"epoch": 1563, "train_loss": "0.447", "train_ntokens": "261128", "train_nsentences": "1750.04", "train_wps": "97482.5", "train_ups": "0.37", "train_wpb": "261128", "train_bsz": "1750", "train_num_updates": "74991", "train_lr": "0.000441588", "train_gnorm": "0.38", "train_loss_scale": "4", "train_train_wall": "46", "train_gb_free": "39.8", "train_wall": "8956"}
[2024-10-10 04:45:30,283][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 04:45:30,297][fairseq.trainer][INFO] - begin training epoch 1564
[2024-10-10 04:45:30,298][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 04:47:06,644][train_inner][INFO] - {"epoch": 1564, "update": 1563.188, "loss": "0.442", "ntokens": "260589", "nsentences": "1749.52", "wps": "100281", "ups": "0.38", "wpb": "260589", "bsz": "1749.5", "num_updates": "75000", "lr": "0.000441576", "gnorm": "0.373", "loss_scale": "4", "train_wall": "197", "gb_free": "39.3", "wall": "9052"}
[2024-10-10 04:47:37,214][fairseq_cli.train][INFO] - end of epoch 1564 (average epoch stats below)
[2024-10-10 04:47:37,216][train][INFO] - {"epoch": 1564, "train_loss": "0.447", "train_ntokens": "260444", "train_nsentences": "1750.04", "train_wps": "98393.1", "train_ups": "0.38", "train_wpb": "260444", "train_bsz": "1750", "train_num_updates": "75039", "train_lr": "0.000441523", "train_gnorm": "0.39", "train_loss_scale": "4", "train_train_wall": "41", "train_gb_free": "39.8", "train_wall": "9083"}
[2024-10-10 04:47:37,313][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 04:47:37,317][fairseq.trainer][INFO] - begin training epoch 1565
[2024-10-10 04:47:37,318][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 04:49:45,450][fairseq_cli.train][INFO] - end of epoch 1565 (average epoch stats below)
[2024-10-10 04:49:45,461][train][INFO] - {"epoch": 1565, "train_loss": "0.442", "train_ntokens": "260634", "train_nsentences": "1750.04", "train_wps": "97553.4", "train_ups": "0.37", "train_wpb": "260634", "train_bsz": "1750", "train_num_updates": "75087", "train_lr": "0.000441458", "train_gnorm": "0.361", "train_loss_scale": "4", "train_train_wall": "57", "train_gb_free": "40.1", "train_wall": "9211"}
[2024-10-10 04:49:45,555][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 04:49:45,558][fairseq.trainer][INFO] - begin training epoch 1566
[2024-10-10 04:49:45,559][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 04:51:52,747][fairseq_cli.train][INFO] - end of epoch 1566 (average epoch stats below)
[2024-10-10 04:51:52,750][train][INFO] - {"epoch": 1566, "train_loss": "0.446", "train_ntokens": "260277", "train_nsentences": "1750.04", "train_wps": "98151.5", "train_ups": "0.38", "train_wpb": "260278", "train_bsz": "1750", "train_num_updates": "75135", "train_lr": "0.000441393", "train_gnorm": "0.378", "train_loss_scale": "4", "train_train_wall": "43", "train_gb_free": "39.7", "train_wall": "9338"}
[2024-10-10 04:51:52,806][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 04:51:52,809][fairseq.trainer][INFO] - begin training epoch 1567
[2024-10-10 04:51:52,810][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 04:54:02,041][fairseq_cli.train][INFO] - end of epoch 1567 (average epoch stats below)
[2024-10-10 04:54:02,044][train][INFO] - {"epoch": 1567, "train_loss": "0.45", "train_ntokens": "260636", "train_nsentences": "1750.04", "train_wps": "96762.2", "train_ups": "0.37", "train_wpb": "260636", "train_bsz": "1750", "train_num_updates": "75183", "train_lr": "0.000441327", "train_gnorm": "0.357", "train_loss_scale": "4", "train_train_wall": "61", "train_gb_free": "39.1", "train_wall": "9467"}
[2024-10-10 04:54:02,100][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 04:54:02,104][fairseq.trainer][INFO] - begin training epoch 1568
[2024-10-10 04:54:02,104][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 04:55:42,152][train_inner][INFO] - {"epoch": 1568, "update": 1567.354, "loss": "0.444", "ntokens": "260671", "nsentences": "1747.02", "wps": "101133", "ups": "0.39", "wpb": "260671", "bsz": "1747", "num_updates": "75200", "lr": "0.000441304", "gnorm": "0.373", "loss_scale": "4", "train_wall": "212", "gb_free": "39.7", "wall": "9568"}
[2024-10-10 04:56:06,350][fairseq_cli.train][INFO] - end of epoch 1568 (average epoch stats below)
[2024-10-10 04:56:06,362][train][INFO] - {"epoch": 1568, "train_loss": "0.438", "train_ntokens": "260459", "train_nsentences": "1750.04", "train_wps": "100576", "train_ups": "0.39", "train_wpb": "260459", "train_bsz": "1750", "train_num_updates": "75231", "train_lr": "0.000441262", "train_gnorm": "0.373", "train_loss_scale": "4", "train_train_wall": "47", "train_gb_free": "39.7", "train_wall": "9592"}
[2024-10-10 04:56:06,502][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 04:56:06,513][fairseq.trainer][INFO] - begin training epoch 1569
[2024-10-10 04:56:06,514][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 04:58:15,287][fairseq_cli.train][INFO] - end of epoch 1569 (average epoch stats below)
[2024-10-10 04:58:15,291][train][INFO] - {"epoch": 1569, "train_loss": "0.442", "train_ntokens": "260701", "train_nsentences": "1750.04", "train_wps": "97060.9", "train_ups": "0.37", "train_wpb": "260701", "train_bsz": "1750", "train_num_updates": "75279", "train_lr": "0.000441197", "train_gnorm": "0.37", "train_loss_scale": "4", "train_train_wall": "57", "train_gb_free": "39.6", "train_wall": "9721"}
[2024-10-10 04:58:15,366][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 04:58:15,381][fairseq.trainer][INFO] - begin training epoch 1570
[2024-10-10 04:58:15,381][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 05:00:22,754][fairseq_cli.train][INFO] - end of epoch 1570 (average epoch stats below)
[2024-10-10 05:00:22,757][train][INFO] - {"epoch": 1570, "train_loss": "0.441", "train_ntokens": "260464", "train_nsentences": "1750.04", "train_wps": "98085.5", "train_ups": "0.38", "train_wpb": "260464", "train_bsz": "1750", "train_num_updates": "75327", "train_lr": "0.000441132", "train_gnorm": "0.361", "train_loss_scale": "4", "train_train_wall": "46", "train_gb_free": "39.6", "train_wall": "9848"}
[2024-10-10 05:00:22,851][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 05:00:22,856][fairseq.trainer][INFO] - begin training epoch 1571
[2024-10-10 05:00:22,857][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 05:02:32,257][fairseq_cli.train][INFO] - end of epoch 1571 (average epoch stats below)
[2024-10-10 05:02:32,260][train][INFO] - {"epoch": 1571, "train_loss": "0.452", "train_ntokens": "260588", "train_nsentences": "1750.04", "train_wps": "96588.5", "train_ups": "0.37", "train_wpb": "260588", "train_bsz": "1750", "train_num_updates": "75375", "train_lr": "0.000441067", "train_gnorm": "0.359", "train_loss_scale": "4", "train_train_wall": "55", "train_gb_free": "40.1", "train_wall": "9978"}
[2024-10-10 05:02:32,391][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 05:02:32,397][fairseq.trainer][INFO] - begin training epoch 1572
[2024-10-10 05:02:32,398][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 05:04:18,658][train_inner][INFO] - {"epoch": 1572, "update": 1571.521, "loss": "0.445", "ntokens": "260593", "nsentences": "1741.65", "wps": "100907", "ups": "0.39", "wpb": "260593", "bsz": "1741.7", "num_updates": "75400", "lr": "0.000441033", "gnorm": "0.361", "loss_scale": "4", "train_wall": "209", "gb_free": "39.2", "wall": "10084"}
[2024-10-10 05:04:37,476][fairseq_cli.train][INFO] - end of epoch 1572 (average epoch stats below)
[2024-10-10 05:04:37,481][train][INFO] - {"epoch": 1572, "train_loss": "0.445", "train_ntokens": "260674", "train_nsentences": "1750.04", "train_wps": "99927", "train_ups": "0.38", "train_wpb": "260674", "train_bsz": "1750", "train_num_updates": "75423", "train_lr": "0.000441001", "train_gnorm": "0.366", "train_loss_scale": "4", "train_train_wall": "47", "train_gb_free": "39.6", "train_wall": "10103"}
[2024-10-10 05:04:37,634][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 05:04:37,638][fairseq.trainer][INFO] - begin training epoch 1573
[2024-10-10 05:04:37,638][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 05:06:42,084][fairseq_cli.train][INFO] - end of epoch 1573 (average epoch stats below)
[2024-10-10 05:06:42,089][train][INFO] - {"epoch": 1573, "train_loss": "0.438", "train_ntokens": "260624", "train_nsentences": "1750.04", "train_wps": "100400", "train_ups": "0.39", "train_wpb": "260624", "train_bsz": "1750", "train_num_updates": "75471", "train_lr": "0.000440936", "train_gnorm": "0.369", "train_loss_scale": "4", "train_train_wall": "54", "train_gb_free": "40.3", "train_wall": "10228"}
[2024-10-10 05:06:42,211][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 05:06:42,214][fairseq.trainer][INFO] - begin training epoch 1574
[2024-10-10 05:06:42,215][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 05:08:50,064][fairseq_cli.train][INFO] - end of epoch 1574 (average epoch stats below)
[2024-10-10 05:08:50,069][train][INFO] - {"epoch": 1574, "train_loss": "0.439", "train_ntokens": "260837", "train_nsentences": "1750.04", "train_wps": "97831.6", "train_ups": "0.38", "train_wpb": "260837", "train_bsz": "1750", "train_num_updates": "75519", "train_lr": "0.000440871", "train_gnorm": "0.347", "train_loss_scale": "4", "train_train_wall": "55", "train_gb_free": "39.8", "train_wall": "10355"}
[2024-10-10 05:08:50,177][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 05:08:50,186][fairseq.trainer][INFO] - begin training epoch 1575
[2024-10-10 05:08:50,186][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 05:10:55,472][fairseq_cli.train][INFO] - end of epoch 1575 (average epoch stats below)
[2024-10-10 05:10:55,476][train][INFO] - {"epoch": 1575, "train_loss": "0.445", "train_ntokens": "260787", "train_nsentences": "1750.04", "train_wps": "99819.7", "train_ups": "0.38", "train_wpb": "260787", "train_bsz": "1750", "train_num_updates": "75567", "train_lr": "0.000440806", "train_gnorm": "0.366", "train_loss_scale": "4", "train_train_wall": "42", "train_gb_free": "39.3", "train_wall": "10481"}
[2024-10-10 05:10:55,558][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 05:10:55,561][fairseq.trainer][INFO] - begin training epoch 1576
[2024-10-10 05:10:55,561][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 05:12:46,582][train_inner][INFO] - {"epoch": 1576, "update": 1575.688, "loss": "0.442", "ntokens": "260920", "nsentences": "1743.49", "wps": "102741", "ups": "0.39", "wpb": "260920", "bsz": "1743.5", "num_updates": "75600", "lr": "0.000440761", "gnorm": "0.364", "loss_scale": "4", "train_wall": "208", "gb_free": "39.7", "wall": "10592"}
[2024-10-10 05:13:02,340][fairseq_cli.train][INFO] - end of epoch 1576 (average epoch stats below)
[2024-10-10 05:13:02,343][train][INFO] - {"epoch": 1576, "train_loss": "0.45", "train_ntokens": "260816", "train_nsentences": "1750.04", "train_wps": "98681.7", "train_ups": "0.38", "train_wpb": "260816", "train_bsz": "1750", "train_num_updates": "75615", "train_lr": "0.00044074", "train_gnorm": "0.369", "train_loss_scale": "4", "train_train_wall": "55", "train_gb_free": "39.6", "train_wall": "10608"}
[2024-10-10 05:13:02,401][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 05:13:02,405][fairseq.trainer][INFO] - begin training epoch 1577
[2024-10-10 05:13:02,406][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 05:15:08,796][fairseq_cli.train][INFO] - end of epoch 1577 (average epoch stats below)
[2024-10-10 05:15:08,801][train][INFO] - {"epoch": 1577, "train_loss": "0.448", "train_ntokens": "260559", "train_nsentences": "1750.04", "train_wps": "98904.1", "train_ups": "0.38", "train_wpb": "260559", "train_bsz": "1750", "train_num_updates": "75663", "train_lr": "0.000440675", "train_gnorm": "0.386", "train_loss_scale": "4", "train_train_wall": "57", "train_gb_free": "39.6", "train_wall": "10734"}
[2024-10-10 05:15:08,857][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 05:15:08,863][fairseq.trainer][INFO] - begin training epoch 1578
[2024-10-10 05:15:08,864][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 05:17:17,412][fairseq_cli.train][INFO] - end of epoch 1578 (average epoch stats below)
[2024-10-10 05:17:17,425][train][INFO] - {"epoch": 1578, "train_loss": "0.451", "train_ntokens": "260733", "train_nsentences": "1750.04", "train_wps": "97302.7", "train_ups": "0.37", "train_wpb": "260733", "train_bsz": "1750", "train_num_updates": "75711", "train_lr": "0.00044061", "train_gnorm": "0.348", "train_loss_scale": "4", "train_train_wall": "56", "train_gb_free": "39.1", "train_wall": "10863"}
[2024-10-10 05:17:17,503][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 05:17:17,506][fairseq.trainer][INFO] - begin training epoch 1579
[2024-10-10 05:17:17,507][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 05:19:31,800][fairseq_cli.train][INFO] - end of epoch 1579 (average epoch stats below)
[2024-10-10 05:19:31,804][train][INFO] - {"epoch": 1579, "train_loss": "0.445", "train_ntokens": "260720", "train_nsentences": "1750.04", "train_wps": "93131.2", "train_ups": "0.36", "train_wpb": "260720", "train_bsz": "1750", "train_num_updates": "75759", "train_lr": "0.000440545", "train_gnorm": "0.353", "train_loss_scale": "4", "train_train_wall": "47", "train_gb_free": "39.6", "train_wall": "10997"}
[2024-10-10 05:19:31,925][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 05:19:31,929][fairseq.trainer][INFO] - begin training epoch 1580
[2024-10-10 05:19:31,930][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 05:21:36,759][train_inner][INFO] - {"epoch": 1580, "update": 1579.854, "loss": "0.447", "ntokens": "260524", "nsentences": "1765.24", "wps": "98281", "ups": "0.38", "wpb": "260524", "bsz": "1765.2", "num_updates": "75800", "lr": "0.000440489", "gnorm": "0.364", "loss_scale": "4", "train_wall": "222", "gb_free": "40.1", "wall": "11122"}
[2024-10-10 05:21:41,611][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 1580 @ 75807 updates
[2024-10-10 05:21:41,612][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-10 05:21:45,108][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-10 05:21:45,111][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 1580 @ 75807 updates, score None) (writing took 3.4996290197595954 seconds)
[2024-10-10 05:21:45,111][fairseq_cli.train][INFO] - end of epoch 1580 (average epoch stats below)
[2024-10-10 05:21:45,113][train][INFO] - {"epoch": 1580, "train_loss": "0.441", "train_ntokens": "260609", "train_nsentences": "1750.04", "train_wps": "93838.4", "train_ups": "0.36", "train_wpb": "260610", "train_bsz": "1750", "train_num_updates": "75807", "train_lr": "0.00044048", "train_gnorm": "0.367", "train_loss_scale": "4", "train_train_wall": "51", "train_gb_free": "40.3", "train_wall": "11131"}
[2024-10-10 05:21:45,174][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 05:21:45,214][fairseq.trainer][INFO] - begin training epoch 1581
[2024-10-10 05:21:45,215][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 05:23:46,829][fairseq_cli.train][INFO] - end of epoch 1581 (average epoch stats below)
[2024-10-10 05:23:46,843][train][INFO] - {"epoch": 1581, "train_loss": "0.451", "train_ntokens": "260641", "train_nsentences": "1750.04", "train_wps": "102787", "train_ups": "0.39", "train_wpb": "260641", "train_bsz": "1750", "train_num_updates": "75855", "train_lr": "0.000440414", "train_gnorm": "0.374", "train_loss_scale": "4", "train_train_wall": "43", "train_gb_free": "39.6", "train_wall": "11252"}
[2024-10-10 05:23:47,018][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 05:23:47,033][fairseq.trainer][INFO] - begin training epoch 1582
[2024-10-10 05:23:47,034][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 05:25:57,583][fairseq_cli.train][INFO] - end of epoch 1582 (average epoch stats below)
[2024-10-10 05:25:57,586][train][INFO] - {"epoch": 1582, "train_loss": "0.447", "train_ntokens": "260538", "train_nsentences": "1750.04", "train_wps": "95654.3", "train_ups": "0.37", "train_wpb": "260538", "train_bsz": "1750", "train_num_updates": "75903", "train_lr": "0.000440349", "train_gnorm": "0.389", "train_loss_scale": "4", "train_train_wall": "58", "train_gb_free": "39.9", "train_wall": "11383"}
[2024-10-10 05:25:57,642][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 05:25:57,646][fairseq.trainer][INFO] - begin training epoch 1583
[2024-10-10 05:25:57,646][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 05:28:05,180][fairseq_cli.train][INFO] - end of epoch 1583 (average epoch stats below)
[2024-10-10 05:28:05,196][train][INFO] - {"epoch": 1583, "train_loss": "0.44", "train_ntokens": "260436", "train_nsentences": "1750.04", "train_wps": "97964.5", "train_ups": "0.38", "train_wpb": "260436", "train_bsz": "1750", "train_num_updates": "75951", "train_lr": "0.000440284", "train_gnorm": "0.349", "train_loss_scale": "4", "train_train_wall": "48", "train_gb_free": "39.4", "train_wall": "11511"}
[2024-10-10 05:28:05,290][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 05:28:05,300][fairseq.trainer][INFO] - begin training epoch 1584
[2024-10-10 05:28:05,301][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 05:30:13,840][fairseq_cli.train][INFO] - end of epoch 1584 (average epoch stats below)
[2024-10-10 05:30:13,843][train][INFO] - {"epoch": 1584, "train_loss": "0.444", "train_ntokens": "260412", "train_nsentences": "1750.04", "train_wps": "97166", "train_ups": "0.37", "train_wpb": "260412", "train_bsz": "1750", "train_num_updates": "75999", "train_lr": "0.000440219", "train_gnorm": "0.365", "train_loss_scale": "4", "train_train_wall": "53", "train_gb_free": "40.1", "train_wall": "11639"}
[2024-10-10 05:30:13,896][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 05:30:13,899][fairseq.trainer][INFO] - begin training epoch 1585
[2024-10-10 05:30:13,900][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 05:31:39,130][train_inner][INFO] - {"epoch": 1585, "update": 1584.021, "loss": "0.446", "ntokens": "260463", "nsentences": "1753.16", "wps": "86483", "ups": "0.33", "wpb": "260463", "bsz": "1753.2", "num_updates": "76000", "lr": "0.000440217", "gnorm": "0.37", "loss_scale": "4", "train_wall": "223", "gb_free": "39.6", "wall": "11725"}
[2024-10-10 05:32:23,648][fairseq_cli.train][INFO] - end of epoch 1585 (average epoch stats below)
[2024-10-10 05:32:23,650][train][INFO] - {"epoch": 1585, "train_loss": "0.444", "train_ntokens": "260791", "train_nsentences": "1750.04", "train_wps": "96437.3", "train_ups": "0.37", "train_wpb": "260791", "train_bsz": "1750", "train_num_updates": "76047", "train_lr": "0.000440154", "train_gnorm": "0.357", "train_loss_scale": "4", "train_train_wall": "60", "train_gb_free": "39.8", "train_wall": "11769"}
[2024-10-10 05:32:23,715][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 05:32:23,722][fairseq.trainer][INFO] - begin training epoch 1586
[2024-10-10 05:32:23,723][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 05:34:22,707][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
[2024-10-10 05:34:32,768][fairseq_cli.train][INFO] - end of epoch 1586 (average epoch stats below)
[2024-10-10 05:34:32,770][train][INFO] - {"epoch": 1586, "train_loss": "0.451", "train_ntokens": "260515", "train_nsentences": "1763.19", "train_wps": "94830.8", "train_ups": "0.36", "train_wpb": "260515", "train_bsz": "1763.2", "train_num_updates": "76094", "train_lr": "0.00044009", "train_gnorm": "0.349", "train_loss_scale": "4", "train_train_wall": "40", "train_gb_free": "39.6", "train_wall": "11898"}
[2024-10-10 05:34:32,826][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 05:34:32,831][fairseq.trainer][INFO] - begin training epoch 1587
[2024-10-10 05:34:32,831][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 05:36:39,593][fairseq_cli.train][INFO] - end of epoch 1587 (average epoch stats below)
[2024-10-10 05:36:39,599][train][INFO] - {"epoch": 1587, "train_loss": "0.45", "train_ntokens": "261054", "train_nsentences": "1750.04", "train_wps": "98803", "train_ups": "0.38", "train_wpb": "261054", "train_bsz": "1750", "train_num_updates": "76142", "train_lr": "0.000440024", "train_gnorm": "0.398", "train_loss_scale": "4", "train_train_wall": "47", "train_gb_free": "39.8", "train_wall": "12025"}
[2024-10-10 05:36:39,665][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 05:36:39,672][fairseq.trainer][INFO] - begin training epoch 1588
[2024-10-10 05:36:39,672][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 05:38:54,500][fairseq_cli.train][INFO] - end of epoch 1588 (average epoch stats below)
[2024-10-10 05:38:54,504][train][INFO] - {"epoch": 1588, "train_loss": "0.445", "train_ntokens": "260788", "train_nsentences": "1750.04", "train_wps": "92792.8", "train_ups": "0.36", "train_wpb": "260788", "train_bsz": "1750", "train_num_updates": "76190", "train_lr": "0.000439959", "train_gnorm": "0.359", "train_loss_scale": "4", "train_train_wall": "57", "train_gb_free": "39.6", "train_wall": "12160"}
[2024-10-10 05:38:54,559][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 05:38:54,562][fairseq.trainer][INFO] - begin training epoch 1589
[2024-10-10 05:38:54,563][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 05:40:25,726][train_inner][INFO] - {"epoch": 1589, "update": 1588.208, "loss": "0.446", "ntokens": "260862", "nsentences": "1746.47", "wps": "99075.5", "ups": "0.38", "wpb": "260862", "bsz": "1746.5", "num_updates": "76200", "lr": "0.000439946", "gnorm": "0.365", "loss_scale": "4", "train_wall": "206", "gb_free": "39.7", "wall": "12251"}
[2024-10-10 05:41:05,633][fairseq_cli.train][INFO] - end of epoch 1589 (average epoch stats below)
[2024-10-10 05:41:05,641][train][INFO] - {"epoch": 1589, "train_loss": "0.443", "train_ntokens": "260931", "train_nsentences": "1750.04", "train_wps": "95515.1", "train_ups": "0.37", "train_wpb": "260931", "train_bsz": "1750", "train_num_updates": "76238", "train_lr": "0.000439894", "train_gnorm": "0.368", "train_loss_scale": "4", "train_train_wall": "58", "train_gb_free": "39.7", "train_wall": "12291"}
[2024-10-10 05:41:05,756][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 05:41:05,769][fairseq.trainer][INFO] - begin training epoch 1590
[2024-10-10 05:41:05,769][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 05:43:14,795][fairseq_cli.train][INFO] - end of epoch 1590 (average epoch stats below)
[2024-10-10 05:43:14,799][train][INFO] - {"epoch": 1590, "train_loss": "0.439", "train_ntokens": "260814", "train_nsentences": "1750.04", "train_wps": "96931.3", "train_ups": "0.37", "train_wpb": "260814", "train_bsz": "1750", "train_num_updates": "76286", "train_lr": "0.000439829", "train_gnorm": "0.364", "train_loss_scale": "4", "train_train_wall": "57", "train_gb_free": "40.1", "train_wall": "12420"}
[2024-10-10 05:43:14,863][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 05:43:14,868][fairseq.trainer][INFO] - begin training epoch 1591
[2024-10-10 05:43:14,868][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 05:45:19,228][fairseq_cli.train][INFO] - end of epoch 1591 (average epoch stats below)
[2024-10-10 05:45:19,255][train][INFO] - {"epoch": 1591, "train_loss": "0.449", "train_ntokens": "260757", "train_nsentences": "1750.04", "train_wps": "100584", "train_ups": "0.39", "train_wpb": "260757", "train_bsz": "1750", "train_num_updates": "76334", "train_lr": "0.000439764", "train_gnorm": "0.353", "train_loss_scale": "4", "train_train_wall": "39", "train_gb_free": "39.9", "train_wall": "12545"}
[2024-10-10 05:45:19,353][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 05:45:19,375][fairseq.trainer][INFO] - begin training epoch 1592
[2024-10-10 05:45:19,376][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 05:47:27,360][fairseq_cli.train][INFO] - end of epoch 1592 (average epoch stats below)
[2024-10-10 05:47:27,364][train][INFO] - {"epoch": 1592, "train_loss": "0.442", "train_ntokens": "260931", "train_nsentences": "1750.04", "train_wps": "97768.7", "train_ups": "0.37", "train_wpb": "260931", "train_bsz": "1750", "train_num_updates": "76382", "train_lr": "0.000439698", "train_gnorm": "0.38", "train_loss_scale": "4", "train_train_wall": "46", "train_gb_free": "39.6", "train_wall": "12673"}
[2024-10-10 05:47:27,461][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 05:47:27,468][fairseq.trainer][INFO] - begin training epoch 1593
[2024-10-10 05:47:27,469][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 05:49:08,789][train_inner][INFO] - {"epoch": 1593, "update": 1592.375, "loss": "0.444", "ntokens": "260704", "nsentences": "1760.62", "wps": "99684.7", "ups": "0.38", "wpb": "260704", "bsz": "1760.6", "num_updates": "76400", "lr": "0.000439674", "gnorm": "0.366", "loss_scale": "4", "train_wall": "203", "gb_free": "39.2", "wall": "12774"}
[2024-10-10 05:49:35,873][fairseq_cli.train][INFO] - end of epoch 1593 (average epoch stats below)
[2024-10-10 05:49:35,875][train][INFO] - {"epoch": 1593, "train_loss": "0.455", "train_ntokens": "261214", "train_nsentences": "1750.04", "train_wps": "97567.9", "train_ups": "0.37", "train_wpb": "261214", "train_bsz": "1750", "train_num_updates": "76430", "train_lr": "0.000439633", "train_gnorm": "0.354", "train_loss_scale": "4", "train_train_wall": "49", "train_gb_free": "40.1", "train_wall": "12801"}
[2024-10-10 05:49:36,010][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 05:49:36,025][fairseq.trainer][INFO] - begin training epoch 1594
[2024-10-10 05:49:36,026][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 05:51:39,755][fairseq_cli.train][INFO] - end of epoch 1594 (average epoch stats below)
[2024-10-10 05:51:39,771][train][INFO] - {"epoch": 1594, "train_loss": "0.447", "train_ntokens": "261066", "train_nsentences": "1750.04", "train_wps": "101156", "train_ups": "0.39", "train_wpb": "261066", "train_bsz": "1750", "train_num_updates": "76478", "train_lr": "0.000439568", "train_gnorm": "0.39", "train_loss_scale": "4", "train_train_wall": "53", "train_gb_free": "40", "train_wall": "12925"}
[2024-10-10 05:51:39,924][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 05:51:39,930][fairseq.trainer][INFO] - begin training epoch 1595
[2024-10-10 05:51:39,930][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 05:53:47,065][fairseq_cli.train][INFO] - end of epoch 1595 (average epoch stats below)
[2024-10-10 05:53:47,077][train][INFO] - {"epoch": 1595, "train_loss": "0.438", "train_ntokens": "260768", "train_nsentences": "1750.04", "train_wps": "98324.2", "train_ups": "0.38", "train_wpb": "260768", "train_bsz": "1750", "train_num_updates": "76526", "train_lr": "0.000439503", "train_gnorm": "0.386", "train_loss_scale": "4", "train_train_wall": "49", "train_gb_free": "39.8", "train_wall": "13053"}
[2024-10-10 05:53:47,143][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 05:53:47,147][fairseq.trainer][INFO] - begin training epoch 1596
[2024-10-10 05:53:47,147][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 05:55:55,622][fairseq_cli.train][INFO] - end of epoch 1596 (average epoch stats below)
[2024-10-10 05:55:55,626][train][INFO] - {"epoch": 1596, "train_loss": "0.434", "train_ntokens": "260589", "train_nsentences": "1750.04", "train_wps": "97306.8", "train_ups": "0.37", "train_wpb": "260589", "train_bsz": "1750", "train_num_updates": "76574", "train_lr": "0.000439437", "train_gnorm": "0.349", "train_loss_scale": "4", "train_train_wall": "56", "train_gb_free": "39.7", "train_wall": "13181"}
[2024-10-10 05:55:55,712][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 05:55:55,723][fairseq.trainer][INFO] - begin training epoch 1597
[2024-10-10 05:55:55,724][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 05:57:47,551][train_inner][INFO] - {"epoch": 1597, "update": 1596.542, "loss": "0.443", "ntokens": "260879", "nsentences": "1751.31", "wps": "100579", "ups": "0.39", "wpb": "260880", "bsz": "1751.3", "num_updates": "76600", "lr": "0.000439402", "gnorm": "0.371", "loss_scale": "4", "train_wall": "218", "gb_free": "40.5", "wall": "13293"}
[2024-10-10 05:58:06,018][fairseq_cli.train][INFO] - end of epoch 1597 (average epoch stats below)
[2024-10-10 05:58:06,021][train][INFO] - {"epoch": 1597, "train_loss": "0.444", "train_ntokens": "261095", "train_nsentences": "1750.04", "train_wps": "96114.5", "train_ups": "0.37", "train_wpb": "261095", "train_bsz": "1750", "train_num_updates": "76622", "train_lr": "0.000439372", "train_gnorm": "0.362", "train_loss_scale": "4", "train_train_wall": "50", "train_gb_free": "39.2", "train_wall": "13311"}
[2024-10-10 05:58:06,084][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 05:58:06,088][fairseq.trainer][INFO] - begin training epoch 1598
[2024-10-10 05:58:06,089][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 06:00:14,974][fairseq_cli.train][INFO] - end of epoch 1598 (average epoch stats below)
[2024-10-10 06:00:14,977][train][INFO] - {"epoch": 1598, "train_loss": "0.441", "train_ntokens": "260242", "train_nsentences": "1750.04", "train_wps": "96870.3", "train_ups": "0.37", "train_wpb": "260242", "train_bsz": "1750", "train_num_updates": "76670", "train_lr": "0.000439307", "train_gnorm": "0.356", "train_loss_scale": "4", "train_train_wall": "42", "train_gb_free": "42.3", "train_wall": "13440"}
[2024-10-10 06:00:15,059][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 06:00:15,062][fairseq.trainer][INFO] - begin training epoch 1599
[2024-10-10 06:00:15,063][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 06:02:18,923][fairseq_cli.train][INFO] - end of epoch 1599 (average epoch stats below)
[2024-10-10 06:02:18,932][train][INFO] - {"epoch": 1599, "train_loss": "0.449", "train_ntokens": "260763", "train_nsentences": "1750.04", "train_wps": "100984", "train_ups": "0.39", "train_wpb": "260763", "train_bsz": "1750", "train_num_updates": "76718", "train_lr": "0.000439242", "train_gnorm": "0.391", "train_loss_scale": "4", "train_train_wall": "60", "train_gb_free": "39.8", "train_wall": "13564"}
[2024-10-10 06:02:19,088][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 06:02:19,101][fairseq.trainer][INFO] - begin training epoch 1600
[2024-10-10 06:02:19,102][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 06:04:27,104][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 1600 @ 76766 updates
[2024-10-10 06:04:27,105][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-10 06:04:30,871][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-10 06:04:30,874][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 1600 @ 76766 updates, score None) (writing took 3.7702870154753327 seconds)
[2024-10-10 06:04:30,874][fairseq_cli.train][INFO] - end of epoch 1600 (average epoch stats below)
[2024-10-10 06:04:30,877][train][INFO] - {"epoch": 1600, "train_loss": "0.445", "train_ntokens": "260666", "train_nsentences": "1750.04", "train_wps": "94837.1", "train_ups": "0.36", "train_wpb": "260666", "train_bsz": "1750", "train_num_updates": "76766", "train_lr": "0.000439177", "train_gnorm": "0.38", "train_loss_scale": "4", "train_train_wall": "55", "train_gb_free": "39.3", "train_wall": "13696"}
[2024-10-10 06:04:30,950][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 06:04:30,980][fairseq.trainer][INFO] - begin training epoch 1601
[2024-10-10 06:04:30,981][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 06:06:26,011][train_inner][INFO] - {"epoch": 1601, "update": 1600.708, "loss": "0.443", "ntokens": "260902", "nsentences": "1744.95", "wps": "100646", "ups": "0.39", "wpb": "260902", "bsz": "1745", "num_updates": "76800", "lr": "0.00043913", "gnorm": "0.373", "loss_scale": "4", "train_wall": "188", "gb_free": "39.6", "wall": "13811"}
[2024-10-10 06:06:38,093][fairseq_cli.train][INFO] - end of epoch 1601 (average epoch stats below)
[2024-10-10 06:06:38,096][train][INFO] - {"epoch": 1601, "train_loss": "0.434", "train_ntokens": "260882", "train_nsentences": "1750.04", "train_wps": "98434.1", "train_ups": "0.38", "train_wpb": "260882", "train_bsz": "1750", "train_num_updates": "76814", "train_lr": "0.000439111", "train_gnorm": "0.367", "train_loss_scale": "4", "train_train_wall": "27", "train_gb_free": "39.8", "train_wall": "13824"}
[2024-10-10 06:06:38,189][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 06:06:38,209][fairseq.trainer][INFO] - begin training epoch 1602
[2024-10-10 06:06:38,210][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 06:08:44,726][fairseq_cli.train][INFO] - end of epoch 1602 (average epoch stats below)
[2024-10-10 06:08:44,738][train][INFO] - {"epoch": 1602, "train_loss": "0.44", "train_ntokens": "260603", "train_nsentences": "1750.04", "train_wps": "98780.4", "train_ups": "0.38", "train_wpb": "260602", "train_bsz": "1750", "train_num_updates": "76862", "train_lr": "0.000439046", "train_gnorm": "0.379", "train_loss_scale": "4", "train_train_wall": "47", "train_gb_free": "39.2", "train_wall": "13950"}
[2024-10-10 06:08:44,861][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 06:08:44,885][fairseq.trainer][INFO] - begin training epoch 1603
[2024-10-10 06:08:44,885][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 06:10:51,646][fairseq_cli.train][INFO] - end of epoch 1603 (average epoch stats below)
[2024-10-10 06:10:51,659][train][INFO] - {"epoch": 1603, "train_loss": "0.442", "train_ntokens": "260508", "train_nsentences": "1750.04", "train_wps": "98523.7", "train_ups": "0.38", "train_wpb": "260508", "train_bsz": "1750", "train_num_updates": "76910", "train_lr": "0.000438981", "train_gnorm": "0.359", "train_loss_scale": "4", "train_train_wall": "45", "train_gb_free": "39.3", "train_wall": "14077"}
[2024-10-10 06:10:51,783][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 06:10:51,787][fairseq.trainer][INFO] - begin training epoch 1604
[2024-10-10 06:10:51,788][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 06:12:58,246][fairseq_cli.train][INFO] - end of epoch 1604 (average epoch stats below)
[2024-10-10 06:12:58,249][train][INFO] - {"epoch": 1604, "train_loss": "0.445", "train_ntokens": "260633", "train_nsentences": "1750.04", "train_wps": "98829.2", "train_ups": "0.38", "train_wpb": "260633", "train_bsz": "1750", "train_num_updates": "76958", "train_lr": "0.000438916", "train_gnorm": "0.37", "train_loss_scale": "4", "train_train_wall": "40", "train_gb_free": "39.6", "train_wall": "14204"}
[2024-10-10 06:12:58,306][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 06:12:58,309][fairseq.trainer][INFO] - begin training epoch 1605
[2024-10-10 06:12:58,310][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 06:15:00,463][train_inner][INFO] - {"epoch": 1605, "update": 1604.875, "loss": "0.443", "ntokens": "260420", "nsentences": "1753.07", "wps": "101244", "ups": "0.39", "wpb": "260420", "bsz": "1753.1", "num_updates": "77000", "lr": "0.000438859", "gnorm": "0.366", "loss_scale": "4", "train_wall": "200", "gb_free": "40.1", "wall": "14326"}
[2024-10-10 06:15:03,082][fairseq_cli.train][INFO] - end of epoch 1605 (average epoch stats below)
[2024-10-10 06:15:03,084][train][INFO] - {"epoch": 1605, "train_loss": "0.449", "train_ntokens": "260620", "train_nsentences": "1750.04", "train_wps": "100213", "train_ups": "0.38", "train_wpb": "260620", "train_bsz": "1750", "train_num_updates": "77006", "train_lr": "0.000438851", "train_gnorm": "0.355", "train_loss_scale": "4", "train_train_wall": "59", "train_gb_free": "39.2", "train_wall": "14329"}
[2024-10-10 06:15:03,187][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 06:15:03,196][fairseq.trainer][INFO] - begin training epoch 1606
[2024-10-10 06:15:03,197][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 06:17:12,132][fairseq_cli.train][INFO] - end of epoch 1606 (average epoch stats below)
[2024-10-10 06:17:12,135][train][INFO] - {"epoch": 1606, "train_loss": "0.435", "train_ntokens": "260979", "train_nsentences": "1750.04", "train_wps": "97073.7", "train_ups": "0.37", "train_wpb": "260979", "train_bsz": "1750", "train_num_updates": "77054", "train_lr": "0.000438785", "train_gnorm": "0.364", "train_loss_scale": "4", "train_train_wall": "49", "train_gb_free": "39.2", "train_wall": "14458"}
[2024-10-10 06:17:12,192][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 06:17:12,195][fairseq.trainer][INFO] - begin training epoch 1607
[2024-10-10 06:17:12,196][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 06:19:16,770][fairseq_cli.train][INFO] - end of epoch 1607 (average epoch stats below)
[2024-10-10 06:19:16,780][train][INFO] - {"epoch": 1607, "train_loss": "0.436", "train_ntokens": "260454", "train_nsentences": "1750.04", "train_wps": "100303", "train_ups": "0.39", "train_wpb": "260454", "train_bsz": "1750", "train_num_updates": "77102", "train_lr": "0.00043872", "train_gnorm": "0.345", "train_loss_scale": "4", "train_train_wall": "53", "train_gb_free": "39.7", "train_wall": "14582"}
[2024-10-10 06:19:16,900][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 06:19:16,903][fairseq.trainer][INFO] - begin training epoch 1608
[2024-10-10 06:19:16,904][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 06:21:25,260][fairseq_cli.train][INFO] - end of epoch 1608 (average epoch stats below)
[2024-10-10 06:21:25,265][train][INFO] - {"epoch": 1608, "train_loss": "0.437", "train_ntokens": "261070", "train_nsentences": "1750.04", "train_wps": "97534.6", "train_ups": "0.37", "train_wpb": "261070", "train_bsz": "1750", "train_num_updates": "77150", "train_lr": "0.000438655", "train_gnorm": "0.387", "train_loss_scale": "4", "train_train_wall": "58", "train_gb_free": "39.3", "train_wall": "14711"}
[2024-10-10 06:21:25,344][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 06:21:25,347][fairseq.trainer][INFO] - begin training epoch 1609
[2024-10-10 06:21:25,348][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 06:23:35,757][fairseq_cli.train][INFO] - end of epoch 1609 (average epoch stats below)
[2024-10-10 06:23:35,759][train][INFO] - {"epoch": 1609, "train_loss": "0.448", "train_ntokens": "260728", "train_nsentences": "1750.04", "train_wps": "95907.4", "train_ups": "0.37", "train_wpb": "260728", "train_bsz": "1750", "train_num_updates": "77198", "train_lr": "0.00043859", "train_gnorm": "0.356", "train_loss_scale": "4", "train_train_wall": "61", "train_gb_free": "40", "train_wall": "14841"}
[2024-10-10 06:23:35,815][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 06:23:35,819][fairseq.trainer][INFO] - begin training epoch 1610
[2024-10-10 06:23:35,819][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 06:24:58,852][train_inner][INFO] - {"epoch": 1610, "update": 1609.042, "loss": "0.439", "ntokens": "260959", "nsentences": "1741.52", "wps": "87224.3", "ups": "0.33", "wpb": "260959", "bsz": "1741.5", "num_updates": "77200", "lr": "0.000438587", "gnorm": "0.362", "loss_scale": "4", "train_wall": "234", "gb_free": "39.4", "wall": "14924"}
[2024-10-10 06:25:43,096][fairseq_cli.train][INFO] - end of epoch 1610 (average epoch stats below)
[2024-10-10 06:25:43,098][train][INFO] - {"epoch": 1610, "train_loss": "0.446", "train_ntokens": "260913", "train_nsentences": "1750.04", "train_wps": "98352.8", "train_ups": "0.38", "train_wpb": "260913", "train_bsz": "1750", "train_num_updates": "77246", "train_lr": "0.000438524", "train_gnorm": "0.396", "train_loss_scale": "4", "train_train_wall": "52", "train_gb_free": "40.1", "train_wall": "14969"}
[2024-10-10 06:25:43,156][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 06:25:43,161][fairseq.trainer][INFO] - begin training epoch 1611
[2024-10-10 06:25:43,162][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 06:27:51,252][fairseq_cli.train][INFO] - end of epoch 1611 (average epoch stats below)
[2024-10-10 06:27:51,254][train][INFO] - {"epoch": 1611, "train_loss": "0.434", "train_ntokens": "260847", "train_nsentences": "1750.04", "train_wps": "97701.1", "train_ups": "0.37", "train_wpb": "260847", "train_bsz": "1750", "train_num_updates": "77294", "train_lr": "0.000438459", "train_gnorm": "0.353", "train_loss_scale": "4", "train_train_wall": "55", "train_gb_free": "40.4", "train_wall": "15097"}
[2024-10-10 06:27:51,325][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 06:27:51,331][fairseq.trainer][INFO] - begin training epoch 1612
[2024-10-10 06:27:51,331][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 06:29:58,177][fairseq_cli.train][INFO] - end of epoch 1612 (average epoch stats below)
[2024-10-10 06:29:58,180][train][INFO] - {"epoch": 1612, "train_loss": "0.443", "train_ntokens": "260763", "train_nsentences": "1750.04", "train_wps": "98616.5", "train_ups": "0.38", "train_wpb": "260763", "train_bsz": "1750", "train_num_updates": "77342", "train_lr": "0.000438394", "train_gnorm": "0.357", "train_loss_scale": "4", "train_train_wall": "55", "train_gb_free": "39.1", "train_wall": "15224"}
[2024-10-10 06:29:58,244][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 06:29:58,248][fairseq.trainer][INFO] - begin training epoch 1613
[2024-10-10 06:29:58,248][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 06:32:05,750][fairseq_cli.train][INFO] - end of epoch 1613 (average epoch stats below)
[2024-10-10 06:32:05,758][train][INFO] - {"epoch": 1613, "train_loss": "0.442", "train_ntokens": "261026", "train_nsentences": "1750.04", "train_wps": "98212.1", "train_ups": "0.38", "train_wpb": "261026", "train_bsz": "1750", "train_num_updates": "77390", "train_lr": "0.000438329", "train_gnorm": "0.375", "train_loss_scale": "4", "train_train_wall": "48", "train_gb_free": "39.6", "train_wall": "15351"}
[2024-10-10 06:32:05,840][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 06:32:05,844][fairseq.trainer][INFO] - begin training epoch 1614
[2024-10-10 06:32:05,845][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 06:33:42,294][train_inner][INFO] - {"epoch": 1614, "update": 1613.208, "loss": "0.441", "ntokens": "260897", "nsentences": "1758.57", "wps": "99686.4", "ups": "0.38", "wpb": "260897", "bsz": "1758.6", "num_updates": "77400", "lr": "0.000438315", "gnorm": "0.369", "loss_scale": "4", "train_wall": "218", "gb_free": "39.3", "wall": "15448"}
[2024-10-10 06:34:14,495][fairseq_cli.train][INFO] - end of epoch 1614 (average epoch stats below)
[2024-10-10 06:34:14,498][train][INFO] - {"epoch": 1614, "train_loss": "0.445", "train_ntokens": "261066", "train_nsentences": "1750.04", "train_wps": "97339.7", "train_ups": "0.37", "train_wpb": "261066", "train_bsz": "1750", "train_num_updates": "77438", "train_lr": "0.000438264", "train_gnorm": "0.374", "train_loss_scale": "4", "train_train_wall": "49", "train_gb_free": "39.4", "train_wall": "15480"}
[2024-10-10 06:34:14,591][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 06:34:14,616][fairseq.trainer][INFO] - begin training epoch 1615
[2024-10-10 06:34:14,616][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 06:36:04,391][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2024-10-10 06:36:22,223][fairseq_cli.train][INFO] - end of epoch 1615 (average epoch stats below)
[2024-10-10 06:36:22,226][train][INFO] - {"epoch": 1615, "train_loss": "0.442", "train_ntokens": "260757", "train_nsentences": "1753.04", "train_wps": "95953.5", "train_ups": "0.37", "train_wpb": "260757", "train_bsz": "1753", "train_num_updates": "77485", "train_lr": "0.0004382", "train_gnorm": "0.375", "train_loss_scale": "2", "train_train_wall": "52", "train_gb_free": "39.8", "train_wall": "15608"}
[2024-10-10 06:36:22,338][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 06:36:22,356][fairseq.trainer][INFO] - begin training epoch 1616
[2024-10-10 06:36:22,356][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 06:38:29,392][fairseq_cli.train][INFO] - end of epoch 1616 (average epoch stats below)
[2024-10-10 06:38:29,404][train][INFO] - {"epoch": 1616, "train_loss": "0.447", "train_ntokens": "260812", "train_nsentences": "1750.04", "train_wps": "98443.5", "train_ups": "0.38", "train_wpb": "260812", "train_bsz": "1750", "train_num_updates": "77533", "train_lr": "0.000438135", "train_gnorm": "0.361", "train_loss_scale": "2", "train_train_wall": "61", "train_gb_free": "39.7", "train_wall": "15735"}
[2024-10-10 06:38:29,522][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 06:38:29,538][fairseq.trainer][INFO] - begin training epoch 1617
[2024-10-10 06:38:29,540][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 06:40:41,297][fairseq_cli.train][INFO] - end of epoch 1617 (average epoch stats below)
[2024-10-10 06:40:41,304][train][INFO] - {"epoch": 1617, "train_loss": "0.443", "train_ntokens": "260811", "train_nsentences": "1750.04", "train_wps": "94917", "train_ups": "0.36", "train_wpb": "260811", "train_bsz": "1750", "train_num_updates": "77581", "train_lr": "0.000438069", "train_gnorm": "0.354", "train_loss_scale": "2", "train_train_wall": "53", "train_gb_free": "41", "train_wall": "15867"}
[2024-10-10 06:40:41,409][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 06:40:41,422][fairseq.trainer][INFO] - begin training epoch 1618
[2024-10-10 06:40:41,422][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 06:42:25,752][train_inner][INFO] - {"epoch": 1618, "update": 1617.396, "loss": "0.443", "ntokens": "260841", "nsentences": "1737.98", "wps": "99661.5", "ups": "0.38", "wpb": "260841", "bsz": "1738", "num_updates": "77600", "lr": "0.000438043", "gnorm": "0.366", "loss_scale": "2", "train_wall": "235", "gb_free": "39.4", "wall": "15971"}
[2024-10-10 06:42:49,965][fairseq_cli.train][INFO] - end of epoch 1618 (average epoch stats below)
[2024-10-10 06:42:49,968][train][INFO] - {"epoch": 1618, "train_loss": "0.443", "train_ntokens": "260872", "train_nsentences": "1750.04", "train_wps": "97325.5", "train_ups": "0.37", "train_wpb": "260872", "train_bsz": "1750", "train_num_updates": "77629", "train_lr": "0.000438004", "train_gnorm": "0.355", "train_loss_scale": "2", "train_train_wall": "61", "train_gb_free": "39.6", "train_wall": "15995"}
[2024-10-10 06:42:50,097][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 06:42:50,103][fairseq.trainer][INFO] - begin training epoch 1619
[2024-10-10 06:42:50,104][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 06:44:58,139][fairseq_cli.train][INFO] - end of epoch 1619 (average epoch stats below)
[2024-10-10 06:44:58,148][train][INFO] - {"epoch": 1619, "train_loss": "0.44", "train_ntokens": "260805", "train_nsentences": "1750.04", "train_wps": "97674.6", "train_ups": "0.37", "train_wpb": "260805", "train_bsz": "1750", "train_num_updates": "77677", "train_lr": "0.000437939", "train_gnorm": "0.35", "train_loss_scale": "2", "train_train_wall": "49", "train_gb_free": "39.3", "train_wall": "16124"}
[2024-10-10 06:44:58,215][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 06:44:58,219][fairseq.trainer][INFO] - begin training epoch 1620
[2024-10-10 06:44:58,220][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 06:47:04,686][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 1620 @ 77725 updates
[2024-10-10 06:47:04,690][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-10 06:47:07,985][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-10 06:47:08,002][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 1620 @ 77725 updates, score None) (writing took 3.31539932731539 seconds)
[2024-10-10 06:47:08,002][fairseq_cli.train][INFO] - end of epoch 1620 (average epoch stats below)
[2024-10-10 06:47:08,005][train][INFO] - {"epoch": 1620, "train_loss": "0.439", "train_ntokens": "260800", "train_nsentences": "1750.04", "train_wps": "96405.1", "train_ups": "0.37", "train_wpb": "260800", "train_bsz": "1750", "train_num_updates": "77725", "train_lr": "0.000437874", "train_gnorm": "0.366", "train_loss_scale": "2", "train_train_wall": "46", "train_gb_free": "41.2", "train_wall": "16253"}
[2024-10-10 06:47:08,082][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 06:47:08,087][fairseq.trainer][INFO] - begin training epoch 1621
[2024-10-10 06:47:08,088][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 06:49:11,557][fairseq_cli.train][INFO] - end of epoch 1621 (average epoch stats below)
[2024-10-10 06:49:11,560][train][INFO] - {"epoch": 1621, "train_loss": "0.443", "train_ntokens": "260643", "train_nsentences": "1750.04", "train_wps": "101260", "train_ups": "0.39", "train_wpb": "260643", "train_bsz": "1750", "train_num_updates": "77773", "train_lr": "0.000437808", "train_gnorm": "0.387", "train_loss_scale": "2", "train_train_wall": "36", "train_gb_free": "39.8", "train_wall": "16377"}
[2024-10-10 06:49:11,683][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 06:49:11,690][fairseq.trainer][INFO] - begin training epoch 1622
[2024-10-10 06:49:11,690][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 06:51:01,324][train_inner][INFO] - {"epoch": 1622, "update": 1621.562, "loss": "0.443", "ntokens": "260736", "nsentences": "1753.34", "wps": "101145", "ups": "0.39", "wpb": "260736", "bsz": "1753.3", "num_updates": "77800", "lr": "0.000437772", "gnorm": "0.364", "loss_scale": "2", "train_wall": "168", "gb_free": "39.3", "wall": "16487"}
[2024-10-10 06:51:18,823][fairseq_cli.train][INFO] - end of epoch 1622 (average epoch stats below)
[2024-10-10 06:51:18,830][train][INFO] - {"epoch": 1622, "train_loss": "0.45", "train_ntokens": "260854", "train_nsentences": "1750.04", "train_wps": "98388", "train_ups": "0.38", "train_wpb": "260854", "train_bsz": "1750", "train_num_updates": "77821", "train_lr": "0.000437743", "train_gnorm": "0.364", "train_loss_scale": "2", "train_train_wall": "28", "train_gb_free": "39.3", "train_wall": "16504"}
[2024-10-10 06:51:18,941][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 06:51:18,962][fairseq.trainer][INFO] - begin training epoch 1623
[2024-10-10 06:51:18,963][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 06:53:27,321][fairseq_cli.train][INFO] - end of epoch 1623 (average epoch stats below)
[2024-10-10 06:53:27,324][train][INFO] - {"epoch": 1623, "train_loss": "0.444", "train_ntokens": "260747", "train_nsentences": "1750.04", "train_wps": "97417", "train_ups": "0.37", "train_wpb": "260747", "train_bsz": "1750", "train_num_updates": "77869", "train_lr": "0.000437678", "train_gnorm": "0.379", "train_loss_scale": "2", "train_train_wall": "60", "train_gb_free": "39.8", "train_wall": "16633"}
[2024-10-10 06:53:27,407][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 06:53:27,414][fairseq.trainer][INFO] - begin training epoch 1624
[2024-10-10 06:53:27,415][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 06:55:34,604][fairseq_cli.train][INFO] - end of epoch 1624 (average epoch stats below)
[2024-10-10 06:55:34,617][train][INFO] - {"epoch": 1624, "train_loss": "0.441", "train_ntokens": "260825", "train_nsentences": "1750.04", "train_wps": "98361.6", "train_ups": "0.38", "train_wpb": "260825", "train_bsz": "1750", "train_num_updates": "77917", "train_lr": "0.000437613", "train_gnorm": "0.363", "train_loss_scale": "2", "train_train_wall": "47", "train_gb_free": "39.3", "train_wall": "16760"}
[2024-10-10 06:55:34,748][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 06:55:34,772][fairseq.trainer][INFO] - begin training epoch 1625
[2024-10-10 06:55:34,773][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 06:57:40,840][fairseq_cli.train][INFO] - end of epoch 1625 (average epoch stats below)
[2024-10-10 06:57:40,855][train][INFO] - {"epoch": 1625, "train_loss": "0.434", "train_ntokens": "260624", "train_nsentences": "1750.04", "train_wps": "99103.4", "train_ups": "0.38", "train_wpb": "260624", "train_bsz": "1750", "train_num_updates": "77965", "train_lr": "0.000437548", "train_gnorm": "0.361", "train_loss_scale": "2", "train_train_wall": "47", "train_gb_free": "39.9", "train_wall": "16886"}
[2024-10-10 06:57:40,943][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 06:57:40,962][fairseq.trainer][INFO] - begin training epoch 1626
[2024-10-10 06:57:40,963][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 06:59:38,222][train_inner][INFO] - {"epoch": 1626, "update": 1625.729, "loss": "0.44", "ntokens": "260618", "nsentences": "1759.51", "wps": "100840", "ups": "0.39", "wpb": "260618", "bsz": "1759.5", "num_updates": "78000", "lr": "0.0004375", "gnorm": "0.371", "loss_scale": "2", "train_wall": "200", "gb_free": "39.8", "wall": "17004"}
[2024-10-10 06:59:50,024][fairseq_cli.train][INFO] - end of epoch 1626 (average epoch stats below)
[2024-10-10 06:59:50,027][train][INFO] - {"epoch": 1626, "train_loss": "0.433", "train_ntokens": "260518", "train_nsentences": "1750.04", "train_wps": "96810.5", "train_ups": "0.37", "train_wpb": "260518", "train_bsz": "1750", "train_num_updates": "78013", "train_lr": "0.000437482", "train_gnorm": "0.376", "train_loss_scale": "2", "train_train_wall": "43", "train_gb_free": "40.1", "train_wall": "17015"}
[2024-10-10 06:59:50,158][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 06:59:50,162][fairseq.trainer][INFO] - begin training epoch 1627
[2024-10-10 06:59:50,162][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 07:01:54,962][fairseq_cli.train][INFO] - end of epoch 1627 (average epoch stats below)
[2024-10-10 07:01:54,977][train][INFO] - {"epoch": 1627, "train_loss": "0.444", "train_ntokens": "260261", "train_nsentences": "1750.04", "train_wps": "99983.1", "train_ups": "0.38", "train_wpb": "260261", "train_bsz": "1750", "train_num_updates": "78061", "train_lr": "0.000437417", "train_gnorm": "0.347", "train_loss_scale": "2", "train_train_wall": "34", "train_gb_free": "39.3", "train_wall": "17140"}
[2024-10-10 07:01:55,101][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 07:01:55,117][fairseq.trainer][INFO] - begin training epoch 1628
[2024-10-10 07:01:55,117][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 07:03:59,647][fairseq_cli.train][INFO] - end of epoch 1628 (average epoch stats below)
[2024-10-10 07:03:59,651][train][INFO] - {"epoch": 1628, "train_loss": "0.445", "train_ntokens": "260814", "train_nsentences": "1750.04", "train_wps": "100417", "train_ups": "0.39", "train_wpb": "260814", "train_bsz": "1750", "train_num_updates": "78109", "train_lr": "0.000437352", "train_gnorm": "0.348", "train_loss_scale": "2", "train_train_wall": "38", "train_gb_free": "41.5", "train_wall": "17265"}
[2024-10-10 07:03:59,753][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 07:03:59,757][fairseq.trainer][INFO] - begin training epoch 1629
[2024-10-10 07:03:59,758][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 07:06:07,317][fairseq_cli.train][INFO] - end of epoch 1629 (average epoch stats below)
[2024-10-10 07:06:07,323][train][INFO] - {"epoch": 1629, "train_loss": "0.437", "train_ntokens": "260944", "train_nsentences": "1750.04", "train_wps": "98109.1", "train_ups": "0.38", "train_wpb": "260944", "train_bsz": "1750", "train_num_updates": "78157", "train_lr": "0.000437287", "train_gnorm": "0.366", "train_loss_scale": "2", "train_train_wall": "34", "train_gb_free": "39.7", "train_wall": "17393"}
[2024-10-10 07:06:07,423][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 07:06:07,444][fairseq.trainer][INFO] - begin training epoch 1630
[2024-10-10 07:06:07,445][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 07:08:12,835][train_inner][INFO] - {"epoch": 1630, "update": 1629.896, "loss": "0.442", "ntokens": "261000", "nsentences": "1738.62", "wps": "101437", "ups": "0.39", "wpb": "261000", "bsz": "1738.6", "num_updates": "78200", "lr": "0.000437228", "gnorm": "0.356", "loss_scale": "2", "train_wall": "160", "gb_free": "39.6", "wall": "17518"}
[2024-10-10 07:08:15,061][fairseq_cli.train][INFO] - end of epoch 1630 (average epoch stats below)
[2024-10-10 07:08:15,063][train][INFO] - {"epoch": 1630, "train_loss": "0.445", "train_ntokens": "260983", "train_nsentences": "1750.04", "train_wps": "98070", "train_ups": "0.38", "train_wpb": "260984", "train_bsz": "1750", "train_num_updates": "78205", "train_lr": "0.000437221", "train_gnorm": "0.369", "train_loss_scale": "2", "train_train_wall": "44", "train_gb_free": "39.8", "train_wall": "17520"}
[2024-10-10 07:08:15,170][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 07:08:15,190][fairseq.trainer][INFO] - begin training epoch 1631
[2024-10-10 07:08:15,191][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 07:10:23,405][fairseq_cli.train][INFO] - end of epoch 1631 (average epoch stats below)
[2024-10-10 07:10:23,412][train][INFO] - {"epoch": 1631, "train_loss": "0.434", "train_ntokens": "260677", "train_nsentences": "1750.04", "train_wps": "97490.6", "train_ups": "0.37", "train_wpb": "260677", "train_bsz": "1750", "train_num_updates": "78253", "train_lr": "0.000437156", "train_gnorm": "0.369", "train_loss_scale": "2", "train_train_wall": "62", "train_gb_free": "40.2", "train_wall": "17649"}
[2024-10-10 07:10:23,497][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 07:10:23,516][fairseq.trainer][INFO] - begin training epoch 1632
[2024-10-10 07:10:23,517][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 07:12:37,929][fairseq_cli.train][INFO] - end of epoch 1632 (average epoch stats below)
[2024-10-10 07:12:37,932][train][INFO] - {"epoch": 1632, "train_loss": "0.442", "train_ntokens": "260602", "train_nsentences": "1750.04", "train_wps": "92991.1", "train_ups": "0.36", "train_wpb": "260602", "train_bsz": "1750", "train_num_updates": "78301", "train_lr": "0.000437091", "train_gnorm": "0.382", "train_loss_scale": "2", "train_train_wall": "68", "train_gb_free": "39.7", "train_wall": "17783"}
[2024-10-10 07:12:37,989][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 07:12:37,994][fairseq.trainer][INFO] - begin training epoch 1633
[2024-10-10 07:12:37,994][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 07:14:44,418][fairseq_cli.train][INFO] - end of epoch 1633 (average epoch stats below)
[2024-10-10 07:14:44,422][train][INFO] - {"epoch": 1633, "train_loss": "0.437", "train_ntokens": "260320", "train_nsentences": "1750.04", "train_wps": "98788.2", "train_ups": "0.38", "train_wpb": "260320", "train_bsz": "1750", "train_num_updates": "78349", "train_lr": "0.000437026", "train_gnorm": "0.373", "train_loss_scale": "2", "train_train_wall": "36", "train_gb_free": "39.8", "train_wall": "17910"}
[2024-10-10 07:14:44,490][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 07:14:44,496][fairseq.trainer][INFO] - begin training epoch 1634
[2024-10-10 07:14:44,497][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 07:16:52,898][fairseq_cli.train][INFO] - end of epoch 1634 (average epoch stats below)
[2024-10-10 07:16:52,903][train][INFO] - {"epoch": 1634, "train_loss": "0.435", "train_ntokens": "260849", "train_nsentences": "1750.04", "train_wps": "97457.7", "train_ups": "0.37", "train_wpb": "260849", "train_bsz": "1750", "train_num_updates": "78397", "train_lr": "0.000436961", "train_gnorm": "0.343", "train_loss_scale": "2", "train_train_wall": "27", "train_gb_free": "39.7", "train_wall": "18038"}
[2024-10-10 07:16:52,978][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 07:16:52,988][fairseq.trainer][INFO] - begin training epoch 1635
[2024-10-10 07:16:52,989][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 07:18:22,909][train_inner][INFO] - {"epoch": 1635, "update": 1634.062, "loss": "0.437", "ntokens": "260450", "nsentences": "1758.13", "wps": "85386.4", "ups": "0.33", "wpb": "260450", "bsz": "1758.1", "num_updates": "78400", "lr": "0.000436957", "gnorm": "0.367", "loss_scale": "2", "train_wall": "208", "gb_free": "39.6", "wall": "18128"}
[2024-10-10 07:19:00,261][fairseq_cli.train][INFO] - end of epoch 1635 (average epoch stats below)
[2024-10-10 07:19:00,283][train][INFO] - {"epoch": 1635, "train_loss": "0.443", "train_ntokens": "261112", "train_nsentences": "1750.04", "train_wps": "98411.9", "train_ups": "0.38", "train_wpb": "261112", "train_bsz": "1750", "train_num_updates": "78445", "train_lr": "0.000436895", "train_gnorm": "0.388", "train_loss_scale": "2", "train_train_wall": "50", "train_gb_free": "39.3", "train_wall": "18166"}
[2024-10-10 07:19:00,374][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 07:19:00,380][fairseq.trainer][INFO] - begin training epoch 1636
[2024-10-10 07:19:00,380][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 07:21:09,343][fairseq_cli.train][INFO] - end of epoch 1636 (average epoch stats below)
[2024-10-10 07:21:09,357][train][INFO] - {"epoch": 1636, "train_loss": "0.443", "train_ntokens": "260912", "train_nsentences": "1750.04", "train_wps": "97031", "train_ups": "0.37", "train_wpb": "260912", "train_bsz": "1750", "train_num_updates": "78493", "train_lr": "0.00043683", "train_gnorm": "0.353", "train_loss_scale": "2", "train_train_wall": "60", "train_gb_free": "39.6", "train_wall": "18295"}
[2024-10-10 07:21:09,454][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 07:21:09,457][fairseq.trainer][INFO] - begin training epoch 1637
[2024-10-10 07:21:09,458][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 07:23:14,267][fairseq_cli.train][INFO] - end of epoch 1637 (average epoch stats below)
[2024-10-10 07:23:14,277][train][INFO] - {"epoch": 1637, "train_loss": "0.436", "train_ntokens": "260646", "train_nsentences": "1750.04", "train_wps": "100155", "train_ups": "0.38", "train_wpb": "260646", "train_bsz": "1750", "train_num_updates": "78541", "train_lr": "0.000436765", "train_gnorm": "0.365", "train_loss_scale": "2", "train_train_wall": "48", "train_gb_free": "39.7", "train_wall": "18420"}
[2024-10-10 07:23:14,390][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 07:23:14,408][fairseq.trainer][INFO] - begin training epoch 1638
[2024-10-10 07:23:14,409][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 07:25:22,805][fairseq_cli.train][INFO] - end of epoch 1638 (average epoch stats below)
[2024-10-10 07:25:22,808][train][INFO] - {"epoch": 1638, "train_loss": "0.434", "train_ntokens": "260426", "train_nsentences": "1750.04", "train_wps": "97259.2", "train_ups": "0.37", "train_wpb": "260426", "train_bsz": "1750", "train_num_updates": "78589", "train_lr": "0.0004367", "train_gnorm": "0.372", "train_loss_scale": "2", "train_train_wall": "51", "train_gb_free": "40.5", "train_wall": "18548"}
[2024-10-10 07:25:22,874][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 07:25:22,878][fairseq.trainer][INFO] - begin training epoch 1639
[2024-10-10 07:25:22,878][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 07:27:01,797][train_inner][INFO] - {"epoch": 1639, "update": 1638.229, "loss": "0.438", "ntokens": "260761", "nsentences": "1752.55", "wps": "100509", "ups": "0.39", "wpb": "260762", "bsz": "1752.5", "num_updates": "78600", "lr": "0.000436685", "gnorm": "0.371", "loss_scale": "2", "train_wall": "231", "gb_free": "39.3", "wall": "18647"}
[2024-10-10 07:27:31,602][fairseq_cli.train][INFO] - end of epoch 1639 (average epoch stats below)
[2024-10-10 07:27:31,615][train][INFO] - {"epoch": 1639, "train_loss": "0.434", "train_ntokens": "260597", "train_nsentences": "1750.04", "train_wps": "97122.8", "train_ups": "0.37", "train_wpb": "260597", "train_bsz": "1750", "train_num_updates": "78637", "train_lr": "0.000436635", "train_gnorm": "0.359", "train_loss_scale": "2", "train_train_wall": "66", "train_gb_free": "39.1", "train_wall": "18677"}
[2024-10-10 07:27:31,733][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 07:27:31,749][fairseq.trainer][INFO] - begin training epoch 1640
[2024-10-10 07:27:31,750][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 07:29:38,990][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 1640 @ 78685 updates
[2024-10-10 07:29:38,991][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-10 07:29:42,493][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-10 07:29:42,508][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 1640 @ 78685 updates, score None) (writing took 3.517861501313746 seconds)
[2024-10-10 07:29:42,508][fairseq_cli.train][INFO] - end of epoch 1640 (average epoch stats below)
[2024-10-10 07:29:42,515][train][INFO] - {"epoch": 1640, "train_loss": "0.436", "train_ntokens": "260601", "train_nsentences": "1750.04", "train_wps": "95566.8", "train_ups": "0.37", "train_wpb": "260601", "train_bsz": "1750", "train_num_updates": "78685", "train_lr": "0.000436569", "train_gnorm": "0.349", "train_loss_scale": "2", "train_train_wall": "52", "train_gb_free": "39.4", "train_wall": "18808"}
[2024-10-10 07:29:42,577][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 07:29:42,581][fairseq.trainer][INFO] - begin training epoch 1641
[2024-10-10 07:29:42,581][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 07:31:44,797][fairseq_cli.train][INFO] - end of epoch 1641 (average epoch stats below)
[2024-10-10 07:31:44,810][train][INFO] - {"epoch": 1641, "train_loss": "0.439", "train_ntokens": "260963", "train_nsentences": "1750.04", "train_wps": "102430", "train_ups": "0.39", "train_wpb": "260963", "train_bsz": "1750", "train_num_updates": "78733", "train_lr": "0.000436504", "train_gnorm": "0.352", "train_loss_scale": "2", "train_train_wall": "45", "train_gb_free": "39.2", "train_wall": "18930"}
[2024-10-10 07:31:44,941][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 07:31:44,945][fairseq.trainer][INFO] - begin training epoch 1642
[2024-10-10 07:31:44,945][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 07:33:52,358][fairseq_cli.train][INFO] - end of epoch 1642 (average epoch stats below)
[2024-10-10 07:33:52,362][train][INFO] - {"epoch": 1642, "train_loss": "0.442", "train_ntokens": "260570", "train_nsentences": "1750.04", "train_wps": "98059.5", "train_ups": "0.38", "train_wpb": "260570", "train_bsz": "1750", "train_num_updates": "78781", "train_lr": "0.000436439", "train_gnorm": "0.382", "train_loss_scale": "2", "train_train_wall": "58", "train_gb_free": "40.3", "train_wall": "19058"}
[2024-10-10 07:33:52,441][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 07:33:52,448][fairseq.trainer][INFO] - begin training epoch 1643
[2024-10-10 07:33:52,448][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 07:35:37,526][train_inner][INFO] - {"epoch": 1643, "update": 1642.396, "loss": "0.439", "ntokens": "260684", "nsentences": "1747.22", "wps": "101095", "ups": "0.39", "wpb": "260684", "bsz": "1747.2", "num_updates": "78800", "lr": "0.000436413", "gnorm": "0.361", "loss_scale": "2", "train_wall": "217", "gb_free": "39.3", "wall": "19163"}
[2024-10-10 07:36:01,725][fairseq_cli.train][INFO] - end of epoch 1643 (average epoch stats below)
[2024-10-10 07:36:01,731][train][INFO] - {"epoch": 1643, "train_loss": "0.439", "train_ntokens": "259856", "train_nsentences": "1750.04", "train_wps": "96420.6", "train_ups": "0.37", "train_wpb": "259856", "train_bsz": "1750", "train_num_updates": "78829", "train_lr": "0.000436374", "train_gnorm": "0.376", "train_loss_scale": "2", "train_train_wall": "56", "train_gb_free": "42.2", "train_wall": "19187"}
[2024-10-10 07:36:01,843][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 07:36:01,849][fairseq.trainer][INFO] - begin training epoch 1644
[2024-10-10 07:36:01,849][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 07:38:12,145][fairseq_cli.train][INFO] - end of epoch 1644 (average epoch stats below)
[2024-10-10 07:38:12,149][train][INFO] - {"epoch": 1644, "train_loss": "0.434", "train_ntokens": "260736", "train_nsentences": "1750.04", "train_wps": "95965.5", "train_ups": "0.37", "train_wpb": "260736", "train_bsz": "1750", "train_num_updates": "78877", "train_lr": "0.000436308", "train_gnorm": "0.363", "train_loss_scale": "2", "train_train_wall": "59", "train_gb_free": "39.6", "train_wall": "19318"}
[2024-10-10 07:38:12,251][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 07:38:12,272][fairseq.trainer][INFO] - begin training epoch 1645
[2024-10-10 07:38:12,273][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 07:40:23,304][fairseq_cli.train][INFO] - end of epoch 1645 (average epoch stats below)
[2024-10-10 07:40:23,308][train][INFO] - {"epoch": 1645, "train_loss": "0.443", "train_ntokens": "260608", "train_nsentences": "1750.04", "train_wps": "95377.7", "train_ups": "0.37", "train_wpb": "260608", "train_bsz": "1750", "train_num_updates": "78925", "train_lr": "0.000436243", "train_gnorm": "0.351", "train_loss_scale": "2", "train_train_wall": "57", "train_gb_free": "39.2", "train_wall": "19449"}
[2024-10-10 07:40:23,384][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 07:40:23,387][fairseq.trainer][INFO] - begin training epoch 1646
[2024-10-10 07:40:23,388][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 07:42:32,304][fairseq_cli.train][INFO] - end of epoch 1646 (average epoch stats below)
[2024-10-10 07:42:32,310][train][INFO] - {"epoch": 1646, "train_loss": "0.442", "train_ntokens": "261230", "train_nsentences": "1750.04", "train_wps": "97203.2", "train_ups": "0.37", "train_wpb": "261230", "train_bsz": "1750", "train_num_updates": "78973", "train_lr": "0.000436178", "train_gnorm": "0.388", "train_loss_scale": "2", "train_train_wall": "47", "train_gb_free": "40.4", "train_wall": "19578"}
[2024-10-10 07:42:32,424][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 07:42:32,430][fairseq.trainer][INFO] - begin training epoch 1647
[2024-10-10 07:42:32,431][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 07:44:24,772][train_inner][INFO] - {"epoch": 1647, "update": 1646.562, "loss": "0.44", "ntokens": "260497", "nsentences": "1753.2", "wps": "98815.1", "ups": "0.38", "wpb": "260497", "bsz": "1753.2", "num_updates": "79000", "lr": "0.000436141", "gnorm": "0.374", "loss_scale": "2", "train_wall": "221", "gb_free": "39.6", "wall": "19690"}
[2024-10-10 07:44:43,989][fairseq_cli.train][INFO] - end of epoch 1647 (average epoch stats below)
[2024-10-10 07:44:43,992][train][INFO] - {"epoch": 1647, "train_loss": "0.445", "train_ntokens": "261006", "train_nsentences": "1750.04", "train_wps": "95143", "train_ups": "0.36", "train_wpb": "261006", "train_bsz": "1750", "train_num_updates": "79021", "train_lr": "0.000436113", "train_gnorm": "0.391", "train_loss_scale": "2", "train_train_wall": "54", "train_gb_free": "40.1", "train_wall": "19709"}
[2024-10-10 07:44:44,080][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 07:44:44,083][fairseq.trainer][INFO] - begin training epoch 1648
[2024-10-10 07:44:44,084][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 07:46:51,235][fairseq_cli.train][INFO] - end of epoch 1648 (average epoch stats below)
[2024-10-10 07:46:51,239][train][INFO] - {"epoch": 1648, "train_loss": "0.436", "train_ntokens": "260534", "train_nsentences": "1750.04", "train_wps": "98281", "train_ups": "0.38", "train_wpb": "260534", "train_bsz": "1750", "train_num_updates": "79069", "train_lr": "0.000436048", "train_gnorm": "0.356", "train_loss_scale": "2", "train_train_wall": "46", "train_gb_free": "39.6", "train_wall": "19837"}
[2024-10-10 07:46:51,294][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 07:46:51,298][fairseq.trainer][INFO] - begin training epoch 1649
[2024-10-10 07:46:51,298][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 07:48:58,086][fairseq_cli.train][INFO] - end of epoch 1649 (average epoch stats below)
[2024-10-10 07:48:58,089][train][INFO] - {"epoch": 1649, "train_loss": "0.441", "train_ntokens": "260580", "train_nsentences": "1750.04", "train_wps": "98607", "train_ups": "0.38", "train_wpb": "260580", "train_bsz": "1750", "train_num_updates": "79117", "train_lr": "0.000435982", "train_gnorm": "0.362", "train_loss_scale": "2", "train_train_wall": "46", "train_gb_free": "39.5", "train_wall": "19964"}
[2024-10-10 07:48:58,152][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 07:48:58,156][fairseq.trainer][INFO] - begin training epoch 1650
[2024-10-10 07:48:58,156][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 07:51:04,254][fairseq_cli.train][INFO] - end of epoch 1650 (average epoch stats below)
[2024-10-10 07:51:04,259][train][INFO] - {"epoch": 1650, "train_loss": "0.442", "train_ntokens": "260717", "train_nsentences": "1750.04", "train_wps": "99189.6", "train_ups": "0.38", "train_wpb": "260717", "train_bsz": "1750", "train_num_updates": "79165", "train_lr": "0.000435917", "train_gnorm": "0.352", "train_loss_scale": "2", "train_train_wall": "48", "train_gb_free": "39.6", "train_wall": "20090"}
[2024-10-10 07:51:04,370][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 07:51:04,385][fairseq.trainer][INFO] - begin training epoch 1651
[2024-10-10 07:51:04,385][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 07:53:00,405][train_inner][INFO] - {"epoch": 1651, "update": 1650.729, "loss": "0.44", "ntokens": "260895", "nsentences": "1735.42", "wps": "101195", "ups": "0.39", "wpb": "260895", "bsz": "1735.4", "num_updates": "79200", "lr": "0.00043587", "gnorm": "0.353", "loss_scale": "2", "train_wall": "204", "gb_free": "39.8", "wall": "20206"}
[2024-10-10 07:53:12,389][fairseq_cli.train][INFO] - end of epoch 1651 (average epoch stats below)
[2024-10-10 07:53:12,392][train][INFO] - {"epoch": 1651, "train_loss": "0.44", "train_ntokens": "260391", "train_nsentences": "1750.04", "train_wps": "97552.6", "train_ups": "0.37", "train_wpb": "260391", "train_bsz": "1750", "train_num_updates": "79213", "train_lr": "0.000435852", "train_gnorm": "0.338", "train_loss_scale": "2", "train_train_wall": "57", "train_gb_free": "39.7", "train_wall": "20218"}
[2024-10-10 07:53:12,531][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 07:53:12,544][fairseq.trainer][INFO] - begin training epoch 1652
[2024-10-10 07:53:12,545][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 07:55:18,641][fairseq_cli.train][INFO] - end of epoch 1652 (average epoch stats below)
[2024-10-10 07:55:18,659][train][INFO] - {"epoch": 1652, "train_loss": "0.436", "train_ntokens": "260815", "train_nsentences": "1750.04", "train_wps": "99161", "train_ups": "0.38", "train_wpb": "260815", "train_bsz": "1750", "train_num_updates": "79261", "train_lr": "0.000435787", "train_gnorm": "0.361", "train_loss_scale": "2", "train_train_wall": "55", "train_gb_free": "40.1", "train_wall": "20344"}
[2024-10-10 07:55:18,768][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 07:55:18,781][fairseq.trainer][INFO] - begin training epoch 1653
[2024-10-10 07:55:18,782][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 07:57:23,918][fairseq_cli.train][INFO] - end of epoch 1653 (average epoch stats below)
[2024-10-10 07:57:23,924][train][INFO] - {"epoch": 1653, "train_loss": "0.434", "train_ntokens": "260697", "train_nsentences": "1750.04", "train_wps": "99900.4", "train_ups": "0.38", "train_wpb": "260697", "train_bsz": "1750", "train_num_updates": "79309", "train_lr": "0.000435721", "train_gnorm": "0.357", "train_loss_scale": "2", "train_train_wall": "52", "train_gb_free": "39.7", "train_wall": "20469"}
[2024-10-10 07:57:24,066][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 07:57:24,082][fairseq.trainer][INFO] - begin training epoch 1654
[2024-10-10 07:57:24,083][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 07:59:31,487][fairseq_cli.train][INFO] - end of epoch 1654 (average epoch stats below)
[2024-10-10 07:59:31,490][train][INFO] - {"epoch": 1654, "train_loss": "0.437", "train_ntokens": "260725", "train_nsentences": "1750.04", "train_wps": "98108.9", "train_ups": "0.38", "train_wpb": "260725", "train_bsz": "1750", "train_num_updates": "79357", "train_lr": "0.000435656", "train_gnorm": "0.375", "train_loss_scale": "2", "train_train_wall": "53", "train_gb_free": "40.5", "train_wall": "20597"}
[2024-10-10 07:59:31,542][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 07:59:31,546][fairseq.trainer][INFO] - begin training epoch 1655
[2024-10-10 07:59:31,546][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 08:01:36,074][train_inner][INFO] - {"epoch": 1655, "update": 1654.896, "loss": "0.438", "ntokens": "260871", "nsentences": "1752.06", "wps": "101178", "ups": "0.39", "wpb": "260871", "bsz": "1752.1", "num_updates": "79400", "lr": "0.000435598", "gnorm": "0.365", "loss_scale": "2", "train_wall": "226", "gb_free": "39.3", "wall": "20722"}
[2024-10-10 08:01:37,914][fairseq_cli.train][INFO] - end of epoch 1655 (average epoch stats below)
[2024-10-10 08:01:37,917][train][INFO] - {"epoch": 1655, "train_loss": "0.444", "train_ntokens": "261111", "train_nsentences": "1750.04", "train_wps": "99138.3", "train_ups": "0.38", "train_wpb": "261111", "train_bsz": "1750", "train_num_updates": "79405", "train_lr": "0.000435591", "train_gnorm": "0.366", "train_loss_scale": "2", "train_train_wall": "57", "train_gb_free": "39.3", "train_wall": "20723"}
[2024-10-10 08:01:37,986][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 08:01:37,990][fairseq.trainer][INFO] - begin training epoch 1656
[2024-10-10 08:01:37,991][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 08:03:48,762][fairseq_cli.train][INFO] - end of epoch 1656 (average epoch stats below)
[2024-10-10 08:03:48,765][train][INFO] - {"epoch": 1656, "train_loss": "0.435", "train_ntokens": "260636", "train_nsentences": "1750.04", "train_wps": "95613.6", "train_ups": "0.37", "train_wpb": "260636", "train_bsz": "1750", "train_num_updates": "79453", "train_lr": "0.000435526", "train_gnorm": "0.375", "train_loss_scale": "2", "train_train_wall": "52", "train_gb_free": "40.1", "train_wall": "20854"}
[2024-10-10 08:03:48,874][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 08:03:48,892][fairseq.trainer][INFO] - begin training epoch 1657
[2024-10-10 08:03:48,893][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 08:06:04,564][fairseq_cli.train][INFO] - end of epoch 1657 (average epoch stats below)
[2024-10-10 08:06:04,576][train][INFO] - {"epoch": 1657, "train_loss": "0.434", "train_ntokens": "260103", "train_nsentences": "1750.04", "train_wps": "91932", "train_ups": "0.35", "train_wpb": "260103", "train_bsz": "1750", "train_num_updates": "79501", "train_lr": "0.000435461", "train_gnorm": "0.359", "train_loss_scale": "2", "train_train_wall": "27", "train_gb_free": "39.3", "train_wall": "20990"}
[2024-10-10 08:06:04,680][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 08:06:04,686][fairseq.trainer][INFO] - begin training epoch 1658
[2024-10-10 08:06:04,687][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 08:08:10,948][fairseq_cli.train][INFO] - end of epoch 1658 (average epoch stats below)
[2024-10-10 08:08:10,952][train][INFO] - {"epoch": 1658, "train_loss": "0.433", "train_ntokens": "260815", "train_nsentences": "1750.04", "train_wps": "99064.9", "train_ups": "0.38", "train_wpb": "260815", "train_bsz": "1750", "train_num_updates": "79549", "train_lr": "0.000435395", "train_gnorm": "0.374", "train_loss_scale": "4", "train_train_wall": "44", "train_gb_free": "39.8", "train_wall": "21116"}
[2024-10-10 08:08:11,057][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 08:08:11,073][fairseq.trainer][INFO] - begin training epoch 1659
[2024-10-10 08:08:11,074][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 08:10:20,918][fairseq_cli.train][INFO] - end of epoch 1659 (average epoch stats below)
[2024-10-10 08:10:20,922][train][INFO] - {"epoch": 1659, "train_loss": "0.431", "train_ntokens": "260561", "train_nsentences": "1750.04", "train_wps": "96232.4", "train_ups": "0.37", "train_wpb": "260561", "train_bsz": "1750", "train_num_updates": "79597", "train_lr": "0.00043533", "train_gnorm": "0.367", "train_loss_scale": "4", "train_train_wall": "59", "train_gb_free": "40", "train_wall": "21246"}
[2024-10-10 08:10:21,024][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 08:10:21,028][fairseq.trainer][INFO] - begin training epoch 1660
[2024-10-10 08:10:21,029][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 08:11:52,299][train_inner][INFO] - {"epoch": 1660, "update": 1659.062, "loss": "0.434", "ntokens": "260305", "nsentences": "1766.99", "wps": "84484.6", "ups": "0.32", "wpb": "260305", "bsz": "1767", "num_updates": "79600", "lr": "0.000435326", "gnorm": "0.368", "loss_scale": "4", "train_wall": "197", "gb_free": "39.8", "wall": "21338"}
[2024-10-10 08:12:26,784][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 1660 @ 79645 updates
[2024-10-10 08:12:26,785][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-10 08:12:30,404][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-10 08:12:30,407][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 1660 @ 79645 updates, score None) (writing took 3.622756644152105 seconds)
[2024-10-10 08:12:30,407][fairseq_cli.train][INFO] - end of epoch 1660 (average epoch stats below)
[2024-10-10 08:12:30,410][train][INFO] - {"epoch": 1660, "train_loss": "0.445", "train_ntokens": "260907", "train_nsentences": "1750.04", "train_wps": "96719.2", "train_ups": "0.37", "train_wpb": "260907", "train_bsz": "1750", "train_num_updates": "79645", "train_lr": "0.000435265", "train_gnorm": "0.372", "train_loss_scale": "4", "train_train_wall": "46", "train_gb_free": "40.3", "train_wall": "21376"}
[2024-10-10 08:12:30,479][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 08:12:30,521][fairseq.trainer][INFO] - begin training epoch 1661
[2024-10-10 08:12:30,521][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 08:14:38,241][fairseq_cli.train][INFO] - end of epoch 1661 (average epoch stats below)
[2024-10-10 08:14:38,255][train][INFO] - {"epoch": 1661, "train_loss": "0.434", "train_ntokens": "260912", "train_nsentences": "1750.04", "train_wps": "97965.8", "train_ups": "0.38", "train_wpb": "260912", "train_bsz": "1750", "train_num_updates": "79693", "train_lr": "0.0004352", "train_gnorm": "0.365", "train_loss_scale": "4", "train_train_wall": "39", "train_gb_free": "39.2", "train_wall": "21504"}
[2024-10-10 08:14:38,390][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 08:14:38,408][fairseq.trainer][INFO] - begin training epoch 1662
[2024-10-10 08:14:38,409][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 08:16:50,615][fairseq_cli.train][INFO] - end of epoch 1662 (average epoch stats below)
[2024-10-10 08:16:50,617][train][INFO] - {"epoch": 1662, "train_loss": "0.44", "train_ntokens": "260722", "train_nsentences": "1750.04", "train_wps": "94551.3", "train_ups": "0.36", "train_wpb": "260722", "train_bsz": "1750", "train_num_updates": "79741", "train_lr": "0.000435135", "train_gnorm": "0.367", "train_loss_scale": "4", "train_train_wall": "55", "train_gb_free": "39.7", "train_wall": "21636"}
[2024-10-10 08:16:50,695][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 08:16:50,700][fairseq.trainer][INFO] - begin training epoch 1663
[2024-10-10 08:16:50,700][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 08:19:00,069][fairseq_cli.train][INFO] - end of epoch 1663 (average epoch stats below)
[2024-10-10 08:19:00,073][train][INFO] - {"epoch": 1663, "train_loss": "0.431", "train_ntokens": "260553", "train_nsentences": "1750.04", "train_wps": "96611.6", "train_ups": "0.37", "train_wpb": "260553", "train_bsz": "1750", "train_num_updates": "79789", "train_lr": "0.000435069", "train_gnorm": "0.357", "train_loss_scale": "4", "train_train_wall": "64", "train_gb_free": "39.6", "train_wall": "21766"}
[2024-10-10 08:19:00,147][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 08:19:00,164][fairseq.trainer][INFO] - begin training epoch 1664
[2024-10-10 08:19:00,165][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 08:20:35,370][train_inner][INFO] - {"epoch": 1664, "update": 1663.229, "loss": "0.437", "ntokens": "260781", "nsentences": "1748.24", "wps": "99713.1", "ups": "0.38", "wpb": "260781", "bsz": "1748.2", "num_updates": "79800", "lr": "0.000435054", "gnorm": "0.366", "loss_scale": "4", "train_wall": "212", "gb_free": "39.6", "wall": "21861"}
[2024-10-10 08:21:06,279][fairseq_cli.train][INFO] - end of epoch 1664 (average epoch stats below)
[2024-10-10 08:21:06,284][train][INFO] - {"epoch": 1664, "train_loss": "0.425", "train_ntokens": "260841", "train_nsentences": "1750.04", "train_wps": "99206.8", "train_ups": "0.38", "train_wpb": "260840", "train_bsz": "1750", "train_num_updates": "79837", "train_lr": "0.000435004", "train_gnorm": "0.37", "train_loss_scale": "4", "train_train_wall": "52", "train_gb_free": "39.6", "train_wall": "21892"}
[2024-10-10 08:21:06,362][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 08:21:06,369][fairseq.trainer][INFO] - begin training epoch 1665
[2024-10-10 08:21:06,370][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 08:23:16,355][fairseq_cli.train][INFO] - end of epoch 1665 (average epoch stats below)
[2024-10-10 08:23:16,359][train][INFO] - {"epoch": 1665, "train_loss": "0.432", "train_ntokens": "260512", "train_nsentences": "1750.04", "train_wps": "96138.2", "train_ups": "0.37", "train_wpb": "260512", "train_bsz": "1750", "train_num_updates": "79885", "train_lr": "0.000434939", "train_gnorm": "0.364", "train_loss_scale": "4", "train_train_wall": "31", "train_gb_free": "40.1", "train_wall": "22022"}
[2024-10-10 08:23:16,469][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 08:23:16,481][fairseq.trainer][INFO] - begin training epoch 1666
[2024-10-10 08:23:16,481][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 08:25:22,313][fairseq_cli.train][INFO] - end of epoch 1666 (average epoch stats below)
[2024-10-10 08:25:22,316][train][INFO] - {"epoch": 1666, "train_loss": "0.428", "train_ntokens": "260989", "train_nsentences": "1750.04", "train_wps": "99461.6", "train_ups": "0.38", "train_wpb": "260989", "train_bsz": "1750", "train_num_updates": "79933", "train_lr": "0.000434874", "train_gnorm": "0.345", "train_loss_scale": "4", "train_train_wall": "55", "train_gb_free": "39.6", "train_wall": "22148"}
[2024-10-10 08:25:22,439][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 08:25:22,448][fairseq.trainer][INFO] - begin training epoch 1667
[2024-10-10 08:25:22,449][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 08:27:29,788][fairseq_cli.train][INFO] - end of epoch 1667 (average epoch stats below)
[2024-10-10 08:27:29,794][train][INFO] - {"epoch": 1667, "train_loss": "0.439", "train_ntokens": "260613", "train_nsentences": "1750.04", "train_wps": "98135.1", "train_ups": "0.38", "train_wpb": "260613", "train_bsz": "1750", "train_num_updates": "79981", "train_lr": "0.000434808", "train_gnorm": "0.362", "train_loss_scale": "4", "train_train_wall": "54", "train_gb_free": "39.8", "train_wall": "22275"}
[2024-10-10 08:27:29,906][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 08:27:29,910][fairseq.trainer][INFO] - begin training epoch 1668
[2024-10-10 08:27:29,911][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 08:29:13,604][train_inner][INFO] - {"epoch": 1668, "update": 1667.396, "loss": "0.431", "ntokens": "260948", "nsentences": "1737.77", "wps": "100708", "ups": "0.39", "wpb": "260948", "bsz": "1737.8", "num_updates": "80000", "lr": "0.000434783", "gnorm": "0.36", "loss_scale": "4", "train_wall": "204", "gb_free": "39.8", "wall": "22379"}
[2024-10-10 08:29:38,459][fairseq_cli.train][INFO] - end of epoch 1668 (average epoch stats below)
[2024-10-10 08:29:38,467][train][INFO] - {"epoch": 1668, "train_loss": "0.433", "train_ntokens": "260696", "train_nsentences": "1750.04", "train_wps": "97258.2", "train_ups": "0.37", "train_wpb": "260696", "train_bsz": "1750", "train_num_updates": "80029", "train_lr": "0.000434743", "train_gnorm": "0.357", "train_loss_scale": "4", "train_train_wall": "57", "train_gb_free": "40.4", "train_wall": "22404"}
[2024-10-10 08:29:38,527][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 08:29:38,531][fairseq.trainer][INFO] - begin training epoch 1669
[2024-10-10 08:29:38,531][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 08:31:50,132][fairseq_cli.train][INFO] - end of epoch 1669 (average epoch stats below)
[2024-10-10 08:31:50,137][train][INFO] - {"epoch": 1669, "train_loss": "0.437", "train_ntokens": "260676", "train_nsentences": "1750.04", "train_wps": "95031.7", "train_ups": "0.36", "train_wpb": "260676", "train_bsz": "1750", "train_num_updates": "80077", "train_lr": "0.000434678", "train_gnorm": "0.357", "train_loss_scale": "4", "train_train_wall": "50", "train_gb_free": "40.3", "train_wall": "22536"}
[2024-10-10 08:31:50,217][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 08:31:50,221][fairseq.trainer][INFO] - begin training epoch 1670
[2024-10-10 08:31:50,221][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 08:33:57,710][fairseq_cli.train][INFO] - end of epoch 1670 (average epoch stats below)
[2024-10-10 08:33:57,714][train][INFO] - {"epoch": 1670, "train_loss": "0.43", "train_ntokens": "260603", "train_nsentences": "1750.04", "train_wps": "98052.7", "train_ups": "0.38", "train_wpb": "260603", "train_bsz": "1750", "train_num_updates": "80125", "train_lr": "0.000434613", "train_gnorm": "0.373", "train_loss_scale": "4", "train_train_wall": "43", "train_gb_free": "40.1", "train_wall": "22663"}
[2024-10-10 08:33:57,817][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 08:33:57,821][fairseq.trainer][INFO] - begin training epoch 1671
[2024-10-10 08:33:57,822][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 08:36:05,118][fairseq_cli.train][INFO] - end of epoch 1671 (average epoch stats below)
[2024-10-10 08:36:05,125][train][INFO] - {"epoch": 1671, "train_loss": "0.432", "train_ntokens": "260581", "train_nsentences": "1750.04", "train_wps": "98175.5", "train_ups": "0.38", "train_wpb": "260581", "train_bsz": "1750", "train_num_updates": "80173", "train_lr": "0.000434548", "train_gnorm": "0.373", "train_loss_scale": "4", "train_train_wall": "47", "train_gb_free": "40.1", "train_wall": "22791"}
[2024-10-10 08:36:05,243][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 08:36:05,255][fairseq.trainer][INFO] - begin training epoch 1672
[2024-10-10 08:36:05,255][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 08:37:54,567][train_inner][INFO] - {"epoch": 1672, "update": 1671.562, "loss": "0.431", "ntokens": "260642", "nsentences": "1746.94", "wps": "100063", "ups": "0.38", "wpb": "260642", "bsz": "1746.9", "num_updates": "80200", "lr": "0.000434511", "gnorm": "0.364", "loss_scale": "4", "train_wall": "199", "gb_free": "39.6", "wall": "22900"}
[2024-10-10 08:38:15,559][fairseq_cli.train][INFO] - end of epoch 1672 (average epoch stats below)
[2024-10-10 08:38:15,561][train][INFO] - {"epoch": 1672, "train_loss": "0.429", "train_ntokens": "260774", "train_nsentences": "1750.04", "train_wps": "95971.8", "train_ups": "0.37", "train_wpb": "260774", "train_bsz": "1750", "train_num_updates": "80221", "train_lr": "0.000434482", "train_gnorm": "0.37", "train_loss_scale": "4", "train_train_wall": "57", "train_gb_free": "39.3", "train_wall": "22921"}
[2024-10-10 08:38:15,620][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 08:38:15,623][fairseq.trainer][INFO] - begin training epoch 1673
[2024-10-10 08:38:15,624][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 08:40:24,970][fairseq_cli.train][INFO] - end of epoch 1673 (average epoch stats below)
[2024-10-10 08:40:24,979][train][INFO] - {"epoch": 1673, "train_loss": "0.437", "train_ntokens": "260590", "train_nsentences": "1750.04", "train_wps": "96654.3", "train_ups": "0.37", "train_wpb": "260590", "train_bsz": "1750", "train_num_updates": "80269", "train_lr": "0.000434417", "train_gnorm": "0.364", "train_loss_scale": "4", "train_train_wall": "63", "train_gb_free": "39.8", "train_wall": "23050"}
[2024-10-10 08:40:25,049][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 08:40:25,052][fairseq.trainer][INFO] - begin training epoch 1674
[2024-10-10 08:40:25,053][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 08:42:30,654][fairseq_cli.train][INFO] - end of epoch 1674 (average epoch stats below)
[2024-10-10 08:42:30,657][train][INFO] - {"epoch": 1674, "train_loss": "0.433", "train_ntokens": "260654", "train_nsentences": "1750.04", "train_wps": "99554.1", "train_ups": "0.38", "train_wpb": "260654", "train_bsz": "1750", "train_num_updates": "80317", "train_lr": "0.000434352", "train_gnorm": "0.352", "train_loss_scale": "4", "train_train_wall": "40", "train_gb_free": "39.6", "train_wall": "23176"}
[2024-10-10 08:42:30,735][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 08:42:30,757][fairseq.trainer][INFO] - begin training epoch 1675
[2024-10-10 08:42:30,758][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 08:44:35,392][fairseq_cli.train][INFO] - end of epoch 1675 (average epoch stats below)
[2024-10-10 08:44:35,397][train][INFO] - {"epoch": 1675, "train_loss": "0.434", "train_ntokens": "260492", "train_nsentences": "1750.04", "train_wps": "100241", "train_ups": "0.38", "train_wpb": "260492", "train_bsz": "1750", "train_num_updates": "80365", "train_lr": "0.000434287", "train_gnorm": "0.376", "train_loss_scale": "4", "train_train_wall": "49", "train_gb_free": "40.4", "train_wall": "23301"}
[2024-10-10 08:44:35,507][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 08:44:35,525][fairseq.trainer][INFO] - begin training epoch 1676
[2024-10-10 08:44:35,525][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 08:46:34,679][train_inner][INFO] - {"epoch": 1676, "update": 1675.729, "loss": "0.434", "ntokens": "260675", "nsentences": "1756.43", "wps": "100239", "ups": "0.38", "wpb": "260675", "bsz": "1756.4", "num_updates": "80400", "lr": "0.000434239", "gnorm": "0.366", "loss_scale": "4", "train_wall": "218", "gb_free": "39.7", "wall": "23420"}
[2024-10-10 08:46:45,629][fairseq_cli.train][INFO] - end of epoch 1676 (average epoch stats below)
[2024-10-10 08:46:45,647][train][INFO] - {"epoch": 1676, "train_loss": "0.432", "train_ntokens": "260575", "train_nsentences": "1750.04", "train_wps": "96042.3", "train_ups": "0.37", "train_wpb": "260575", "train_bsz": "1750", "train_num_updates": "80413", "train_lr": "0.000434221", "train_gnorm": "0.355", "train_loss_scale": "4", "train_train_wall": "55", "train_gb_free": "39.3", "train_wall": "23431"}
[2024-10-10 08:46:45,754][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 08:46:45,763][fairseq.trainer][INFO] - begin training epoch 1677
[2024-10-10 08:46:45,763][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 08:48:53,352][fairseq_cli.train][INFO] - end of epoch 1677 (average epoch stats below)
[2024-10-10 08:48:53,356][train][INFO] - {"epoch": 1677, "train_loss": "0.437", "train_ntokens": "261211", "train_nsentences": "1750.04", "train_wps": "98180.9", "train_ups": "0.38", "train_wpb": "261211", "train_bsz": "1750", "train_num_updates": "80461", "train_lr": "0.000434156", "train_gnorm": "0.353", "train_loss_scale": "4", "train_train_wall": "56", "train_gb_free": "39.4", "train_wall": "23559"}
[2024-10-10 08:48:53,447][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 08:48:53,476][fairseq.trainer][INFO] - begin training epoch 1678
[2024-10-10 08:48:53,476][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 08:51:02,457][fairseq_cli.train][INFO] - end of epoch 1678 (average epoch stats below)
[2024-10-10 08:51:02,460][train][INFO] - {"epoch": 1678, "train_loss": "0.433", "train_ntokens": "260607", "train_nsentences": "1750.04", "train_wps": "96894.7", "train_ups": "0.37", "train_wpb": "260607", "train_bsz": "1750", "train_num_updates": "80509", "train_lr": "0.000434091", "train_gnorm": "0.369", "train_loss_scale": "4", "train_train_wall": "29", "train_gb_free": "39.2", "train_wall": "23688"}
[2024-10-10 08:51:02,520][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 08:51:02,523][fairseq.trainer][INFO] - begin training epoch 1679
[2024-10-10 08:51:02,524][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 08:53:08,622][fairseq_cli.train][INFO] - end of epoch 1679 (average epoch stats below)
[2024-10-10 08:53:08,639][train][INFO] - {"epoch": 1679, "train_loss": "0.43", "train_ntokens": "260357", "train_nsentences": "1750.04", "train_wps": "99057.2", "train_ups": "0.38", "train_wpb": "260357", "train_bsz": "1750", "train_num_updates": "80557", "train_lr": "0.000434026", "train_gnorm": "0.373", "train_loss_scale": "4", "train_train_wall": "50", "train_gb_free": "40.1", "train_wall": "23814"}
[2024-10-10 08:53:08,756][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 08:53:08,769][fairseq.trainer][INFO] - begin training epoch 1680
[2024-10-10 08:53:08,770][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 08:55:14,164][train_inner][INFO] - {"epoch": 1680, "update": 1679.896, "loss": "0.434", "ntokens": "260455", "nsentences": "1751.33", "wps": "100276", "ups": "0.39", "wpb": "260455", "bsz": "1751.3", "num_updates": "80600", "lr": "0.000433967", "gnorm": "0.362", "loss_scale": "4", "train_wall": "184", "gb_free": "39.2", "wall": "23940"}
[2024-10-10 08:55:15,709][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 1680 @ 80605 updates
[2024-10-10 08:55:15,709][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-10 08:55:19,384][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-10 08:55:19,398][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 1680 @ 80605 updates, score None) (writing took 3.6897273315116763 seconds)
[2024-10-10 08:55:19,399][fairseq_cli.train][INFO] - end of epoch 1680 (average epoch stats below)
[2024-10-10 08:55:19,400][train][INFO] - {"epoch": 1680, "train_loss": "0.435", "train_ntokens": "260556", "train_nsentences": "1750.04", "train_wps": "95648.7", "train_ups": "0.37", "train_wpb": "260556", "train_bsz": "1750", "train_num_updates": "80605", "train_lr": "0.000433961", "train_gnorm": "0.362", "train_loss_scale": "4", "train_train_wall": "40", "train_gb_free": "39.5", "train_wall": "23945"}
[2024-10-10 08:55:19,457][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 08:55:19,461][fairseq.trainer][INFO] - begin training epoch 1681
[2024-10-10 08:55:19,461][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 08:57:27,103][fairseq_cli.train][INFO] - end of epoch 1681 (average epoch stats below)
[2024-10-10 08:57:27,111][train][INFO] - {"epoch": 1681, "train_loss": "0.429", "train_ntokens": "260530", "train_nsentences": "1750.04", "train_wps": "97925.4", "train_ups": "0.38", "train_wpb": "260530", "train_bsz": "1750", "train_num_updates": "80653", "train_lr": "0.000433895", "train_gnorm": "0.346", "train_loss_scale": "4", "train_train_wall": "33", "train_gb_free": "39.6", "train_wall": "24073"}
[2024-10-10 08:57:27,195][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 08:57:27,201][fairseq.trainer][INFO] - begin training epoch 1682
[2024-10-10 08:57:27,203][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 08:59:34,537][fairseq_cli.train][INFO] - end of epoch 1682 (average epoch stats below)
[2024-10-10 08:59:34,541][train][INFO] - {"epoch": 1682, "train_loss": "0.432", "train_ntokens": "260874", "train_nsentences": "1750.04", "train_wps": "98270.8", "train_ups": "0.38", "train_wpb": "260874", "train_bsz": "1750", "train_num_updates": "80701", "train_lr": "0.00043383", "train_gnorm": "0.361", "train_loss_scale": "4", "train_train_wall": "63", "train_gb_free": "39.6", "train_wall": "24200"}
[2024-10-10 08:59:34,689][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 08:59:34,705][fairseq.trainer][INFO] - begin training epoch 1683
[2024-10-10 08:59:34,706][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 09:01:43,722][fairseq_cli.train][INFO] - end of epoch 1683 (average epoch stats below)
[2024-10-10 09:01:43,725][train][INFO] - {"epoch": 1683, "train_loss": "0.439", "train_ntokens": "260644", "train_nsentences": "1750.04", "train_wps": "96862.9", "train_ups": "0.37", "train_wpb": "260644", "train_bsz": "1750", "train_num_updates": "80749", "train_lr": "0.000433765", "train_gnorm": "0.364", "train_loss_scale": "4", "train_train_wall": "54", "train_gb_free": "39.6", "train_wall": "24329"}
[2024-10-10 09:01:43,780][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 09:01:43,785][fairseq.trainer][INFO] - begin training epoch 1684
[2024-10-10 09:01:43,785][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 09:03:50,159][fairseq_cli.train][INFO] - end of epoch 1684 (average epoch stats below)
[2024-10-10 09:03:50,163][train][INFO] - {"epoch": 1684, "train_loss": "0.427", "train_ntokens": "260534", "train_nsentences": "1750.04", "train_wps": "98910.6", "train_ups": "0.38", "train_wpb": "260534", "train_bsz": "1750", "train_num_updates": "80797", "train_lr": "0.0004337", "train_gnorm": "0.342", "train_loss_scale": "4", "train_train_wall": "47", "train_gb_free": "39.2", "train_wall": "24456"}
[2024-10-10 09:03:50,305][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 09:03:50,308][fairseq.trainer][INFO] - begin training epoch 1685
[2024-10-10 09:03:50,309][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 09:05:17,181][train_inner][INFO] - {"epoch": 1685, "update": 1684.062, "loss": "0.432", "ntokens": "260659", "nsentences": "1748.03", "wps": "86454.3", "ups": "0.33", "wpb": "260659", "bsz": "1748", "num_updates": "80800", "lr": "0.000433696", "gnorm": "0.353", "loss_scale": "4", "train_wall": "206", "gb_free": "39.6", "wall": "24543"}
[2024-10-10 09:05:56,397][fairseq_cli.train][INFO] - end of epoch 1685 (average epoch stats below)
[2024-10-10 09:05:56,399][train][INFO] - {"epoch": 1685, "train_loss": "0.438", "train_ntokens": "260653", "train_nsentences": "1750.04", "train_wps": "99113.1", "train_ups": "0.38", "train_wpb": "260653", "train_bsz": "1750", "train_num_updates": "80845", "train_lr": "0.000433635", "train_gnorm": "0.379", "train_loss_scale": "4", "train_train_wall": "47", "train_gb_free": "39.3", "train_wall": "24582"}
[2024-10-10 09:05:56,504][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 09:05:56,528][fairseq.trainer][INFO] - begin training epoch 1686
[2024-10-10 09:05:56,529][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 09:08:11,127][fairseq_cli.train][INFO] - end of epoch 1686 (average epoch stats below)
[2024-10-10 09:08:11,131][train][INFO] - {"epoch": 1686, "train_loss": "0.432", "train_ntokens": "260672", "train_nsentences": "1750.04", "train_wps": "92870.9", "train_ups": "0.36", "train_wpb": "260672", "train_bsz": "1750", "train_num_updates": "80893", "train_lr": "0.000433569", "train_gnorm": "0.353", "train_loss_scale": "4", "train_train_wall": "53", "train_gb_free": "39.1", "train_wall": "24717"}
[2024-10-10 09:08:11,226][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 09:08:11,234][fairseq.trainer][INFO] - begin training epoch 1687
[2024-10-10 09:08:11,237][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 09:10:18,150][fairseq_cli.train][INFO] - end of epoch 1687 (average epoch stats below)
[2024-10-10 09:10:18,167][train][INFO] - {"epoch": 1687, "train_loss": "0.436", "train_ntokens": "260630", "train_nsentences": "1750.04", "train_wps": "98492.1", "train_ups": "0.38", "train_wpb": "260630", "train_bsz": "1750", "train_num_updates": "80941", "train_lr": "0.000433504", "train_gnorm": "0.351", "train_loss_scale": "4", "train_train_wall": "50", "train_gb_free": "39.8", "train_wall": "24844"}
[2024-10-10 09:10:18,270][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 09:10:18,276][fairseq.trainer][INFO] - begin training epoch 1688
[2024-10-10 09:10:18,277][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 09:12:27,678][fairseq_cli.train][INFO] - end of epoch 1688 (average epoch stats below)
[2024-10-10 09:12:27,683][train][INFO] - {"epoch": 1688, "train_loss": "0.426", "train_ntokens": "260591", "train_nsentences": "1750.04", "train_wps": "96581.5", "train_ups": "0.37", "train_wpb": "260591", "train_bsz": "1750", "train_num_updates": "80989", "train_lr": "0.000433439", "train_gnorm": "0.356", "train_loss_scale": "4", "train_train_wall": "61", "train_gb_free": "39.6", "train_wall": "24973"}
[2024-10-10 09:12:27,799][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 09:12:27,809][fairseq.trainer][INFO] - begin training epoch 1689
[2024-10-10 09:12:27,809][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 09:14:00,449][train_inner][INFO] - {"epoch": 1689, "update": 1688.229, "loss": "0.433", "ntokens": "260453", "nsentences": "1758.05", "wps": "99551", "ups": "0.38", "wpb": "260453", "bsz": "1758", "num_updates": "81000", "lr": "0.000433424", "gnorm": "0.36", "loss_scale": "4", "train_wall": "215", "gb_free": "39.3", "wall": "25066"}
[2024-10-10 09:14:35,117][fairseq_cli.train][INFO] - end of epoch 1689 (average epoch stats below)
[2024-10-10 09:14:35,119][train][INFO] - {"epoch": 1689, "train_loss": "0.433", "train_ntokens": "260535", "train_nsentences": "1750.04", "train_wps": "98136.5", "train_ups": "0.38", "train_wpb": "260535", "train_bsz": "1750", "train_num_updates": "81037", "train_lr": "0.000433374", "train_gnorm": "0.374", "train_loss_scale": "4", "train_train_wall": "38", "train_gb_free": "39.6", "train_wall": "25101"}
[2024-10-10 09:14:35,184][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 09:14:35,187][fairseq.trainer][INFO] - begin training epoch 1690
[2024-10-10 09:14:35,188][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 09:16:42,398][fairseq_cli.train][INFO] - end of epoch 1690 (average epoch stats below)
[2024-10-10 09:16:42,401][train][INFO] - {"epoch": 1690, "train_loss": "0.43", "train_ntokens": "260615", "train_nsentences": "1750.04", "train_wps": "98285.5", "train_ups": "0.38", "train_wpb": "260615", "train_bsz": "1750", "train_num_updates": "81085", "train_lr": "0.000433308", "train_gnorm": "0.378", "train_loss_scale": "4", "train_train_wall": "57", "train_gb_free": "39.2", "train_wall": "25228"}
[2024-10-10 09:16:42,520][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 09:16:42,524][fairseq.trainer][INFO] - begin training epoch 1691
[2024-10-10 09:16:42,524][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 09:18:55,175][fairseq_cli.train][INFO] - end of epoch 1691 (average epoch stats below)
[2024-10-10 09:18:55,185][train][INFO] - {"epoch": 1691, "train_loss": "0.431", "train_ntokens": "261000", "train_nsentences": "1750.04", "train_wps": "94356.5", "train_ups": "0.36", "train_wpb": "261000", "train_bsz": "1750", "train_num_updates": "81133", "train_lr": "0.000433243", "train_gnorm": "0.378", "train_loss_scale": "4", "train_train_wall": "51", "train_gb_free": "39.6", "train_wall": "25361"}
[2024-10-10 09:18:55,294][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 09:18:55,301][fairseq.trainer][INFO] - begin training epoch 1692
[2024-10-10 09:18:55,302][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 09:21:02,390][fairseq_cli.train][INFO] - end of epoch 1692 (average epoch stats below)
[2024-10-10 09:21:02,394][train][INFO] - {"epoch": 1692, "train_loss": "0.425", "train_ntokens": "261043", "train_nsentences": "1750.04", "train_wps": "98503.7", "train_ups": "0.38", "train_wpb": "261043", "train_bsz": "1750", "train_num_updates": "81181", "train_lr": "0.000433178", "train_gnorm": "0.343", "train_loss_scale": "4", "train_train_wall": "54", "train_gb_free": "40.3", "train_wall": "25488"}
[2024-10-10 09:21:02,451][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 09:21:02,455][fairseq.trainer][INFO] - begin training epoch 1693
[2024-10-10 09:21:02,455][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 09:22:47,962][train_inner][INFO] - {"epoch": 1693, "update": 1692.396, "loss": "0.43", "ntokens": "260881", "nsentences": "1752.4", "wps": "98911.6", "ups": "0.38", "wpb": "260882", "bsz": "1752.4", "num_updates": "81200", "lr": "0.000433152", "gnorm": "0.369", "loss_scale": "4", "train_wall": "205", "gb_free": "40.5", "wall": "25593"}
[2024-10-10 09:23:09,462][fairseq_cli.train][INFO] - end of epoch 1693 (average epoch stats below)
[2024-10-10 09:23:09,475][train][INFO] - {"epoch": 1693, "train_loss": "0.428", "train_ntokens": "260638", "train_nsentences": "1750.04", "train_wps": "98456.8", "train_ups": "0.38", "train_wpb": "260638", "train_bsz": "1750", "train_num_updates": "81229", "train_lr": "0.000433113", "train_gnorm": "0.383", "train_loss_scale": "4", "train_train_wall": "39", "train_gb_free": "40", "train_wall": "25615"}
[2024-10-10 09:23:09,578][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 09:23:09,586][fairseq.trainer][INFO] - begin training epoch 1694
[2024-10-10 09:23:09,587][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 09:25:16,486][fairseq_cli.train][INFO] - end of epoch 1694 (average epoch stats below)
[2024-10-10 09:25:16,496][train][INFO] - {"epoch": 1694, "train_loss": "0.433", "train_ntokens": "260791", "train_nsentences": "1750.04", "train_wps": "98552.7", "train_ups": "0.38", "train_wpb": "260791", "train_bsz": "1750", "train_num_updates": "81277", "train_lr": "0.000433048", "train_gnorm": "0.352", "train_loss_scale": "4", "train_train_wall": "58", "train_gb_free": "39.6", "train_wall": "25742"}
[2024-10-10 09:25:16,608][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 09:25:16,623][fairseq.trainer][INFO] - begin training epoch 1695
[2024-10-10 09:25:16,623][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 09:27:26,414][fairseq_cli.train][INFO] - end of epoch 1695 (average epoch stats below)
[2024-10-10 09:27:26,418][train][INFO] - {"epoch": 1695, "train_loss": "0.435", "train_ntokens": "261021", "train_nsentences": "1750.04", "train_wps": "96439", "train_ups": "0.37", "train_wpb": "261021", "train_bsz": "1750", "train_num_updates": "81325", "train_lr": "0.000432982", "train_gnorm": "0.351", "train_loss_scale": "4", "train_train_wall": "53", "train_gb_free": "39.6", "train_wall": "25872"}
[2024-10-10 09:27:26,483][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 09:27:26,487][fairseq.trainer][INFO] - begin training epoch 1696
[2024-10-10 09:27:26,487][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 09:29:32,169][fairseq_cli.train][INFO] - end of epoch 1696 (average epoch stats below)
[2024-10-10 09:29:32,179][train][INFO] - {"epoch": 1696, "train_loss": "0.434", "train_ntokens": "260819", "train_nsentences": "1750.04", "train_wps": "99552", "train_ups": "0.38", "train_wpb": "260819", "train_bsz": "1750", "train_num_updates": "81373", "train_lr": "0.000432917", "train_gnorm": "0.347", "train_loss_scale": "4", "train_train_wall": "43", "train_gb_free": "39.7", "train_wall": "25998"}
[2024-10-10 09:29:32,244][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 09:29:32,248][fairseq.trainer][INFO] - begin training epoch 1697
[2024-10-10 09:29:32,249][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 09:31:19,638][train_inner][INFO] - {"epoch": 1697, "update": 1696.562, "loss": "0.432", "ntokens": "260839", "nsentences": "1732.86", "wps": "101956", "ups": "0.39", "wpb": "260839", "bsz": "1732.9", "num_updates": "81400", "lr": "0.00043288", "gnorm": "0.362", "loss_scale": "4", "train_wall": "215", "gb_free": "40.8", "wall": "26105"}
[2024-10-10 09:31:36,787][fairseq_cli.train][INFO] - end of epoch 1697 (average epoch stats below)
[2024-10-10 09:31:36,791][train][INFO] - {"epoch": 1697, "train_loss": "0.431", "train_ntokens": "260490", "train_nsentences": "1750.04", "train_wps": "100345", "train_ups": "0.39", "train_wpb": "260490", "train_bsz": "1750", "train_num_updates": "81421", "train_lr": "0.000432852", "train_gnorm": "0.396", "train_loss_scale": "4", "train_train_wall": "56", "train_gb_free": "39.6", "train_wall": "26122"}
[2024-10-10 09:31:36,954][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 09:31:36,958][fairseq.trainer][INFO] - begin training epoch 1698
[2024-10-10 09:31:36,959][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 09:33:41,579][fairseq_cli.train][INFO] - end of epoch 1698 (average epoch stats below)
[2024-10-10 09:33:41,595][train][INFO] - {"epoch": 1698, "train_loss": "0.431", "train_ntokens": "260584", "train_nsentences": "1750.04", "train_wps": "100227", "train_ups": "0.38", "train_wpb": "260584", "train_bsz": "1750", "train_num_updates": "81469", "train_lr": "0.000432787", "train_gnorm": "0.339", "train_loss_scale": "4", "train_train_wall": "48", "train_gb_free": "39.7", "train_wall": "26247"}
[2024-10-10 09:33:41,722][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 09:33:41,735][fairseq.trainer][INFO] - begin training epoch 1699
[2024-10-10 09:33:41,735][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 09:35:47,908][fairseq_cli.train][INFO] - end of epoch 1699 (average epoch stats below)
[2024-10-10 09:35:47,928][train][INFO] - {"epoch": 1699, "train_loss": "0.438", "train_ntokens": "260818", "train_nsentences": "1750.04", "train_wps": "99101.1", "train_ups": "0.38", "train_wpb": "260818", "train_bsz": "1750", "train_num_updates": "81517", "train_lr": "0.000432721", "train_gnorm": "0.371", "train_loss_scale": "4", "train_train_wall": "50", "train_gb_free": "40.2", "train_wall": "26373"}
[2024-10-10 09:35:48,052][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 09:35:48,056][fairseq.trainer][INFO] - begin training epoch 1700
[2024-10-10 09:35:48,057][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 09:37:54,514][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
[2024-10-10 09:37:55,928][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 1700 @ 81564 updates
[2024-10-10 09:37:55,929][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-10 09:37:59,233][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-10 09:37:59,235][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 1700 @ 81564 updates, score None) (writing took 3.3069562735036016 seconds)
[2024-10-10 09:37:59,236][fairseq_cli.train][INFO] - end of epoch 1700 (average epoch stats below)
[2024-10-10 09:37:59,239][train][INFO] - {"epoch": 1700, "train_loss": "0.431", "train_ntokens": "260643", "train_nsentences": "1744.72", "train_wps": "93295.3", "train_ups": "0.36", "train_wpb": "260643", "train_bsz": "1744.7", "train_num_updates": "81564", "train_lr": "0.000432658", "train_gnorm": "0.394", "train_loss_scale": "4", "train_train_wall": "48", "train_gb_free": "39.5", "train_wall": "26505"}
[2024-10-10 09:37:59,324][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 09:37:59,328][fairseq.trainer][INFO] - begin training epoch 1701
[2024-10-10 09:37:59,328][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 09:39:51,342][train_inner][INFO] - {"epoch": 1701, "update": 1700.75, "loss": "0.433", "ntokens": "260662", "nsentences": "1764.92", "wps": "101881", "ups": "0.39", "wpb": "260662", "bsz": "1764.9", "num_updates": "81600", "lr": "0.000432609", "gnorm": "0.37", "loss_scale": "4", "train_wall": "182", "gb_free": "39.9", "wall": "26617"}
[2024-10-10 09:40:02,707][fairseq_cli.train][INFO] - end of epoch 1701 (average epoch stats below)
[2024-10-10 09:40:02,713][train][INFO] - {"epoch": 1701, "train_loss": "0.428", "train_ntokens": "260294", "train_nsentences": "1750.04", "train_wps": "101193", "train_ups": "0.39", "train_wpb": "260294", "train_bsz": "1750", "train_num_updates": "81612", "train_lr": "0.000432592", "train_gnorm": "0.367", "train_loss_scale": "4", "train_train_wall": "31", "train_gb_free": "39.8", "train_wall": "26628"}
[2024-10-10 09:40:02,820][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 09:40:02,831][fairseq.trainer][INFO] - begin training epoch 1702
[2024-10-10 09:40:02,832][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 09:42:08,903][fairseq_cli.train][INFO] - end of epoch 1702 (average epoch stats below)
[2024-10-10 09:42:08,908][train][INFO] - {"epoch": 1702, "train_loss": "0.436", "train_ntokens": "260512", "train_nsentences": "1750.04", "train_wps": "99094.3", "train_ups": "0.38", "train_wpb": "260512", "train_bsz": "1750", "train_num_updates": "81660", "train_lr": "0.000432527", "train_gnorm": "0.369", "train_loss_scale": "4", "train_train_wall": "54", "train_gb_free": "39.2", "train_wall": "26754"}
[2024-10-10 09:42:08,994][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 09:42:08,998][fairseq.trainer][INFO] - begin training epoch 1703
[2024-10-10 09:42:08,998][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 09:44:15,408][fairseq_cli.train][INFO] - end of epoch 1703 (average epoch stats below)
[2024-10-10 09:44:15,412][train][INFO] - {"epoch": 1703, "train_loss": "0.429", "train_ntokens": "260661", "train_nsentences": "1750.04", "train_wps": "98907.8", "train_ups": "0.38", "train_wpb": "260661", "train_bsz": "1750", "train_num_updates": "81708", "train_lr": "0.000432462", "train_gnorm": "0.361", "train_loss_scale": "4", "train_train_wall": "40", "train_gb_free": "39.6", "train_wall": "26881"}
[2024-10-10 09:44:15,513][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 09:44:15,520][fairseq.trainer][INFO] - begin training epoch 1704
[2024-10-10 09:44:15,521][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 09:46:23,395][fairseq_cli.train][INFO] - end of epoch 1704 (average epoch stats below)
[2024-10-10 09:46:23,398][train][INFO] - {"epoch": 1704, "train_loss": "0.433", "train_ntokens": "260551", "train_nsentences": "1750.04", "train_wps": "97720.3", "train_ups": "0.38", "train_wpb": "260551", "train_bsz": "1750", "train_num_updates": "81756", "train_lr": "0.000432397", "train_gnorm": "0.359", "train_loss_scale": "4", "train_train_wall": "48", "train_gb_free": "40.2", "train_wall": "27009"}
[2024-10-10 09:46:23,513][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 09:46:23,536][fairseq.trainer][INFO] - begin training epoch 1705
[2024-10-10 09:46:23,537][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 09:48:29,430][train_inner][INFO] - {"epoch": 1705, "update": 1704.917, "loss": "0.433", "ntokens": "260577", "nsentences": "1742.69", "wps": "100593", "ups": "0.39", "wpb": "260577", "bsz": "1742.7", "num_updates": "81800", "lr": "0.000432337", "gnorm": "0.368", "loss_scale": "4", "train_wall": "206", "gb_free": "39.8", "wall": "27135"}
[2024-10-10 09:48:31,166][fairseq_cli.train][INFO] - end of epoch 1705 (average epoch stats below)
[2024-10-10 09:48:31,170][train][INFO] - {"epoch": 1705, "train_loss": "0.435", "train_ntokens": "260739", "train_nsentences": "1750.04", "train_wps": "97955.5", "train_ups": "0.38", "train_wpb": "260739", "train_bsz": "1750", "train_num_updates": "81804", "train_lr": "0.000432332", "train_gnorm": "0.376", "train_loss_scale": "4", "train_train_wall": "55", "train_gb_free": "40", "train_wall": "27137"}
[2024-10-10 09:48:31,265][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 09:48:31,268][fairseq.trainer][INFO] - begin training epoch 1706
[2024-10-10 09:48:31,269][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 09:50:41,169][fairseq_cli.train][INFO] - end of epoch 1706 (average epoch stats below)
[2024-10-10 09:50:41,172][train][INFO] - {"epoch": 1706, "train_loss": "0.436", "train_ntokens": "260390", "train_nsentences": "1750.04", "train_wps": "96145.7", "train_ups": "0.37", "train_wpb": "260390", "train_bsz": "1750", "train_num_updates": "81852", "train_lr": "0.000432266", "train_gnorm": "0.359", "train_loss_scale": "4", "train_train_wall": "56", "train_gb_free": "40.1", "train_wall": "27267"}
[2024-10-10 09:50:41,255][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 09:50:41,265][fairseq.trainer][INFO] - begin training epoch 1707
[2024-10-10 09:50:41,265][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 09:52:48,814][fairseq_cli.train][INFO] - end of epoch 1707 (average epoch stats below)
[2024-10-10 09:52:48,822][train][INFO] - {"epoch": 1707, "train_loss": "0.426", "train_ntokens": "260572", "train_nsentences": "1750.04", "train_wps": "97985.6", "train_ups": "0.38", "train_wpb": "260572", "train_bsz": "1750", "train_num_updates": "81900", "train_lr": "0.000432201", "train_gnorm": "0.369", "train_loss_scale": "4", "train_train_wall": "55", "train_gb_free": "40.5", "train_wall": "27394"}
[2024-10-10 09:52:48,898][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 09:52:48,902][fairseq.trainer][INFO] - begin training epoch 1708
[2024-10-10 09:52:48,902][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 09:54:56,151][fairseq_cli.train][INFO] - end of epoch 1708 (average epoch stats below)
[2024-10-10 09:54:56,163][train][INFO] - {"epoch": 1708, "train_loss": "0.437", "train_ntokens": "260855", "train_nsentences": "1750.04", "train_wps": "98340.8", "train_ups": "0.38", "train_wpb": "260856", "train_bsz": "1750", "train_num_updates": "81948", "train_lr": "0.000432136", "train_gnorm": "0.353", "train_loss_scale": "4", "train_train_wall": "41", "train_gb_free": "40.5", "train_wall": "27522"}
[2024-10-10 09:54:56,285][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 09:54:56,289][fairseq.trainer][INFO] - begin training epoch 1709
[2024-10-10 09:54:56,290][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 09:57:08,424][fairseq_cli.train][INFO] - end of epoch 1709 (average epoch stats below)
[2024-10-10 09:57:08,434][train][INFO] - {"epoch": 1709, "train_loss": "0.431", "train_ntokens": "260923", "train_nsentences": "1750.04", "train_wps": "94694.1", "train_ups": "0.36", "train_wpb": "260923", "train_bsz": "1750", "train_num_updates": "81996", "train_lr": "0.000432071", "train_gnorm": "0.354", "train_loss_scale": "4", "train_train_wall": "48", "train_gb_free": "39.6", "train_wall": "27654"}
[2024-10-10 09:57:08,520][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 09:57:08,523][fairseq.trainer][INFO] - begin training epoch 1710
[2024-10-10 09:57:08,524][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 09:58:38,944][train_inner][INFO] - {"epoch": 1710, "update": 1709.083, "loss": "0.433", "ntokens": "260606", "nsentences": "1754.92", "wps": "85516.2", "ups": "0.33", "wpb": "260606", "bsz": "1754.9", "num_updates": "82000", "lr": "0.000432065", "gnorm": "0.358", "loss_scale": "4", "train_wall": "221", "gb_free": "39.6", "wall": "27744"}
[2024-10-10 09:59:19,449][fairseq_cli.train][INFO] - end of epoch 1710 (average epoch stats below)
[2024-10-10 09:59:19,451][train][INFO] - {"epoch": 1710, "train_loss": "0.43", "train_ntokens": "260958", "train_nsentences": "1750.04", "train_wps": "95609", "train_ups": "0.37", "train_wpb": "260958", "train_bsz": "1750", "train_num_updates": "82044", "train_lr": "0.000432005", "train_gnorm": "0.354", "train_loss_scale": "4", "train_train_wall": "47", "train_gb_free": "40.5", "train_wall": "27785"}
[2024-10-10 09:59:19,533][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 09:59:19,541][fairseq.trainer][INFO] - begin training epoch 1711
[2024-10-10 09:59:19,541][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 10:01:27,251][fairseq_cli.train][INFO] - end of epoch 1711 (average epoch stats below)
[2024-10-10 10:01:27,255][train][INFO] - {"epoch": 1711, "train_loss": "0.433", "train_ntokens": "260738", "train_nsentences": "1750.04", "train_wps": "97930.2", "train_ups": "0.38", "train_wpb": "260738", "train_bsz": "1750", "train_num_updates": "82092", "train_lr": "0.00043194", "train_gnorm": "0.362", "train_loss_scale": "4", "train_train_wall": "36", "train_gb_free": "39.3", "train_wall": "27913"}
[2024-10-10 10:01:27,311][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 10:01:27,314][fairseq.trainer][INFO] - begin training epoch 1712
[2024-10-10 10:01:27,315][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 10:03:34,524][fairseq_cli.train][INFO] - end of epoch 1712 (average epoch stats below)
[2024-10-10 10:03:34,529][train][INFO] - {"epoch": 1712, "train_loss": "0.433", "train_ntokens": "260630", "train_nsentences": "1750.04", "train_wps": "98297.1", "train_ups": "0.38", "train_wpb": "260630", "train_bsz": "1750", "train_num_updates": "82140", "train_lr": "0.000431875", "train_gnorm": "0.362", "train_loss_scale": "4", "train_train_wall": "66", "train_gb_free": "40.6", "train_wall": "28040"}
[2024-10-10 10:03:34,649][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 10:03:34,652][fairseq.trainer][INFO] - begin training epoch 1713
[2024-10-10 10:03:34,653][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 10:05:09,368][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2024-10-10 10:05:50,466][fairseq_cli.train][INFO] - end of epoch 1713 (average epoch stats below)
[2024-10-10 10:05:50,482][train][INFO] - {"epoch": 1713, "train_loss": "0.44", "train_ntokens": "260815", "train_nsentences": "1737.55", "train_wps": "90177.2", "train_ups": "0.35", "train_wpb": "260815", "train_bsz": "1737.6", "train_num_updates": "82187", "train_lr": "0.000431811", "train_gnorm": "0.36", "train_loss_scale": "2", "train_train_wall": "40", "train_gb_free": "40.3", "train_wall": "28176"}
[2024-10-10 10:05:50,587][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 10:05:50,595][fairseq.trainer][INFO] - begin training epoch 1714
[2024-10-10 10:05:50,595][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 10:07:23,540][train_inner][INFO] - {"epoch": 1714, "update": 1713.271, "loss": "0.434", "ntokens": "260924", "nsentences": "1737.07", "wps": "99480.9", "ups": "0.38", "wpb": "260924", "bsz": "1737.1", "num_updates": "82200", "lr": "0.000431793", "gnorm": "0.359", "loss_scale": "2", "train_wall": "184", "gb_free": "40", "wall": "28269"}
[2024-10-10 10:07:58,520][fairseq_cli.train][INFO] - end of epoch 1714 (average epoch stats below)
[2024-10-10 10:07:58,522][train][INFO] - {"epoch": 1714, "train_loss": "0.435", "train_ntokens": "260912", "train_nsentences": "1750.04", "train_wps": "97816", "train_ups": "0.37", "train_wpb": "260912", "train_bsz": "1750", "train_num_updates": "82235", "train_lr": "0.000431746", "train_gnorm": "0.355", "train_loss_scale": "2", "train_train_wall": "50", "train_gb_free": "40.3", "train_wall": "28304"}
[2024-10-10 10:07:58,629][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 10:07:58,646][fairseq.trainer][INFO] - begin training epoch 1715
[2024-10-10 10:07:58,646][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 10:10:03,556][fairseq_cli.train][INFO] - end of epoch 1715 (average epoch stats below)
[2024-10-10 10:10:03,567][train][INFO] - {"epoch": 1715, "train_loss": "0.432", "train_ntokens": "260799", "train_nsentences": "1750.04", "train_wps": "100121", "train_ups": "0.38", "train_wpb": "260799", "train_bsz": "1750", "train_num_updates": "82283", "train_lr": "0.000431681", "train_gnorm": "0.36", "train_loss_scale": "2", "train_train_wall": "53", "train_gb_free": "39.2", "train_wall": "28429"}
[2024-10-10 10:10:03,719][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 10:10:03,723][fairseq.trainer][INFO] - begin training epoch 1716
[2024-10-10 10:10:03,724][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 10:12:12,484][fairseq_cli.train][INFO] - end of epoch 1716 (average epoch stats below)
[2024-10-10 10:12:12,489][train][INFO] - {"epoch": 1716, "train_loss": "0.43", "train_ntokens": "260986", "train_nsentences": "1750.04", "train_wps": "97173.5", "train_ups": "0.37", "train_wpb": "260986", "train_bsz": "1750", "train_num_updates": "82331", "train_lr": "0.000431615", "train_gnorm": "0.348", "train_loss_scale": "2", "train_train_wall": "50", "train_gb_free": "39.8", "train_wall": "28558"}
[2024-10-10 10:12:12,607][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 10:12:12,611][fairseq.trainer][INFO] - begin training epoch 1717
[2024-10-10 10:12:12,611][fairseq_cli.train][INFO] - Start iterating over samples
