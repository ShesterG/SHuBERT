[2024-10-05 02:00:44,772][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:14402', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'gloss'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_800k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-05 02:00:46,156][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:14697', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'gloss'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_800k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-05 02:00:46,730][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:15954', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'gloss'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_800k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-05 02:00:46,770][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:13068', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'gloss'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_800k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-05 02:00:47,014][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:18996', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'gloss'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_800k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-05 02:00:47,255][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:19063', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'gloss'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_800k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-05 02:00:47,381][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:12461', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'gloss'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_800k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-05 02:00:47,750][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:19455', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'gloss'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_800k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-05 02:00:49,658][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-05 02:00:49,668][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-05 02:00:49,668][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-05 02:00:49,668][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-05 02:00:49,669][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-05 02:00:49,675][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-05 02:00:49,965][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-05 02:00:49,967][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-05 02:00:49,967][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-05 02:00:49,967][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-05 02:00:49,968][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-05 02:00:49,975][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-05 02:00:50,395][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-05 02:00:50,397][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-05 02:00:50,397][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-05 02:00:50,397][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-05 02:00:50,398][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-05 02:00:50,398][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-05 02:00:50,406][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-05 02:00:50,427][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-05 02:00:50,428][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-05 02:00:50,428][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-05 02:00:50,428][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-05 02:00:50,429][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-05 02:00:51,478][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-05 02:00:51,480][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-05 02:00:51,480][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-05 02:00:51,480][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-05 02:00:51,480][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-05 02:00:51,481][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-05 02:00:53,801][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-05 02:00:53,840][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-05 02:00:53,840][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-05 02:00:53,840][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-05 02:00:53,841][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-05 02:00:53,841][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-05 02:00:53,884][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-05 02:00:54,397][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-05 02:00:55,515][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-05 02:01:01,271][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-05 02:01:01,312][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-05 02:01:01,331][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-05 02:01:01,331][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-05 02:01:01,331][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-05 02:01:01,332][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-05 02:01:01,332][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-05 02:01:01,966][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-05 02:01:06,429][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-05 02:01:12,789][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-05 02:01:12,876][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-05 02:01:12,876][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-05 02:01:12,876][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-05 02:01:12,877][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-05 02:01:12,877][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-05 02:01:43,045][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-05 02:02:00,164][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-05 02:03:26,187][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-05 02:03:26,194][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 02:03:26,194][fairseq.utils][INFO] - rank   1: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 02:03:26,194][fairseq.utils][INFO] - rank   2: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 02:03:26,194][fairseq.utils][INFO] - rank   3: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 02:03:26,194][fairseq.utils][INFO] - rank   4: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 02:03:26,194][fairseq.utils][INFO] - rank   5: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 02:03:26,194][fairseq.utils][INFO] - rank   6: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 02:03:26,194][fairseq.utils][INFO] - rank   7: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 02:03:26,194][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-05 02:03:26,195][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-05 02:03:26,198][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-05 02:03:26,198][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 02:04:00,192][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 19 @ 8614 updates)
[2024-10-05 02:04:00,194][fairseq.trainer][INFO] - loading train data for epoch 19
[2024-10-05 02:04:07,806][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-05 02:04:49,261][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 02:04:49,271][fairseq.trainer][INFO] - begin training epoch 19
[2024-10-05 02:04:49,272][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 02:06:27,633][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-05 02:06:27,648][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 02:06:27,648][fairseq.utils][INFO] - rank   1: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 02:06:27,648][fairseq.utils][INFO] - rank   2: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 02:06:27,648][fairseq.utils][INFO] - rank   3: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 02:06:27,648][fairseq.utils][INFO] - rank   4: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 02:06:27,648][fairseq.utils][INFO] - rank   5: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 02:06:27,648][fairseq.utils][INFO] - rank   6: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 02:06:27,649][fairseq.utils][INFO] - rank   7: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 02:06:27,649][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-05 02:06:27,649][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-05 02:06:27,649][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-05 02:06:27,650][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 02:07:42,363][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-05 02:07:42,382][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 02:07:42,382][fairseq.utils][INFO] - rank   1: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 02:07:42,382][fairseq.utils][INFO] - rank   2: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 02:07:42,382][fairseq.utils][INFO] - rank   3: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 02:07:42,383][fairseq.utils][INFO] - rank   4: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 02:07:42,383][fairseq.utils][INFO] - rank   5: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 02:07:42,383][fairseq.utils][INFO] - rank   6: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 02:07:42,383][fairseq.utils][INFO] - rank   7: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 02:07:42,383][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-05 02:07:42,383][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-05 02:07:42,390][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-05 02:07:42,391][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 02:07:48,489][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 19 @ 8614 updates)
[2024-10-05 02:07:48,658][fairseq.trainer][INFO] - loading train data for epoch 19
[2024-10-05 02:08:10,884][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-05 02:08:14,780][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 19 @ 8614 updates)
[2024-10-05 02:08:14,781][fairseq.trainer][INFO] - loading train data for epoch 19
[2024-10-05 02:08:21,105][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-05 02:08:50,803][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-05 02:08:50,804][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 02:08:50,804][fairseq.utils][INFO] - rank   1: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 02:08:50,804][fairseq.utils][INFO] - rank   2: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 02:08:50,804][fairseq.utils][INFO] - rank   3: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 02:08:50,810][fairseq.utils][INFO] - rank   4: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 02:08:50,810][fairseq.utils][INFO] - rank   5: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 02:08:50,810][fairseq.utils][INFO] - rank   6: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 02:08:50,810][fairseq.utils][INFO] - rank   7: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 02:08:50,818][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-05 02:08:50,818][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-05 02:08:50,819][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-05 02:08:50,819][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 02:09:16,674][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 19 @ 8614 updates)
[2024-10-05 02:09:16,676][fairseq.trainer][INFO] - loading train data for epoch 19
[2024-10-05 02:09:24,799][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-05 02:10:13,904][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 02:10:13,912][fairseq.trainer][INFO] - begin training epoch 19
[2024-10-05 02:10:13,912][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 02:10:32,346][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-05 02:10:32,347][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 02:10:32,347][fairseq.utils][INFO] - rank   1: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 02:10:32,347][fairseq.utils][INFO] - rank   2: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 02:10:32,347][fairseq.utils][INFO] - rank   3: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 02:10:32,347][fairseq.utils][INFO] - rank   4: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 02:10:32,347][fairseq.utils][INFO] - rank   5: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 02:10:32,347][fairseq.utils][INFO] - rank   6: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 02:10:32,347][fairseq.utils][INFO] - rank   7: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 02:10:32,347][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-05 02:10:32,348][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-05 02:10:32,348][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-05 02:10:32,354][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 02:11:12,485][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 19 @ 8614 updates)
[2024-10-05 02:11:12,510][fairseq.trainer][INFO] - loading train data for epoch 19
[2024-10-05 02:11:21,329][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-05 02:12:48,002][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 02:12:48,012][fairseq.trainer][INFO] - begin training epoch 19
[2024-10-05 02:12:48,013][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 02:12:56,179][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 02:12:56,192][fairseq.trainer][INFO] - begin training epoch 19
[2024-10-05 02:12:56,193][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 02:13:08,567][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-05 02:13:08,578][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 02:13:08,578][fairseq.utils][INFO] - rank   1: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 02:13:08,579][fairseq.utils][INFO] - rank   2: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 02:13:08,579][fairseq.utils][INFO] - rank   3: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 02:13:08,579][fairseq.utils][INFO] - rank   4: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 02:13:08,579][fairseq.utils][INFO] - rank   5: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 02:13:08,579][fairseq.utils][INFO] - rank   6: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 02:13:08,579][fairseq.utils][INFO] - rank   7: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 02:13:08,579][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-05 02:13:08,579][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-05 02:13:08,579][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-05 02:13:08,580][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 02:14:09,737][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 19 @ 8614 updates)
[2024-10-05 02:14:09,740][fairseq.trainer][INFO] - loading train data for epoch 19
[2024-10-05 02:14:33,024][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-05 02:14:48,798][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-05 02:14:48,799][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 02:14:48,799][fairseq.utils][INFO] - rank   1: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 02:14:48,799][fairseq.utils][INFO] - rank   2: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 02:14:48,799][fairseq.utils][INFO] - rank   3: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 02:14:48,799][fairseq.utils][INFO] - rank   4: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 02:14:48,799][fairseq.utils][INFO] - rank   5: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 02:14:48,799][fairseq.utils][INFO] - rank   6: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 02:14:48,799][fairseq.utils][INFO] - rank   7: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 02:14:48,799][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-05 02:14:48,799][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-05 02:14:48,804][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-05 02:14:48,806][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 02:14:59,140][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-05 02:14:59,140][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 02:14:59,141][fairseq.utils][INFO] - rank   1: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 02:14:59,141][fairseq.utils][INFO] - rank   2: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 02:14:59,141][fairseq.utils][INFO] - rank   3: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 02:14:59,141][fairseq.utils][INFO] - rank   4: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 02:14:59,141][fairseq.utils][INFO] - rank   5: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 02:14:59,141][fairseq.utils][INFO] - rank   6: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 02:14:59,141][fairseq.utils][INFO] - rank   7: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 02:14:59,141][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-05 02:14:59,141][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-05 02:14:59,141][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-05 02:14:59,142][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 02:14:59,433][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 02:14:59,444][fairseq.trainer][INFO] - begin training epoch 19
[2024-10-05 02:14:59,444][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 02:15:15,761][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 19 @ 8614 updates)
[2024-10-05 02:15:15,845][fairseq.trainer][INFO] - loading train data for epoch 19
[2024-10-05 02:15:15,903][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 19 @ 8614 updates)
[2024-10-05 02:15:16,154][fairseq.trainer][INFO] - loading train data for epoch 19
[2024-10-05 02:15:18,319][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 02:15:18,323][fairseq.trainer][INFO] - begin training epoch 19
[2024-10-05 02:15:18,341][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 02:15:18,576][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-05 02:15:19,098][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-05 02:16:11,477][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 02:16:11,487][fairseq.trainer][INFO] - begin training epoch 19
[2024-10-05 02:16:11,490][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 02:16:12,985][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 02:16:12,992][fairseq.trainer][INFO] - begin training epoch 19
[2024-10-05 02:16:12,992][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 03:07:30,534][train_inner][INFO] - {"epoch": 19, "update": 18.388, "loss": "1.289", "ntokens": "239532", "nsentences": "1752.97", "wps": "25157.9", "ups": "0.11", "wpb": "239532", "bsz": "1753", "num_updates": "8800", "lr": "0.0001375", "gnorm": "0.87", "loss_scale": "2", "train_wall": "2079", "gb_free": "39.6", "wall": "3418"}
[2024-10-05 03:07:30,632][train_inner][INFO] - {"epoch": 19, "update": 18.388, "loss": "1.289", "ntokens": "239532", "nsentences": "1752.97", "wps": "22093.1", "ups": "0.09", "wpb": "239532", "bsz": "1753", "num_updates": "8800", "lr": "0.0001375", "gnorm": "0.879", "loss_scale": "2", "train_wall": "1969", "gb_free": "39.6", "wall": "3519"}
[2024-10-05 03:26:33,943][train_inner][INFO] - {"epoch": 19, "update": 18.806, "loss": "1.289", "ntokens": "240137", "nsentences": "1740.41", "wps": "42006.2", "ups": "0.17", "wpb": "240136", "bsz": "1740.4", "num_updates": "9000", "lr": "0.000140625", "gnorm": "0.917", "loss_scale": "2", "train_wall": "1120", "gb_free": "39.8", "wall": "4562"}
[2024-10-05 03:26:33,991][train_inner][INFO] - {"epoch": 19, "update": 18.806, "loss": "1.288", "ntokens": "240137", "nsentences": "1740.41", "wps": "42006", "ups": "0.17", "wpb": "240136", "bsz": "1740.4", "num_updates": "9000", "lr": "0.000140625", "gnorm": "0.896", "loss_scale": "2", "train_wall": "1119", "gb_free": "39.8", "wall": "4663"}
[2024-10-05 03:33:16,848][fairseq_cli.train][INFO] - end of epoch 19 (average epoch stats below)
[2024-10-05 03:33:16,993][train][INFO] - {"epoch": 19, "train_loss": "1.289", "train_ntokens": "239316", "train_nsentences": "1753.71", "train_wps": "34581.1", "train_ups": "0.14", "train_wpb": "239316", "train_bsz": "1753.7", "train_num_updates": "9093", "train_lr": "0.000142078", "train_gnorm": "0.896", "train_loss_scale": "2", "train_train_wall": "3599", "train_gb_free": "39.7", "train_wall": "4965"}
[2024-10-05 03:33:17,271][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 03:33:17,310][fairseq.trainer][INFO] - begin training epoch 20
[2024-10-05 03:33:17,315][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 03:33:48,865][fairseq_cli.train][INFO] - end of epoch 19 (average epoch stats below)
[2024-10-05 03:33:48,893][train][INFO] - {"epoch": 19, "train_loss": "1.289", "train_ntokens": "239316", "train_nsentences": "1753.71", "train_wps": "31914.8", "train_ups": "0.13", "train_wpb": "239316", "train_bsz": "1753.7", "train_num_updates": "9093", "train_lr": "0.000142078", "train_gnorm": "0.892", "train_loss_scale": "2", "train_train_wall": "3518", "train_gb_free": "39.7", "train_wall": "5098"}
[2024-10-05 03:33:49,154][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 03:33:49,160][fairseq.trainer][INFO] - begin training epoch 20
[2024-10-05 03:33:49,161][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 03:42:33,114][train_inner][INFO] - {"epoch": 20, "update": 19.223, "loss": "1.281", "ntokens": "238565", "nsentences": "1788.14", "wps": "49747.4", "ups": "0.21", "wpb": "238565", "bsz": "1788.1", "num_updates": "9200", "lr": "0.00014375", "gnorm": "0.898", "loss_scale": "2", "train_wall": "602", "gb_free": "39.4", "wall": "5521"}
[2024-10-05 03:42:53,415][train_inner][INFO] - {"epoch": 20, "update": 19.223, "loss": "1.282", "ntokens": "238565", "nsentences": "1788.14", "wps": "48715.7", "ups": "0.2", "wpb": "238565", "bsz": "1788.1", "num_updates": "9200", "lr": "0.00014375", "gnorm": "0.94", "loss_scale": "2", "train_wall": "614", "gb_free": "39.4", "wall": "5643"}
[2024-10-05 03:48:27,636][train_inner][INFO] - {"epoch": 20, "update": 19.641, "loss": "1.279", "ntokens": "239607", "nsentences": "1741.61", "wps": "135181", "ups": "0.56", "wpb": "239608", "bsz": "1741.6", "num_updates": "9400", "lr": "0.000146875", "gnorm": "0.938", "loss_scale": "2", "train_wall": "347", "gb_free": "39.6", "wall": "5875"}
[2024-10-05 03:48:32,847][train_inner][INFO] - {"epoch": 20, "update": 19.641, "loss": "1.279", "ntokens": "239607", "nsentences": "1741.61", "wps": "141215", "ups": "0.59", "wpb": "239608", "bsz": "1741.6", "num_updates": "9400", "lr": "0.000146875", "gnorm": "0.925", "loss_scale": "2", "train_wall": "332", "gb_free": "39.6", "wall": "5982"}
[2024-10-05 03:53:05,761][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 20 @ 9572 updates
[2024-10-05 03:53:05,767][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 03:53:26,045][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 20 @ 9572 updates
[2024-10-05 03:53:26,045][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 03:53:32,798][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 03:53:32,987][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 20 @ 9572 updates, score None) (writing took 27.226638610474765 seconds)
[2024-10-05 03:53:32,988][fairseq_cli.train][INFO] - end of epoch 20 (average epoch stats below)
[2024-10-05 03:53:33,043][train][INFO] - {"epoch": 20, "train_loss": "1.277", "train_ntokens": "239103", "train_nsentences": "1753.71", "train_wps": "94186.6", "train_ups": "0.39", "train_wpb": "239103", "train_bsz": "1753.7", "train_num_updates": "9572", "train_lr": "0.000149562", "train_gnorm": "0.925", "train_loss_scale": "2", "train_train_wall": "821", "train_gb_free": "39.7", "train_wall": "6181"}
[2024-10-05 03:53:33,329][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 03:53:33,356][fairseq.trainer][INFO] - begin training epoch 21
[2024-10-05 03:53:33,356][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 03:59:17,991][train_inner][INFO] - {"epoch": 21, "update": 20.058, "loss": "1.274", "ntokens": "238329", "nsentences": "1748.26", "wps": "73292.4", "ups": "0.31", "wpb": "238329", "bsz": "1748.3", "num_updates": "9600", "lr": "0.00015", "gnorm": "0.92", "loss_scale": "2", "train_wall": "320", "gb_free": "39.8", "wall": "6526"}
[2024-10-05 04:01:58,907][train_inner][INFO] - {"epoch": 21, "update": 20.476, "loss": "1.265", "ntokens": "239390", "nsentences": "1762.52", "wps": "297545", "ups": "1.24", "wpb": "239390", "bsz": "1762.5", "num_updates": "9800", "lr": "0.000153125", "gnorm": "0.823", "loss_scale": "2", "train_wall": "157", "gb_free": "39.3", "wall": "6687"}
[2024-10-05 04:02:48,613][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
[2024-10-05 04:05:03,559][train_inner][INFO] - {"epoch": 21, "update": 20.896, "loss": "1.268", "ntokens": "239961", "nsentences": "1749.92", "wps": "259912", "ups": "1.08", "wpb": "239961", "bsz": "1749.9", "num_updates": "10000", "lr": "0.00015625", "gnorm": "0.875", "loss_scale": "1", "train_wall": "181", "gb_free": "39.6", "wall": "6871"}
[2024-10-05 04:06:11,512][fairseq_cli.train][INFO] - end of epoch 21 (average epoch stats below)
[2024-10-05 04:06:11,539][train][INFO] - {"epoch": 21, "train_loss": "1.266", "train_ntokens": "239274", "train_nsentences": "1754.69", "train_wps": "150791", "train_ups": "0.63", "train_wpb": "239274", "train_bsz": "1754.7", "train_num_updates": "10050", "train_lr": "0.000157031", "train_gnorm": "0.849", "train_loss_scale": "1", "train_train_wall": "453", "train_gb_free": "39.3", "train_wall": "6939"}
[2024-10-05 04:06:11,799][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 04:06:11,802][fairseq.trainer][INFO] - begin training epoch 22
[2024-10-05 04:06:11,803][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 04:13:05,410][train_inner][INFO] - {"epoch": 22, "update": 21.313, "loss": "1.258", "ntokens": "238730", "nsentences": "1753.72", "wps": "99089.8", "ups": "0.42", "wpb": "238730", "bsz": "1753.7", "num_updates": "10200", "lr": "0.000159375", "gnorm": "0.816", "loss_scale": "1", "train_wall": "218", "gb_free": "39.6", "wall": "7353"}
[2024-10-05 04:16:18,760][train_inner][INFO] - {"epoch": 22, "update": 21.731, "loss": "1.258", "ntokens": "240601", "nsentences": "1723.69", "wps": "248882", "ups": "1.03", "wpb": "240601", "bsz": "1723.7", "num_updates": "10400", "lr": "0.0001625", "gnorm": "0.85", "loss_scale": "1", "train_wall": "189", "gb_free": "40.1", "wall": "7546"}
[2024-10-05 04:18:30,195][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 22 @ 10529 updates
[2024-10-05 04:18:30,197][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 04:18:35,614][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 04:18:35,631][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 22 @ 10529 updates, score None) (writing took 5.436702891252935 seconds)
[2024-10-05 04:18:35,632][fairseq_cli.train][INFO] - end of epoch 22 (average epoch stats below)
[2024-10-05 04:18:35,647][train][INFO] - {"epoch": 22, "train_loss": "1.257", "train_ntokens": "239481", "train_nsentences": "1753.71", "train_wps": "154163", "train_ups": "0.64", "train_wpb": "239481", "train_bsz": "1753.7", "train_num_updates": "10529", "train_lr": "0.000164516", "train_gnorm": "0.834", "train_loss_scale": "1", "train_train_wall": "469", "train_gb_free": "39.3", "train_wall": "7683"}
[2024-10-05 04:18:35,725][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 04:18:35,742][fairseq.trainer][INFO] - begin training epoch 23
[2024-10-05 04:18:35,742][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 04:24:21,013][train_inner][INFO] - {"epoch": 23, "update": 22.148, "loss": "1.253", "ntokens": "237784", "nsentences": "1790.45", "wps": "98614.4", "ups": "0.41", "wpb": "237784", "bsz": "1790.5", "num_updates": "10600", "lr": "0.000165625", "gnorm": "0.857", "loss_scale": "1", "train_wall": "201", "gb_free": "39.6", "wall": "8029"}
[2024-10-05 04:27:05,513][train_inner][INFO] - {"epoch": 23, "update": 22.566, "loss": "1.246", "ntokens": "239338", "nsentences": "1752.6", "wps": "290991", "ups": "1.22", "wpb": "239338", "bsz": "1752.6", "num_updates": "10800", "lr": "0.00016875", "gnorm": "0.797", "loss_scale": "1", "train_wall": "161", "gb_free": "39.3", "wall": "8193"}
[2024-10-05 04:30:04,015][train_inner][INFO] - {"epoch": 23, "update": 22.983, "loss": "1.248", "ntokens": "240038", "nsentences": "1751.47", "wps": "269002", "ups": "1.12", "wpb": "240038", "bsz": "1751.5", "num_updates": "11000", "lr": "0.000171875", "gnorm": "0.859", "loss_scale": "1", "train_wall": "175", "gb_free": "40.1", "wall": "8372"}
[2024-10-05 04:30:36,858][fairseq_cli.train][INFO] - end of epoch 23 (average epoch stats below)
[2024-10-05 04:30:36,862][train][INFO] - {"epoch": 23, "train_loss": "1.247", "train_ntokens": "239085", "train_nsentences": "1753.71", "train_wps": "158791", "train_ups": "0.66", "train_wpb": "239086", "train_bsz": "1753.7", "train_num_updates": "11008", "train_lr": "0.000172", "train_gnorm": "0.838", "train_loss_scale": "1", "train_train_wall": "441", "train_gb_free": "39.6", "train_wall": "8405"}
[2024-10-05 04:30:36,993][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 04:30:37,012][fairseq.trainer][INFO] - begin training epoch 24
[2024-10-05 04:30:37,013][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 04:38:09,950][train_inner][INFO] - {"epoch": 24, "update": 23.401, "loss": "1.24", "ntokens": "238136", "nsentences": "1774.84", "wps": "98014.2", "ups": "0.41", "wpb": "238136", "bsz": "1774.8", "num_updates": "11200", "lr": "0.000175", "gnorm": "0.835", "loss_scale": "1", "train_wall": "222", "gb_free": "39.2", "wall": "8858"}
[2024-10-05 04:41:29,437][train_inner][INFO] - {"epoch": 24, "update": 23.818, "loss": "1.242", "ntokens": "239730", "nsentences": "1758.32", "wps": "240361", "ups": "1", "wpb": "239730", "bsz": "1758.3", "num_updates": "11400", "lr": "0.000178125", "gnorm": "0.832", "loss_scale": "1", "train_wall": "196", "gb_free": "40.4", "wall": "9057"}
[2024-10-05 04:42:39,392][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 24 @ 11487 updates
[2024-10-05 04:42:39,394][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 04:42:50,722][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 04:42:50,725][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 24 @ 11487 updates, score None) (writing took 11.332653544843197 seconds)
[2024-10-05 04:42:50,725][fairseq_cli.train][INFO] - end of epoch 24 (average epoch stats below)
[2024-10-05 04:42:50,747][train][INFO] - {"epoch": 24, "train_loss": "1.24", "train_ntokens": "239335", "train_nsentences": "1753.71", "train_wps": "156216", "train_ups": "0.65", "train_wpb": "239335", "train_bsz": "1753.7", "train_num_updates": "11487", "train_lr": "0.000179484", "train_gnorm": "0.824", "train_loss_scale": "1", "train_train_wall": "454", "train_gb_free": "39.8", "train_wall": "9138"}
[2024-10-05 04:42:50,921][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 04:42:50,944][fairseq.trainer][INFO] - begin training epoch 25
[2024-10-05 04:42:50,951][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 04:49:17,025][train_inner][INFO] - {"epoch": 25, "update": 24.236, "loss": "1.233", "ntokens": "239698", "nsentences": "1691.97", "wps": "102527", "ups": "0.43", "wpb": "239698", "bsz": "1692", "num_updates": "11600", "lr": "0.00018125", "gnorm": "0.792", "loss_scale": "1", "train_wall": "164", "gb_free": "40.5", "wall": "9525"}
[2024-10-05 04:52:19,082][train_inner][INFO] - {"epoch": 25, "update": 24.653, "loss": "1.233", "ntokens": "239609", "nsentences": "1757.44", "wps": "263231", "ups": "1.1", "wpb": "239609", "bsz": "1757.4", "num_updates": "11800", "lr": "0.000184375", "gnorm": "0.735", "loss_scale": "1", "train_wall": "179", "gb_free": "39.6", "wall": "9707"}
[2024-10-05 04:54:59,709][fairseq_cli.train][INFO] - end of epoch 25 (average epoch stats below)
[2024-10-05 04:54:59,723][train][INFO] - {"epoch": 25, "train_loss": "1.232", "train_ntokens": "239461", "train_nsentences": "1753.71", "train_wps": "157347", "train_ups": "0.66", "train_wpb": "239461", "train_bsz": "1753.7", "train_num_updates": "11966", "train_lr": "0.000186969", "train_gnorm": "0.76", "train_loss_scale": "2", "train_train_wall": "431", "train_gb_free": "41.5", "train_wall": "9867"}
[2024-10-05 04:55:00,164][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 04:55:00,199][fairseq.trainer][INFO] - begin training epoch 26
[2024-10-05 04:55:00,200][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 05:00:20,244][train_inner][INFO] - {"epoch": 26, "update": 25.071, "loss": "1.231", "ntokens": "238926", "nsentences": "1783.14", "wps": "99313.3", "ups": "0.42", "wpb": "238926", "bsz": "1783.1", "num_updates": "12000", "lr": "0.0001875", "gnorm": "0.779", "loss_scale": "2", "train_wall": "196", "gb_free": "40.3", "wall": "10188"}
[2024-10-05 05:03:44,072][train_inner][INFO] - {"epoch": 26, "update": 25.489, "loss": "1.221", "ntokens": "240145", "nsentences": "1737.99", "wps": "235641", "ups": "0.98", "wpb": "240146", "bsz": "1738", "num_updates": "12200", "lr": "0.000190625", "gnorm": "0.712", "loss_scale": "2", "train_wall": "199", "gb_free": "40.2", "wall": "10392"}
[2024-10-05 05:07:23,295][train_inner][INFO] - {"epoch": 26, "update": 25.906, "loss": "1.226", "ntokens": "239052", "nsentences": "1767.12", "wps": "218201", "ups": "0.91", "wpb": "239052", "bsz": "1767.1", "num_updates": "12400", "lr": "0.00019375", "gnorm": "0.765", "loss_scale": "2", "train_wall": "167", "gb_free": "39.6", "wall": "10611"}
[2024-10-05 05:08:20,015][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 26 @ 12445 updates
[2024-10-05 05:08:20,016][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 05:08:32,810][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 05:08:32,813][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 26 @ 12445 updates, score None) (writing took 12.797591987997293 seconds)
[2024-10-05 05:08:32,813][fairseq_cli.train][INFO] - end of epoch 26 (average epoch stats below)
[2024-10-05 05:08:32,815][train][INFO] - {"epoch": 26, "train_loss": "1.223", "train_ntokens": "239050", "train_nsentences": "1753.71", "train_wps": "140827", "train_ups": "0.59", "train_wpb": "239050", "train_bsz": "1753.7", "train_num_updates": "12445", "train_lr": "0.000194453", "train_gnorm": "0.745", "train_loss_scale": "2", "train_train_wall": "457", "train_gb_free": "39.8", "train_wall": "10680"}
[2024-10-05 05:08:32,949][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 05:08:32,954][fairseq.trainer][INFO] - begin training epoch 27
[2024-10-05 05:08:32,954][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 05:15:22,573][train_inner][INFO] - {"epoch": 27, "update": 26.324, "loss": "1.217", "ntokens": "238645", "nsentences": "1751.08", "wps": "99592.4", "ups": "0.42", "wpb": "238645", "bsz": "1751.1", "num_updates": "12600", "lr": "0.000196875", "gnorm": "0.727", "loss_scale": "2", "train_wall": "188", "gb_free": "39.6", "wall": "11090"}
[2024-10-05 05:18:12,579][train_inner][INFO] - {"epoch": 27, "update": 26.741, "loss": "1.217", "ntokens": "239815", "nsentences": "1735.57", "wps": "282170", "ups": "1.18", "wpb": "239815", "bsz": "1735.6", "num_updates": "12800", "lr": "0.0002", "gnorm": "0.696", "loss_scale": "2", "train_wall": "158", "gb_free": "40.1", "wall": "11260"}
[2024-10-05 05:20:22,495][fairseq_cli.train][INFO] - end of epoch 27 (average epoch stats below)
[2024-10-05 05:20:22,556][train][INFO] - {"epoch": 27, "train_loss": "1.217", "train_ntokens": "239343", "train_nsentences": "1753.71", "train_wps": "161533", "train_ups": "0.67", "train_wpb": "239343", "train_bsz": "1753.7", "train_num_updates": "12924", "train_lr": "0.000201937", "train_gnorm": "0.725", "train_loss_scale": "2", "train_train_wall": "416", "train_gb_free": "39.6", "train_wall": "11390"}
[2024-10-05 05:20:23,141][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 05:20:23,203][fairseq.trainer][INFO] - begin training epoch 28
[2024-10-05 05:20:23,204][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 05:26:01,186][train_inner][INFO] - {"epoch": 28, "update": 27.159, "loss": "1.217", "ntokens": "238403", "nsentences": "1778.34", "wps": "101752", "ups": "0.43", "wpb": "238403", "bsz": "1778.3", "num_updates": "13000", "lr": "0.000203125", "gnorm": "0.739", "loss_scale": "2", "train_wall": "190", "gb_free": "39.3", "wall": "11729"}
[2024-10-05 05:28:51,667][train_inner][INFO] - {"epoch": 28, "update": 27.576, "loss": "1.209", "ntokens": "240258", "nsentences": "1724.73", "wps": "281865", "ups": "1.17", "wpb": "240258", "bsz": "1724.7", "num_updates": "13200", "lr": "0.00020625", "gnorm": "0.674", "loss_scale": "2", "train_wall": "167", "gb_free": "40.1", "wall": "11899"}
[2024-10-05 05:32:03,759][train_inner][INFO] - {"epoch": 28, "update": 27.994, "loss": "1.212", "ntokens": "239872", "nsentences": "1779.24", "wps": "249775", "ups": "1.04", "wpb": "239872", "bsz": "1779.2", "num_updates": "13400", "lr": "0.000209375", "gnorm": "0.691", "loss_scale": "2", "train_wall": "128", "gb_free": "40", "wall": "12091"}
[2024-10-05 05:32:17,044][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 28 @ 13403 updates
[2024-10-05 05:32:17,045][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 05:32:33,252][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 05:32:33,284][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 28 @ 13403 updates, score None) (writing took 16.239103333093226 seconds)
[2024-10-05 05:32:33,291][fairseq_cli.train][INFO] - end of epoch 28 (average epoch stats below)
[2024-10-05 05:32:33,299][train][INFO] - {"epoch": 28, "train_loss": "1.211", "train_ntokens": "239553", "train_nsentences": "1753.71", "train_wps": "157030", "train_ups": "0.66", "train_wpb": "239553", "train_bsz": "1753.7", "train_num_updates": "13403", "train_lr": "0.000209422", "train_gnorm": "0.685", "train_loss_scale": "2", "train_train_wall": "373", "train_gb_free": "39.7", "train_wall": "12121"}
[2024-10-05 05:32:33,529][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 05:32:33,628][fairseq.trainer][INFO] - begin training epoch 29
[2024-10-05 05:32:33,628][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 05:39:51,584][train_inner][INFO] - {"epoch": 29, "update": 28.411, "loss": "1.206", "ntokens": "238019", "nsentences": "1748.58", "wps": "101760", "ups": "0.43", "wpb": "238019", "bsz": "1748.6", "num_updates": "13600", "lr": "0.0002125", "gnorm": "0.697", "loss_scale": "2", "train_wall": "157", "gb_free": "39.6", "wall": "12559"}
[2024-10-05 05:43:15,900][train_inner][INFO] - {"epoch": 29, "update": 28.829, "loss": "1.204", "ntokens": "239301", "nsentences": "1778.82", "wps": "234260", "ups": "0.98", "wpb": "239301", "bsz": "1778.8", "num_updates": "13800", "lr": "0.000215625", "gnorm": "0.719", "loss_scale": "2", "train_wall": "144", "gb_free": "40.1", "wall": "12764"}
[2024-10-05 05:44:43,579][fairseq_cli.train][INFO] - end of epoch 29 (average epoch stats below)
[2024-10-05 05:44:43,679][train][INFO] - {"epoch": 29, "train_loss": "1.204", "train_ntokens": "238890", "train_nsentences": "1753.71", "train_wps": "156678", "train_ups": "0.66", "train_wpb": "238890", "train_bsz": "1753.7", "train_num_updates": "13882", "train_lr": "0.000216906", "train_gnorm": "0.702", "train_loss_scale": "2", "train_train_wall": "354", "train_gb_free": "40.1", "train_wall": "12851"}
[2024-10-05 05:44:43,955][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 05:44:43,980][fairseq.trainer][INFO] - begin training epoch 30
[2024-10-05 05:44:43,981][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 05:50:39,904][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2024-10-05 05:51:00,643][train_inner][INFO] - {"epoch": 30, "update": 29.248, "loss": "1.199", "ntokens": "239547", "nsentences": "1713.4", "wps": "103089", "ups": "0.43", "wpb": "239547", "bsz": "1713.4", "num_updates": "14000", "lr": "0.00021875", "gnorm": "0.641", "loss_scale": "2", "train_wall": "170", "gb_free": "39.8", "wall": "13228"}
[2024-10-05 05:54:06,917][train_inner][INFO] - {"epoch": 30, "update": 29.666, "loss": "1.199", "ntokens": "239474", "nsentences": "1797.37", "wps": "257127", "ups": "1.07", "wpb": "239474", "bsz": "1797.4", "num_updates": "14200", "lr": "0.000221875", "gnorm": "0.678", "loss_scale": "2", "train_wall": "177", "gb_free": "40.1", "wall": "13415"}
[2024-10-05 05:56:47,687][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 30 @ 14360 updates
[2024-10-05 05:56:47,711][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 05:56:58,554][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 05:56:58,561][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 30 @ 14360 updates, score None) (writing took 10.873163488693535 seconds)
[2024-10-05 05:56:58,561][fairseq_cli.train][INFO] - end of epoch 30 (average epoch stats below)
[2024-10-05 05:56:58,564][train][INFO] - {"epoch": 30, "train_loss": "1.199", "train_ntokens": "239619", "train_nsentences": "1753.61", "train_wps": "155864", "train_ups": "0.65", "train_wpb": "239619", "train_bsz": "1753.6", "train_num_updates": "14360", "train_lr": "0.000224375", "train_gnorm": "0.656", "train_loss_scale": "2", "train_train_wall": "419", "train_gb_free": "40.1", "train_wall": "13586"}
[2024-10-05 05:56:58,636][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 05:56:58,641][fairseq.trainer][INFO] - begin training epoch 31
[2024-10-05 05:56:58,642][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 06:01:57,856][train_inner][INFO] - {"epoch": 31, "update": 30.084, "loss": "1.198", "ntokens": "238964", "nsentences": "1724.53", "wps": "101486", "ups": "0.42", "wpb": "238964", "bsz": "1724.5", "num_updates": "14400", "lr": "0.000225", "gnorm": "0.686", "loss_scale": "2", "train_wall": "170", "gb_free": "40.4", "wall": "13886"}
[2024-10-05 06:04:53,177][train_inner][INFO] - {"epoch": 31, "update": 30.501, "loss": "1.191", "ntokens": "239423", "nsentences": "1760.64", "wps": "273139", "ups": "1.14", "wpb": "239423", "bsz": "1760.6", "num_updates": "14600", "lr": "0.000228125", "gnorm": "0.645", "loss_scale": "2", "train_wall": "150", "gb_free": "39.6", "wall": "14061"}
[2024-10-05 06:08:07,352][train_inner][INFO] - {"epoch": 31, "update": 30.919, "loss": "1.193", "ntokens": "239758", "nsentences": "1764.35", "wps": "246959", "ups": "1.03", "wpb": "239758", "bsz": "1764.4", "num_updates": "14800", "lr": "0.00023125", "gnorm": "0.66", "loss_scale": "2", "train_wall": "143", "gb_free": "39.7", "wall": "14255"}
[2024-10-05 06:08:44,187][fairseq_cli.train][INFO] - end of epoch 31 (average epoch stats below)
[2024-10-05 06:08:44,200][train][INFO] - {"epoch": 31, "train_loss": "1.192", "train_ntokens": "239283", "train_nsentences": "1753.71", "train_wps": "162430", "train_ups": "0.68", "train_wpb": "239283", "train_bsz": "1753.7", "train_num_updates": "14839", "train_lr": "0.000231859", "train_gnorm": "0.656", "train_loss_scale": "2", "train_train_wall": "352", "train_gb_free": "39.2", "train_wall": "14292"}
[2024-10-05 06:08:46,860][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 06:08:47,106][fairseq.trainer][INFO] - begin training epoch 32
[2024-10-05 06:08:47,107][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 06:15:53,447][train_inner][INFO] - {"epoch": 32, "update": 31.336, "loss": "1.187", "ntokens": "238238", "nsentences": "1775.36", "wps": "102228", "ups": "0.43", "wpb": "238238", "bsz": "1775.4", "num_updates": "15000", "lr": "0.000234375", "gnorm": "0.585", "loss_scale": "2", "train_wall": "165", "gb_free": "39.6", "wall": "14721"}
[2024-10-05 06:19:08,466][train_inner][INFO] - {"epoch": 32, "update": 31.754, "loss": "1.189", "ntokens": "239314", "nsentences": "1759.56", "wps": "245431", "ups": "1.03", "wpb": "239314", "bsz": "1759.6", "num_updates": "15200", "lr": "0.0002375", "gnorm": "0.6", "loss_scale": "2", "train_wall": "190", "gb_free": "39.6", "wall": "14916"}
[2024-10-05 06:21:00,953][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 32 @ 15318 updates
[2024-10-05 06:21:01,027][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 06:21:14,734][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 06:21:14,739][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 32 @ 15318 updates, score None) (writing took 13.78631572239101 seconds)
[2024-10-05 06:21:14,739][fairseq_cli.train][INFO] - end of epoch 32 (average epoch stats below)
[2024-10-05 06:21:14,742][train][INFO] - {"epoch": 32, "train_loss": "1.188", "train_ntokens": "239077", "train_nsentences": "1753.71", "train_wps": "152583", "train_ups": "0.64", "train_wpb": "239077", "train_bsz": "1753.7", "train_num_updates": "15318", "train_lr": "0.000239344", "train_gnorm": "0.611", "train_loss_scale": "2", "train_train_wall": "434", "train_gb_free": "40.2", "train_wall": "15042"}
[2024-10-05 06:21:14,799][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 06:21:14,803][fairseq.trainer][INFO] - begin training epoch 33
[2024-10-05 06:21:14,804][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 06:27:02,870][train_inner][INFO] - {"epoch": 33, "update": 32.171, "loss": "1.185", "ntokens": "238845", "nsentences": "1739.36", "wps": "100696", "ups": "0.42", "wpb": "238846", "bsz": "1739.4", "num_updates": "15400", "lr": "0.000240625", "gnorm": "0.623", "loss_scale": "2", "train_wall": "167", "gb_free": "39.6", "wall": "15391"}
[2024-10-05 06:29:47,287][train_inner][INFO] - {"epoch": 33, "update": 32.589, "loss": "1.183", "ntokens": "240074", "nsentences": "1749.54", "wps": "292052", "ups": "1.22", "wpb": "240074", "bsz": "1749.5", "num_updates": "15600", "lr": "0.00024375", "gnorm": "0.606", "loss_scale": "2", "train_wall": "116", "gb_free": "39.3", "wall": "15555"}
[2024-10-05 06:32:52,556][fairseq_cli.train][INFO] - end of epoch 33 (average epoch stats below)
[2024-10-05 06:32:52,607][train][INFO] - {"epoch": 33, "train_loss": "1.183", "train_ntokens": "239476", "train_nsentences": "1753.71", "train_wps": "164374", "train_ups": "0.69", "train_wpb": "239476", "train_bsz": "1753.7", "train_num_updates": "15797", "train_lr": "0.000246828", "train_gnorm": "0.601", "train_loss_scale": "2", "train_train_wall": "341", "train_gb_free": "39.7", "train_wall": "15740"}
[2024-10-05 06:32:52,798][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 06:32:52,809][fairseq.trainer][INFO] - begin training epoch 34
[2024-10-05 06:32:52,809][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 06:37:43,476][train_inner][INFO] - {"epoch": 34, "update": 33.006, "loss": "1.183", "ntokens": "239256", "nsentences": "1726.47", "wps": "100489", "ups": "0.42", "wpb": "239256", "bsz": "1726.5", "num_updates": "15800", "lr": "0.000246875", "gnorm": "0.604", "loss_scale": "2", "train_wall": "169", "gb_free": "40.1", "wall": "16031"}
[2024-10-05 06:40:28,573][train_inner][INFO] - {"epoch": 34, "update": 33.424, "loss": "1.176", "ntokens": "239571", "nsentences": "1768.03", "wps": "290225", "ups": "1.21", "wpb": "239571", "bsz": "1768", "num_updates": "16000", "lr": "0.00025", "gnorm": "0.57", "loss_scale": "2", "train_wall": "161", "gb_free": "39.2", "wall": "16196"}
[2024-10-05 06:41:43,793][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2024-10-05 06:43:46,884][train_inner][INFO] - {"epoch": 34, "update": 33.843, "loss": "1.18", "ntokens": "239766", "nsentences": "1746.74", "wps": "241819", "ups": "1.01", "wpb": "239766", "bsz": "1746.7", "num_updates": "16200", "lr": "0.000253125", "gnorm": "0.581", "loss_scale": "2", "train_wall": "195", "gb_free": "39.8", "wall": "16395"}
[2024-10-05 06:44:53,160][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 34 @ 16275 updates
[2024-10-05 06:44:53,161][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 06:44:59,797][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 06:44:59,800][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 34 @ 16275 updates, score None) (writing took 6.6399744506925344 seconds)
[2024-10-05 06:44:59,800][fairseq_cli.train][INFO] - end of epoch 34 (average epoch stats below)
[2024-10-05 06:44:59,802][train][INFO] - {"epoch": 34, "train_loss": "1.178", "train_ntokens": "239260", "train_nsentences": "1753.11", "train_wps": "157271", "train_ups": "0.66", "train_wpb": "239260", "train_bsz": "1753.1", "train_num_updates": "16275", "train_lr": "0.000254297", "train_gnorm": "0.569", "train_loss_scale": "2", "train_train_wall": "427", "train_gb_free": "39.3", "train_wall": "16467"}
[2024-10-05 06:44:59,878][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 06:44:59,882][fairseq.trainer][INFO] - begin training epoch 35
[2024-10-05 06:44:59,883][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 06:51:32,762][train_inner][INFO] - {"epoch": 35, "update": 34.261, "loss": "1.175", "ntokens": "238712", "nsentences": "1749.59", "wps": "102480", "ups": "0.43", "wpb": "238712", "bsz": "1749.6", "num_updates": "16400", "lr": "0.00025625", "gnorm": "0.538", "loss_scale": "2", "train_wall": "181", "gb_free": "39.6", "wall": "16860"}
[2024-10-05 06:54:31,450][train_inner][INFO] - {"epoch": 35, "update": 34.678, "loss": "1.175", "ntokens": "239829", "nsentences": "1753.63", "wps": "268439", "ups": "1.12", "wpb": "239830", "bsz": "1753.6", "num_updates": "16600", "lr": "0.000259375", "gnorm": "0.597", "loss_scale": "2", "train_wall": "175", "gb_free": "39.6", "wall": "17039"}
[2024-10-05 06:57:13,829][fairseq_cli.train][INFO] - end of epoch 35 (average epoch stats below)
[2024-10-05 06:57:13,860][train][INFO] - {"epoch": 35, "train_loss": "1.175", "train_ntokens": "239347", "train_nsentences": "1753.71", "train_wps": "156184", "train_ups": "0.65", "train_wpb": "239347", "train_bsz": "1753.7", "train_num_updates": "16754", "train_lr": "0.000261781", "train_gnorm": "0.572", "train_loss_scale": "2", "train_train_wall": "449", "train_gb_free": "39.6", "train_wall": "17202"}
[2024-10-05 06:57:14,081][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 06:57:14,087][fairseq.trainer][INFO] - begin training epoch 36
[2024-10-05 06:57:14,087][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 07:02:41,595][train_inner][INFO] - {"epoch": 36, "update": 35.096, "loss": "1.175", "ntokens": "238547", "nsentences": "1748.25", "wps": "97338.4", "ups": "0.41", "wpb": "238547", "bsz": "1748.2", "num_updates": "16800", "lr": "0.0002625", "gnorm": "0.566", "loss_scale": "2", "train_wall": "210", "gb_free": "39.3", "wall": "17529"}
[2024-10-05 07:05:29,553][train_inner][INFO] - {"epoch": 36, "update": 35.514, "loss": "1.168", "ntokens": "239416", "nsentences": "1784.41", "wps": "285103", "ups": "1.19", "wpb": "239416", "bsz": "1784.4", "num_updates": "17000", "lr": "0.000265625", "gnorm": "0.554", "loss_scale": "2", "train_wall": "164", "gb_free": "39.6", "wall": "17697"}
[2024-10-05 07:08:39,440][train_inner][INFO] - {"epoch": 36, "update": 35.931, "loss": "1.171", "ntokens": "239940", "nsentences": "1733.78", "wps": "252724", "ups": "1.05", "wpb": "239940", "bsz": "1733.8", "num_updates": "17200", "lr": "0.00026875", "gnorm": "0.588", "loss_scale": "2", "train_wall": "186", "gb_free": "39.8", "wall": "17887"}
[2024-10-05 07:09:36,175][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 36 @ 17233 updates
[2024-10-05 07:09:36,176][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 07:09:39,543][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 07:09:39,546][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 36 @ 17233 updates, score None) (writing took 3.3707916336134076 seconds)
[2024-10-05 07:09:39,546][fairseq_cli.train][INFO] - end of epoch 36 (average epoch stats below)
[2024-10-05 07:09:39,551][train][INFO] - {"epoch": 36, "train_loss": "1.17", "train_ntokens": "239300", "train_nsentences": "1753.71", "train_wps": "153717", "train_ups": "0.64", "train_wpb": "239300", "train_bsz": "1753.7", "train_num_updates": "17233", "train_lr": "0.000269266", "train_gnorm": "0.566", "train_loss_scale": "2", "train_train_wall": "459", "train_gb_free": "40.5", "train_wall": "17947"}
[2024-10-05 07:09:39,603][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 07:09:39,638][fairseq.trainer][INFO] - begin training epoch 37
[2024-10-05 07:09:39,639][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 07:16:31,146][train_inner][INFO] - {"epoch": 37, "update": 36.349, "loss": "1.166", "ntokens": "238947", "nsentences": "1746.41", "wps": "101312", "ups": "0.42", "wpb": "238947", "bsz": "1746.4", "num_updates": "17400", "lr": "0.000271875", "gnorm": "0.524", "loss_scale": "2", "train_wall": "207", "gb_free": "39.8", "wall": "18359"}
[2024-10-05 07:19:50,191][train_inner][INFO] - {"epoch": 37, "update": 36.766, "loss": "1.167", "ntokens": "239594", "nsentences": "1784.03", "wps": "240752", "ups": "1", "wpb": "239594", "bsz": "1784", "num_updates": "17600", "lr": "0.000275", "gnorm": "0.529", "loss_scale": "2", "train_wall": "195", "gb_free": "39.6", "wall": "18558"}
[2024-10-05 07:21:48,684][fairseq_cli.train][INFO] - end of epoch 37 (average epoch stats below)
[2024-10-05 07:21:48,702][train][INFO] - {"epoch": 37, "train_loss": "1.166", "train_ntokens": "239494", "train_nsentences": "1753.71", "train_wps": "157331", "train_ups": "0.66", "train_wpb": "239494", "train_bsz": "1753.7", "train_num_updates": "17712", "train_lr": "0.00027675", "train_gnorm": "0.532", "train_loss_scale": "2", "train_train_wall": "460", "train_gb_free": "39.7", "train_wall": "18676"}
[2024-10-05 07:21:48,966][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 07:21:48,973][fairseq.trainer][INFO] - begin training epoch 38
[2024-10-05 07:21:48,973][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 07:27:53,921][train_inner][INFO] - {"epoch": 38, "update": 37.184, "loss": "1.165", "ntokens": "239309", "nsentences": "1728.52", "wps": "98944.4", "ups": "0.41", "wpb": "239309", "bsz": "1728.5", "num_updates": "17800", "lr": "0.000278125", "gnorm": "0.524", "loss_scale": "2", "train_wall": "208", "gb_free": "39.3", "wall": "19042"}
[2024-10-05 07:31:05,137][train_inner][INFO] - {"epoch": 38, "update": 37.601, "loss": "1.161", "ntokens": "239872", "nsentences": "1755.53", "wps": "250898", "ups": "1.05", "wpb": "239872", "bsz": "1755.5", "num_updates": "18000", "lr": "0.00028125", "gnorm": "0.534", "loss_scale": "2", "train_wall": "188", "gb_free": "39.3", "wall": "19233"}
[2024-10-05 07:34:03,922][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2024-10-05 07:34:13,913][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 38 @ 18190 updates
[2024-10-05 07:34:13,913][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 07:34:18,993][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 07:34:18,995][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 38 @ 18190 updates, score None) (writing took 5.082428333349526 seconds)
[2024-10-05 07:34:18,996][fairseq_cli.train][INFO] - end of epoch 38 (average epoch stats below)
[2024-10-05 07:34:18,999][train][INFO] - {"epoch": 38, "train_loss": "1.162", "train_ntokens": "239214", "train_nsentences": "1754.05", "train_wps": "152400", "train_ups": "0.64", "train_wpb": "239214", "train_bsz": "1754.1", "train_num_updates": "18190", "train_lr": "0.000284219", "train_gnorm": "0.518", "train_loss_scale": "2", "train_train_wall": "466", "train_gb_free": "40.1", "train_wall": "19427"}
[2024-10-05 07:34:19,078][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 07:34:19,116][fairseq.trainer][INFO] - begin training epoch 39
[2024-10-05 07:34:19,117][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 07:39:06,958][train_inner][INFO] - {"epoch": 39, "update": 38.021, "loss": "1.163", "ntokens": "238387", "nsentences": "1742.08", "wps": "98953.4", "ups": "0.42", "wpb": "238387", "bsz": "1742.1", "num_updates": "18200", "lr": "0.000284375", "gnorm": "0.517", "loss_scale": "2", "train_wall": "213", "gb_free": "40.3", "wall": "19715"}
[2024-10-05 07:42:00,046][train_inner][INFO] - {"epoch": 39, "update": 38.438, "loss": "1.157", "ntokens": "240144", "nsentences": "1746.96", "wps": "277493", "ups": "1.16", "wpb": "240144", "bsz": "1747", "num_updates": "18400", "lr": "0.0002875", "gnorm": "0.501", "loss_scale": "2", "train_wall": "169", "gb_free": "39.8", "wall": "19888"}
[2024-10-05 07:45:27,685][train_inner][INFO] - {"epoch": 39, "update": 38.856, "loss": "1.158", "ntokens": "239473", "nsentences": "1748.69", "wps": "230673", "ups": "0.96", "wpb": "239473", "bsz": "1748.7", "num_updates": "18600", "lr": "0.000290625", "gnorm": "0.523", "loss_scale": "2", "train_wall": "204", "gb_free": "39.3", "wall": "20095"}
[2024-10-05 07:46:43,964][fairseq_cli.train][INFO] - end of epoch 39 (average epoch stats below)
[2024-10-05 07:46:44,006][train][INFO] - {"epoch": 39, "train_loss": "1.159", "train_ntokens": "239351", "train_nsentences": "1753.71", "train_wps": "153892", "train_ups": "0.64", "train_wpb": "239351", "train_bsz": "1753.7", "train_num_updates": "18669", "train_lr": "0.000291703", "train_gnorm": "0.529", "train_loss_scale": "2", "train_train_wall": "475", "train_gb_free": "39.6", "train_wall": "20172"}
[2024-10-05 07:46:44,237][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 07:46:44,258][fairseq.trainer][INFO] - begin training epoch 40
[2024-10-05 07:46:44,258][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 07:53:15,600][train_inner][INFO] - {"epoch": 40, "update": 39.273, "loss": "1.157", "ntokens": "238929", "nsentences": "1737.15", "wps": "102126", "ups": "0.43", "wpb": "238929", "bsz": "1737.2", "num_updates": "18800", "lr": "0.00029375", "gnorm": "0.537", "loss_scale": "2", "train_wall": "189", "gb_free": "39.3", "wall": "20563"}
[2024-10-05 07:56:22,760][train_inner][INFO] - {"epoch": 40, "update": 39.691, "loss": "1.157", "ntokens": "239610", "nsentences": "1776.13", "wps": "256066", "ups": "1.07", "wpb": "239610", "bsz": "1776.1", "num_updates": "19000", "lr": "0.000296875", "gnorm": "0.491", "loss_scale": "2", "train_wall": "184", "gb_free": "39.8", "wall": "20750"}
[2024-10-05 07:58:42,588][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 40 @ 19148 updates
[2024-10-05 07:58:42,595][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 07:58:50,662][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 07:58:50,706][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 40 @ 19148 updates, score None) (writing took 8.118435204960406 seconds)
[2024-10-05 07:58:50,710][fairseq_cli.train][INFO] - end of epoch 40 (average epoch stats below)
[2024-10-05 07:58:50,712][train][INFO] - {"epoch": 40, "train_loss": "1.156", "train_ntokens": "239460", "train_nsentences": "1753.71", "train_wps": "157838", "train_ups": "0.66", "train_wpb": "239460", "train_bsz": "1753.7", "train_num_updates": "19148", "train_lr": "0.000299187", "train_gnorm": "0.515", "train_loss_scale": "2", "train_train_wall": "434", "train_gb_free": "39.6", "train_wall": "20898"}
[2024-10-05 07:58:50,911][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 07:58:50,930][fairseq.trainer][INFO] - begin training epoch 41
[2024-10-05 07:58:50,931][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 08:04:14,601][train_inner][INFO] - {"epoch": 41, "update": 40.109, "loss": "1.155", "ntokens": "239286", "nsentences": "1760.88", "wps": "101429", "ups": "0.42", "wpb": "239286", "bsz": "1760.9", "num_updates": "19200", "lr": "0.0003", "gnorm": "0.523", "loss_scale": "2", "train_wall": "183", "gb_free": "40.5", "wall": "21222"}
[2024-10-05 08:07:03,476][train_inner][INFO] - {"epoch": 41, "update": 40.526, "loss": "1.152", "ntokens": "239679", "nsentences": "1736.98", "wps": "283869", "ups": "1.18", "wpb": "239679", "bsz": "1737", "num_updates": "19400", "lr": "0.000303125", "gnorm": "0.482", "loss_scale": "2", "train_wall": "165", "gb_free": "39.6", "wall": "21391"}
[2024-10-05 08:10:12,219][train_inner][INFO] - {"epoch": 41, "update": 40.944, "loss": "1.152", "ntokens": "238977", "nsentences": "1784.48", "wps": "253235", "ups": "1.06", "wpb": "238977", "bsz": "1784.5", "num_updates": "19600", "lr": "0.00030625", "gnorm": "0.515", "loss_scale": "2", "train_wall": "185", "gb_free": "39.3", "wall": "21580"}
[2024-10-05 08:10:30,374][fairseq_cli.train][INFO] - end of epoch 41 (average epoch stats below)
[2024-10-05 08:10:30,396][train][INFO] - {"epoch": 41, "train_loss": "1.152", "train_ntokens": "239115", "train_nsentences": "1753.71", "train_wps": "163698", "train_ups": "0.68", "train_wpb": "239116", "train_bsz": "1753.7", "train_num_updates": "19627", "train_lr": "0.000306672", "train_gnorm": "0.496", "train_loss_scale": "2", "train_train_wall": "413", "train_gb_free": "39.6", "train_wall": "21598"}
[2024-10-05 08:10:30,677][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 08:10:30,700][fairseq.trainer][INFO] - begin training epoch 42
[2024-10-05 08:10:30,700][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 08:17:54,094][train_inner][INFO] - {"epoch": 42, "update": 41.361, "loss": "1.151", "ntokens": "239012", "nsentences": "1725.38", "wps": "103500", "ups": "0.43", "wpb": "239012", "bsz": "1725.4", "num_updates": "19800", "lr": "0.000309375", "gnorm": "0.47", "loss_scale": "2", "train_wall": "158", "gb_free": "40.5", "wall": "22042"}
[2024-10-05 08:20:58,321][train_inner][INFO] - {"epoch": 42, "update": 41.779, "loss": "1.15", "ntokens": "240259", "nsentences": "1737.51", "wps": "260838", "ups": "1.09", "wpb": "240258", "bsz": "1737.5", "num_updates": "20000", "lr": "0.0003125", "gnorm": "0.459", "loss_scale": "2", "train_wall": "180", "gb_free": "39.3", "wall": "22226"}
[2024-10-05 08:22:29,025][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 42 @ 20106 updates
[2024-10-05 08:22:29,026][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 08:22:34,343][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 08:22:34,362][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 42 @ 20106 updates, score None) (writing took 5.337086175568402 seconds)
[2024-10-05 08:22:34,376][fairseq_cli.train][INFO] - end of epoch 42 (average epoch stats below)
[2024-10-05 08:22:34,383][train][INFO] - {"epoch": 42, "train_loss": "1.15", "train_ntokens": "239617", "train_nsentences": "1753.71", "train_wps": "158536", "train_ups": "0.66", "train_wpb": "239617", "train_bsz": "1753.7", "train_num_updates": "20106", "train_lr": "0.000314156", "train_gnorm": "0.457", "train_loss_scale": "2", "train_train_wall": "409", "train_gb_free": "39.3", "train_wall": "22322"}
[2024-10-05 08:22:34,481][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 08:22:34,489][fairseq.trainer][INFO] - begin training epoch 43
[2024-10-05 08:22:34,489][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 08:29:10,382][train_inner][INFO] - {"epoch": 43, "update": 42.196, "loss": "1.147", "ntokens": "238662", "nsentences": "1769.88", "wps": "97006.1", "ups": "0.41", "wpb": "238662", "bsz": "1769.9", "num_updates": "20200", "lr": "0.000315625", "gnorm": "0.475", "loss_scale": "2", "train_wall": "187", "gb_free": "39.6", "wall": "22718"}
[2024-10-05 08:33:12,104][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2024-10-05 08:34:57,174][train_inner][INFO] - {"epoch": 43, "update": 42.616, "loss": "1.147", "ntokens": "239386", "nsentences": "1795.53", "wps": "138089", "ups": "0.58", "wpb": "239386", "bsz": "1795.5", "num_updates": "20400", "lr": "0.00031875", "gnorm": "0.475", "loss_scale": "2", "train_wall": "269", "gb_free": "39.2", "wall": "23065"}
[2024-10-05 08:38:45,753][fairseq_cli.train][INFO] - end of epoch 43 (average epoch stats below)
[2024-10-05 08:38:45,772][train][INFO] - {"epoch": 43, "train_loss": "1.147", "train_ntokens": "239269", "train_nsentences": "1753.76", "train_wps": "117740", "train_ups": "0.49", "train_wpb": "239270", "train_bsz": "1753.8", "train_num_updates": "20584", "train_lr": "0.000321625", "train_gnorm": "0.472", "train_loss_scale": "2", "train_train_wall": "592", "train_gb_free": "39.3", "train_wall": "23293"}
[2024-10-05 08:38:45,929][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 08:38:45,938][fairseq.trainer][INFO] - begin training epoch 44
[2024-10-05 08:38:45,938][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 08:44:00,933][train_inner][INFO] - {"epoch": 44, "update": 43.033, "loss": "1.148", "ntokens": "239036", "nsentences": "1722.24", "wps": "87928", "ups": "0.37", "wpb": "239036", "bsz": "1722.2", "num_updates": "20600", "lr": "0.000321875", "gnorm": "0.46", "loss_scale": "2", "train_wall": "251", "gb_free": "40.6", "wall": "23609"}
[2024-10-05 08:47:29,609][train_inner][INFO] - {"epoch": 44, "update": 43.451, "loss": "1.14", "ntokens": "239168", "nsentences": "1773.4", "wps": "229244", "ups": "0.96", "wpb": "239168", "bsz": "1773.4", "num_updates": "20800", "lr": "0.000325", "gnorm": "0.448", "loss_scale": "2", "train_wall": "206", "gb_free": "39.3", "wall": "23817"}
[2024-10-05 08:50:41,532][train_inner][INFO] - {"epoch": 44, "update": 43.868, "loss": "1.145", "ntokens": "240169", "nsentences": "1741.31", "wps": "250322", "ups": "1.04", "wpb": "240169", "bsz": "1741.3", "num_updates": "21000", "lr": "0.000328125", "gnorm": "0.442", "loss_scale": "2", "train_wall": "174", "gb_free": "39.6", "wall": "24009"}
[2024-10-05 08:51:46,206][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 44 @ 21063 updates
[2024-10-05 08:51:46,207][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 08:51:51,283][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 08:51:51,299][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 44 @ 21063 updates, score None) (writing took 5.092763704247773 seconds)
[2024-10-05 08:51:51,299][fairseq_cli.train][INFO] - end of epoch 44 (average epoch stats below)
[2024-10-05 08:51:51,301][train][INFO] - {"epoch": 44, "train_loss": "1.144", "train_ntokens": "239336", "train_nsentences": "1753.71", "train_wps": "145943", "train_ups": "0.61", "train_wpb": "239336", "train_bsz": "1753.7", "train_num_updates": "21063", "train_lr": "0.000329109", "train_gnorm": "0.45", "train_loss_scale": "2", "train_train_wall": "471", "train_gb_free": "39.2", "train_wall": "24079"}
[2024-10-05 08:51:51,425][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 08:51:51,455][fairseq.trainer][INFO] - begin training epoch 45
[2024-10-05 08:51:51,456][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 08:59:09,646][train_inner][INFO] - {"epoch": 45, "update": 44.286, "loss": "1.139", "ntokens": "238235", "nsentences": "1771.76", "wps": "93777.8", "ups": "0.39", "wpb": "238235", "bsz": "1771.8", "num_updates": "21200", "lr": "0.00033125", "gnorm": "0.452", "loss_scale": "2", "train_wall": "228", "gb_free": "40.1", "wall": "24517"}
[2024-10-05 09:02:20,359][train_inner][INFO] - {"epoch": 45, "update": 44.704, "loss": "1.143", "ntokens": "240015", "nsentences": "1753.42", "wps": "251736", "ups": "1.05", "wpb": "240015", "bsz": "1753.4", "num_updates": "21400", "lr": "0.000334375", "gnorm": "0.445", "loss_scale": "2", "train_wall": "188", "gb_free": "39.3", "wall": "24708"}
[2024-10-05 09:05:05,469][fairseq_cli.train][INFO] - end of epoch 45 (average epoch stats below)
[2024-10-05 09:05:05,486][train][INFO] - {"epoch": 45, "train_loss": "1.141", "train_ntokens": "239305", "train_nsentences": "1753.71", "train_wps": "144333", "train_ups": "0.6", "train_wpb": "239305", "train_bsz": "1753.7", "train_num_updates": "21542", "train_lr": "0.000336594", "train_gnorm": "0.437", "train_loss_scale": "2", "train_train_wall": "515", "train_gb_free": "39.2", "train_wall": "24873"}
[2024-10-05 09:05:05,590][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 09:05:05,595][fairseq.trainer][INFO] - begin training epoch 46
[2024-10-05 09:05:05,596][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 09:10:21,732][train_inner][INFO] - {"epoch": 46, "update": 45.121, "loss": "1.143", "ntokens": "238866", "nsentences": "1719.64", "wps": "99248", "ups": "0.42", "wpb": "238866", "bsz": "1719.6", "num_updates": "21600", "lr": "0.0003375", "gnorm": "0.425", "loss_scale": "2", "train_wall": "221", "gb_free": "39.8", "wall": "25189"}
[2024-10-05 09:13:15,642][train_inner][INFO] - {"epoch": 46, "update": 45.539, "loss": "1.136", "ntokens": "239955", "nsentences": "1745.31", "wps": "275964", "ups": "1.15", "wpb": "239955", "bsz": "1745.3", "num_updates": "21800", "lr": "0.000340625", "gnorm": "0.471", "loss_scale": "2", "train_wall": "170", "gb_free": "39.3", "wall": "25363"}
[2024-10-05 09:16:53,883][train_inner][INFO] - {"epoch": 46, "update": 45.956, "loss": "1.142", "ntokens": "239640", "nsentences": "1769.87", "wps": "219618", "ups": "0.92", "wpb": "239640", "bsz": "1769.9", "num_updates": "22000", "lr": "0.00034375", "gnorm": "0.44", "loss_scale": "2", "train_wall": "200", "gb_free": "39.3", "wall": "25582"}
[2024-10-05 09:17:37,107][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 46 @ 22021 updates
[2024-10-05 09:17:37,108][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 09:17:44,981][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 09:17:44,985][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 46 @ 22021 updates, score None) (writing took 7.8776403879746795 seconds)
[2024-10-05 09:17:44,985][fairseq_cli.train][INFO] - end of epoch 46 (average epoch stats below)
[2024-10-05 09:17:44,998][train][INFO] - {"epoch": 46, "train_loss": "1.139", "train_ntokens": "239190", "train_nsentences": "1753.71", "train_wps": "150852", "train_ups": "0.63", "train_wpb": "239190", "train_bsz": "1753.7", "train_num_updates": "22021", "train_lr": "0.000344078", "train_gnorm": "0.458", "train_loss_scale": "2", "train_train_wall": "471", "train_gb_free": "39.8", "train_wall": "25633"}
[2024-10-05 09:17:45,083][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 09:17:45,089][fairseq.trainer][INFO] - begin training epoch 47
[2024-10-05 09:17:45,089][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 09:25:09,690][train_inner][INFO] - {"epoch": 47, "update": 46.374, "loss": "1.136", "ntokens": "238514", "nsentences": "1750.8", "wps": "96213.5", "ups": "0.4", "wpb": "238514", "bsz": "1750.8", "num_updates": "22200", "lr": "0.000346875", "gnorm": "0.444", "loss_scale": "2", "train_wall": "211", "gb_free": "39.8", "wall": "26077"}
[2024-10-05 09:28:24,076][train_inner][INFO] - {"epoch": 47, "update": 46.791, "loss": "1.138", "ntokens": "239305", "nsentences": "1786.97", "wps": "246248", "ups": "1.03", "wpb": "239305", "bsz": "1787", "num_updates": "22400", "lr": "0.00035", "gnorm": "0.455", "loss_scale": "2", "train_wall": "191", "gb_free": "40.3", "wall": "26272"}
[2024-10-05 09:29:09,232][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2024-10-05 09:30:07,116][fairseq_cli.train][INFO] - end of epoch 47 (average epoch stats below)
[2024-10-05 09:30:07,167][train][INFO] - {"epoch": 47, "train_loss": "1.137", "train_ntokens": "239260", "train_nsentences": "1752.78", "train_wps": "154103", "train_ups": "0.64", "train_wpb": "239260", "train_bsz": "1752.8", "train_num_updates": "22499", "train_lr": "0.000351547", "train_gnorm": "0.435", "train_loss_scale": "2", "train_train_wall": "460", "train_gb_free": "39.1", "train_wall": "26375"}
[2024-10-05 09:30:07,361][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 09:30:07,384][fairseq.trainer][INFO] - begin training epoch 48
[2024-10-05 09:30:07,384][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 09:37:19,990][train_inner][INFO] - {"epoch": 48, "update": 47.211, "loss": "1.135", "ntokens": "238618", "nsentences": "1729.25", "wps": "89054.7", "ups": "0.37", "wpb": "238618", "bsz": "1729.2", "num_updates": "22600", "lr": "0.000353125", "gnorm": "0.437", "loss_scale": "2", "train_wall": "265", "gb_free": "39.6", "wall": "26808"}
[2024-10-05 09:40:12,073][train_inner][INFO] - {"epoch": 48, "update": 47.628, "loss": "1.133", "ntokens": "239544", "nsentences": "1758.98", "wps": "278427", "ups": "1.16", "wpb": "239544", "bsz": "1759", "num_updates": "22800", "lr": "0.00035625", "gnorm": "0.41", "loss_scale": "2", "train_wall": "169", "gb_free": "40.1", "wall": "26980"}
[2024-10-05 09:43:07,767][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 48 @ 22978 updates
[2024-10-05 09:43:07,775][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 09:43:13,719][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 09:43:13,733][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 48 @ 22978 updates, score None) (writing took 5.966289296746254 seconds)
[2024-10-05 09:43:13,734][fairseq_cli.train][INFO] - end of epoch 48 (average epoch stats below)
[2024-10-05 09:43:13,738][train][INFO] - {"epoch": 48, "train_loss": "1.134", "train_ntokens": "239053", "train_nsentences": "1753.71", "train_wps": "145578", "train_ups": "0.61", "train_wpb": "239053", "train_bsz": "1753.7", "train_num_updates": "22978", "train_lr": "0.000359031", "train_gnorm": "0.436", "train_loss_scale": "2", "train_train_wall": "505", "train_gb_free": "39.3", "train_wall": "27161"}
[2024-10-05 09:43:13,846][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 09:43:13,867][fairseq.trainer][INFO] - begin training epoch 49
[2024-10-05 09:43:13,868][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 09:48:30,251][train_inner][INFO] - {"epoch": 49, "update": 48.046, "loss": "1.136", "ntokens": "238546", "nsentences": "1753.06", "wps": "95770.8", "ups": "0.4", "wpb": "238546", "bsz": "1753.1", "num_updates": "23000", "lr": "0.000359375", "gnorm": "0.444", "loss_scale": "2", "train_wall": "219", "gb_free": "40.2", "wall": "27478"}
[2024-10-05 09:51:33,845][train_inner][INFO] - {"epoch": 49, "update": 48.463, "loss": "1.128", "ntokens": "239570", "nsentences": "1760.23", "wps": "261000", "ups": "1.09", "wpb": "239570", "bsz": "1760.2", "num_updates": "23200", "lr": "0.0003625", "gnorm": "0.415", "loss_scale": "2", "train_wall": "156", "gb_free": "40.5", "wall": "27661"}
[2024-10-05 09:54:43,874][train_inner][INFO] - {"epoch": 49, "update": 48.881, "loss": "1.136", "ntokens": "240329", "nsentences": "1758.82", "wps": "252954", "ups": "1.05", "wpb": "240329", "bsz": "1758.8", "num_updates": "23400", "lr": "0.000365625", "gnorm": "0.427", "loss_scale": "2", "train_wall": "178", "gb_free": "40.3", "wall": "27852"}
[2024-10-05 09:55:51,698][fairseq_cli.train][INFO] - end of epoch 49 (average epoch stats below)
[2024-10-05 09:55:51,700][train][INFO] - {"epoch": 49, "train_loss": "1.133", "train_ntokens": "239482", "train_nsentences": "1753.71", "train_wps": "151343", "train_ups": "0.63", "train_wpb": "239482", "train_bsz": "1753.7", "train_num_updates": "23457", "train_lr": "0.000366516", "train_gnorm": "0.418", "train_loss_scale": "2", "train_train_wall": "448", "train_gb_free": "39.6", "train_wall": "27919"}
[2024-10-05 09:55:51,815][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 09:55:51,835][fairseq.trainer][INFO] - begin training epoch 50
[2024-10-05 09:55:51,835][fairseq_cli.train][INFO] - Start iterating over samples
