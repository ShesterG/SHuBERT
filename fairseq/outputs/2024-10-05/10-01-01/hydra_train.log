[2024-10-05 10:01:27,181][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:12620', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'gloss'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_800k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-05 10:01:29,044][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:19501', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'gloss'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_800k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-05 10:01:29,106][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:16452', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'gloss'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_800k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-05 10:01:29,826][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:19156', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'gloss'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_800k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-05 10:01:31,763][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-05 10:01:31,765][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-05 10:01:31,765][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-05 10:01:31,765][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-05 10:01:31,766][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-05 10:01:31,766][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-05 10:01:32,172][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-05 10:01:32,173][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-05 10:01:32,174][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-05 10:01:32,174][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-05 10:01:32,182][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-05 10:01:32,183][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-05 10:01:32,360][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-05 10:01:32,362][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-05 10:01:32,362][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-05 10:01:32,362][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-05 10:01:32,371][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-05 10:01:32,371][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-05 10:01:34,124][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-05 10:01:34,688][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-05 10:01:35,481][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-05 10:01:37,458][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:10384', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'gloss'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_800k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-05 10:01:37,962][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:12324', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'gloss'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_800k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-05 10:01:38,514][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:15673', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'gloss'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_800k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-05 10:01:38,928][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:18443', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'gloss'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_800k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-05 10:01:41,527][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-05 10:01:41,529][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-05 10:01:41,529][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-05 10:01:41,529][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-05 10:01:41,530][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-05 10:01:41,530][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-05 10:01:42,286][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-05 10:01:42,288][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-05 10:01:42,288][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-05 10:01:42,288][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-05 10:01:42,289][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-05 10:01:42,289][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-05 10:01:44,218][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-05 10:01:44,601][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-05 10:01:44,624][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-05 10:01:44,624][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-05 10:01:44,624][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-05 10:01:44,625][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-05 10:01:44,625][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-05 10:01:45,077][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-05 10:01:45,114][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-05 10:01:45,114][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-05 10:01:45,114][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-05 10:01:45,115][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-05 10:01:45,116][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-05 10:01:45,240][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-05 10:01:45,241][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-05 10:01:45,241][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-05 10:01:45,241][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-05 10:01:45,402][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-05 10:01:45,443][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-05 10:01:48,134][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-05 10:01:54,810][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-05 10:01:56,066][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-05 10:02:12,201][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-05 10:04:50,615][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-05 10:04:50,616][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 10:04:50,616][fairseq.utils][INFO] - rank   1: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 10:04:50,616][fairseq.utils][INFO] - rank   2: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 10:04:50,616][fairseq.utils][INFO] - rank   3: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 10:04:50,616][fairseq.utils][INFO] - rank   4: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 10:04:50,616][fairseq.utils][INFO] - rank   5: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 10:04:50,616][fairseq.utils][INFO] - rank   6: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 10:04:50,616][fairseq.utils][INFO] - rank   7: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 10:04:50,616][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-05 10:04:50,617][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-05 10:04:50,668][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-05 10:04:50,669][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 10:05:12,783][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-05 10:05:12,784][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 10:05:12,784][fairseq.utils][INFO] - rank   1: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 10:05:12,784][fairseq.utils][INFO] - rank   2: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 10:05:12,784][fairseq.utils][INFO] - rank   3: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 10:05:12,784][fairseq.utils][INFO] - rank   4: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 10:05:12,784][fairseq.utils][INFO] - rank   5: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 10:05:12,784][fairseq.utils][INFO] - rank   6: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 10:05:12,784][fairseq.utils][INFO] - rank   7: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 10:05:12,784][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-05 10:05:12,784][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-05 10:05:12,784][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-05 10:05:12,806][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 10:05:38,009][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 49 @ 22978 updates)
[2024-10-05 10:05:38,010][fairseq.trainer][INFO] - loading train data for epoch 49
[2024-10-05 10:05:42,049][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-05 10:06:20,189][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 49 @ 22978 updates)
[2024-10-05 10:06:20,190][fairseq.trainer][INFO] - loading train data for epoch 49
[2024-10-05 10:07:02,815][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-05 10:07:56,596][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 10:07:56,601][fairseq.trainer][INFO] - begin training epoch 49
[2024-10-05 10:07:56,606][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 10:11:01,593][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 10:11:01,620][fairseq.trainer][INFO] - begin training epoch 49
[2024-10-05 10:11:01,627][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 10:13:10,162][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-05 10:13:10,225][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 10:13:10,225][fairseq.utils][INFO] - rank   1: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 10:13:10,225][fairseq.utils][INFO] - rank   2: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 10:13:10,225][fairseq.utils][INFO] - rank   3: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 10:13:10,225][fairseq.utils][INFO] - rank   4: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 10:13:10,225][fairseq.utils][INFO] - rank   5: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 10:13:10,225][fairseq.utils][INFO] - rank   6: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 10:13:10,225][fairseq.utils][INFO] - rank   7: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 10:13:10,226][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-05 10:13:10,226][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-05 10:13:10,235][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-05 10:13:10,236][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 10:13:52,628][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 49 @ 22978 updates)
[2024-10-05 10:13:52,630][fairseq.trainer][INFO] - loading train data for epoch 49
[2024-10-05 10:14:00,746][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-05 10:15:58,984][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 10:15:58,990][fairseq.trainer][INFO] - begin training epoch 49
[2024-10-05 10:15:58,990][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 10:16:07,418][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-05 10:16:07,418][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 10:16:07,418][fairseq.utils][INFO] - rank   1: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 10:16:07,418][fairseq.utils][INFO] - rank   2: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 10:16:07,418][fairseq.utils][INFO] - rank   3: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 10:16:07,418][fairseq.utils][INFO] - rank   4: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 10:16:07,419][fairseq.utils][INFO] - rank   5: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 10:16:07,419][fairseq.utils][INFO] - rank   6: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 10:16:07,419][fairseq.utils][INFO] - rank   7: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 10:16:07,419][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-05 10:16:07,419][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-05 10:16:07,419][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-05 10:16:07,426][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 10:16:18,858][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-05 10:16:18,858][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 10:16:18,858][fairseq.utils][INFO] - rank   1: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 10:16:18,858][fairseq.utils][INFO] - rank   2: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 10:16:18,858][fairseq.utils][INFO] - rank   3: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 10:16:18,858][fairseq.utils][INFO] - rank   4: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 10:16:18,858][fairseq.utils][INFO] - rank   5: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 10:16:18,858][fairseq.utils][INFO] - rank   6: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 10:16:18,858][fairseq.utils][INFO] - rank   7: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 10:16:18,858][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-05 10:16:18,859][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-05 10:16:18,859][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-05 10:16:18,864][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 10:16:25,776][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-05 10:16:25,776][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 10:16:25,776][fairseq.utils][INFO] - rank   1: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 10:16:25,776][fairseq.utils][INFO] - rank   2: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 10:16:25,776][fairseq.utils][INFO] - rank   3: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 10:16:25,776][fairseq.utils][INFO] - rank   4: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 10:16:25,776][fairseq.utils][INFO] - rank   5: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 10:16:25,776][fairseq.utils][INFO] - rank   6: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 10:16:25,776][fairseq.utils][INFO] - rank   7: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 10:16:25,776][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-05 10:16:25,776][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-05 10:16:25,777][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-05 10:16:25,779][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 10:16:35,818][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 49 @ 22978 updates)
[2024-10-05 10:16:35,884][fairseq.trainer][INFO] - loading train data for epoch 49
[2024-10-05 10:16:36,932][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 49 @ 22978 updates)
[2024-10-05 10:16:36,994][fairseq.trainer][INFO] - loading train data for epoch 49
[2024-10-05 10:16:39,077][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-05 10:16:39,851][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-05 10:16:42,184][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 49 @ 22978 updates)
[2024-10-05 10:16:42,261][fairseq.trainer][INFO] - loading train data for epoch 49
[2024-10-05 10:16:44,606][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-05 10:17:30,631][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-05 10:17:30,632][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 10:17:30,632][fairseq.utils][INFO] - rank   1: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 10:17:30,632][fairseq.utils][INFO] - rank   2: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 10:17:30,632][fairseq.utils][INFO] - rank   3: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 10:17:30,632][fairseq.utils][INFO] - rank   4: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 10:17:30,635][fairseq.utils][INFO] - rank   5: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 10:17:30,635][fairseq.utils][INFO] - rank   6: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 10:17:30,635][fairseq.utils][INFO] - rank   7: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 10:17:30,635][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-05 10:17:30,635][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-05 10:17:30,636][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-05 10:17:30,695][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 10:17:41,306][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-05 10:17:41,355][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 10:17:41,355][fairseq.utils][INFO] - rank   1: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 10:17:41,355][fairseq.utils][INFO] - rank   2: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 10:17:41,355][fairseq.utils][INFO] - rank   3: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 10:17:41,355][fairseq.utils][INFO] - rank   4: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 10:17:41,355][fairseq.utils][INFO] - rank   5: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 10:17:41,355][fairseq.utils][INFO] - rank   6: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 10:17:41,355][fairseq.utils][INFO] - rank   7: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 10:17:41,355][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-05 10:17:41,356][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-05 10:17:41,356][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-05 10:17:41,575][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 10:17:48,973][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 10:17:48,980][fairseq.trainer][INFO] - begin training epoch 49
[2024-10-05 10:17:48,987][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 10:17:50,823][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 10:17:50,836][fairseq.trainer][INFO] - begin training epoch 49
[2024-10-05 10:17:50,836][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 10:17:57,963][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 10:17:57,966][fairseq.trainer][INFO] - begin training epoch 49
[2024-10-05 10:17:57,967][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 10:18:13,330][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 49 @ 22978 updates)
[2024-10-05 10:18:13,369][fairseq.trainer][INFO] - loading train data for epoch 49
[2024-10-05 10:18:21,193][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 49 @ 22978 updates)
[2024-10-05 10:18:21,276][fairseq.trainer][INFO] - loading train data for epoch 49
[2024-10-05 10:18:23,212][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-05 10:18:24,857][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-05 10:19:31,519][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 10:19:31,522][fairseq.trainer][INFO] - begin training epoch 49
[2024-10-05 10:19:31,523][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 10:19:41,823][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 10:19:41,842][fairseq.trainer][INFO] - begin training epoch 49
[2024-10-05 10:19:41,843][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 10:48:29,052][train_inner][INFO] - {"epoch": 49, "update": 48.046, "loss": "1.134", "ntokens": "238832", "nsentences": "1784.91", "wps": "17189.1", "ups": "0.07", "wpb": "238832", "bsz": "1784.9", "num_updates": "23000", "lr": "0.000359375", "gnorm": "0.414", "loss_scale": "2", "train_wall": "595", "gb_free": "39.7", "wall": "1930"}
[2024-10-05 10:48:37,147][train_inner][INFO] - {"epoch": 49, "update": 48.046, "loss": "1.134", "ntokens": "238832", "nsentences": "1784.91", "wps": "36003.3", "ups": "0.15", "wpb": "238832", "bsz": "1784.9", "num_updates": "23000", "lr": "0.000359375", "gnorm": "0.414", "loss_scale": "2", "train_wall": "589", "gb_free": "39.7", "wall": "1866"}
[2024-10-05 11:09:37,267][train_inner][INFO] - {"epoch": 49, "update": 48.463, "loss": "1.131", "ntokens": "239642", "nsentences": "1760.23", "wps": "37794.7", "ups": "0.16", "wpb": "239642", "bsz": "1760.2", "num_updates": "23200", "lr": "0.0003625", "gnorm": "0.454", "loss_scale": "2", "train_wall": "1011", "gb_free": "40.5", "wall": "3198"}
[2024-10-05 11:09:37,267][train_inner][INFO] - {"epoch": 49, "update": 48.463, "loss": "1.131", "ntokens": "239642", "nsentences": "1760.23", "wps": "38034.9", "ups": "0.16", "wpb": "239642", "bsz": "1760.2", "num_updates": "23200", "lr": "0.0003625", "gnorm": "0.454", "loss_scale": "2", "train_wall": "1007", "gb_free": "40.5", "wall": "3127"}
[2024-10-05 11:35:01,335][train_inner][INFO] - {"epoch": 49, "update": 48.881, "loss": "1.136", "ntokens": "239822", "nsentences": "1758.82", "wps": "31473.1", "ups": "0.13", "wpb": "239822", "bsz": "1758.8", "num_updates": "23400", "lr": "0.000365625", "gnorm": "0.412", "loss_scale": "2", "train_wall": "1409", "gb_free": "40.3", "wall": "4651"}
[2024-10-05 11:35:03,366][train_inner][INFO] - {"epoch": 49, "update": 48.881, "loss": "1.136", "ntokens": "239822", "nsentences": "1758.82", "wps": "31431.1", "ups": "0.13", "wpb": "239822", "bsz": "1758.8", "num_updates": "23400", "lr": "0.000365625", "gnorm": "0.412", "loss_scale": "2", "train_wall": "1410", "gb_free": "40.3", "wall": "4725"}
[2024-10-05 11:38:46,734][fairseq_cli.train][INFO] - end of epoch 49 (average epoch stats below)
[2024-10-05 11:38:46,907][train][INFO] - {"epoch": 49, "train_loss": "1.134", "train_ntokens": "239234", "train_nsentences": "1753.71", "train_wps": "36313.8", "train_ups": "0.15", "train_wpb": "239234", "train_bsz": "1753.7", "train_num_updates": "23457", "train_lr": "0.000366516", "train_gnorm": "0.431", "train_loss_scale": "2", "train_train_wall": "3230", "train_gb_free": "39.6", "train_wall": "4876"}
[2024-10-05 11:38:47,214][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 11:38:47,256][fairseq.trainer][INFO] - begin training epoch 50
[2024-10-05 11:38:47,263][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 11:39:20,330][fairseq_cli.train][INFO] - end of epoch 49 (average epoch stats below)
[2024-10-05 11:39:20,426][train][INFO] - {"epoch": 49, "train_loss": "1.134", "train_ntokens": "239234", "train_nsentences": "1753.71", "train_wps": "34200.4", "train_ups": "0.14", "train_wpb": "239234", "train_bsz": "1753.7", "train_num_updates": "23457", "train_lr": "0.000366516", "train_gnorm": "0.431", "train_loss_scale": "2", "train_train_wall": "3272", "train_gb_free": "39.6", "train_wall": "4981"}
[2024-10-05 11:39:20,584][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 11:39:20,589][fairseq.trainer][INFO] - begin training epoch 50
[2024-10-05 11:39:20,589][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 11:53:15,449][train_inner][INFO] - {"epoch": 50, "update": 49.299, "loss": "1.132", "ntokens": "238839", "nsentences": "1775.07", "wps": "43664.3", "ups": "0.18", "wpb": "238839", "bsz": "1775.1", "num_updates": "23600", "lr": "0.00036875", "gnorm": "0.391", "loss_scale": "2", "train_wall": "692", "gb_free": "39.6", "wall": "5745"}
[2024-10-05 11:53:15,815][train_inner][INFO] - {"epoch": 50, "update": 49.299, "loss": "1.132", "ntokens": "238839", "nsentences": "1775.07", "wps": "43726", "ups": "0.18", "wpb": "238839", "bsz": "1775.1", "num_updates": "23600", "lr": "0.00036875", "gnorm": "0.395", "loss_scale": "2", "train_wall": "725", "gb_free": "39.6", "wall": "5817"}
[2024-10-05 12:01:07,703][train_inner][INFO] - {"epoch": 50, "update": 49.716, "loss": "1.131", "ntokens": "239712", "nsentences": "1752.01", "wps": "101605", "ups": "0.42", "wpb": "239712", "bsz": "1752", "num_updates": "23800", "lr": "0.000371875", "gnorm": "0.407", "loss_scale": "2", "train_wall": "466", "gb_free": "39.6", "wall": "6289"}
[2024-10-05 12:01:10,851][train_inner][INFO] - {"epoch": 50, "update": 49.716, "loss": "1.131", "ntokens": "239712", "nsentences": "1752.01", "wps": "100868", "ups": "0.42", "wpb": "239712", "bsz": "1752", "num_updates": "23800", "lr": "0.000371875", "gnorm": "0.412", "loss_scale": "2", "train_wall": "470", "gb_free": "39.6", "wall": "6220"}
[2024-10-05 12:05:31,768][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 50 @ 23936 updates
[2024-10-05 12:05:31,771][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 12:05:34,688][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 50 @ 23936 updates
[2024-10-05 12:05:34,723][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 12:05:50,859][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 12:05:51,191][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 50 @ 23936 updates, score None) (writing took 16.50257551111281 seconds)
[2024-10-05 12:05:51,202][fairseq_cli.train][INFO] - end of epoch 50 (average epoch stats below)
[2024-10-05 12:05:51,225][train][INFO] - {"epoch": 50, "train_loss": "1.131", "train_ntokens": "239400", "train_nsentences": "1753.71", "train_wps": "70598.4", "train_ups": "0.29", "train_wpb": "239400", "train_bsz": "1753.7", "train_num_updates": "23936", "train_lr": "0.000374", "train_gnorm": "0.409", "train_loss_scale": "2", "train_train_wall": "1177", "train_gb_free": "39.2", "train_wall": "6501"}
[2024-10-05 12:05:51,546][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 12:05:51,584][fairseq.trainer][INFO] - begin training epoch 51
[2024-10-05 12:05:51,584][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 12:14:12,058][train_inner][INFO] - {"epoch": 51, "update": 50.134, "loss": "1.131", "ntokens": "238938", "nsentences": "1713.01", "wps": "61171.8", "ups": "0.26", "wpb": "238938", "bsz": "1713", "num_updates": "24000", "lr": "0.000375", "gnorm": "0.425", "loss_scale": "2", "train_wall": "392", "gb_free": "39.3", "wall": "7001"}
[2024-10-05 12:21:06,786][train_inner][INFO] - {"epoch": 51, "update": 50.551, "loss": "1.13", "ntokens": "239996", "nsentences": "1747.09", "wps": "115754", "ups": "0.48", "wpb": "239996", "bsz": "1747.1", "num_updates": "24200", "lr": "0.000378125", "gnorm": "0.4", "loss_scale": "2", "train_wall": "329", "gb_free": "39.8", "wall": "7416"}
[2024-10-05 12:28:19,395][train_inner][INFO] - {"epoch": 51, "update": 50.969, "loss": "1.131", "ntokens": "239676", "nsentences": "1769.43", "wps": "110816", "ups": "0.46", "wpb": "239676", "bsz": "1769.4", "num_updates": "24400", "lr": "0.00038125", "gnorm": "0.406", "loss_scale": "2", "train_wall": "431", "gb_free": "39.3", "wall": "7849"}
[2024-10-05 12:28:56,874][fairseq_cli.train][INFO] - end of epoch 51 (average epoch stats below)
[2024-10-05 12:28:56,893][train][INFO] - {"epoch": 51, "train_loss": "1.13", "train_ntokens": "239422", "train_nsentences": "1753.71", "train_wps": "82764", "train_ups": "0.35", "train_wpb": "239422", "train_bsz": "1753.7", "train_num_updates": "24415", "train_lr": "0.000381484", "train_gnorm": "0.402", "train_loss_scale": "2", "train_train_wall": "949", "train_gb_free": "39.6", "train_wall": "7886"}
[2024-10-05 12:28:57,153][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 12:28:57,170][fairseq.trainer][INFO] - begin training epoch 52
[2024-10-05 12:28:57,170][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 12:38:49,310][train_inner][INFO] - {"epoch": 52, "update": 51.386, "loss": "1.127", "ntokens": "238948", "nsentences": "1723.22", "wps": "75867.6", "ups": "0.32", "wpb": "238948", "bsz": "1723.2", "num_updates": "24600", "lr": "0.000384375", "gnorm": "0.401", "loss_scale": "2", "train_wall": "280", "gb_free": "39.6", "wall": "8479"}
[2024-10-05 12:44:28,173][train_inner][INFO] - {"epoch": 52, "update": 51.804, "loss": "1.129", "ntokens": "238719", "nsentences": "1808", "wps": "140916", "ups": "0.59", "wpb": "238719", "bsz": "1808", "num_updates": "24800", "lr": "0.0003875", "gnorm": "0.416", "loss_scale": "2", "train_wall": "236", "gb_free": "40.1", "wall": "8818"}
[2024-10-05 12:46:34,848][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 52 @ 24894 updates
[2024-10-05 12:46:34,857][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 12:46:42,105][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 12:46:42,108][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 52 @ 24894 updates, score None) (writing took 7.260105951689184 seconds)
[2024-10-05 12:46:42,109][fairseq_cli.train][INFO] - end of epoch 52 (average epoch stats below)
[2024-10-05 12:46:42,111][train][INFO] - {"epoch": 52, "train_loss": "1.129", "train_ntokens": "239267", "train_nsentences": "1753.71", "train_wps": "107592", "train_ups": "0.45", "train_wpb": "239267", "train_bsz": "1753.7", "train_num_updates": "24894", "train_lr": "0.000388969", "train_gnorm": "0.414", "train_loss_scale": "2", "train_train_wall": "604", "train_gb_free": "39.1", "train_wall": "8951"}
[2024-10-05 12:46:42,278][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 12:46:42,315][fairseq.trainer][INFO] - begin training epoch 53
[2024-10-05 12:46:42,315][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 12:53:30,302][train_inner][INFO] - {"epoch": 53, "update": 52.221, "loss": "1.127", "ntokens": "239140", "nsentences": "1721.96", "wps": "88233.1", "ups": "0.37", "wpb": "239140", "bsz": "1722", "num_updates": "25000", "lr": "0.000390625", "gnorm": "0.397", "loss_scale": "2", "train_wall": "222", "gb_free": "40.1", "wall": "9360"}
[2024-10-05 12:54:50,682][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2024-10-05 12:57:06,526][train_inner][INFO] - {"epoch": 53, "update": 52.641, "loss": "1.125", "ntokens": "239602", "nsentences": "1749.73", "wps": "221662", "ups": "0.93", "wpb": "239602", "bsz": "1749.7", "num_updates": "25200", "lr": "0.00039375", "gnorm": "0.405", "loss_scale": "2", "train_wall": "189", "gb_free": "39.6", "wall": "9576"}
[2024-10-05 12:59:55,115][fairseq_cli.train][INFO] - end of epoch 53 (average epoch stats below)
[2024-10-05 12:59:55,139][train][INFO] - {"epoch": 53, "train_loss": "1.126", "train_ntokens": "239064", "train_nsentences": "1753.06", "train_wps": "144098", "train_ups": "0.6", "train_wpb": "239064", "train_bsz": "1753.1", "train_num_updates": "25372", "train_lr": "0.000396437", "train_gnorm": "0.396", "train_loss_scale": "2", "train_train_wall": "431", "train_gb_free": "40.1", "train_wall": "9744"}
[2024-10-05 12:59:55,377][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 12:59:55,396][fairseq.trainer][INFO] - begin training epoch 54
[2024-10-05 12:59:55,396][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 13:05:28,100][train_inner][INFO] - {"epoch": 54, "update": 53.058, "loss": "1.131", "ntokens": "238521", "nsentences": "1762.05", "wps": "95109.7", "ups": "0.4", "wpb": "238521", "bsz": "1762", "num_updates": "25400", "lr": "0.000396875", "gnorm": "0.413", "loss_scale": "2", "train_wall": "205", "gb_free": "39.6", "wall": "10077"}
[2024-10-05 13:10:25,973][train_inner][INFO] - {"epoch": 54, "update": 53.476, "loss": "1.123", "ntokens": "240512", "nsentences": "1727.08", "wps": "161534", "ups": "0.67", "wpb": "240512", "bsz": "1727.1", "num_updates": "25600", "lr": "0.0004", "gnorm": "0.391", "loss_scale": "2", "train_wall": "268", "gb_free": "39.6", "wall": "10375"}
[2024-10-05 13:15:29,832][train_inner][INFO] - {"epoch": 54, "update": 53.894, "loss": "1.126", "ntokens": "239605", "nsentences": "1787.59", "wps": "157731", "ups": "0.66", "wpb": "239605", "bsz": "1787.6", "num_updates": "25800", "lr": "0.000403125", "gnorm": "0.384", "loss_scale": "2", "train_wall": "274", "gb_free": "39.6", "wall": "10679"}
[2024-10-05 13:16:07,097][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 54 @ 25851 updates
[2024-10-05 13:16:07,098][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 13:16:16,603][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 13:16:16,628][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 54 @ 25851 updates, score None) (writing took 9.530897932127118 seconds)
[2024-10-05 13:16:16,641][fairseq_cli.train][INFO] - end of epoch 54 (average epoch stats below)
[2024-10-05 13:16:16,659][train][INFO] - {"epoch": 54, "train_loss": "1.126", "train_ntokens": "239505", "train_nsentences": "1753.71", "train_wps": "116885", "train_ups": "0.49", "train_wpb": "239505", "train_bsz": "1753.7", "train_num_updates": "25851", "train_lr": "0.000403922", "train_gnorm": "0.402", "train_loss_scale": "2", "train_train_wall": "638", "train_gb_free": "39.8", "train_wall": "10726"}
[2024-10-05 13:16:16,833][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 13:16:16,861][fairseq.trainer][INFO] - begin training epoch 55
[2024-10-05 13:16:16,861][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 13:25:31,402][train_inner][INFO] - {"epoch": 55, "update": 54.311, "loss": "1.124", "ntokens": "238427", "nsentences": "1750.37", "wps": "79269.1", "ups": "0.33", "wpb": "238427", "bsz": "1750.4", "num_updates": "26000", "lr": "0.00040625", "gnorm": "0.457", "loss_scale": "2", "train_wall": "293", "gb_free": "39.7", "wall": "11281"}
[2024-10-05 13:29:53,446][train_inner][INFO] - {"epoch": 55, "update": 54.729, "loss": "1.124", "ntokens": "239661", "nsentences": "1764.68", "wps": "182932", "ups": "0.76", "wpb": "239661", "bsz": "1764.7", "num_updates": "26200", "lr": "0.000409375", "gnorm": "0.405", "loss_scale": "2", "train_wall": "259", "gb_free": "39.3", "wall": "11543"}
[2024-10-05 13:32:48,305][fairseq_cli.train][INFO] - end of epoch 55 (average epoch stats below)
[2024-10-05 13:32:48,343][train][INFO] - {"epoch": 55, "train_loss": "1.124", "train_ntokens": "239149", "train_nsentences": "1753.71", "train_wps": "115514", "train_ups": "0.48", "train_wpb": "239149", "train_bsz": "1753.7", "train_num_updates": "26330", "train_lr": "0.000411406", "train_gnorm": "0.414", "train_loss_scale": "2", "train_train_wall": "690", "train_gb_free": "39.3", "train_wall": "11718"}
[2024-10-05 13:32:48,571][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 13:32:48,591][fairseq.trainer][INFO] - begin training epoch 56
[2024-10-05 13:32:48,592][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 13:39:55,634][train_inner][INFO] - {"epoch": 56, "update": 55.146, "loss": "1.125", "ntokens": "238420", "nsentences": "1736.26", "wps": "79187.3", "ups": "0.33", "wpb": "238420", "bsz": "1736.3", "num_updates": "26400", "lr": "0.0004125", "gnorm": "0.39", "loss_scale": "2", "train_wall": "286", "gb_free": "39.6", "wall": "12145"}
[2024-10-05 13:44:51,433][train_inner][INFO] - {"epoch": 56, "update": 55.564, "loss": "1.12", "ntokens": "239769", "nsentences": "1753.06", "wps": "162126", "ups": "0.68", "wpb": "239769", "bsz": "1753.1", "num_updates": "26600", "lr": "0.000415625", "gnorm": "0.381", "loss_scale": "2", "train_wall": "259", "gb_free": "39.1", "wall": "12441"}
[2024-10-05 13:49:51,695][train_inner][INFO] - {"epoch": 56, "update": 55.981, "loss": "1.126", "ntokens": "239529", "nsentences": "1743.24", "wps": "159598", "ups": "0.67", "wpb": "239529", "bsz": "1743.2", "num_updates": "26800", "lr": "0.00041875", "gnorm": "0.393", "loss_scale": "2", "train_wall": "229", "gb_free": "39.4", "wall": "12741"}
[2024-10-05 13:50:23,369][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 56 @ 26809 updates
[2024-10-05 13:50:23,369][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 13:50:30,253][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 13:50:30,257][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 56 @ 26809 updates, score None) (writing took 6.888336080126464 seconds)
[2024-10-05 13:50:30,257][fairseq_cli.train][INFO] - end of epoch 56 (average epoch stats below)
[2024-10-05 13:50:30,259][train][INFO] - {"epoch": 56, "train_loss": "1.123", "train_ntokens": "239039", "train_nsentences": "1753.71", "train_wps": "107824", "train_ups": "0.45", "train_wpb": "239039", "train_bsz": "1753.7", "train_num_updates": "26809", "train_lr": "0.000418891", "train_gnorm": "0.388", "train_loss_scale": "2", "train_train_wall": "631", "train_gb_free": "39.2", "train_wall": "12780"}
[2024-10-05 13:50:30,364][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 13:50:30,372][fairseq.trainer][INFO] - begin training epoch 57
[2024-10-05 13:50:30,372][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 13:59:48,817][train_inner][INFO] - {"epoch": 57, "update": 56.399, "loss": "1.12", "ntokens": "237973", "nsentences": "1777.92", "wps": "79708", "ups": "0.33", "wpb": "237973", "bsz": "1777.9", "num_updates": "27000", "lr": "0.000421875", "gnorm": "0.388", "loss_scale": "2", "train_wall": "271", "gb_free": "39.3", "wall": "13338"}
[2024-10-05 14:02:44,674][train_inner][INFO] - {"epoch": 57, "update": 56.816, "loss": "1.123", "ntokens": "239565", "nsentences": "1764.41", "wps": "272474", "ups": "1.14", "wpb": "239565", "bsz": "1764.4", "num_updates": "27200", "lr": "0.000425", "gnorm": "0.365", "loss_scale": "4", "train_wall": "172", "gb_free": "39.8", "wall": "13514"}
[2024-10-05 14:04:05,086][fairseq_cli.train][INFO] - end of epoch 57 (average epoch stats below)
[2024-10-05 14:04:05,171][train][INFO] - {"epoch": 57, "train_loss": "1.122", "train_ntokens": "239145", "train_nsentences": "1753.71", "train_wps": "140569", "train_ups": "0.59", "train_wpb": "239145", "train_bsz": "1753.7", "train_num_updates": "27288", "train_lr": "0.000426375", "train_gnorm": "0.382", "train_loss_scale": "4", "train_train_wall": "490", "train_gb_free": "39.7", "train_wall": "13595"}
[2024-10-05 14:04:05,330][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 14:04:05,341][fairseq.trainer][INFO] - begin training epoch 58
[2024-10-05 14:04:05,341][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 14:13:34,313][train_inner][INFO] - {"epoch": 58, "update": 57.234, "loss": "1.12", "ntokens": "238749", "nsentences": "1716.34", "wps": "73503.8", "ups": "0.31", "wpb": "238750", "bsz": "1716.3", "num_updates": "27400", "lr": "0.000428125", "gnorm": "0.394", "loss_scale": "4", "train_wall": "320", "gb_free": "39.6", "wall": "14164"}
[2024-10-05 14:17:04,502][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2024-10-05 14:20:25,542][train_inner][INFO] - {"epoch": 58, "update": 57.653, "loss": "1.121", "ntokens": "239856", "nsentences": "1757.26", "wps": "116669", "ups": "0.49", "wpb": "239856", "bsz": "1757.3", "num_updates": "27600", "lr": "0.00043125", "gnorm": "0.386", "loss_scale": "2", "train_wall": "387", "gb_free": "39.6", "wall": "14575"}
[2024-10-05 14:24:42,807][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 58 @ 27766 updates
[2024-10-05 14:24:42,813][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 14:24:49,485][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 14:24:49,506][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 58 @ 27766 updates, score None) (writing took 6.699039334431291 seconds)
[2024-10-05 14:24:49,514][fairseq_cli.train][INFO] - end of epoch 58 (average epoch stats below)
[2024-10-05 14:24:49,524][train][INFO] - {"epoch": 58, "train_loss": "1.121", "train_ntokens": "239266", "train_nsentences": "1753.13", "train_wps": "91910.8", "train_ups": "0.38", "train_wpb": "239266", "train_bsz": "1753.1", "train_num_updates": "27766", "train_lr": "0.000433844", "train_gnorm": "0.385", "train_loss_scale": "2", "train_train_wall": "845", "train_gb_free": "40.2", "train_wall": "14839"}
[2024-10-05 14:24:49,678][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 14:24:49,720][fairseq.trainer][INFO] - begin training epoch 59
[2024-10-05 14:24:49,720][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 14:31:26,233][train_inner][INFO] - {"epoch": 59, "update": 58.071, "loss": "1.123", "ntokens": "238483", "nsentences": "1766.65", "wps": "72196.1", "ups": "0.3", "wpb": "238483", "bsz": "1766.7", "num_updates": "27800", "lr": "0.000434375", "gnorm": "0.384", "loss_scale": "2", "train_wall": "323", "gb_free": "40.1", "wall": "15236"}
[2024-10-05 14:38:43,628][train_inner][INFO] - {"epoch": 59, "update": 58.489, "loss": "1.118", "ntokens": "239137", "nsentences": "1777.04", "wps": "109373", "ups": "0.46", "wpb": "239137", "bsz": "1777", "num_updates": "28000", "lr": "0.0004375", "gnorm": "0.391", "loss_scale": "2", "train_wall": "411", "gb_free": "39.8", "wall": "15673"}
[2024-10-05 14:44:45,390][train_inner][INFO] - {"epoch": 59, "update": 58.906, "loss": "1.122", "ntokens": "239905", "nsentences": "1756.62", "wps": "132654", "ups": "0.55", "wpb": "239905", "bsz": "1756.6", "num_updates": "28200", "lr": "0.000440625", "gnorm": "0.373", "loss_scale": "2", "train_wall": "270", "gb_free": "40.1", "wall": "16035"}
[2024-10-05 14:45:58,955][fairseq_cli.train][INFO] - end of epoch 59 (average epoch stats below)
[2024-10-05 14:45:58,995][train][INFO] - {"epoch": 59, "train_loss": "1.12", "train_ntokens": "239246", "train_nsentences": "1753.71", "train_wps": "90274.2", "train_ups": "0.38", "train_wpb": "239246", "train_bsz": "1753.7", "train_num_updates": "28245", "train_lr": "0.000441328", "train_gnorm": "0.38", "train_loss_scale": "2", "train_train_wall": "858", "train_gb_free": "39.6", "train_wall": "16108"}
[2024-10-05 14:45:59,170][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 14:45:59,183][fairseq.trainer][INFO] - begin training epoch 60
[2024-10-05 14:45:59,184][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 14:55:43,761][train_inner][INFO] - {"epoch": 60, "update": 59.324, "loss": "1.119", "ntokens": "239193", "nsentences": "1731.59", "wps": "72668.9", "ups": "0.3", "wpb": "239193", "bsz": "1731.6", "num_updates": "28400", "lr": "0.00044375", "gnorm": "0.369", "loss_scale": "2", "train_wall": "297", "gb_free": "39.6", "wall": "16693"}
[2024-10-05 15:00:42,520][train_inner][INFO] - {"epoch": 60, "update": 59.741, "loss": "1.118", "ntokens": "239359", "nsentences": "1776.23", "wps": "160312", "ups": "0.67", "wpb": "239359", "bsz": "1776.2", "num_updates": "28600", "lr": "0.000446875", "gnorm": "0.375", "loss_scale": "2", "train_wall": "288", "gb_free": "39.3", "wall": "16992"}
[2024-10-05 15:02:37,565][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 60 @ 28724 updates
[2024-10-05 15:02:37,571][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 15:02:49,879][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 15:02:49,882][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 60 @ 28724 updates, score None) (writing took 12.316756003536284 seconds)
[2024-10-05 15:02:49,915][fairseq_cli.train][INFO] - end of epoch 60 (average epoch stats below)
[2024-10-05 15:02:49,933][train][INFO] - {"epoch": 60, "train_loss": "1.119", "train_ntokens": "239268", "train_nsentences": "1753.71", "train_wps": "113371", "train_ups": "0.47", "train_wpb": "239268", "train_bsz": "1753.7", "train_num_updates": "28724", "train_lr": "0.000448812", "train_gnorm": "0.39", "train_loss_scale": "2", "train_train_wall": "612", "train_gb_free": "39.6", "train_wall": "17119"}
[2024-10-05 15:02:50,141][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 15:02:50,172][fairseq.trainer][INFO] - begin training epoch 61
[2024-10-05 15:02:50,172][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 15:09:23,526][train_inner][INFO] - {"epoch": 61, "update": 60.159, "loss": "1.12", "ntokens": "239196", "nsentences": "1684.65", "wps": "91821.4", "ups": "0.38", "wpb": "239196", "bsz": "1684.7", "num_updates": "28800", "lr": "0.00045", "gnorm": "0.419", "loss_scale": "2", "train_wall": "179", "gb_free": "40.3", "wall": "17513"}
[2024-10-05 15:13:09,377][train_inner][INFO] - {"epoch": 61, "update": 60.576, "loss": "1.115", "ntokens": "239694", "nsentences": "1740.75", "wps": "212263", "ups": "0.89", "wpb": "239694", "bsz": "1740.8", "num_updates": "29000", "lr": "0.000453125", "gnorm": "0.393", "loss_scale": "2", "train_wall": "216", "gb_free": "39.3", "wall": "17739"}
[2024-10-05 15:18:14,547][train_inner][INFO] - {"epoch": 61, "update": 60.994, "loss": "1.122", "ntokens": "239162", "nsentences": "1814.68", "wps": "156777", "ups": "0.66", "wpb": "239162", "bsz": "1814.7", "num_updates": "29200", "lr": "0.00045625", "gnorm": "0.354", "loss_scale": "2", "train_wall": "269", "gb_free": "39.6", "wall": "18044"}
[2024-10-05 15:18:24,686][fairseq_cli.train][INFO] - end of epoch 61 (average epoch stats below)
[2024-10-05 15:18:24,759][train][INFO] - {"epoch": 61, "train_loss": "1.118", "train_ntokens": "239207", "train_nsentences": "1753.71", "train_wps": "122570", "train_ups": "0.51", "train_wpb": "239207", "train_bsz": "1753.7", "train_num_updates": "29203", "train_lr": "0.000456297", "train_gnorm": "0.379", "train_loss_scale": "2", "train_train_wall": "574", "train_gb_free": "41.5", "train_wall": "18054"}
[2024-10-05 15:18:25,041][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 15:18:25,062][fairseq.trainer][INFO] - begin training epoch 62
[2024-10-05 15:18:25,063][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 15:29:18,639][train_inner][INFO] - {"epoch": 62, "update": 61.411, "loss": "1.115", "ntokens": "239144", "nsentences": "1719.91", "wps": "72023.1", "ups": "0.3", "wpb": "239144", "bsz": "1719.9", "num_updates": "29400", "lr": "0.000459375", "gnorm": "0.383", "loss_scale": "2", "train_wall": "337", "gb_free": "39.3", "wall": "18708"}
[2024-10-05 15:35:39,802][train_inner][INFO] - {"epoch": 62, "update": 61.829, "loss": "1.12", "ntokens": "239259", "nsentences": "1767.62", "wps": "125553", "ups": "0.52", "wpb": "239259", "bsz": "1767.6", "num_updates": "29600", "lr": "0.0004625", "gnorm": "0.381", "loss_scale": "4", "train_wall": "379", "gb_free": "39.6", "wall": "19089"}
[2024-10-05 15:36:15,540][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2024-10-05 15:37:43,290][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 62 @ 29681 updates
[2024-10-05 15:37:43,296][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 15:37:51,497][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 15:37:51,500][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 62 @ 29681 updates, score None) (writing took 8.209543839097023 seconds)
[2024-10-05 15:37:51,501][fairseq_cli.train][INFO] - end of epoch 62 (average epoch stats below)
[2024-10-05 15:37:51,502][train][INFO] - {"epoch": 62, "train_loss": "1.119", "train_ntokens": "239199", "train_nsentences": "1753.37", "train_wps": "97997.2", "train_ups": "0.41", "train_wpb": "239199", "train_bsz": "1753.4", "train_num_updates": "29681", "train_lr": "0.000463766", "train_gnorm": "0.381", "train_loss_scale": "2", "train_train_wall": "828", "train_gb_free": "39.3", "train_wall": "19221"}
[2024-10-05 15:37:51,626][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 15:37:51,650][fairseq.trainer][INFO] - begin training epoch 63
[2024-10-05 15:37:51,651][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 15:44:36,145][train_inner][INFO] - {"epoch": 63, "update": 62.248, "loss": "1.118", "ntokens": "238763", "nsentences": "1742.06", "wps": "89041.7", "ups": "0.37", "wpb": "238763", "bsz": "1742.1", "num_updates": "29800", "lr": "0.000465625", "gnorm": "0.372", "loss_scale": "2", "train_wall": "239", "gb_free": "39.2", "wall": "19626"}
[2024-10-05 15:47:35,616][train_inner][INFO] - {"epoch": 63, "update": 62.666, "loss": "1.118", "ntokens": "238808", "nsentences": "1787.53", "wps": "266132", "ups": "1.11", "wpb": "238808", "bsz": "1787.5", "num_updates": "30000", "lr": "0.00046875", "gnorm": "0.37", "loss_scale": "2", "train_wall": "175", "gb_free": "39.3", "wall": "19805"}
[2024-10-05 15:50:34,858][fairseq_cli.train][INFO] - end of epoch 63 (average epoch stats below)
[2024-10-05 15:50:34,898][train][INFO] - {"epoch": 63, "train_loss": "1.117", "train_ntokens": "239170", "train_nsentences": "1753.71", "train_wps": "150071", "train_ups": "0.63", "train_wpb": "239170", "train_bsz": "1753.7", "train_num_updates": "30160", "train_lr": "0.00047125", "train_gnorm": "0.362", "train_loss_scale": "2", "train_train_wall": "463", "train_gb_free": "39.6", "train_wall": "19984"}
[2024-10-05 15:50:35,158][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 15:50:35,176][fairseq.trainer][INFO] - begin training epoch 64
[2024-10-05 15:50:35,177][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 15:56:12,020][train_inner][INFO] - {"epoch": 64, "update": 63.084, "loss": "1.118", "ntokens": "238929", "nsentences": "1736.65", "wps": "92536.5", "ups": "0.39", "wpb": "238929", "bsz": "1736.7", "num_updates": "30200", "lr": "0.000471875", "gnorm": "0.349", "loss_scale": "2", "train_wall": "229", "gb_free": "40.2", "wall": "20321"}
[2024-10-05 15:58:56,648][train_inner][INFO] - {"epoch": 64, "update": 63.501, "loss": "1.113", "ntokens": "239934", "nsentences": "1749.68", "wps": "291494", "ups": "1.21", "wpb": "239934", "bsz": "1749.7", "num_updates": "30400", "lr": "0.000475", "gnorm": "0.368", "loss_scale": "2", "train_wall": "161", "gb_free": "39.6", "wall": "20486"}
[2024-10-05 16:01:56,320][train_inner][INFO] - {"epoch": 64, "update": 63.919, "loss": "1.12", "ntokens": "239410", "nsentences": "1781.88", "wps": "266500", "ups": "1.11", "wpb": "239410", "bsz": "1781.9", "num_updates": "30600", "lr": "0.000478125", "gnorm": "0.369", "loss_scale": "2", "train_wall": "175", "gb_free": "39.7", "wall": "20666"}
[2024-10-05 16:02:38,664][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 64 @ 30639 updates
[2024-10-05 16:02:38,665][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 16:02:49,932][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 16:02:49,962][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 64 @ 30639 updates, score None) (writing took 11.297973979264498 seconds)
[2024-10-05 16:02:49,978][fairseq_cli.train][INFO] - end of epoch 64 (average epoch stats below)
[2024-10-05 16:02:49,980][train][INFO] - {"epoch": 64, "train_loss": "1.117", "train_ntokens": "239270", "train_nsentences": "1753.71", "train_wps": "155916", "train_ups": "0.65", "train_wpb": "239270", "train_bsz": "1753.7", "train_num_updates": "30639", "train_lr": "0.000478734", "train_gnorm": "0.366", "train_loss_scale": "2", "train_train_wall": "435", "train_gb_free": "39.6", "train_wall": "20719"}
[2024-10-05 16:02:50,052][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 16:02:50,069][fairseq.trainer][INFO] - begin training epoch 65
[2024-10-05 16:02:50,070][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 16:09:59,422][train_inner][INFO] - {"epoch": 65, "update": 64.336, "loss": "1.117", "ntokens": "238647", "nsentences": "1750.89", "wps": "98798.3", "ups": "0.41", "wpb": "238647", "bsz": "1750.9", "num_updates": "30800", "lr": "0.00048125", "gnorm": "0.385", "loss_scale": "2", "train_wall": "203", "gb_free": "40.1", "wall": "21149"}
[2024-10-05 16:13:00,591][train_inner][INFO] - {"epoch": 65, "update": 64.754, "loss": "1.116", "ntokens": "239814", "nsentences": "1737.63", "wps": "264749", "ups": "1.1", "wpb": "239814", "bsz": "1737.6", "num_updates": "31000", "lr": "0.000484375", "gnorm": "0.359", "loss_scale": "2", "train_wall": "177", "gb_free": "39.6", "wall": "21330"}
[2024-10-05 16:14:52,745][fairseq_cli.train][INFO] - end of epoch 65 (average epoch stats below)
[2024-10-05 16:14:52,767][train][INFO] - {"epoch": 65, "train_loss": "1.117", "train_ntokens": "239294", "train_nsentences": "1753.71", "train_wps": "158585", "train_ups": "0.66", "train_wpb": "239294", "train_bsz": "1753.7", "train_num_updates": "31118", "train_lr": "0.000486219", "train_gnorm": "0.368", "train_loss_scale": "2", "train_train_wall": "447", "train_gb_free": "39.8", "train_wall": "21442"}
[2024-10-05 16:14:52,941][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 16:14:52,992][fairseq.trainer][INFO] - begin training epoch 66
[2024-10-05 16:14:52,993][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 16:20:56,243][train_inner][INFO] - {"epoch": 66, "update": 65.171, "loss": "1.12", "ntokens": "238513", "nsentences": "1800.12", "wps": "100290", "ups": "0.42", "wpb": "238513", "bsz": "1800.1", "num_updates": "31200", "lr": "0.0004875", "gnorm": "0.359", "loss_scale": "2", "train_wall": "189", "gb_free": "40.1", "wall": "21806"}
[2024-10-05 16:23:52,668][train_inner][INFO] - {"epoch": 66, "update": 65.589, "loss": "1.115", "ntokens": "240208", "nsentences": "1736.87", "wps": "272328", "ups": "1.13", "wpb": "240208", "bsz": "1736.9", "num_updates": "31400", "lr": "0.000490625", "gnorm": "0.35", "loss_scale": "2", "train_wall": "173", "gb_free": "39.7", "wall": "21982"}
[2024-10-05 16:27:27,615][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 66 @ 31597 updates
[2024-10-05 16:27:27,616][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 16:27:30,697][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 16:27:30,700][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 66 @ 31597 updates, score None) (writing took 3.084854781627655 seconds)
[2024-10-05 16:27:30,701][fairseq_cli.train][INFO] - end of epoch 66 (average epoch stats below)
[2024-10-05 16:27:30,703][train][INFO] - {"epoch": 66, "train_loss": "1.118", "train_ntokens": "239643", "train_nsentences": "1753.71", "train_wps": "151450", "train_ups": "0.63", "train_wpb": "239643", "train_bsz": "1753.7", "train_num_updates": "31597", "train_lr": "0.000493703", "train_gnorm": "0.356", "train_loss_scale": "2", "train_train_wall": "464", "train_gb_free": "39.2", "train_wall": "22200"}
[2024-10-05 16:27:30,744][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 16:27:30,748][fairseq.trainer][INFO] - begin training epoch 67
[2024-10-05 16:27:30,748][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 16:32:42,225][train_inner][INFO] - {"epoch": 67, "update": 66.006, "loss": "1.12", "ntokens": "239457", "nsentences": "1717.6", "wps": "90437.5", "ups": "0.38", "wpb": "239457", "bsz": "1717.6", "num_updates": "31600", "lr": "0.00049375", "gnorm": "0.358", "loss_scale": "2", "train_wall": "216", "gb_free": "39.8", "wall": "22512"}
[2024-10-05 16:38:22,643][train_inner][INFO] - {"epoch": 67, "update": 66.424, "loss": "1.113", "ntokens": "239641", "nsentences": "1767.88", "wps": "140824", "ups": "0.59", "wpb": "239641", "bsz": "1767.9", "num_updates": "31800", "lr": "0.000496875", "gnorm": "0.352", "loss_scale": "4", "train_wall": "247", "gb_free": "39.6", "wall": "22852"}
[2024-10-05 16:43:23,586][train_inner][INFO] - {"epoch": 67, "update": 66.841, "loss": "1.119", "ntokens": "240205", "nsentences": "1758.47", "wps": "159637", "ups": "0.66", "wpb": "240205", "bsz": "1758.5", "num_updates": "32000", "lr": "0.0005", "gnorm": "0.343", "loss_scale": "4", "train_wall": "269", "gb_free": "39.6", "wall": "23153"}
[2024-10-05 16:44:35,187][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2024-10-05 16:44:58,410][fairseq_cli.train][INFO] - end of epoch 67 (average epoch stats below)
[2024-10-05 16:44:58,446][train][INFO] - {"epoch": 67, "train_loss": "1.116", "train_ntokens": "239554", "train_nsentences": "1754.05", "train_wps": "109290", "train_ups": "0.46", "train_wpb": "239554", "train_bsz": "1754", "train_num_updates": "32075", "train_lr": "0.000499898", "train_gnorm": "0.35", "train_loss_scale": "2", "train_train_wall": "600", "train_gb_free": "39.2", "train_wall": "23248"}
[2024-10-05 16:44:58,700][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 16:44:58,714][fairseq.trainer][INFO] - begin training epoch 68
[2024-10-05 16:44:58,714][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 16:52:17,944][train_inner][INFO] - {"epoch": 68, "update": 67.261, "loss": "1.114", "ntokens": "239261", "nsentences": "1719.14", "wps": "89558.2", "ups": "0.37", "wpb": "239261", "bsz": "1719.1", "num_updates": "32200", "lr": "0.000499728", "gnorm": "0.343", "loss_scale": "2", "train_wall": "194", "gb_free": "39.8", "wall": "23687"}
[2024-10-05 16:55:10,163][train_inner][INFO] - {"epoch": 68, "update": 67.678, "loss": "1.112", "ntokens": "239422", "nsentences": "1768.86", "wps": "278162", "ups": "1.16", "wpb": "239422", "bsz": "1768.9", "num_updates": "32400", "lr": "0.000499457", "gnorm": "0.336", "loss_scale": "2", "train_wall": "159", "gb_free": "39.3", "wall": "23860"}
[2024-10-05 16:58:48,352][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 68 @ 32554 updates
[2024-10-05 16:58:48,463][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 16:59:08,724][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 16:59:08,730][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 68 @ 32554 updates, score None) (writing took 20.377610492520034 seconds)
[2024-10-05 16:59:08,731][fairseq_cli.train][INFO] - end of epoch 68 (average epoch stats below)
[2024-10-05 16:59:08,733][train][INFO] - {"epoch": 68, "train_loss": "1.114", "train_ntokens": "239287", "train_nsentences": "1753.71", "train_wps": "134801", "train_ups": "0.56", "train_wpb": "239287", "train_bsz": "1753.7", "train_num_updates": "32554", "train_lr": "0.000499247", "train_gnorm": "0.338", "train_loss_scale": "2", "train_train_wall": "442", "train_gb_free": "39.7", "train_wall": "24098"}
[2024-10-05 16:59:08,857][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 16:59:08,888][fairseq.trainer][INFO] - begin training epoch 69
[2024-10-05 16:59:08,889][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 17:04:42,408][train_inner][INFO] - {"epoch": 69, "update": 68.096, "loss": "1.119", "ntokens": "238560", "nsentences": "1763.76", "wps": "83389.2", "ups": "0.35", "wpb": "238560", "bsz": "1763.8", "num_updates": "32600", "lr": "0.000499185", "gnorm": "0.348", "loss_scale": "2", "train_wall": "217", "gb_free": "39.6", "wall": "24432"}
[2024-10-05 17:09:01,416][train_inner][INFO] - {"epoch": 69, "update": 68.514, "loss": "1.109", "ntokens": "238712", "nsentences": "1774.33", "wps": "184389", "ups": "0.77", "wpb": "238712", "bsz": "1774.3", "num_updates": "32800", "lr": "0.000498913", "gnorm": "0.342", "loss_scale": "2", "train_wall": "256", "gb_free": "39.8", "wall": "24691"}
[2024-10-05 17:12:24,804][train_inner][INFO] - {"epoch": 69, "update": 68.931, "loss": "1.116", "ntokens": "240559", "nsentences": "1729.59", "wps": "236640", "ups": "0.98", "wpb": "240559", "bsz": "1729.6", "num_updates": "33000", "lr": "0.000498641", "gnorm": "0.368", "loss_scale": "2", "train_wall": "193", "gb_free": "39.3", "wall": "24894"}
[2024-10-05 17:13:16,776][fairseq_cli.train][INFO] - end of epoch 69 (average epoch stats below)
[2024-10-05 17:13:16,835][train][INFO] - {"epoch": 69, "train_loss": "1.113", "train_ntokens": "239342", "train_nsentences": "1753.71", "train_wps": "135182", "train_ups": "0.56", "train_wpb": "239342", "train_bsz": "1753.7", "train_num_updates": "33033", "train_lr": "0.000498596", "train_gnorm": "0.351", "train_loss_scale": "2", "train_train_wall": "544", "train_gb_free": "39.6", "train_wall": "24946"}
[2024-10-05 17:13:17,050][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 17:13:17,061][fairseq.trainer][INFO] - begin training epoch 70
[2024-10-05 17:13:17,062][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 17:23:22,152][train_inner][INFO] - {"epoch": 70, "update": 69.349, "loss": "1.11", "ntokens": "239248", "nsentences": "1759.8", "wps": "72793.1", "ups": "0.3", "wpb": "239248", "bsz": "1759.8", "num_updates": "33200", "lr": "0.00049837", "gnorm": "0.319", "loss_scale": "2", "train_wall": "371", "gb_free": "39.6", "wall": "25552"}
[2024-10-05 17:26:58,283][train_inner][INFO] - {"epoch": 70, "update": 69.766, "loss": "1.11", "ntokens": "239600", "nsentences": "1749.29", "wps": "221732", "ups": "0.93", "wpb": "239600", "bsz": "1749.3", "num_updates": "33400", "lr": "0.000498098", "gnorm": "0.345", "loss_scale": "2", "train_wall": "202", "gb_free": "40.5", "wall": "25768"}
[2024-10-05 17:28:47,343][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 70 @ 33512 updates
[2024-10-05 17:28:47,451][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 17:29:04,943][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 17:29:04,948][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 70 @ 33512 updates, score None) (writing took 17.605012647807598 seconds)
[2024-10-05 17:29:04,948][fairseq_cli.train][INFO] - end of epoch 70 (average epoch stats below)
[2024-10-05 17:29:04,952][train][INFO] - {"epoch": 70, "train_loss": "1.111", "train_ntokens": "239361", "train_nsentences": "1753.71", "train_wps": "120929", "train_ups": "0.51", "train_wpb": "239361", "train_bsz": "1753.7", "train_num_updates": "33512", "train_lr": "0.000497946", "train_gnorm": "0.332", "train_loss_scale": "2", "train_train_wall": "630", "train_gb_free": "40.1", "train_wall": "25894"}
[2024-10-05 17:29:05,082][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 17:29:05,109][fairseq.trainer][INFO] - begin training epoch 71
[2024-10-05 17:29:05,110][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 17:34:49,776][train_inner][INFO] - {"epoch": 71, "update": 70.184, "loss": "1.111", "ntokens": "238988", "nsentences": "1727.41", "wps": "101389", "ups": "0.42", "wpb": "238988", "bsz": "1727.4", "num_updates": "33600", "lr": "0.000497826", "gnorm": "0.328", "loss_scale": "2", "train_wall": "177", "gb_free": "39.6", "wall": "26239"}
[2024-10-05 17:37:55,794][train_inner][INFO] - {"epoch": 71, "update": 70.601, "loss": "1.109", "ntokens": "239736", "nsentences": "1765.97", "wps": "257764", "ups": "1.08", "wpb": "239736", "bsz": "1766", "num_updates": "33800", "lr": "0.000497554", "gnorm": "0.323", "loss_scale": "2", "train_wall": "181", "gb_free": "39.3", "wall": "26425"}
[2024-10-05 17:41:21,329][fairseq_cli.train][INFO] - end of epoch 71 (average epoch stats below)
[2024-10-05 17:41:21,499][train][INFO] - {"epoch": 71, "train_loss": "1.109", "train_ntokens": "239402", "train_nsentences": "1753.71", "train_wps": "155694", "train_ups": "0.65", "train_wpb": "239402", "train_bsz": "1753.7", "train_num_updates": "33991", "train_lr": "0.000497295", "train_gnorm": "0.341", "train_loss_scale": "2", "train_train_wall": "404", "train_gb_free": "39.3", "train_wall": "26631"}
[2024-10-05 17:41:21,710][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 17:41:21,756][fairseq.trainer][INFO] - begin training epoch 72
[2024-10-05 17:41:21,757][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 17:46:27,631][train_inner][INFO] - {"epoch": 72, "update": 71.019, "loss": "1.112", "ntokens": "238788", "nsentences": "1764.02", "wps": "93307.6", "ups": "0.39", "wpb": "238788", "bsz": "1764", "num_updates": "34000", "lr": "0.000497283", "gnorm": "0.364", "loss_scale": "2", "train_wall": "166", "gb_free": "39.4", "wall": "26937"}
[2024-10-05 17:51:08,619][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2024-10-05 17:52:10,060][train_inner][INFO] - {"epoch": 72, "update": 71.438, "loss": "1.103", "ntokens": "238771", "nsentences": "1778.28", "wps": "139488", "ups": "0.58", "wpb": "238771", "bsz": "1778.3", "num_updates": "34200", "lr": "0.000497011", "gnorm": "0.353", "loss_scale": "2", "train_wall": "254", "gb_free": "40.8", "wall": "27279"}
[2024-10-05 17:56:06,377][train_inner][INFO] - {"epoch": 72, "update": 71.856, "loss": "1.106", "ntokens": "239609", "nsentences": "1755.86", "wps": "202796", "ups": "0.85", "wpb": "239609", "bsz": "1755.9", "num_updates": "34400", "lr": "0.000496739", "gnorm": "0.327", "loss_scale": "2", "train_wall": "127", "gb_free": "39.6", "wall": "27516"}
[2024-10-05 17:57:28,567][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 72 @ 34469 updates
[2024-10-05 17:57:28,587][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 17:57:45,237][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-05 17:57:45,242][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 72 @ 34469 updates, score None) (writing took 16.674859394319355 seconds)
[2024-10-05 17:57:45,243][fairseq_cli.train][INFO] - end of epoch 72 (average epoch stats below)
[2024-10-05 17:57:45,245][train][INFO] - {"epoch": 72, "train_loss": "1.106", "train_ntokens": "238942", "train_nsentences": "1754.72", "train_wps": "116102", "train_ups": "0.49", "train_wpb": "238942", "train_bsz": "1754.7", "train_num_updates": "34469", "train_lr": "0.000496645", "train_gnorm": "0.351", "train_loss_scale": "2", "train_train_wall": "464", "train_gb_free": "39.3", "train_wall": "27615"}
[2024-10-05 17:57:45,356][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 17:57:45,387][fairseq.trainer][INFO] - begin training epoch 73
[2024-10-05 17:57:45,388][fairseq_cli.train][INFO] - Start iterating over samples
