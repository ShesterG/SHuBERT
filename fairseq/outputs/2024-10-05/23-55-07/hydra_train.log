[2024-10-05 23:55:46,169][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:13253', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'channel'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_800k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-05 23:55:48,984][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:11721', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'channel'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_800k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-05 23:55:49,548][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-05 23:55:49,550][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-05 23:55:49,550][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-05 23:55:49,550][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-05 23:55:49,551][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-05 23:55:49,566][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-05 23:55:50,569][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:15738', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'channel'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_800k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-05 23:55:50,901][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:16219', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'channel'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_800k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-05 23:55:51,373][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:16769', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'channel'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_800k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-05 23:55:51,396][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:19446', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'channel'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_800k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-05 23:55:51,761][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:10828', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'channel'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_800k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-05 23:55:52,489][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:19081', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'channel'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_800k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-05 23:55:53,575][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-05 23:55:53,577][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-05 23:55:53,577][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-05 23:55:53,577][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-05 23:55:53,578][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-05 23:55:53,579][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-05 23:55:55,461][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-05 23:55:55,463][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-05 23:55:55,483][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-05 23:55:55,483][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-05 23:55:55,490][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-05 23:55:55,491][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-05 23:55:55,572][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-05 23:55:55,578][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-05 23:55:55,578][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-05 23:55:55,578][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-05 23:55:55,579][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-05 23:55:55,580][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-05 23:55:59,034][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-05 23:55:59,321][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-05 23:55:59,861][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-05 23:55:59,874][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-05 23:55:59,874][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-05 23:55:59,874][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-05 23:55:59,875][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-05 23:55:59,876][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-05 23:56:01,546][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-05 23:56:01,549][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-05 23:56:01,549][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-05 23:56:01,549][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-05 23:56:01,612][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-05 23:56:01,612][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-05 23:56:02,158][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-05 23:56:04,927][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-05 23:56:06,324][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-05 23:56:09,405][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-05 23:56:09,440][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-05 23:56:09,445][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-05 23:56:09,446][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-05 23:56:09,447][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-05 23:56:09,447][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-05 23:56:14,889][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-05 23:56:15,037][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-05 23:56:15,038][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-05 23:56:15,038][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-05 23:56:15,039][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-05 23:56:15,039][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-05 23:56:18,772][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-05 23:56:51,212][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-05 23:56:59,706][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-05 23:59:38,478][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-05 23:59:38,484][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 23:59:38,484][fairseq.utils][INFO] - rank   1: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 23:59:38,484][fairseq.utils][INFO] - rank   2: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 23:59:38,484][fairseq.utils][INFO] - rank   3: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 23:59:38,484][fairseq.utils][INFO] - rank   4: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 23:59:38,484][fairseq.utils][INFO] - rank   5: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 23:59:38,484][fairseq.utils][INFO] - rank   6: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 23:59:38,484][fairseq.utils][INFO] - rank   7: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 23:59:38,484][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-05 23:59:38,485][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-05 23:59:38,486][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-05 23:59:38,487][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-06 00:00:16,641][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt (epoch 53 @ 24892 updates)
[2024-10-06 00:00:16,653][fairseq.trainer][INFO] - loading train data for epoch 53
[2024-10-06 00:00:21,512][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-06 00:00:51,170][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 00:00:51,170][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 00:00:51,170][fairseq.utils][INFO] - rank   1: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 00:00:51,170][fairseq.utils][INFO] - rank   2: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 00:00:51,170][fairseq.utils][INFO] - rank   3: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 00:00:51,170][fairseq.utils][INFO] - rank   4: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 00:00:51,170][fairseq.utils][INFO] - rank   5: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 00:00:51,170][fairseq.utils][INFO] - rank   6: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 00:00:51,170][fairseq.utils][INFO] - rank   7: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 00:00:51,170][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 00:00:51,170][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-06 00:00:51,171][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-06 00:00:51,171][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-06 00:01:02,083][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 00:01:02,083][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 00:01:02,083][fairseq.utils][INFO] - rank   1: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 00:01:02,083][fairseq.utils][INFO] - rank   2: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 00:01:02,083][fairseq.utils][INFO] - rank   3: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 00:01:02,083][fairseq.utils][INFO] - rank   4: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 00:01:02,083][fairseq.utils][INFO] - rank   5: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 00:01:02,084][fairseq.utils][INFO] - rank   6: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 00:01:02,084][fairseq.utils][INFO] - rank   7: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 00:01:02,084][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 00:01:02,084][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-06 00:01:02,084][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-06 00:01:02,085][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-06 00:01:03,459][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 00:01:03,475][fairseq.trainer][INFO] - begin training epoch 53
[2024-10-06 00:01:03,476][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 00:02:20,038][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt (epoch 53 @ 24892 updates)
[2024-10-06 00:02:20,112][fairseq.trainer][INFO] - loading train data for epoch 53
[2024-10-06 00:02:31,565][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt (epoch 53 @ 24892 updates)
[2024-10-06 00:02:31,611][fairseq.trainer][INFO] - loading train data for epoch 53
[2024-10-06 00:02:41,892][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-06 00:02:46,958][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-06 00:05:36,033][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 00:05:36,040][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 00:05:36,040][fairseq.utils][INFO] - rank   1: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 00:05:36,040][fairseq.utils][INFO] - rank   2: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 00:05:36,040][fairseq.utils][INFO] - rank   3: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 00:05:36,040][fairseq.utils][INFO] - rank   4: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 00:05:36,040][fairseq.utils][INFO] - rank   5: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 00:05:36,040][fairseq.utils][INFO] - rank   6: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 00:05:36,041][fairseq.utils][INFO] - rank   7: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 00:05:36,041][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 00:05:36,041][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-06 00:05:36,046][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-06 00:05:36,048][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-06 00:06:07,783][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt (epoch 53 @ 24892 updates)
[2024-10-06 00:06:07,828][fairseq.trainer][INFO] - loading train data for epoch 53
[2024-10-06 00:06:08,371][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 00:06:08,372][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 00:06:08,372][fairseq.utils][INFO] - rank   1: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 00:06:08,372][fairseq.utils][INFO] - rank   2: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 00:06:08,372][fairseq.utils][INFO] - rank   3: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 00:06:08,372][fairseq.utils][INFO] - rank   4: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 00:06:08,372][fairseq.utils][INFO] - rank   5: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 00:06:08,372][fairseq.utils][INFO] - rank   6: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 00:06:08,372][fairseq.utils][INFO] - rank   7: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 00:06:08,372][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 00:06:08,372][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-06 00:06:08,373][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-06 00:06:08,373][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-06 00:06:14,485][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-06 00:06:21,179][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 00:06:21,179][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 00:06:21,179][fairseq.utils][INFO] - rank   1: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 00:06:21,179][fairseq.utils][INFO] - rank   2: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 00:06:21,179][fairseq.utils][INFO] - rank   3: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 00:06:21,179][fairseq.utils][INFO] - rank   4: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 00:06:21,179][fairseq.utils][INFO] - rank   5: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 00:06:21,179][fairseq.utils][INFO] - rank   6: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 00:06:21,179][fairseq.utils][INFO] - rank   7: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 00:06:21,179][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 00:06:21,179][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-06 00:06:21,180][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-06 00:06:21,180][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-06 00:06:25,359][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 00:06:25,360][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 00:06:25,360][fairseq.utils][INFO] - rank   1: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 00:06:25,360][fairseq.utils][INFO] - rank   2: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 00:06:25,360][fairseq.utils][INFO] - rank   3: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 00:06:25,360][fairseq.utils][INFO] - rank   4: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 00:06:25,360][fairseq.utils][INFO] - rank   5: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 00:06:25,360][fairseq.utils][INFO] - rank   6: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 00:06:25,360][fairseq.utils][INFO] - rank   7: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 00:06:25,360][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 00:06:25,361][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-06 00:06:25,361][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-06 00:06:25,362][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-06 00:06:33,373][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 00:06:33,500][fairseq.trainer][INFO] - begin training epoch 53
[2024-10-06 00:06:33,501][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 00:06:47,653][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 00:06:47,654][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 00:06:47,654][fairseq.utils][INFO] - rank   1: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 00:06:47,658][fairseq.utils][INFO] - rank   2: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 00:06:47,658][fairseq.utils][INFO] - rank   3: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 00:06:47,658][fairseq.utils][INFO] - rank   4: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 00:06:47,658][fairseq.utils][INFO] - rank   5: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 00:06:47,658][fairseq.utils][INFO] - rank   6: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 00:06:47,658][fairseq.utils][INFO] - rank   7: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 00:06:47,658][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 00:06:47,658][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-06 00:06:47,659][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-06 00:06:47,660][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-06 00:06:52,711][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt (epoch 53 @ 24892 updates)
[2024-10-06 00:06:52,713][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 00:06:52,760][fairseq.trainer][INFO] - begin training epoch 53
[2024-10-06 00:06:52,760][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 00:06:53,008][fairseq.trainer][INFO] - loading train data for epoch 53
[2024-10-06 00:06:56,122][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt (epoch 53 @ 24892 updates)
[2024-10-06 00:06:56,528][fairseq.trainer][INFO] - loading train data for epoch 53
[2024-10-06 00:07:03,667][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-06 00:07:07,464][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-06 00:07:08,014][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt (epoch 53 @ 24892 updates)
[2024-10-06 00:07:08,182][fairseq.trainer][INFO] - loading train data for epoch 53
[2024-10-06 00:07:14,466][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-06 00:07:31,723][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt (epoch 53 @ 24892 updates)
[2024-10-06 00:07:32,068][fairseq.trainer][INFO] - loading train data for epoch 53
[2024-10-06 00:07:32,180][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 00:07:32,191][fairseq.trainer][INFO] - begin training epoch 53
[2024-10-06 00:07:32,191][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 00:07:36,594][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-06 00:08:40,413][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 00:08:40,419][fairseq.trainer][INFO] - begin training epoch 53
[2024-10-06 00:08:40,420][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 00:08:42,369][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 00:08:42,387][fairseq.trainer][INFO] - begin training epoch 53
[2024-10-06 00:08:42,388][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 00:08:48,258][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 00:08:48,264][fairseq.trainer][INFO] - begin training epoch 53
[2024-10-06 00:08:48,265][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 00:08:49,013][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 00:08:49,031][fairseq.trainer][INFO] - begin training epoch 53
[2024-10-06 00:08:49,031][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 00:38:22,951][train_inner][INFO] - {"epoch": 53, "update": 52.225, "loss": "2.547", "ntokens": "358290", "nsentences": "1746.35", "wps": "87463.3", "ups": "0.24", "wpb": "358290", "bsz": "1746.4", "num_updates": "25000", "lr": "0.000390625", "gnorm": "1.295", "loss_scale": "0.25", "train_wall": "204", "gb_free": "39.8", "wall": "2251"}
[2024-10-06 00:38:23,393][train_inner][INFO] - {"epoch": 53, "update": 52.225, "loss": "2.547", "ntokens": "358290", "nsentences": "1746.35", "wps": "87329.5", "ups": "0.24", "wpb": "358290", "bsz": "1746.4", "num_updates": "25000", "lr": "0.000390625", "gnorm": "1.291", "loss_scale": "0.25", "train_wall": "204", "gb_free": "39.8", "wall": "1967"}
[2024-10-06 00:54:34,301][train_inner][INFO] - {"epoch": 53, "update": 52.643, "loss": "2.554", "ntokens": "358383", "nsentences": "1748.95", "wps": "73797.6", "ups": "0.21", "wpb": "358383", "bsz": "1749", "num_updates": "25200", "lr": "0.00039375", "gnorm": "1.296", "loss_scale": "0.25", "train_wall": "880", "gb_free": "39.3", "wall": "3223"}
[2024-10-06 00:54:39,698][train_inner][INFO] - {"epoch": 53, "update": 52.643, "loss": "2.554", "ntokens": "358383", "nsentences": "1748.95", "wps": "73416.8", "ups": "0.2", "wpb": "358383", "bsz": "1749", "num_updates": "25200", "lr": "0.00039375", "gnorm": "1.295", "loss_scale": "0.25", "train_wall": "950", "gb_free": "39.3", "wall": "2944"}
[2024-10-06 01:08:45,339][fairseq_cli.train][INFO] - end of epoch 53 (average epoch stats below)
[2024-10-06 01:08:45,381][train][INFO] - {"epoch": 53, "train_loss": "2.558", "train_ntokens": "357674", "train_nsentences": "1753.71", "train_wps": "75613.8", "train_ups": "0.21", "train_wpb": "357674", "train_bsz": "1753.7", "train_num_updates": "25371", "train_lr": "0.000396422", "train_gnorm": "1.316", "train_loss_scale": "0.25", "train_train_wall": "1700", "train_gb_free": "40.5", "train_wall": "4074"}
[2024-10-06 01:08:48,440][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 01:08:48,470][fairseq.trainer][INFO] - begin training epoch 54
[2024-10-06 01:08:48,470][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 01:09:02,667][fairseq_cli.train][INFO] - end of epoch 53 (average epoch stats below)
[2024-10-06 01:09:02,673][train][INFO] - {"epoch": 53, "train_loss": "2.558", "train_ntokens": "357674", "train_nsentences": "1753.71", "train_wps": "75040.2", "train_ups": "0.21", "train_wpb": "357674", "train_bsz": "1753.7", "train_num_updates": "25371", "train_lr": "0.000396422", "train_gnorm": "1.336", "train_loss_scale": "0.25", "train_train_wall": "1884", "train_gb_free": "40.5", "train_wall": "3807"}
[2024-10-06 01:09:02,792][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 01:09:02,798][fairseq.trainer][INFO] - begin training epoch 54
[2024-10-06 01:09:02,798][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 01:21:17,460][train_inner][INFO] - {"epoch": 54, "update": 53.061, "loss": "2.566", "ntokens": "356604", "nsentences": "1765.25", "wps": "44490.1", "ups": "0.12", "wpb": "356604", "bsz": "1765.2", "num_updates": "25400", "lr": "0.000396875", "gnorm": "1.363", "loss_scale": "0.25", "train_wall": "897", "gb_free": "39.2", "wall": "4826"}
[2024-10-06 01:21:17,699][train_inner][INFO] - {"epoch": 54, "update": 53.061, "loss": "2.567", "ntokens": "356604", "nsentences": "1765.25", "wps": "44631.4", "ups": "0.13", "wpb": "356604", "bsz": "1765.2", "num_updates": "25400", "lr": "0.000396875", "gnorm": "1.408", "loss_scale": "0.25", "train_wall": "1013", "gb_free": "39.2", "wall": "4542"}
[2024-10-06 01:41:27,912][train_inner][INFO] - {"epoch": 54, "update": 53.478, "loss": "2.54", "ntokens": "358403", "nsentences": "1729.11", "wps": "59222.4", "ups": "0.17", "wpb": "358403", "bsz": "1729.1", "num_updates": "25600", "lr": "0.0004", "gnorm": "1.363", "loss_scale": "0.25", "train_wall": "1169", "gb_free": "39.6", "wall": "6037"}
[2024-10-06 01:41:33,949][train_inner][INFO] - {"epoch": 54, "update": 53.478, "loss": "2.54", "ntokens": "358403", "nsentences": "1729.11", "wps": "58936.5", "ups": "0.16", "wpb": "358403", "bsz": "1729.1", "num_updates": "25600", "lr": "0.0004", "gnorm": "1.37", "loss_scale": "0.25", "train_wall": "1166", "gb_free": "39.6", "wall": "5758"}
[2024-10-06 01:56:13,140][train_inner][INFO] - {"epoch": 54, "update": 53.896, "loss": "2.563", "ntokens": "358373", "nsentences": "1782.35", "wps": "80976.6", "ups": "0.23", "wpb": "358373", "bsz": "1782.4", "num_updates": "25800", "lr": "0.000403125", "gnorm": "1.348", "loss_scale": "0.25", "train_wall": "837", "gb_free": "40.5", "wall": "6922"}
[2024-10-06 01:56:13,338][train_inner][INFO] - {"epoch": 54, "update": 53.896, "loss": "2.566", "ntokens": "358373", "nsentences": "1782.35", "wps": "81505.8", "ups": "0.23", "wpb": "358373", "bsz": "1782.4", "num_updates": "25800", "lr": "0.000403125", "gnorm": "1.378", "loss_scale": "0.25", "train_wall": "831", "gb_free": "40.5", "wall": "6637"}
[2024-10-06 01:58:35,738][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 54 @ 25850 updates
[2024-10-06 01:58:35,742][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-06 01:58:38,691][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 54 @ 25850 updates
[2024-10-06 01:58:38,698][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-06 01:58:45,309][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-06 01:58:45,312][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt (epoch 54 @ 25850 updates, score None) (writing took 9.574058815836906 seconds)
[2024-10-06 01:58:45,313][fairseq_cli.train][INFO] - end of epoch 54 (average epoch stats below)
[2024-10-06 01:58:45,330][train][INFO] - {"epoch": 54, "train_loss": "2.554", "train_ntokens": "357674", "train_nsentences": "1753.71", "train_wps": "57109.9", "train_ups": "0.16", "train_wpb": "357674", "train_bsz": "1753.7", "train_num_updates": "25850", "train_lr": "0.000403906", "train_gnorm": "1.375", "train_loss_scale": "0.25", "train_train_wall": "2429", "train_gb_free": "39.3", "train_wall": "7074"}
[2024-10-06 01:58:45,455][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 01:58:45,496][fairseq.trainer][INFO] - begin training epoch 55
[2024-10-06 01:58:45,497][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 02:19:33,209][train_inner][INFO] - {"epoch": 55, "update": 54.313, "loss": "2.555", "ntokens": "356844", "nsentences": "1751.24", "wps": "50976.7", "ups": "0.14", "wpb": "356844", "bsz": "1751.2", "num_updates": "26000", "lr": "0.00040625", "gnorm": "1.398", "loss_scale": "0.25", "train_wall": "978", "gb_free": "39.6", "wall": "8322"}
[2024-10-06 02:34:36,318][train_inner][INFO] - {"epoch": 55, "update": 54.731, "loss": "2.561", "ntokens": "358181", "nsentences": "1767.84", "wps": "79325", "ups": "0.22", "wpb": "358181", "bsz": "1767.8", "num_updates": "26200", "lr": "0.000409375", "gnorm": "1.347", "loss_scale": "0.25", "train_wall": "763", "gb_free": "39.6", "wall": "9225"}
[2024-10-06 02:42:59,148][fairseq_cli.train][INFO] - end of epoch 55 (average epoch stats below)
[2024-10-06 02:42:59,152][train][INFO] - {"epoch": 55, "train_loss": "2.556", "train_ntokens": "357674", "train_nsentences": "1753.71", "train_wps": "64558.2", "train_ups": "0.18", "train_wpb": "357674", "train_bsz": "1753.7", "train_num_updates": "26329", "train_lr": "0.000411391", "train_gnorm": "1.365", "train_loss_scale": "0.25", "train_train_wall": "2102", "train_gb_free": "39.3", "train_wall": "9728"}
[2024-10-06 02:42:59,522][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 02:42:59,541][fairseq.trainer][INFO] - begin training epoch 56
[2024-10-06 02:42:59,542][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 02:53:54,826][train_inner][INFO] - {"epoch": 56, "update": 55.148, "loss": "2.557", "ntokens": "356829", "nsentences": "1731.02", "wps": "61603.6", "ups": "0.17", "wpb": "356828", "bsz": "1731", "num_updates": "26400", "lr": "0.0004125", "gnorm": "1.411", "loss_scale": "0.25", "train_wall": "755", "gb_free": "39.8", "wall": "10384"}
[2024-10-06 03:10:39,612][train_inner][INFO] - {"epoch": 56, "update": 55.566, "loss": "2.549", "ntokens": "358251", "nsentences": "1755.14", "wps": "71313.6", "ups": "0.2", "wpb": "358251", "bsz": "1755.1", "num_updates": "26600", "lr": "0.000415625", "gnorm": "1.269", "loss_scale": "0.25", "train_wall": "958", "gb_free": "42", "wall": "11388"}
[2024-10-06 03:25:50,246][train_inner][INFO] - {"epoch": 56, "update": 55.983, "loss": "2.564", "ntokens": "358400", "nsentences": "1747.32", "wps": "78724.3", "ups": "0.22", "wpb": "358400", "bsz": "1747.3", "num_updates": "26800", "lr": "0.00041875", "gnorm": "1.376", "loss_scale": "0.25", "train_wall": "702", "gb_free": "39.6", "wall": "12299"}
[2024-10-06 03:26:12,509][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 56 @ 26808 updates
[2024-10-06 03:26:12,531][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-06 03:26:31,280][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-06 03:26:31,348][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt (epoch 56 @ 26808 updates, score None) (writing took 18.83967932406813 seconds)
[2024-10-06 03:26:31,349][fairseq_cli.train][INFO] - end of epoch 56 (average epoch stats below)
[2024-10-06 03:26:31,355][train][INFO] - {"epoch": 56, "train_loss": "2.557", "train_ntokens": "357674", "train_nsentences": "1753.71", "train_wps": "65586.9", "train_ups": "0.18", "train_wpb": "357674", "train_bsz": "1753.7", "train_num_updates": "26808", "train_lr": "0.000418875", "train_gnorm": "1.344", "train_loss_scale": "0.25", "train_train_wall": "1935", "train_gb_free": "39.2", "train_wall": "12340"}
[2024-10-06 03:26:31,438][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 03:26:31,457][fairseq.trainer][INFO] - begin training epoch 57
[2024-10-06 03:26:31,457][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 03:43:56,764][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.25
[2024-10-06 03:44:04,230][train_inner][INFO] - {"epoch": 57, "update": 56.403, "loss": "2.549", "ntokens": "356852", "nsentences": "1773.06", "wps": "65241.2", "ups": "0.18", "wpb": "356852", "bsz": "1773.1", "num_updates": "27000", "lr": "0.000421875", "gnorm": "1.405", "loss_scale": "0.25", "train_wall": "654", "gb_free": "40.1", "wall": "13393"}
[2024-10-06 03:52:36,512][train_inner][INFO] - {"epoch": 57, "update": 56.82, "loss": "2.556", "ntokens": "358210", "nsentences": "1766.37", "wps": "139854", "ups": "0.39", "wpb": "358210", "bsz": "1766.4", "num_updates": "27200", "lr": "0.000425", "gnorm": "1.448", "loss_scale": "0.25", "train_wall": "494", "gb_free": "39.3", "wall": "13905"}
[2024-10-06 03:55:21,110][fairseq_cli.train][INFO] - end of epoch 57 (average epoch stats below)
[2024-10-06 03:55:21,145][train][INFO] - {"epoch": 57, "train_loss": "2.552", "train_ntokens": "357672", "train_nsentences": "1754.74", "train_wps": "98837.2", "train_ups": "0.28", "train_wpb": "357672", "train_bsz": "1754.7", "train_num_updates": "27286", "train_lr": "0.000426344", "train_gnorm": "1.429", "train_loss_scale": "0.25", "train_train_wall": "1290", "train_gb_free": "40.2", "train_wall": "14070"}
[2024-10-06 03:55:21,586][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 03:55:21,656][fairseq.trainer][INFO] - begin training epoch 58
[2024-10-06 03:55:21,658][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 04:04:00,337][train_inner][INFO] - {"epoch": 58, "update": 57.238, "loss": "2.55", "ntokens": "356826", "nsentences": "1719.23", "wps": "104367", "ups": "0.29", "wpb": "356826", "bsz": "1719.2", "num_updates": "27400", "lr": "0.000428125", "gnorm": "1.359", "loss_scale": "0.25", "train_wall": "362", "gb_free": "40.6", "wall": "14589"}
[2024-10-06 04:08:18,439][train_inner][INFO] - {"epoch": 58, "update": 57.656, "loss": "2.553", "ntokens": "358253", "nsentences": "1755.69", "wps": "277612", "ups": "0.77", "wpb": "358253", "bsz": "1755.7", "num_updates": "27600", "lr": "0.00043125", "gnorm": "1.365", "loss_scale": "0.25", "train_wall": "253", "gb_free": "39.6", "wall": "14847"}
[2024-10-06 04:12:20,801][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 58 @ 27765 updates
[2024-10-06 04:12:20,802][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-06 04:12:24,220][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-06 04:12:24,222][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt (epoch 58 @ 27765 updates, score None) (writing took 3.421744960360229 seconds)
[2024-10-06 04:12:24,223][fairseq_cli.train][INFO] - end of epoch 58 (average epoch stats below)
[2024-10-06 04:12:24,224][train][INFO] - {"epoch": 58, "train_loss": "2.556", "train_ntokens": "357674", "train_nsentences": "1753.71", "train_wps": "167462", "train_ups": "0.47", "train_wpb": "357674", "train_bsz": "1753.7", "train_num_updates": "27765", "train_lr": "0.000433828", "train_gnorm": "1.4", "train_loss_scale": "0.25", "train_train_wall": "691", "train_gb_free": "40.2", "train_wall": "15093"}
[2024-10-06 04:12:24,271][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 04:12:24,275][fairseq.trainer][INFO] - begin training epoch 59
[2024-10-06 04:12:24,275][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 04:18:47,093][train_inner][INFO] - {"epoch": 59, "update": 58.073, "loss": "2.567", "ntokens": "356820", "nsentences": "1767.66", "wps": "113520", "ups": "0.32", "wpb": "356820", "bsz": "1767.7", "num_updates": "27800", "lr": "0.000434375", "gnorm": "1.528", "loss_scale": "0.25", "train_wall": "307", "gb_free": "39.8", "wall": "15476"}
[2024-10-06 04:23:03,044][train_inner][INFO] - {"epoch": 59, "update": 58.491, "loss": "2.552", "ntokens": "358370", "nsentences": "1774.18", "wps": "280037", "ups": "0.78", "wpb": "358370", "bsz": "1774.2", "num_updates": "28000", "lr": "0.0004375", "gnorm": "1.293", "loss_scale": "0.25", "train_wall": "252", "gb_free": "39.4", "wall": "15732"}
[2024-10-06 04:27:29,470][train_inner][INFO] - {"epoch": 59, "update": 58.908, "loss": "2.56", "ntokens": "358254", "nsentences": "1758.65", "wps": "268940", "ups": "0.75", "wpb": "358254", "bsz": "1758.7", "num_updates": "28200", "lr": "0.000440625", "gnorm": "1.361", "loss_scale": "0.25", "train_wall": "263", "gb_free": "39.3", "wall": "15998"}
[2024-10-06 04:28:04,703][fairseq_cli.train][INFO] - end of epoch 59 (average epoch stats below)
[2024-10-06 04:28:04,711][train][INFO] - {"epoch": 59, "train_loss": "2.557", "train_ntokens": "357674", "train_nsentences": "1753.71", "train_wps": "182168", "train_ups": "0.51", "train_wpb": "357674", "train_bsz": "1753.7", "train_num_updates": "28244", "train_lr": "0.000441313", "train_gnorm": "1.337", "train_loss_scale": "0.25", "train_train_wall": "617", "train_gb_free": "40.1", "train_wall": "16034"}
[2024-10-06 04:28:04,895][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 04:28:04,927][fairseq.trainer][INFO] - begin training epoch 60
[2024-10-06 04:28:04,931][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 04:37:22,703][train_inner][INFO] - {"epoch": 60, "update": 59.326, "loss": "2.549", "ntokens": "356669", "nsentences": "1734.95", "wps": "120247", "ups": "0.34", "wpb": "356669", "bsz": "1735", "num_updates": "28400", "lr": "0.00044375", "gnorm": "1.364", "loss_scale": "0.25", "train_wall": "241", "gb_free": "39.6", "wall": "16592"}
[2024-10-06 04:41:33,687][train_inner][INFO] - {"epoch": 60, "update": 59.743, "loss": "2.564", "ntokens": "358439", "nsentences": "1775.97", "wps": "285638", "ups": "0.8", "wpb": "358439", "bsz": "1776", "num_updates": "28600", "lr": "0.000446875", "gnorm": "1.415", "loss_scale": "0.25", "train_wall": "247", "gb_free": "39.6", "wall": "16843"}
[2024-10-06 04:44:03,457][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 60 @ 28723 updates
[2024-10-06 04:44:03,458][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-06 04:44:10,090][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-06 04:44:10,108][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt (epoch 60 @ 28723 updates, score None) (writing took 6.6515744170174 seconds)
[2024-10-06 04:44:10,109][fairseq_cli.train][INFO] - end of epoch 60 (average epoch stats below)
[2024-10-06 04:44:10,122][train][INFO] - {"epoch": 60, "train_loss": "2.558", "train_ntokens": "357674", "train_nsentences": "1753.71", "train_wps": "177466", "train_ups": "0.5", "train_wpb": "357674", "train_bsz": "1753.7", "train_num_updates": "28723", "train_lr": "0.000448797", "train_gnorm": "1.435", "train_loss_scale": "0.25", "train_train_wall": "601", "train_gb_free": "39.6", "train_wall": "16999"}
[2024-10-06 04:44:10,329][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 04:44:10,351][fairseq.trainer][INFO] - begin training epoch 61
[2024-10-06 04:44:10,352][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 04:51:25,067][train_inner][INFO] - {"epoch": 61, "update": 60.161, "loss": "2.549", "ntokens": "356804", "nsentences": "1681.8", "wps": "120672", "ups": "0.34", "wpb": "356804", "bsz": "1681.8", "num_updates": "28800", "lr": "0.00045", "gnorm": "1.404", "loss_scale": "0.25", "train_wall": "256", "gb_free": "39.6", "wall": "17434"}
[2024-10-06 04:55:32,493][train_inner][INFO] - {"epoch": 61, "update": 60.578, "loss": "2.558", "ntokens": "358427", "nsentences": "1743.34", "wps": "289731", "ups": "0.81", "wpb": "358427", "bsz": "1743.3", "num_updates": "29000", "lr": "0.000453125", "gnorm": "1.39", "loss_scale": "0.25", "train_wall": "243", "gb_free": "39.1", "wall": "17681"}
[2024-10-06 04:57:16,881][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.25
[2024-10-06 04:59:51,438][train_inner][INFO] - {"epoch": 61, "update": 60.998, "loss": "2.574", "ntokens": "358180", "nsentences": "1812.61", "wps": "276654", "ups": "0.77", "wpb": "358180", "bsz": "1812.6", "num_updates": "29200", "lr": "0.00045625", "gnorm": "1.311", "loss_scale": "0.25", "train_wall": "254", "gb_free": "40.3", "wall": "17940"}
[2024-10-06 04:59:52,059][fairseq_cli.train][INFO] - end of epoch 61 (average epoch stats below)
[2024-10-06 04:59:52,098][train][INFO] - {"epoch": 61, "train_loss": "2.56", "train_ntokens": "357672", "train_nsentences": "1753.43", "train_wps": "181500", "train_ups": "0.51", "train_wpb": "357672", "train_bsz": "1753.4", "train_num_updates": "29201", "train_lr": "0.000456266", "train_gnorm": "1.333", "train_loss_scale": "0.25", "train_train_wall": "606", "train_gb_free": "40.1", "train_wall": "17941"}
[2024-10-06 04:59:52,363][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 04:59:52,388][fairseq.trainer][INFO] - begin training epoch 62
[2024-10-06 04:59:52,389][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 05:09:47,152][train_inner][INFO] - {"epoch": 62, "update": 61.415, "loss": "2.548", "ntokens": "356690", "nsentences": "1721.83", "wps": "119757", "ups": "0.34", "wpb": "356690", "bsz": "1721.8", "num_updates": "29400", "lr": "0.000459375", "gnorm": "1.361", "loss_scale": "0.25", "train_wall": "233", "gb_free": "39.8", "wall": "18536"}
[2024-10-06 05:14:09,664][train_inner][INFO] - {"epoch": 62, "update": 61.833, "loss": "2.567", "ntokens": "358361", "nsentences": "1769.89", "wps": "273037", "ups": "0.76", "wpb": "358361", "bsz": "1769.9", "num_updates": "29600", "lr": "0.0004625", "gnorm": "1.496", "loss_scale": "0.25", "train_wall": "258", "gb_free": "39.6", "wall": "18798"}
[2024-10-06 05:15:46,604][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 62 @ 29680 updates
[2024-10-06 05:15:46,610][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-06 05:15:54,562][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-06 05:15:54,568][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt (epoch 62 @ 29680 updates, score None) (writing took 7.963736840523779 seconds)
[2024-10-06 05:15:54,568][fairseq_cli.train][INFO] - end of epoch 62 (average epoch stats below)
[2024-10-06 05:15:54,586][train][INFO] - {"epoch": 62, "train_loss": "2.561", "train_ntokens": "357674", "train_nsentences": "1753.71", "train_wps": "178006", "train_ups": "0.5", "train_wpb": "357674", "train_bsz": "1753.7", "train_num_updates": "29680", "train_lr": "0.00046375", "train_gnorm": "1.427", "train_loss_scale": "0.25", "train_train_wall": "585", "train_gb_free": "39.3", "train_wall": "18903"}
[2024-10-06 05:15:54,666][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 05:15:54,703][fairseq.trainer][INFO] - begin training epoch 63
[2024-10-06 05:15:54,703][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 05:23:36,025][train_inner][INFO] - {"epoch": 63, "update": 62.251, "loss": "2.56", "ntokens": "356776", "nsentences": "1736.69", "wps": "125991", "ups": "0.35", "wpb": "356776", "bsz": "1736.7", "num_updates": "29800", "lr": "0.000465625", "gnorm": "1.449", "loss_scale": "0.25", "train_wall": "217", "gb_free": "39.6", "wall": "19365"}
[2024-10-06 05:28:00,181][train_inner][INFO] - {"epoch": 63, "update": 62.668, "loss": "2.563", "ntokens": "358320", "nsentences": "1787.78", "wps": "271300", "ups": "0.76", "wpb": "358320", "bsz": "1787.8", "num_updates": "30000", "lr": "0.00046875", "gnorm": "1.369", "loss_scale": "0.25", "train_wall": "190", "gb_free": "39.8", "wall": "19629"}
[2024-10-06 05:31:34,905][fairseq_cli.train][INFO] - end of epoch 63 (average epoch stats below)
[2024-10-06 05:31:34,914][train][INFO] - {"epoch": 63, "train_loss": "2.561", "train_ntokens": "357674", "train_nsentences": "1753.71", "train_wps": "182199", "train_ups": "0.51", "train_wpb": "357674", "train_bsz": "1753.7", "train_num_updates": "30159", "train_lr": "0.000471234", "train_gnorm": "1.406", "train_loss_scale": "0.25", "train_train_wall": "422", "train_gb_free": "39.6", "train_wall": "19844"}
[2024-10-06 05:31:35,051][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 05:31:35,056][fairseq.trainer][INFO] - begin training epoch 64
[2024-10-06 05:31:35,057][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 05:38:11,153][train_inner][INFO] - {"epoch": 64, "update": 63.086, "loss": "2.566", "ntokens": "356836", "nsentences": "1740.43", "wps": "116810", "ups": "0.33", "wpb": "356836", "bsz": "1740.4", "num_updates": "30200", "lr": "0.000471875", "gnorm": "1.429", "loss_scale": "0.25", "train_wall": "187", "gb_free": "39.2", "wall": "20240"}
[2024-10-06 05:42:21,017][train_inner][INFO] - {"epoch": 64, "update": 63.503, "loss": "2.555", "ntokens": "358285", "nsentences": "1744.73", "wps": "286790", "ups": "0.8", "wpb": "358285", "bsz": "1744.7", "num_updates": "30400", "lr": "0.000475", "gnorm": "1.337", "loss_scale": "0.25", "train_wall": "246", "gb_free": "39.8", "wall": "20490"}
[2024-10-06 05:47:02,804][train_inner][INFO] - {"epoch": 64, "update": 63.921, "loss": "2.573", "ntokens": "358360", "nsentences": "1783.13", "wps": "254352", "ups": "0.71", "wpb": "358360", "bsz": "1783.1", "num_updates": "30600", "lr": "0.000478125", "gnorm": "1.378", "loss_scale": "0.25", "train_wall": "277", "gb_free": "40.5", "wall": "20772"}
[2024-10-06 05:48:06,618][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 64 @ 30638 updates
[2024-10-06 05:48:06,619][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-06 05:48:10,548][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-06 05:48:10,553][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt (epoch 64 @ 30638 updates, score None) (writing took 3.9349329536780715 seconds)
[2024-10-06 05:48:10,553][fairseq_cli.train][INFO] - end of epoch 64 (average epoch stats below)
[2024-10-06 05:48:10,555][train][INFO] - {"epoch": 64, "train_loss": "2.564", "train_ntokens": "357674", "train_nsentences": "1753.71", "train_wps": "172077", "train_ups": "0.48", "train_wpb": "357674", "train_bsz": "1753.7", "train_num_updates": "30638", "train_lr": "0.000478719", "train_gnorm": "1.385", "train_loss_scale": "0.25", "train_train_wall": "665", "train_gb_free": "40.1", "train_wall": "20839"}
[2024-10-06 05:48:10,613][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 05:48:10,619][fairseq.trainer][INFO] - begin training epoch 65
[2024-10-06 05:48:10,619][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 05:57:21,257][train_inner][INFO] - {"epoch": 65, "update": 64.338, "loss": "2.563", "ntokens": "356792", "nsentences": "1757.81", "wps": "115383", "ups": "0.32", "wpb": "356792", "bsz": "1757.8", "num_updates": "30800", "lr": "0.00048125", "gnorm": "1.474", "loss_scale": "0.25", "train_wall": "281", "gb_free": "39.6", "wall": "21390"}
[2024-10-06 06:01:31,434][train_inner][INFO] - {"epoch": 65, "update": 64.756, "loss": "2.565", "ntokens": "358314", "nsentences": "1734.7", "wps": "286521", "ups": "0.8", "wpb": "358314", "bsz": "1734.7", "num_updates": "31000", "lr": "0.000484375", "gnorm": "1.42", "loss_scale": "0.25", "train_wall": "241", "gb_free": "39.1", "wall": "21640"}
[2024-10-06 06:03:59,715][fairseq_cli.train][INFO] - end of epoch 65 (average epoch stats below)
[2024-10-06 06:03:59,750][train][INFO] - {"epoch": 65, "train_loss": "2.565", "train_ntokens": "357674", "train_nsentences": "1753.71", "train_wps": "180499", "train_ups": "0.5", "train_wpb": "357674", "train_bsz": "1753.7", "train_num_updates": "31117", "train_lr": "0.000486203", "train_gnorm": "1.415", "train_loss_scale": "0.25", "train_train_wall": "604", "train_gb_free": "39.8", "train_wall": "21789"}
[2024-10-06 06:04:00,178][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 06:04:00,239][fairseq.trainer][INFO] - begin training epoch 66
[2024-10-06 06:04:00,240][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 06:12:05,474][train_inner][INFO] - {"epoch": 66, "update": 65.173, "loss": "2.573", "ntokens": "356734", "nsentences": "1803.17", "wps": "112528", "ups": "0.32", "wpb": "356734", "bsz": "1803.2", "num_updates": "31200", "lr": "0.0004875", "gnorm": "1.321", "loss_scale": "0.5", "train_wall": "268", "gb_free": "39.3", "wall": "22274"}
[2024-10-06 06:13:15,647][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.25
[2024-10-06 06:16:25,877][train_inner][INFO] - {"epoch": 66, "update": 65.593, "loss": "2.562", "ntokens": "358245", "nsentences": "1732.52", "wps": "275154", "ups": "0.77", "wpb": "358245", "bsz": "1732.5", "num_updates": "31400", "lr": "0.000490625", "gnorm": "1.376", "loss_scale": "0.25", "train_wall": "257", "gb_free": "39.6", "wall": "22535"}
[2024-10-06 06:20:25,963][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 66 @ 31595 updates
[2024-10-06 06:20:25,970][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-06 06:20:39,816][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-06 06:20:40,335][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt (epoch 66 @ 31595 updates, score None) (writing took 14.371537174098194 seconds)
[2024-10-06 06:20:40,352][fairseq_cli.train][INFO] - end of epoch 66 (average epoch stats below)
[2024-10-06 06:20:40,367][train][INFO] - {"epoch": 66, "train_loss": "2.571", "train_ntokens": "357672", "train_nsentences": "1754.87", "train_wps": "170864", "train_ups": "0.48", "train_wpb": "357672", "train_bsz": "1754.9", "train_num_updates": "31595", "train_lr": "0.000493672", "train_gnorm": "1.428", "train_loss_scale": "0.25", "train_train_wall": "615", "train_gb_free": "39.7", "train_wall": "22789"}
[2024-10-06 06:20:40,599][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 06:20:40,627][fairseq.trainer][INFO] - begin training epoch 67
[2024-10-06 06:20:40,628][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 06:26:45,616][train_inner][INFO] - {"epoch": 67, "update": 66.01, "loss": "2.577", "ntokens": "356813", "nsentences": "1720.39", "wps": "115151", "ups": "0.32", "wpb": "356813", "bsz": "1720.4", "num_updates": "31600", "lr": "0.00049375", "gnorm": "1.546", "loss_scale": "0.25", "train_wall": "258", "gb_free": "39.6", "wall": "23154"}
[2024-10-06 06:28:01,800][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.125
[2024-10-06 06:30:52,155][train_inner][INFO] - {"epoch": 67, "update": 66.43, "loss": "2.565", "ntokens": "358470", "nsentences": "1753.38", "wps": "290809", "ups": "0.81", "wpb": "358470", "bsz": "1753.4", "num_updates": "31800", "lr": "0.000496875", "gnorm": "1.494", "loss_scale": "0.125", "train_wall": "231", "gb_free": "41", "wall": "23401"}
[2024-10-06 06:35:26,119][train_inner][INFO] - {"epoch": 67, "update": 66.848, "loss": "2.574", "ntokens": "358209", "nsentences": "1767.37", "wps": "261507", "ups": "0.73", "wpb": "358209", "bsz": "1767.4", "num_updates": "32000", "lr": "0.0005", "gnorm": "1.401", "loss_scale": "0.125", "train_wall": "262", "gb_free": "39.6", "wall": "23675"}
[2024-10-06 06:36:32,420][fairseq_cli.train][INFO] - end of epoch 67 (average epoch stats below)
[2024-10-06 06:36:32,436][train][INFO] - {"epoch": 67, "train_loss": "2.57", "train_ntokens": "357685", "train_nsentences": "1750.56", "train_wps": "179582", "train_ups": "0.5", "train_wpb": "357685", "train_bsz": "1750.6", "train_num_updates": "32073", "train_lr": "0.000499901", "train_gnorm": "1.479", "train_loss_scale": "0.125", "train_train_wall": "580", "train_gb_free": "39.2", "train_wall": "23741"}
[2024-10-06 06:36:32,821][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 06:36:32,873][fairseq.trainer][INFO] - begin training epoch 68
[2024-10-06 06:36:32,873][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 06:45:26,653][train_inner][INFO] - {"epoch": 68, "update": 67.265, "loss": "2.564", "ntokens": "356885", "nsentences": "1715.96", "wps": "118858", "ups": "0.33", "wpb": "356885", "bsz": "1716", "num_updates": "32200", "lr": "0.000499728", "gnorm": "1.475", "loss_scale": "0.125", "train_wall": "233", "gb_free": "40.5", "wall": "24275"}
[2024-10-06 06:50:09,613][train_inner][INFO] - {"epoch": 68, "update": 67.683, "loss": "2.571", "ntokens": "358165", "nsentences": "1778.73", "wps": "253160", "ups": "0.71", "wpb": "358165", "bsz": "1778.7", "num_updates": "32400", "lr": "0.000499457", "gnorm": "1.528", "loss_scale": "0.125", "train_wall": "278", "gb_free": "39.8", "wall": "24558"}
[2024-10-06 06:53:02,672][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 68 @ 32552 updates
[2024-10-06 06:53:02,679][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-06 06:53:16,355][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-06 06:53:16,831][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt (epoch 68 @ 32552 updates, score None) (writing took 14.158739847131073 seconds)
[2024-10-06 06:53:16,845][fairseq_cli.train][INFO] - end of epoch 68 (average epoch stats below)
[2024-10-06 06:53:16,854][train][INFO] - {"epoch": 68, "train_loss": "2.569", "train_ntokens": "357674", "train_nsentences": "1753.71", "train_wps": "170575", "train_ups": "0.48", "train_wpb": "357674", "train_bsz": "1753.7", "train_num_updates": "32552", "train_lr": "0.00049925", "train_gnorm": "1.439", "train_loss_scale": "0.125", "train_train_wall": "616", "train_gb_free": "40.2", "train_wall": "24746"}
[2024-10-06 06:53:17,094][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 06:53:17,133][fairseq.trainer][INFO] - begin training epoch 69
[2024-10-06 06:53:17,133][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 07:00:24,568][train_inner][INFO] - {"epoch": 69, "update": 68.1, "loss": "2.572", "ntokens": "356832", "nsentences": "1760.06", "wps": "116054", "ups": "0.33", "wpb": "356832", "bsz": "1760.1", "num_updates": "32600", "lr": "0.000499185", "gnorm": "1.377", "loss_scale": "0.125", "train_wall": "271", "gb_free": "39.1", "wall": "25173"}
[2024-10-06 07:04:23,894][train_inner][INFO] - {"epoch": 69, "update": 68.518, "loss": "2.563", "ntokens": "358201", "nsentences": "1769.27", "wps": "299348", "ups": "0.84", "wpb": "358201", "bsz": "1769.3", "num_updates": "32800", "lr": "0.000498913", "gnorm": "1.4", "loss_scale": "0.125", "train_wall": "235", "gb_free": "39.8", "wall": "25413"}
[2024-10-06 07:09:11,588][train_inner][INFO] - {"epoch": 69, "update": 68.935, "loss": "2.563", "ntokens": "358433", "nsentences": "1726.62", "wps": "249180", "ups": "0.7", "wpb": "358433", "bsz": "1726.6", "num_updates": "33000", "lr": "0.000498641", "gnorm": "1.479", "loss_scale": "0.125", "train_wall": "283", "gb_free": "39.3", "wall": "25700"}
[2024-10-06 07:09:39,329][fairseq_cli.train][INFO] - end of epoch 69 (average epoch stats below)
[2024-10-06 07:09:39,378][train][INFO] - {"epoch": 69, "train_loss": "2.564", "train_ntokens": "357674", "train_nsentences": "1753.71", "train_wps": "174377", "train_ups": "0.49", "train_wpb": "357674", "train_bsz": "1753.7", "train_num_updates": "33031", "train_lr": "0.000498599", "train_gnorm": "1.434", "train_loss_scale": "0.125", "train_train_wall": "646", "train_gb_free": "40.1", "train_wall": "25728"}
[2024-10-06 07:09:39,812][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 07:09:39,864][fairseq.trainer][INFO] - begin training epoch 70
[2024-10-06 07:09:39,865][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 07:19:21,540][train_inner][INFO] - {"epoch": 70, "update": 69.353, "loss": "2.558", "ntokens": "356851", "nsentences": "1759.99", "wps": "117010", "ups": "0.33", "wpb": "356851", "bsz": "1760", "num_updates": "33200", "lr": "0.00049837", "gnorm": "1.54", "loss_scale": "0.125", "train_wall": "251", "gb_free": "39.3", "wall": "26310"}
[2024-10-06 07:23:29,768][train_inner][INFO] - {"epoch": 70, "update": 69.77, "loss": "2.567", "ntokens": "358381", "nsentences": "1752.84", "wps": "288758", "ups": "0.81", "wpb": "358381", "bsz": "1752.8", "num_updates": "33400", "lr": "0.000498098", "gnorm": "1.57", "loss_scale": "0.125", "train_wall": "244", "gb_free": "39.6", "wall": "26559"}
[2024-10-06 07:26:01,988][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 70 @ 33510 updates
[2024-10-06 07:26:01,998][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-06 07:26:09,964][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-06 07:26:10,017][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt (epoch 70 @ 33510 updates, score None) (writing took 8.029052322730422 seconds)
[2024-10-06 07:26:10,018][fairseq_cli.train][INFO] - end of epoch 70 (average epoch stats below)
[2024-10-06 07:26:10,046][train][INFO] - {"epoch": 70, "train_loss": "2.564", "train_ntokens": "357674", "train_nsentences": "1753.71", "train_wps": "172945", "train_ups": "0.48", "train_wpb": "357674", "train_bsz": "1753.7", "train_num_updates": "33510", "train_lr": "0.000497948", "train_gnorm": "1.532", "train_loss_scale": "0.125", "train_train_wall": "618", "train_gb_free": "40.1", "train_wall": "26719"}
[2024-10-06 07:26:10,131][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 07:26:10,177][fairseq.trainer][INFO] - begin training epoch 71
[2024-10-06 07:26:10,178][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 07:33:41,007][train_inner][INFO] - {"epoch": 71, "update": 70.188, "loss": "2.557", "ntokens": "356646", "nsentences": "1723.51", "wps": "116700", "ups": "0.33", "wpb": "356646", "bsz": "1723.5", "num_updates": "33600", "lr": "0.000497826", "gnorm": "1.406", "loss_scale": "0.125", "train_wall": "261", "gb_free": "39.6", "wall": "27170"}
[2024-10-06 07:38:18,216][train_inner][INFO] - {"epoch": 71, "update": 70.605, "loss": "2.554", "ntokens": "358202", "nsentences": "1761.41", "wps": "258439", "ups": "0.72", "wpb": "358202", "bsz": "1761.4", "num_updates": "33800", "lr": "0.000497554", "gnorm": "1.416", "loss_scale": "0.25", "train_wall": "273", "gb_free": "39.7", "wall": "27447"}
[2024-10-06 07:43:10,459][fairseq_cli.train][INFO] - end of epoch 71 (average epoch stats below)
[2024-10-06 07:43:10,477][train][INFO] - {"epoch": 71, "train_loss": "2.556", "train_ntokens": "357674", "train_nsentences": "1753.71", "train_wps": "167896", "train_ups": "0.47", "train_wpb": "357674", "train_bsz": "1753.7", "train_num_updates": "33989", "train_lr": "0.000497298", "train_gnorm": "1.392", "train_loss_scale": "0.25", "train_train_wall": "672", "train_gb_free": "40.3", "train_wall": "27739"}
[2024-10-06 07:43:10,889][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 07:43:10,901][fairseq.trainer][INFO] - begin training epoch 72
[2024-10-06 07:43:10,902][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 07:49:37,202][train_inner][INFO] - {"epoch": 72, "update": 71.023, "loss": "2.566", "ntokens": "356851", "nsentences": "1771.17", "wps": "105114", "ups": "0.29", "wpb": "356851", "bsz": "1771.2", "num_updates": "34000", "lr": "0.000497283", "gnorm": "1.386", "loss_scale": "0.25", "train_wall": "330", "gb_free": "39.6", "wall": "28126"}
[2024-10-06 07:53:48,455][train_inner][INFO] - {"epoch": 72, "update": 71.441, "loss": "2.547", "ntokens": "358289", "nsentences": "1775.31", "wps": "285207", "ups": "0.8", "wpb": "358289", "bsz": "1775.3", "num_updates": "34200", "lr": "0.000497011", "gnorm": "1.468", "loss_scale": "0.25", "train_wall": "248", "gb_free": "39.6", "wall": "28377"}
