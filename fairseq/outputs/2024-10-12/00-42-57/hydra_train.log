[2024-10-12 00:43:30,082][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:13029', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-12 00:43:33,556][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-12 00:43:33,558][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-12 00:43:33,567][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-12 00:43:33,567][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-12 00:43:33,568][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-12 00:43:33,575][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-12 00:43:34,087][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-12 00:43:36,697][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:18631', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-12 00:43:36,991][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:10660', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-12 00:43:37,879][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:12424', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-12 00:43:38,587][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:13351', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-12 00:43:38,823][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:17690', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-12 00:43:39,936][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:16178', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-12 00:43:40,985][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:17636', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-12 00:43:41,657][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-12 00:43:41,668][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-12 00:43:41,668][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-12 00:43:41,668][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-12 00:43:41,669][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-12 00:43:41,669][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-12 00:43:42,434][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-12 00:43:42,444][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-12 00:43:42,444][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-12 00:43:42,444][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-12 00:43:42,445][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-12 00:43:42,451][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-12 00:43:42,793][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-12 00:43:43,223][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-12 00:43:43,683][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-12 00:43:43,685][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-12 00:43:43,691][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-12 00:43:43,691][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-12 00:43:43,692][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-12 00:43:43,699][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-12 00:43:44,338][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-12 00:43:44,456][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-12 00:43:44,463][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-12 00:43:44,463][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-12 00:43:44,463][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-12 00:43:44,464][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-12 00:43:44,465][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-12 00:43:45,196][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-12 00:43:52,124][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-12 00:43:52,126][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-12 00:43:52,126][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-12 00:43:52,126][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-12 00:43:52,143][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-12 00:43:52,143][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-12 00:43:53,258][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-12 00:44:17,565][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-12 00:44:18,282][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-12 00:44:18,282][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-12 00:44:18,282][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-12 00:44:18,284][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-12 00:44:18,284][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-12 00:44:30,781][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-12 00:44:38,758][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-12 00:44:38,759][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-12 00:44:38,765][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-12 00:44:38,765][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-12 00:44:38,765][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-12 00:44:38,765][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-12 00:44:38,765][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-12 00:44:38,765][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-12 00:44:38,765][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-12 00:44:38,766][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-12 00:44:38,766][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-12 00:44:38,766][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-12 00:44:38,768][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-12 00:44:38,814][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-12 00:44:39,086][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-12 00:44:39,086][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-12 00:44:39,086][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-12 00:44:39,087][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-12 00:44:39,088][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-12 00:44:52,464][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-12 00:45:02,671][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 2141 @ 102677 updates)
[2024-10-12 00:45:02,672][fairseq.trainer][INFO] - loading train data for epoch 2141
[2024-10-12 00:45:05,082][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-12 00:45:18,696][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-12 00:45:18,696][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-12 00:45:18,696][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-12 00:45:18,696][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-12 00:45:18,697][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-12 00:45:18,697][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-12 00:45:18,697][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-12 00:45:18,697][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-12 00:45:18,697][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-12 00:45:18,697][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-12 00:45:18,697][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-12 00:45:18,697][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-12 00:45:18,712][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-12 00:45:19,714][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 00:45:19,721][fairseq.trainer][INFO] - begin training epoch 2141
[2024-10-12 00:45:19,722][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 00:45:28,462][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-12 00:45:28,462][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-12 00:45:28,462][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-12 00:45:28,462][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-12 00:45:28,462][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-12 00:45:28,463][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-12 00:45:28,463][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-12 00:45:28,552][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-12 00:45:28,552][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-12 00:45:28,552][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-12 00:45:28,552][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-12 00:45:28,553][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-12 00:45:28,553][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-12 00:45:51,415][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 2141 @ 102677 updates)
[2024-10-12 00:45:51,424][fairseq.trainer][INFO] - loading train data for epoch 2141
[2024-10-12 00:45:51,844][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-12 00:45:59,990][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 00:45:59,995][fairseq.trainer][INFO] - begin training epoch 2141
[2024-10-12 00:45:59,996][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 00:46:09,150][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-12 00:46:09,150][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-12 00:46:09,151][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-12 00:46:09,151][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-12 00:46:09,151][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-12 00:46:09,151][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-12 00:46:09,151][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-12 00:46:09,151][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-12 00:46:09,151][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-12 00:46:09,151][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-12 00:46:09,151][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-12 00:46:09,151][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-12 00:46:09,152][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-12 00:46:22,006][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 2141 @ 102677 updates)
[2024-10-12 00:46:22,008][fairseq.trainer][INFO] - loading train data for epoch 2141
[2024-10-12 00:46:22,366][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-12 00:46:32,531][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 00:46:32,536][fairseq.trainer][INFO] - begin training epoch 2141
[2024-10-12 00:46:32,537][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 00:46:36,668][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 2141 @ 102677 updates)
[2024-10-12 00:46:36,670][fairseq.trainer][INFO] - loading train data for epoch 2141
[2024-10-12 00:46:37,243][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-12 00:46:48,455][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 00:46:48,464][fairseq.trainer][INFO] - begin training epoch 2141
[2024-10-12 00:46:48,467][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 00:47:01,312][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-12 00:47:01,312][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-12 00:47:01,312][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-12 00:47:01,312][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-12 00:47:01,312][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-12 00:47:01,312][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-12 00:47:01,312][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-12 00:47:01,312][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-12 00:47:01,312][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-12 00:47:01,313][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-12 00:47:01,313][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-12 00:47:01,313][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-12 00:47:01,327][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-12 00:47:23,832][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-12 00:47:23,832][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-12 00:47:23,832][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-12 00:47:23,832][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-12 00:47:23,832][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-12 00:47:23,833][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-12 00:47:23,833][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-12 00:47:23,833][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-12 00:47:23,833][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-12 00:47:23,833][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-12 00:47:23,833][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-12 00:47:23,833][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-12 00:47:23,834][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-12 00:47:29,785][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-12 00:47:29,785][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-12 00:47:29,785][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-12 00:47:29,785][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-12 00:47:29,785][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-12 00:47:29,785][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-12 00:47:29,785][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-12 00:47:29,785][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-12 00:47:29,785][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-12 00:47:29,785][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-12 00:47:29,786][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-12 00:47:29,786][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-12 00:47:29,835][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-12 00:47:31,557][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-12 00:47:31,572][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-12 00:47:31,572][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-12 00:47:31,573][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-12 00:47:31,573][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-12 00:47:31,573][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-12 00:47:31,573][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-12 00:47:31,573][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-12 00:47:31,573][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-12 00:47:31,573][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-12 00:47:31,573][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-12 00:47:31,574][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-12 00:47:31,574][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-12 00:47:38,938][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 2141 @ 102677 updates)
[2024-10-12 00:47:38,940][fairseq.trainer][INFO] - loading train data for epoch 2141
[2024-10-12 00:47:39,540][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-12 00:47:48,833][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 00:47:48,839][fairseq.trainer][INFO] - begin training epoch 2141
[2024-10-12 00:47:48,851][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 00:47:57,684][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 2141 @ 102677 updates)
[2024-10-12 00:47:57,686][fairseq.trainer][INFO] - loading train data for epoch 2141
[2024-10-12 00:47:58,166][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-12 00:48:03,964][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 2141 @ 102677 updates)
[2024-10-12 00:48:03,965][fairseq.trainer][INFO] - loading train data for epoch 2141
[2024-10-12 00:48:04,768][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-12 00:48:07,776][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 00:48:07,789][fairseq.trainer][INFO] - begin training epoch 2141
[2024-10-12 00:48:07,789][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 00:48:13,531][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 00:48:13,544][fairseq.trainer][INFO] - begin training epoch 2141
[2024-10-12 00:48:13,545][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 00:48:59,549][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 2141 @ 102677 updates)
[2024-10-12 00:48:59,569][fairseq.trainer][INFO] - loading train data for epoch 2141
[2024-10-12 00:49:00,518][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-12 00:49:19,700][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 00:49:19,709][fairseq.trainer][INFO] - begin training epoch 2141
[2024-10-12 00:49:19,709][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 00:55:43,671][fairseq.trainer][WARNING] - OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 24.00 MiB. GPU 
[2024-10-12 00:55:43,679][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 1            |        cudaMalloc retries: 1         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   1902 MiB |   1969 MiB |   2230 MiB | 335639 KiB |
|       from large pool |   1863 MiB |   1930 MiB |   2119 MiB | 261721 KiB |
|       from small pool |     38 MiB |    103 MiB |    110 MiB |  73918 KiB |
|---------------------------------------------------------------------------|
| Active memory         |   1902 MiB |   1969 MiB |   2230 MiB | 335639 KiB |
|       from large pool |   1863 MiB |   1930 MiB |   2119 MiB | 261721 KiB |
|       from small pool |     38 MiB |    103 MiB |    110 MiB |  73918 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |   1891 MiB |   1958 MiB |   2218 MiB | 334056 KiB |
|       from large pool |   1853 MiB |   1920 MiB |   2107 MiB | 260141 KiB |
|       from small pool |     38 MiB |    102 MiB |    110 MiB |  73915 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   2088 MiB |   2088 MiB |   2088 MiB |      0 B   |
|       from large pool |   1984 MiB |   1984 MiB |   1984 MiB |      0 B   |
|       from small pool |    104 MiB |    104 MiB |    104 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory | 116235 KiB | 152238 KiB | 477209 KiB | 360973 KiB |
|       from large pool |  49224 KiB |  85178 KiB | 299543 KiB | 250319 KiB |
|       from small pool |  67011 KiB |  68933 KiB | 177665 KiB | 110654 KiB |
|---------------------------------------------------------------------------|
| Allocations           |    1916    |    2693    |    2754    |     838    |
|       from large pool |     100    |     103    |     109    |       9    |
|       from small pool |    1816    |    2615    |    2645    |     829    |
|---------------------------------------------------------------------------|
| Active allocs         |    1916    |    2693    |    2754    |     838    |
|       from large pool |     100    |     103    |     109    |       9    |
|       from small pool |    1816    |    2615    |    2645    |     829    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      84    |      84    |      84    |       0    |
|       from large pool |      32    |      32    |      32    |       0    |
|       from small pool |      52    |      52    |      52    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     265    |     268    |     487    |     222    |
|       from large pool |      16    |      19    |      31    |      15    |
|       from small pool |     249    |     251    |     456    |     207    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-12 00:55:43,680][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-12 00:55:43,680][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-12 00:55:43,695][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-12 00:55:43,695][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 4                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-12 00:55:43,696][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 5                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-12 00:55:43,696][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 6                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-12 00:55:43,701][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 7                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-12 00:55:43,701][fairseq.trainer][WARNING] - attempting to recover from OOM in forward/backward pass
[2024-10-12 00:59:05,502][fairseq_cli.train][INFO] - end of epoch 2141 (average epoch stats below)
[2024-10-12 00:59:05,589][train][INFO] - {"epoch": 2141, "train_loss": "0.392", "train_ntokens": "260571", "train_nsentences": "1750.04", "train_wps": "50229.1", "train_ups": "0.19", "train_wpb": "260571", "train_bsz": "1750", "train_num_updates": "102725", "train_lr": "0.000403906", "train_gnorm": "0.341", "train_loss_scale": "4", "train_train_wall": "170", "train_gb_free": "40", "train_wall": "867"}
[2024-10-12 00:59:05,754][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 00:59:05,777][fairseq.trainer][INFO] - begin training epoch 2142
[2024-10-12 00:59:05,778][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 01:06:39,459][fairseq_cli.train][INFO] - end of epoch 2142 (average epoch stats below)
[2024-10-12 01:06:39,491][train][INFO] - {"epoch": 2142, "train_loss": "0.393", "train_ntokens": "260670", "train_nsentences": "1750.04", "train_wps": "27567", "train_ups": "0.11", "train_wpb": "260670", "train_bsz": "1750", "train_num_updates": "102773", "train_lr": "0.000403841", "train_gnorm": "0.344", "train_loss_scale": "4", "train_train_wall": "177", "train_gb_free": "40.2", "train_wall": "1321"}
[2024-10-12 01:06:39,621][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 01:06:39,624][fairseq.trainer][INFO] - begin training epoch 2143
[2024-10-12 01:06:39,625][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 01:08:47,071][train_inner][INFO] - {"epoch": 2143, "update": 2142.562, "loss": "0.39", "ntokens": "260969", "nsentences": "1737.23", "wps": "38573.9", "ups": "0.15", "wpb": "260969", "bsz": "1737.2", "num_updates": "102800", "lr": "0.000403804", "gnorm": "0.343", "loss_scale": "4", "train_wall": "376", "gb_free": "40.5", "wall": "1448"}
[2024-10-12 01:09:09,029][fairseq_cli.train][INFO] - end of epoch 2143 (average epoch stats below)
[2024-10-12 01:09:09,031][train][INFO] - {"epoch": 2143, "train_loss": "0.384", "train_ntokens": "260571", "train_nsentences": "1750.04", "train_wps": "83640", "train_ups": "0.32", "train_wpb": "260571", "train_bsz": "1750", "train_num_updates": "102821", "train_lr": "0.000403776", "train_gnorm": "0.35", "train_loss_scale": "4", "train_train_wall": "51", "train_gb_free": "39.7", "train_wall": "1470"}
[2024-10-12 01:09:09,100][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 01:09:09,104][fairseq.trainer][INFO] - begin training epoch 2144
[2024-10-12 01:09:09,104][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 01:11:21,957][fairseq_cli.train][INFO] - end of epoch 2144 (average epoch stats below)
[2024-10-12 01:11:21,961][train][INFO] - {"epoch": 2144, "train_loss": "0.395", "train_ntokens": "260784", "train_nsentences": "1750.04", "train_wps": "94169.4", "train_ups": "0.36", "train_wpb": "260784", "train_bsz": "1750", "train_num_updates": "102869", "train_lr": "0.000403711", "train_gnorm": "0.365", "train_loss_scale": "4", "train_train_wall": "53", "train_gb_free": "39.7", "train_wall": "1603"}
[2024-10-12 01:11:22,017][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 01:11:22,020][fairseq.trainer][INFO] - begin training epoch 2145
[2024-10-12 01:11:22,020][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 01:13:30,276][fairseq_cli.train][INFO] - end of epoch 2145 (average epoch stats below)
[2024-10-12 01:13:30,280][train][INFO] - {"epoch": 2145, "train_loss": "0.397", "train_ntokens": "260702", "train_nsentences": "1750.04", "train_wps": "97522", "train_ups": "0.37", "train_wpb": "260702", "train_bsz": "1750", "train_num_updates": "102917", "train_lr": "0.000403645", "train_gnorm": "0.358", "train_loss_scale": "4", "train_train_wall": "56", "train_gb_free": "39.6", "train_wall": "1732"}
[2024-10-12 01:13:30,356][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 01:13:30,360][fairseq.trainer][INFO] - begin training epoch 2146
[2024-10-12 01:13:30,360][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 01:15:39,293][fairseq_cli.train][INFO] - end of epoch 2146 (average epoch stats below)
[2024-10-12 01:15:39,298][train][INFO] - {"epoch": 2146, "train_loss": "0.395", "train_ntokens": "260812", "train_nsentences": "1750.04", "train_wps": "97034.9", "train_ups": "0.37", "train_wpb": "260812", "train_bsz": "1750", "train_num_updates": "102965", "train_lr": "0.00040358", "train_gnorm": "0.356", "train_loss_scale": "4", "train_train_wall": "52", "train_gb_free": "39.9", "train_wall": "1861"}
[2024-10-12 01:15:39,441][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 01:15:39,445][fairseq.trainer][INFO] - begin training epoch 2147
[2024-10-12 01:15:39,445][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 01:17:39,454][train_inner][INFO] - {"epoch": 2147, "update": 2146.729, "loss": "0.395", "ntokens": "260574", "nsentences": "1765.94", "wps": "97890.1", "ups": "0.38", "wpb": "260574", "bsz": "1765.9", "num_updates": "103000", "lr": "0.000403533", "gnorm": "0.354", "loss_scale": "4", "train_wall": "231", "gb_free": "39.7", "wall": "1981"}
[2024-10-12 01:17:51,765][fairseq_cli.train][INFO] - end of epoch 2147 (average epoch stats below)
[2024-10-12 01:17:51,773][train][INFO] - {"epoch": 2147, "train_loss": "0.398", "train_ntokens": "260706", "train_nsentences": "1750.04", "train_wps": "94467.6", "train_ups": "0.36", "train_wpb": "260706", "train_bsz": "1750", "train_num_updates": "103013", "train_lr": "0.000403515", "train_gnorm": "0.34", "train_loss_scale": "4", "train_train_wall": "61", "train_gb_free": "39.8", "train_wall": "1993"}
[2024-10-12 01:17:51,867][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 01:17:51,882][fairseq.trainer][INFO] - begin training epoch 2148
[2024-10-12 01:17:51,882][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 01:20:00,720][fairseq_cli.train][INFO] - end of epoch 2148 (average epoch stats below)
[2024-10-12 01:20:00,725][train][INFO] - {"epoch": 2148, "train_loss": "0.392", "train_ntokens": "260661", "train_nsentences": "1750.04", "train_wps": "97030.3", "train_ups": "0.37", "train_wpb": "260661", "train_bsz": "1750", "train_num_updates": "103061", "train_lr": "0.00040345", "train_gnorm": "0.356", "train_loss_scale": "4", "train_train_wall": "61", "train_gb_free": "39.4", "train_wall": "2122"}
[2024-10-12 01:20:00,844][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 01:20:00,858][fairseq.trainer][INFO] - begin training epoch 2149
[2024-10-12 01:20:00,858][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 01:22:06,940][fairseq_cli.train][INFO] - end of epoch 2149 (average epoch stats below)
[2024-10-12 01:22:06,942][train][INFO] - {"epoch": 2149, "train_loss": "0.4", "train_ntokens": "260763", "train_nsentences": "1750.04", "train_wps": "99168.7", "train_ups": "0.38", "train_wpb": "260763", "train_bsz": "1750", "train_num_updates": "103109", "train_lr": "0.000403385", "train_gnorm": "0.358", "train_loss_scale": "4", "train_train_wall": "53", "train_gb_free": "40.1", "train_wall": "2248"}
[2024-10-12 01:22:07,003][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 01:22:07,006][fairseq.trainer][INFO] - begin training epoch 2150
[2024-10-12 01:22:07,007][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 01:24:11,634][fairseq_cli.train][INFO] - end of epoch 2150 (average epoch stats below)
[2024-10-12 01:24:11,643][train][INFO] - {"epoch": 2150, "train_loss": "0.404", "train_ntokens": "260717", "train_nsentences": "1750.04", "train_wps": "100360", "train_ups": "0.38", "train_wpb": "260717", "train_bsz": "1750", "train_num_updates": "103157", "train_lr": "0.000403319", "train_gnorm": "0.358", "train_loss_scale": "4", "train_train_wall": "60", "train_gb_free": "39.7", "train_wall": "2373"}
[2024-10-12 01:24:11,775][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 01:24:11,791][fairseq.trainer][INFO] - begin training epoch 2151
[2024-10-12 01:24:11,792][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 01:26:19,608][train_inner][INFO] - {"epoch": 2151, "update": 2150.896, "loss": "0.398", "ntokens": "260664", "nsentences": "1741.72", "wps": "100226", "ups": "0.38", "wpb": "260664", "bsz": "1741.7", "num_updates": "103200", "lr": "0.000403261", "gnorm": "0.356", "loss_scale": "4", "train_wall": "229", "gb_free": "39.7", "wall": "2501"}
[2024-10-12 01:26:20,840][fairseq_cli.train][INFO] - end of epoch 2151 (average epoch stats below)
[2024-10-12 01:26:20,843][train][INFO] - {"epoch": 2151, "train_loss": "0.397", "train_ntokens": "260713", "train_nsentences": "1750.04", "train_wps": "96861.5", "train_ups": "0.37", "train_wpb": "260713", "train_bsz": "1750", "train_num_updates": "103205", "train_lr": "0.000403254", "train_gnorm": "0.352", "train_loss_scale": "4", "train_train_wall": "44", "train_gb_free": "39.8", "train_wall": "2502"}
[2024-10-12 01:26:20,917][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 01:26:20,920][fairseq.trainer][INFO] - begin training epoch 2152
[2024-10-12 01:26:20,921][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 01:28:26,731][fairseq_cli.train][INFO] - end of epoch 2152 (average epoch stats below)
[2024-10-12 01:28:26,747][train][INFO] - {"epoch": 2152, "train_loss": "0.388", "train_ntokens": "260437", "train_nsentences": "1750.04", "train_wps": "99301.1", "train_ups": "0.38", "train_wpb": "260437", "train_bsz": "1750", "train_num_updates": "103253", "train_lr": "0.000403189", "train_gnorm": "0.358", "train_loss_scale": "4", "train_train_wall": "45", "train_gb_free": "39.3", "train_wall": "2628"}
[2024-10-12 01:28:26,847][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 01:28:26,861][fairseq.trainer][INFO] - begin training epoch 2153
[2024-10-12 01:28:26,862][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 01:30:34,660][fairseq_cli.train][INFO] - end of epoch 2153 (average epoch stats below)
[2024-10-12 01:30:34,667][train][INFO] - {"epoch": 2153, "train_loss": "0.399", "train_ntokens": "260266", "train_nsentences": "1750.04", "train_wps": "97664.9", "train_ups": "0.38", "train_wpb": "260266", "train_bsz": "1750", "train_num_updates": "103301", "train_lr": "0.000403124", "train_gnorm": "0.348", "train_loss_scale": "4", "train_train_wall": "36", "train_gb_free": "39.7", "train_wall": "2756"}
[2024-10-12 01:30:34,764][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 01:30:34,770][fairseq.trainer][INFO] - begin training epoch 2154
[2024-10-12 01:30:34,771][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 01:32:45,187][fairseq_cli.train][INFO] - end of epoch 2154 (average epoch stats below)
[2024-10-12 01:32:45,193][train][INFO] - {"epoch": 2154, "train_loss": "0.392", "train_ntokens": "260857", "train_nsentences": "1750.04", "train_wps": "95930.5", "train_ups": "0.37", "train_wpb": "260857", "train_bsz": "1750", "train_num_updates": "103349", "train_lr": "0.000403058", "train_gnorm": "0.354", "train_loss_scale": "4", "train_train_wall": "32", "train_gb_free": "39.3", "train_wall": "2886"}
[2024-10-12 01:32:45,277][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 01:32:45,283][fairseq.trainer][INFO] - begin training epoch 2155
[2024-10-12 01:32:45,283][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 01:34:52,266][fairseq_cli.train][INFO] - end of epoch 2155 (average epoch stats below)
[2024-10-12 01:34:52,280][train][INFO] - {"epoch": 2155, "train_loss": "0.402", "train_ntokens": "260913", "train_nsentences": "1750.04", "train_wps": "98547.8", "train_ups": "0.38", "train_wpb": "260913", "train_bsz": "1750", "train_num_updates": "103397", "train_lr": "0.000402993", "train_gnorm": "0.365", "train_loss_scale": "4", "train_train_wall": "48", "train_gb_free": "39.8", "train_wall": "3014"}
[2024-10-12 01:34:52,429][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 01:34:52,438][fairseq.trainer][INFO] - begin training epoch 2156
[2024-10-12 01:34:52,439][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 01:36:19,318][train_inner][INFO] - {"epoch": 2156, "update": 2155.062, "loss": "0.396", "ntokens": "260611", "nsentences": "1752.73", "wps": "86914.6", "ups": "0.33", "wpb": "260611", "bsz": "1752.7", "num_updates": "103400", "lr": "0.000402989", "gnorm": "0.357", "loss_scale": "4", "train_wall": "171", "gb_free": "39.6", "wall": "3101"}
[2024-10-12 01:37:03,297][fairseq_cli.train][INFO] - end of epoch 2156 (average epoch stats below)
[2024-10-12 01:37:03,305][train][INFO] - {"epoch": 2156, "train_loss": "0.396", "train_ntokens": "260873", "train_nsentences": "1750.04", "train_wps": "95574.1", "train_ups": "0.37", "train_wpb": "260873", "train_bsz": "1750", "train_num_updates": "103445", "train_lr": "0.000402928", "train_gnorm": "0.359", "train_loss_scale": "4", "train_train_wall": "52", "train_gb_free": "40", "train_wall": "3145"}
[2024-10-12 01:37:03,394][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 01:37:03,398][fairseq.trainer][INFO] - begin training epoch 2157
[2024-10-12 01:37:03,398][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 01:39:06,949][fairseq_cli.train][INFO] - end of epoch 2157 (average epoch stats below)
[2024-10-12 01:39:06,954][train][INFO] - {"epoch": 2157, "train_loss": "0.387", "train_ntokens": "260758", "train_nsentences": "1750.04", "train_wps": "101233", "train_ups": "0.39", "train_wpb": "260758", "train_bsz": "1750", "train_num_updates": "103493", "train_lr": "0.000402863", "train_gnorm": "0.358", "train_loss_scale": "4", "train_train_wall": "60", "train_gb_free": "39.6", "train_wall": "3268"}
[2024-10-12 01:39:07,061][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 01:39:07,076][fairseq.trainer][INFO] - begin training epoch 2158
[2024-10-12 01:39:07,076][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 01:41:12,434][fairseq_cli.train][INFO] - end of epoch 2158 (average epoch stats below)
[2024-10-12 01:41:12,466][train][INFO] - {"epoch": 2158, "train_loss": "0.396", "train_ntokens": "261349", "train_nsentences": "1750.04", "train_wps": "99953.6", "train_ups": "0.38", "train_wpb": "261349", "train_bsz": "1750", "train_num_updates": "103541", "train_lr": "0.000402798", "train_gnorm": "0.349", "train_loss_scale": "4", "train_train_wall": "49", "train_gb_free": "39.3", "train_wall": "3394"}
[2024-10-12 01:41:12,602][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 01:41:12,618][fairseq.trainer][INFO] - begin training epoch 2159
[2024-10-12 01:41:12,626][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 01:43:23,612][fairseq_cli.train][INFO] - end of epoch 2159 (average epoch stats below)
[2024-10-12 01:43:23,615][train][INFO] - {"epoch": 2159, "train_loss": "0.395", "train_ntokens": "260881", "train_nsentences": "1750.04", "train_wps": "95483.7", "train_ups": "0.37", "train_wpb": "260882", "train_bsz": "1750", "train_num_updates": "103589", "train_lr": "0.000402732", "train_gnorm": "0.344", "train_loss_scale": "4", "train_train_wall": "49", "train_gb_free": "39.3", "train_wall": "3525"}
[2024-10-12 01:43:23,677][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 01:43:23,681][fairseq.trainer][INFO] - begin training epoch 2160
[2024-10-12 01:43:23,681][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 01:45:03,495][train_inner][INFO] - {"epoch": 2160, "update": 2159.229, "loss": "0.393", "ntokens": "261020", "nsentences": "1744.68", "wps": "99593.3", "ups": "0.38", "wpb": "261020", "bsz": "1744.7", "num_updates": "103600", "lr": "0.000402717", "gnorm": "0.352", "loss_scale": "4", "train_wall": "230", "gb_free": "39.6", "wall": "3625"}
[2024-10-12 01:45:33,265][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 2160 @ 103637 updates
[2024-10-12 01:45:33,266][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-12 01:45:37,089][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-12 01:45:37,109][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 2160 @ 103637 updates, score None) (writing took 3.844389921054244 seconds)
[2024-10-12 01:45:37,110][fairseq_cli.train][INFO] - end of epoch 2160 (average epoch stats below)
[2024-10-12 01:45:37,113][train][INFO] - {"epoch": 2160, "train_loss": "0.395", "train_ntokens": "260642", "train_nsentences": "1750.04", "train_wps": "93718.1", "train_ups": "0.36", "train_wpb": "260642", "train_bsz": "1750", "train_num_updates": "103637", "train_lr": "0.000402667", "train_gnorm": "0.367", "train_loss_scale": "4", "train_train_wall": "57", "train_gb_free": "39.2", "train_wall": "3658"}
[2024-10-12 01:45:37,169][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 01:45:37,209][fairseq.trainer][INFO] - begin training epoch 2161
[2024-10-12 01:45:37,209][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 01:47:40,288][fairseq_cli.train][INFO] - end of epoch 2161 (average epoch stats below)
[2024-10-12 01:47:40,299][train][INFO] - {"epoch": 2161, "train_loss": "0.392", "train_ntokens": "260845", "train_nsentences": "1750.04", "train_wps": "101647", "train_ups": "0.39", "train_wpb": "260845", "train_bsz": "1750", "train_num_updates": "103685", "train_lr": "0.000402602", "train_gnorm": "0.354", "train_loss_scale": "4", "train_train_wall": "54", "train_gb_free": "40.1", "train_wall": "3782"}
[2024-10-12 01:47:40,405][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 01:47:40,412][fairseq.trainer][INFO] - begin training epoch 2162
[2024-10-12 01:47:40,412][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 01:49:48,635][fairseq_cli.train][INFO] - end of epoch 2162 (average epoch stats below)
[2024-10-12 01:49:48,638][train][INFO] - {"epoch": 2162, "train_loss": "0.392", "train_ntokens": "260694", "train_nsentences": "1750.04", "train_wps": "97503.5", "train_ups": "0.37", "train_wpb": "260694", "train_bsz": "1750", "train_num_updates": "103733", "train_lr": "0.000402537", "train_gnorm": "0.381", "train_loss_scale": "4", "train_train_wall": "60", "train_gb_free": "39.6", "train_wall": "3910"}
[2024-10-12 01:49:48,695][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 01:49:48,699][fairseq.trainer][INFO] - begin training epoch 2163
[2024-10-12 01:49:48,700][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 01:51:54,594][fairseq_cli.train][INFO] - end of epoch 2163 (average epoch stats below)
[2024-10-12 01:51:54,598][train][INFO] - {"epoch": 2163, "train_loss": "0.399", "train_ntokens": "260510", "train_nsentences": "1750.04", "train_wps": "99275.5", "train_ups": "0.38", "train_wpb": "260510", "train_bsz": "1750", "train_num_updates": "103781", "train_lr": "0.000402471", "train_gnorm": "0.356", "train_loss_scale": "4", "train_train_wall": "31", "train_gb_free": "39.2", "train_wall": "4036"}
[2024-10-12 01:51:54,734][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 01:51:54,738][fairseq.trainer][INFO] - begin training epoch 2164
[2024-10-12 01:51:54,739][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 01:53:22,269][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2024-10-12 01:53:36,820][train_inner][INFO] - {"epoch": 2164, "update": 2163.417, "loss": "0.393", "ntokens": "260705", "nsentences": "1753.64", "wps": "101576", "ups": "0.39", "wpb": "260705", "bsz": "1753.6", "num_updates": "103800", "lr": "0.000402446", "gnorm": "0.363", "loss_scale": "2", "train_wall": "205", "gb_free": "40.1", "wall": "4138"}
[2024-10-12 01:54:02,484][fairseq_cli.train][INFO] - end of epoch 2164 (average epoch stats below)
[2024-10-12 01:54:02,487][train][INFO] - {"epoch": 2164, "train_loss": "0.387", "train_ntokens": "260546", "train_nsentences": "1748.7", "train_wps": "95754.7", "train_ups": "0.37", "train_wpb": "260546", "train_bsz": "1748.7", "train_num_updates": "103828", "train_lr": "0.000402408", "train_gnorm": "0.36", "train_loss_scale": "2", "train_train_wall": "53", "train_gb_free": "39.4", "train_wall": "4164"}
[2024-10-12 01:54:02,544][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 01:54:02,548][fairseq.trainer][INFO] - begin training epoch 2165
[2024-10-12 01:54:02,548][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 01:56:09,476][fairseq_cli.train][INFO] - end of epoch 2165 (average epoch stats below)
[2024-10-12 01:56:09,481][train][INFO] - {"epoch": 2165, "train_loss": "0.393", "train_ntokens": "260560", "train_nsentences": "1750.04", "train_wps": "98488.3", "train_ups": "0.38", "train_wpb": "260560", "train_bsz": "1750", "train_num_updates": "103876", "train_lr": "0.000402342", "train_gnorm": "0.382", "train_loss_scale": "2", "train_train_wall": "54", "train_gb_free": "39.8", "train_wall": "4291"}
[2024-10-12 01:56:09,543][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 01:56:09,547][fairseq.trainer][INFO] - begin training epoch 2166
[2024-10-12 01:56:09,547][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 01:58:17,828][fairseq_cli.train][INFO] - end of epoch 2166 (average epoch stats below)
[2024-10-12 01:58:17,831][train][INFO] - {"epoch": 2166, "train_loss": "0.389", "train_ntokens": "260456", "train_nsentences": "1750.04", "train_wps": "97406.8", "train_ups": "0.37", "train_wpb": "260456", "train_bsz": "1750", "train_num_updates": "103924", "train_lr": "0.000402277", "train_gnorm": "0.363", "train_loss_scale": "2", "train_train_wall": "44", "train_gb_free": "39.3", "train_wall": "4419"}
[2024-10-12 01:58:17,884][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 01:58:17,889][fairseq.trainer][INFO] - begin training epoch 2167
[2024-10-12 01:58:17,889][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 02:00:24,696][fairseq_cli.train][INFO] - end of epoch 2167 (average epoch stats below)
[2024-10-12 02:00:24,699][train][INFO] - {"epoch": 2167, "train_loss": "0.381", "train_ntokens": "260892", "train_nsentences": "1750.04", "train_wps": "98709.2", "train_ups": "0.38", "train_wpb": "260892", "train_bsz": "1750", "train_num_updates": "103972", "train_lr": "0.000402212", "train_gnorm": "0.357", "train_loss_scale": "2", "train_train_wall": "44", "train_gb_free": "39.3", "train_wall": "4546"}
[2024-10-12 02:00:24,770][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 02:00:24,777][fairseq.trainer][INFO] - begin training epoch 2168
[2024-10-12 02:00:24,777][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 02:02:14,305][train_inner][INFO] - {"epoch": 2168, "update": 2167.583, "loss": "0.389", "ntokens": "260491", "nsentences": "1747.95", "wps": "100676", "ups": "0.39", "wpb": "260491", "bsz": "1748", "num_updates": "104000", "lr": "0.000402174", "gnorm": "0.365", "loss_scale": "2", "train_wall": "196", "gb_free": "40.1", "wall": "4656"}
[2024-10-12 02:02:31,381][fairseq_cli.train][INFO] - end of epoch 2168 (average epoch stats below)
[2024-10-12 02:02:31,387][train][INFO] - {"epoch": 2168, "train_loss": "0.398", "train_ntokens": "260650", "train_nsentences": "1750.04", "train_wps": "98761.4", "train_ups": "0.38", "train_wpb": "260650", "train_bsz": "1750", "train_num_updates": "104020", "train_lr": "0.000402147", "train_gnorm": "0.352", "train_loss_scale": "2", "train_train_wall": "46", "train_gb_free": "40.6", "train_wall": "4673"}
[2024-10-12 02:02:31,518][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 02:02:31,522][fairseq.trainer][INFO] - begin training epoch 2169
[2024-10-12 02:02:31,522][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 02:04:40,527][fairseq_cli.train][INFO] - end of epoch 2169 (average epoch stats below)
[2024-10-12 02:04:40,532][train][INFO] - {"epoch": 2169, "train_loss": "0.392", "train_ntokens": "260780", "train_nsentences": "1750.04", "train_wps": "96927.3", "train_ups": "0.37", "train_wpb": "260780", "train_bsz": "1750", "train_num_updates": "104068", "train_lr": "0.000402082", "train_gnorm": "0.367", "train_loss_scale": "2", "train_train_wall": "44", "train_gb_free": "40", "train_wall": "4802"}
[2024-10-12 02:04:40,594][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 02:04:40,597][fairseq.trainer][INFO] - begin training epoch 2170
[2024-10-12 02:04:40,597][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 02:06:43,428][fairseq_cli.train][INFO] - end of epoch 2170 (average epoch stats below)
[2024-10-12 02:06:43,446][train][INFO] - {"epoch": 2170, "train_loss": "0.401", "train_ntokens": "260569", "train_nsentences": "1750.04", "train_wps": "101759", "train_ups": "0.39", "train_wpb": "260569", "train_bsz": "1750", "train_num_updates": "104116", "train_lr": "0.000402016", "train_gnorm": "0.352", "train_loss_scale": "2", "train_train_wall": "51", "train_gb_free": "39.6", "train_wall": "4925"}
[2024-10-12 02:06:43,564][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 02:06:43,573][fairseq.trainer][INFO] - begin training epoch 2171
[2024-10-12 02:06:43,576][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 02:08:53,149][fairseq_cli.train][INFO] - end of epoch 2171 (average epoch stats below)
[2024-10-12 02:08:53,172][train][INFO] - {"epoch": 2171, "train_loss": "0.4", "train_ntokens": "260469", "train_nsentences": "1750.04", "train_wps": "96386.6", "train_ups": "0.37", "train_wpb": "260469", "train_bsz": "1750", "train_num_updates": "104164", "train_lr": "0.000401951", "train_gnorm": "0.372", "train_loss_scale": "2", "train_train_wall": "61", "train_gb_free": "39.7", "train_wall": "5054"}
[2024-10-12 02:08:53,290][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 02:08:53,309][fairseq.trainer][INFO] - begin training epoch 2172
[2024-10-12 02:08:53,310][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 02:10:49,533][train_inner][INFO] - {"epoch": 2172, "update": 2171.75, "loss": "0.397", "ntokens": "260805", "nsentences": "1734.51", "wps": "101239", "ups": "0.39", "wpb": "260805", "bsz": "1734.5", "num_updates": "104200", "lr": "0.000401902", "gnorm": "0.361", "loss_scale": "2", "train_wall": "215", "gb_free": "39.3", "wall": "5171"}
[2024-10-12 02:11:00,617][fairseq_cli.train][INFO] - end of epoch 2172 (average epoch stats below)
[2024-10-12 02:11:00,619][train][INFO] - {"epoch": 2172, "train_loss": "0.394", "train_ntokens": "260466", "train_nsentences": "1750.04", "train_wps": "98100.2", "train_ups": "0.38", "train_wpb": "260466", "train_bsz": "1750", "train_num_updates": "104212", "train_lr": "0.000401886", "train_gnorm": "0.35", "train_loss_scale": "2", "train_train_wall": "53", "train_gb_free": "39.6", "train_wall": "5182"}
[2024-10-12 02:11:00,766][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 02:11:00,770][fairseq.trainer][INFO] - begin training epoch 2173
[2024-10-12 02:11:00,770][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 02:13:07,014][fairseq_cli.train][INFO] - end of epoch 2173 (average epoch stats below)
[2024-10-12 02:13:07,020][train][INFO] - {"epoch": 2173, "train_loss": "0.396", "train_ntokens": "260868", "train_nsentences": "1750.04", "train_wps": "99065.2", "train_ups": "0.38", "train_wpb": "260868", "train_bsz": "1750", "train_num_updates": "104260", "train_lr": "0.000401821", "train_gnorm": "0.344", "train_loss_scale": "2", "train_train_wall": "50", "train_gb_free": "39.7", "train_wall": "5308"}
[2024-10-12 02:13:07,140][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 02:13:07,144][fairseq.trainer][INFO] - begin training epoch 2174
[2024-10-12 02:13:07,144][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 02:15:12,774][fairseq_cli.train][INFO] - end of epoch 2174 (average epoch stats below)
[2024-10-12 02:15:12,777][train][INFO] - {"epoch": 2174, "train_loss": "0.397", "train_ntokens": "260590", "train_nsentences": "1750.04", "train_wps": "99465.7", "train_ups": "0.38", "train_wpb": "260590", "train_bsz": "1750", "train_num_updates": "104308", "train_lr": "0.000401755", "train_gnorm": "0.356", "train_loss_scale": "2", "train_train_wall": "53", "train_gb_free": "40", "train_wall": "5434"}
[2024-10-12 02:15:12,839][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 02:15:12,844][fairseq.trainer][INFO] - begin training epoch 2175
[2024-10-12 02:15:12,844][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 02:17:19,638][fairseq_cli.train][INFO] - end of epoch 2175 (average epoch stats below)
[2024-10-12 02:17:19,655][train][INFO] - {"epoch": 2175, "train_loss": "0.39", "train_ntokens": "260902", "train_nsentences": "1750.04", "train_wps": "98707.4", "train_ups": "0.38", "train_wpb": "260902", "train_bsz": "1750", "train_num_updates": "104356", "train_lr": "0.00040169", "train_gnorm": "0.346", "train_loss_scale": "2", "train_train_wall": "54", "train_gb_free": "40", "train_wall": "5561"}
[2024-10-12 02:17:19,762][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 02:17:19,777][fairseq.trainer][INFO] - begin training epoch 2176
[2024-10-12 02:17:19,778][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 02:19:27,059][train_inner][INFO] - {"epoch": 2176, "update": 2175.917, "loss": "0.394", "ntokens": "260514", "nsentences": "1768.5", "wps": "100681", "ups": "0.39", "wpb": "260514", "bsz": "1768.5", "num_updates": "104400", "lr": "0.00040163", "gnorm": "0.347", "loss_scale": "2", "train_wall": "220", "gb_free": "39.2", "wall": "5688"}
[2024-10-12 02:19:28,026][fairseq_cli.train][INFO] - end of epoch 2176 (average epoch stats below)
[2024-10-12 02:19:28,028][train][INFO] - {"epoch": 2176, "train_loss": "0.391", "train_ntokens": "260737", "train_nsentences": "1750.04", "train_wps": "97494.1", "train_ups": "0.37", "train_wpb": "260737", "train_bsz": "1750", "train_num_updates": "104404", "train_lr": "0.000401625", "train_gnorm": "0.344", "train_loss_scale": "2", "train_train_wall": "53", "train_gb_free": "40.1", "train_wall": "5689"}
[2024-10-12 02:19:28,093][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 02:19:28,100][fairseq.trainer][INFO] - begin training epoch 2177
[2024-10-12 02:19:28,101][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 02:21:34,841][fairseq_cli.train][INFO] - end of epoch 2177 (average epoch stats below)
[2024-10-12 02:21:34,844][train][INFO] - {"epoch": 2177, "train_loss": "0.391", "train_ntokens": "260589", "train_nsentences": "1750.04", "train_wps": "98635", "train_ups": "0.38", "train_wpb": "260589", "train_bsz": "1750", "train_num_updates": "104452", "train_lr": "0.00040156", "train_gnorm": "0.337", "train_loss_scale": "2", "train_train_wall": "48", "train_gb_free": "39.7", "train_wall": "5816"}
[2024-10-12 02:21:34,902][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 02:21:34,905][fairseq.trainer][INFO] - begin training epoch 2178
[2024-10-12 02:21:34,906][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 02:23:44,147][fairseq_cli.train][INFO] - end of epoch 2178 (average epoch stats below)
[2024-10-12 02:23:44,150][train][INFO] - {"epoch": 2178, "train_loss": "0.391", "train_ntokens": "260918", "train_nsentences": "1750.04", "train_wps": "96858.2", "train_ups": "0.37", "train_wpb": "260918", "train_bsz": "1750", "train_num_updates": "104500", "train_lr": "0.000401495", "train_gnorm": "0.341", "train_loss_scale": "2", "train_train_wall": "47", "train_gb_free": "39.3", "train_wall": "5945"}
[2024-10-12 02:23:44,207][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 02:23:44,211][fairseq.trainer][INFO] - begin training epoch 2179
[2024-10-12 02:23:44,211][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 02:25:48,117][fairseq_cli.train][INFO] - end of epoch 2179 (average epoch stats below)
[2024-10-12 02:25:48,131][train][INFO] - {"epoch": 2179, "train_loss": "0.393", "train_ntokens": "260722", "train_nsentences": "1750.04", "train_wps": "100950", "train_ups": "0.39", "train_wpb": "260722", "train_bsz": "1750", "train_num_updates": "104548", "train_lr": "0.000401429", "train_gnorm": "0.357", "train_loss_scale": "2", "train_train_wall": "49", "train_gb_free": "39.3", "train_wall": "6069"}
[2024-10-12 02:25:48,244][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 02:25:48,257][fairseq.trainer][INFO] - begin training epoch 2180
[2024-10-12 02:25:48,258][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 02:27:58,269][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 2180 @ 104596 updates
[2024-10-12 02:27:58,269][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-12 02:28:01,667][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-12 02:28:01,669][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 2180 @ 104596 updates, score None) (writing took 3.4002138040959835 seconds)
[2024-10-12 02:28:01,669][fairseq_cli.train][INFO] - end of epoch 2180 (average epoch stats below)
[2024-10-12 02:28:01,671][train][INFO] - {"epoch": 2180, "train_loss": "0.397", "train_ntokens": "260989", "train_nsentences": "1750.04", "train_wps": "93812.3", "train_ups": "0.36", "train_wpb": "260989", "train_bsz": "1750", "train_num_updates": "104596", "train_lr": "0.000401364", "train_gnorm": "0.36", "train_loss_scale": "2", "train_train_wall": "64", "train_gb_free": "39.3", "train_wall": "6203"}
[2024-10-12 02:28:01,739][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 02:28:01,766][fairseq.trainer][INFO] - begin training epoch 2181
[2024-10-12 02:28:01,766][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 02:29:27,893][train_inner][INFO] - {"epoch": 2181, "update": 2180.083, "loss": "0.394", "ntokens": "260888", "nsentences": "1746.6", "wps": "86842.6", "ups": "0.33", "wpb": "260888", "bsz": "1746.6", "num_updates": "104600", "lr": "0.000401359", "gnorm": "0.349", "loss_scale": "2", "train_wall": "220", "gb_free": "39.3", "wall": "6289"}
[2024-10-12 02:30:08,603][fairseq_cli.train][INFO] - end of epoch 2181 (average epoch stats below)
[2024-10-12 02:30:08,605][train][INFO] - {"epoch": 2181, "train_loss": "0.394", "train_ntokens": "260754", "train_nsentences": "1750.04", "train_wps": "98606", "train_ups": "0.38", "train_wpb": "260754", "train_bsz": "1750", "train_num_updates": "104644", "train_lr": "0.000401299", "train_gnorm": "0.358", "train_loss_scale": "2", "train_train_wall": "50", "train_gb_free": "39.8", "train_wall": "6330"}
[2024-10-12 02:30:08,715][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 02:30:08,724][fairseq.trainer][INFO] - begin training epoch 2182
[2024-10-12 02:30:08,724][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 02:32:15,569][fairseq_cli.train][INFO] - end of epoch 2182 (average epoch stats below)
[2024-10-12 02:32:15,577][train][INFO] - {"epoch": 2182, "train_loss": "0.394", "train_ntokens": "260719", "train_nsentences": "1750.04", "train_wps": "98564.2", "train_ups": "0.38", "train_wpb": "260719", "train_bsz": "1750", "train_num_updates": "104692", "train_lr": "0.000401234", "train_gnorm": "0.359", "train_loss_scale": "2", "train_train_wall": "48", "train_gb_free": "39.6", "train_wall": "6457"}
[2024-10-12 02:32:15,749][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 02:32:15,769][fairseq.trainer][INFO] - begin training epoch 2183
[2024-10-12 02:32:15,769][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 02:34:23,884][fairseq_cli.train][INFO] - end of epoch 2183 (average epoch stats below)
[2024-10-12 02:34:23,891][train][INFO] - {"epoch": 2183, "train_loss": "0.393", "train_ntokens": "261086", "train_nsentences": "1750.04", "train_wps": "97668.8", "train_ups": "0.37", "train_wpb": "261086", "train_bsz": "1750", "train_num_updates": "104740", "train_lr": "0.000401168", "train_gnorm": "0.349", "train_loss_scale": "2", "train_train_wall": "63", "train_gb_free": "39.6", "train_wall": "6585"}
[2024-10-12 02:34:24,004][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 02:34:24,008][fairseq.trainer][INFO] - begin training epoch 2184
[2024-10-12 02:34:24,008][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 02:36:29,499][fairseq_cli.train][INFO] - end of epoch 2184 (average epoch stats below)
[2024-10-12 02:36:29,503][train][INFO] - {"epoch": 2184, "train_loss": "0.397", "train_ntokens": "260911", "train_nsentences": "1750.04", "train_wps": "99704.9", "train_ups": "0.38", "train_wpb": "260911", "train_bsz": "1750", "train_num_updates": "104788", "train_lr": "0.000401103", "train_gnorm": "0.383", "train_loss_scale": "2", "train_train_wall": "33", "train_gb_free": "40.2", "train_wall": "6711"}
[2024-10-12 02:36:29,604][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 02:36:29,613][fairseq.trainer][INFO] - begin training epoch 2185
[2024-10-12 02:36:29,614][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 02:38:02,262][train_inner][INFO] - {"epoch": 2185, "update": 2184.25, "loss": "0.394", "ntokens": "260804", "nsentences": "1751.56", "wps": "101408", "ups": "0.39", "wpb": "260804", "bsz": "1751.6", "num_updates": "104800", "lr": "0.000401087", "gnorm": "0.362", "loss_scale": "2", "train_wall": "213", "gb_free": "39.6", "wall": "6803"}
[2024-10-12 02:38:35,143][fairseq_cli.train][INFO] - end of epoch 2185 (average epoch stats below)
[2024-10-12 02:38:35,148][train][INFO] - {"epoch": 2185, "train_loss": "0.398", "train_ntokens": "260714", "train_nsentences": "1750.04", "train_wps": "99602.7", "train_ups": "0.38", "train_wpb": "260714", "train_bsz": "1750", "train_num_updates": "104836", "train_lr": "0.000401038", "train_gnorm": "0.361", "train_loss_scale": "2", "train_train_wall": "62", "train_gb_free": "40.3", "train_wall": "6836"}
[2024-10-12 02:38:35,258][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 02:38:35,267][fairseq.trainer][INFO] - begin training epoch 2186
[2024-10-12 02:38:35,268][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 02:40:40,859][fairseq_cli.train][INFO] - end of epoch 2186 (average epoch stats below)
[2024-10-12 02:40:40,864][train][INFO] - {"epoch": 2186, "train_loss": "0.394", "train_ntokens": "260444", "train_nsentences": "1750.04", "train_wps": "99445.8", "train_ups": "0.38", "train_wpb": "260444", "train_bsz": "1750", "train_num_updates": "104884", "train_lr": "0.000400973", "train_gnorm": "0.343", "train_loss_scale": "2", "train_train_wall": "47", "train_gb_free": "39.6", "train_wall": "6962"}
[2024-10-12 02:40:40,968][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 02:40:40,985][fairseq.trainer][INFO] - begin training epoch 2187
[2024-10-12 02:40:40,986][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 02:42:50,300][fairseq_cli.train][INFO] - end of epoch 2187 (average epoch stats below)
[2024-10-12 02:42:50,303][train][INFO] - {"epoch": 2187, "train_loss": "0.388", "train_ntokens": "260971", "train_nsentences": "1750.04", "train_wps": "96777.6", "train_ups": "0.37", "train_wpb": "260971", "train_bsz": "1750", "train_num_updates": "104932", "train_lr": "0.000400908", "train_gnorm": "0.345", "train_loss_scale": "2", "train_train_wall": "64", "train_gb_free": "39.4", "train_wall": "7092"}
[2024-10-12 02:42:50,366][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 02:42:50,370][fairseq.trainer][INFO] - begin training epoch 2188
[2024-10-12 02:42:50,370][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 02:44:56,675][fairseq_cli.train][INFO] - end of epoch 2188 (average epoch stats below)
[2024-10-12 02:44:56,682][train][INFO] - {"epoch": 2188, "train_loss": "0.396", "train_ntokens": "260814", "train_nsentences": "1750.04", "train_wps": "99062.3", "train_ups": "0.38", "train_wpb": "260814", "train_bsz": "1750", "train_num_updates": "104980", "train_lr": "0.000400842", "train_gnorm": "0.364", "train_loss_scale": "2", "train_train_wall": "39", "train_gb_free": "39.7", "train_wall": "7218"}
[2024-10-12 02:44:56,788][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 02:44:56,805][fairseq.trainer][INFO] - begin training epoch 2189
[2024-10-12 02:44:56,805][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 02:46:45,231][train_inner][INFO] - {"epoch": 2189, "update": 2188.417, "loss": "0.393", "ntokens": "260765", "nsentences": "1741.03", "wps": "99725.4", "ups": "0.38", "wpb": "260764", "bsz": "1741", "num_updates": "105000", "lr": "0.000400815", "gnorm": "0.352", "loss_scale": "2", "train_wall": "218", "gb_free": "40.5", "wall": "7326"}
[2024-10-12 02:47:05,703][fairseq_cli.train][INFO] - end of epoch 2189 (average epoch stats below)
[2024-10-12 02:47:05,712][train][INFO] - {"epoch": 2189, "train_loss": "0.388", "train_ntokens": "260650", "train_nsentences": "1750.04", "train_wps": "96971.2", "train_ups": "0.37", "train_wpb": "260650", "train_bsz": "1750", "train_num_updates": "105028", "train_lr": "0.000400777", "train_gnorm": "0.352", "train_loss_scale": "2", "train_train_wall": "56", "train_gb_free": "39.6", "train_wall": "7347"}
[2024-10-12 02:47:05,843][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 02:47:05,849][fairseq.trainer][INFO] - begin training epoch 2190
[2024-10-12 02:47:05,849][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 02:49:15,563][fairseq_cli.train][INFO] - end of epoch 2190 (average epoch stats below)
[2024-10-12 02:49:15,567][train][INFO] - {"epoch": 2190, "train_loss": "0.392", "train_ntokens": "260934", "train_nsentences": "1750.04", "train_wps": "96455.5", "train_ups": "0.37", "train_wpb": "260934", "train_bsz": "1750", "train_num_updates": "105076", "train_lr": "0.000400712", "train_gnorm": "0.374", "train_loss_scale": "2", "train_train_wall": "51", "train_gb_free": "39.6", "train_wall": "7477"}
[2024-10-12 02:49:15,624][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 02:49:15,632][fairseq.trainer][INFO] - begin training epoch 2191
[2024-10-12 02:49:15,633][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 02:51:23,410][fairseq_cli.train][INFO] - end of epoch 2191 (average epoch stats below)
[2024-10-12 02:51:23,414][train][INFO] - {"epoch": 2191, "train_loss": "0.392", "train_ntokens": "260795", "train_nsentences": "1750.04", "train_wps": "97917.4", "train_ups": "0.38", "train_wpb": "260795", "train_bsz": "1750", "train_num_updates": "105124", "train_lr": "0.000400647", "train_gnorm": "0.359", "train_loss_scale": "2", "train_train_wall": "50", "train_gb_free": "39.3", "train_wall": "7605"}
[2024-10-12 02:51:23,495][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 02:51:23,499][fairseq.trainer][INFO] - begin training epoch 2192
[2024-10-12 02:51:23,499][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 02:53:34,498][fairseq_cli.train][INFO] - end of epoch 2192 (average epoch stats below)
[2024-10-12 02:53:34,509][train][INFO] - {"epoch": 2192, "train_loss": "0.386", "train_ntokens": "260647", "train_nsentences": "1750.04", "train_wps": "95438.4", "train_ups": "0.37", "train_wpb": "260647", "train_bsz": "1750", "train_num_updates": "105172", "train_lr": "0.000400582", "train_gnorm": "0.359", "train_loss_scale": "2", "train_train_wall": "53", "train_gb_free": "40.3", "train_wall": "7736"}
[2024-10-12 02:53:34,624][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 02:53:34,635][fairseq.trainer][INFO] - begin training epoch 2193
[2024-10-12 02:53:34,636][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 02:55:22,524][train_inner][INFO] - {"epoch": 2193, "update": 2192.583, "loss": "0.389", "ntokens": "260805", "nsentences": "1749.42", "wps": "100836", "ups": "0.39", "wpb": "260805", "bsz": "1749.4", "num_updates": "105200", "lr": "0.000400543", "gnorm": "0.362", "loss_scale": "2", "train_wall": "207", "gb_free": "39.6", "wall": "7844"}
[2024-10-12 02:55:39,986][fairseq_cli.train][INFO] - end of epoch 2193 (average epoch stats below)
[2024-10-12 02:55:39,989][train][INFO] - {"epoch": 2193, "train_loss": "0.393", "train_ntokens": "260688", "train_nsentences": "1750.04", "train_wps": "99724.4", "train_ups": "0.38", "train_wpb": "260688", "train_bsz": "1750", "train_num_updates": "105220", "train_lr": "0.000400516", "train_gnorm": "0.357", "train_loss_scale": "2", "train_train_wall": "49", "train_gb_free": "39.8", "train_wall": "7861"}
[2024-10-12 02:55:40,048][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 02:55:40,054][fairseq.trainer][INFO] - begin training epoch 2194
[2024-10-12 02:55:40,055][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 02:57:45,051][fairseq_cli.train][INFO] - end of epoch 2194 (average epoch stats below)
[2024-10-12 02:57:45,072][train][INFO] - {"epoch": 2194, "train_loss": "0.403", "train_ntokens": "260783", "train_nsentences": "1750.04", "train_wps": "100076", "train_ups": "0.38", "train_wpb": "260783", "train_bsz": "1750", "train_num_updates": "105268", "train_lr": "0.000400451", "train_gnorm": "0.373", "train_loss_scale": "2", "train_train_wall": "57", "train_gb_free": "40.3", "train_wall": "7986"}
[2024-10-12 02:57:45,197][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 02:57:45,213][fairseq.trainer][INFO] - begin training epoch 2195
[2024-10-12 02:57:45,214][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 02:59:55,833][fairseq_cli.train][INFO] - end of epoch 2195 (average epoch stats below)
[2024-10-12 02:59:55,838][train][INFO] - {"epoch": 2195, "train_loss": "0.398", "train_ntokens": "260668", "train_nsentences": "1750.04", "train_wps": "95685.2", "train_ups": "0.37", "train_wpb": "260668", "train_bsz": "1750", "train_num_updates": "105316", "train_lr": "0.000400386", "train_gnorm": "0.354", "train_loss_scale": "2", "train_train_wall": "55", "train_gb_free": "39.9", "train_wall": "8117"}
[2024-10-12 02:59:55,909][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 02:59:55,913][fairseq.trainer][INFO] - begin training epoch 2196
[2024-10-12 02:59:55,913][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 03:02:02,441][fairseq_cli.train][INFO] - end of epoch 2196 (average epoch stats below)
[2024-10-12 03:02:02,457][train][INFO] - {"epoch": 2196, "train_loss": "0.398", "train_ntokens": "260670", "train_nsentences": "1750.04", "train_wps": "98824.7", "train_ups": "0.38", "train_wpb": "260670", "train_bsz": "1750", "train_num_updates": "105364", "train_lr": "0.000400321", "train_gnorm": "0.348", "train_loss_scale": "2", "train_train_wall": "56", "train_gb_free": "39.3", "train_wall": "8244"}
[2024-10-12 03:02:02,535][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 03:02:02,541][fairseq.trainer][INFO] - begin training epoch 2197
[2024-10-12 03:02:02,541][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 03:03:58,499][train_inner][INFO] - {"epoch": 2197, "update": 2196.75, "loss": "0.398", "ntokens": "260760", "nsentences": "1757.62", "wps": "101076", "ups": "0.39", "wpb": "260760", "bsz": "1757.6", "num_updates": "105400", "lr": "0.000400272", "gnorm": "0.354", "loss_scale": "2", "train_wall": "228", "gb_free": "39.6", "wall": "8360"}
[2024-10-12 03:04:12,577][fairseq_cli.train][INFO] - end of epoch 2197 (average epoch stats below)
[2024-10-12 03:04:12,579][train][INFO] - {"epoch": 2197, "train_loss": "0.385", "train_ntokens": "260775", "train_nsentences": "1750.04", "train_wps": "96197.8", "train_ups": "0.37", "train_wpb": "260775", "train_bsz": "1750", "train_num_updates": "105412", "train_lr": "0.000400255", "train_gnorm": "0.333", "train_loss_scale": "2", "train_train_wall": "57", "train_gb_free": "39.8", "train_wall": "8374"}
[2024-10-12 03:04:12,634][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 03:04:12,638][fairseq.trainer][INFO] - begin training epoch 2198
[2024-10-12 03:04:12,639][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 03:06:17,647][fairseq_cli.train][INFO] - end of epoch 2198 (average epoch stats below)
[2024-10-12 03:06:17,651][train][INFO] - {"epoch": 2198, "train_loss": "0.392", "train_ntokens": "260692", "train_nsentences": "1750.04", "train_wps": "100051", "train_ups": "0.38", "train_wpb": "260692", "train_bsz": "1750", "train_num_updates": "105460", "train_lr": "0.00040019", "train_gnorm": "0.367", "train_loss_scale": "2", "train_train_wall": "37", "train_gb_free": "40.1", "train_wall": "8499"}
[2024-10-12 03:06:17,707][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 03:06:17,711][fairseq.trainer][INFO] - begin training epoch 2199
[2024-10-12 03:06:17,712][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 03:08:26,758][fairseq_cli.train][INFO] - end of epoch 2199 (average epoch stats below)
[2024-10-12 03:08:26,775][train][INFO] - {"epoch": 2199, "train_loss": "0.394", "train_ntokens": "260399", "train_nsentences": "1750.04", "train_wps": "96808.2", "train_ups": "0.37", "train_wpb": "260399", "train_bsz": "1750", "train_num_updates": "105508", "train_lr": "0.000400125", "train_gnorm": "0.359", "train_loss_scale": "2", "train_train_wall": "54", "train_gb_free": "39.8", "train_wall": "8628"}
[2024-10-12 03:08:26,880][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 03:08:26,896][fairseq.trainer][INFO] - begin training epoch 2200
[2024-10-12 03:08:26,897][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 03:10:37,337][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 2200 @ 105556 updates
[2024-10-12 03:10:37,340][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-12 03:10:40,844][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-12 03:10:40,847][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 2200 @ 105556 updates, score None) (writing took 3.50974287558347 seconds)
[2024-10-12 03:10:40,847][fairseq_cli.train][INFO] - end of epoch 2200 (average epoch stats below)
[2024-10-12 03:10:40,850][train][INFO] - {"epoch": 2200, "train_loss": "0.398", "train_ntokens": "260479", "train_nsentences": "1750.04", "train_wps": "93256.8", "train_ups": "0.36", "train_wpb": "260479", "train_bsz": "1750", "train_num_updates": "105556", "train_lr": "0.00040006", "train_gnorm": "0.362", "train_loss_scale": "2", "train_train_wall": "53", "train_gb_free": "39.1", "train_wall": "8762"}
[2024-10-12 03:10:40,920][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 03:10:40,925][fairseq.trainer][INFO] - begin training epoch 2201
[2024-10-12 03:10:40,925][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 03:12:42,630][train_inner][INFO] - {"epoch": 2201, "update": 2200.917, "loss": "0.394", "ntokens": "260426", "nsentences": "1752.43", "wps": "99375.2", "ups": "0.38", "wpb": "260426", "bsz": "1752.4", "num_updates": "105600", "lr": "0.0004", "gnorm": "0.363", "loss_scale": "2", "train_wall": "189", "gb_free": "39.3", "wall": "8884"}
[2024-10-12 03:12:44,146][fairseq_cli.train][INFO] - end of epoch 2201 (average epoch stats below)
[2024-10-12 03:12:44,148][train][INFO] - {"epoch": 2201, "train_loss": "0.394", "train_ntokens": "260622", "train_nsentences": "1750.04", "train_wps": "101462", "train_ups": "0.39", "train_wpb": "260622", "train_bsz": "1750", "train_num_updates": "105604", "train_lr": "0.000399995", "train_gnorm": "0.372", "train_loss_scale": "2", "train_train_wall": "32", "train_gb_free": "39.3", "train_wall": "8885"}
[2024-10-12 03:12:44,246][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 03:12:44,253][fairseq.trainer][INFO] - begin training epoch 2202
[2024-10-12 03:12:44,255][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 03:14:49,769][fairseq_cli.train][INFO] - end of epoch 2202 (average epoch stats below)
[2024-10-12 03:14:49,772][train][INFO] - {"epoch": 2202, "train_loss": "0.392", "train_ntokens": "260803", "train_nsentences": "1750.04", "train_wps": "99652.9", "train_ups": "0.38", "train_wpb": "260802", "train_bsz": "1750", "train_num_updates": "105652", "train_lr": "0.000399929", "train_gnorm": "0.369", "train_loss_scale": "2", "train_train_wall": "55", "train_gb_free": "40.3", "train_wall": "9011"}
[2024-10-12 03:14:49,874][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 03:14:49,879][fairseq.trainer][INFO] - begin training epoch 2203
[2024-10-12 03:14:49,879][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 03:16:52,967][fairseq_cli.train][INFO] - end of epoch 2203 (average epoch stats below)
[2024-10-12 03:16:52,996][train][INFO] - {"epoch": 2203, "train_loss": "0.393", "train_ntokens": "260799", "train_nsentences": "1750.04", "train_wps": "101600", "train_ups": "0.39", "train_wpb": "260799", "train_bsz": "1750", "train_num_updates": "105700", "train_lr": "0.000399864", "train_gnorm": "0.366", "train_loss_scale": "2", "train_train_wall": "37", "train_gb_free": "39.6", "train_wall": "9134"}
[2024-10-12 03:16:53,111][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 03:16:53,129][fairseq.trainer][INFO] - begin training epoch 2204
[2024-10-12 03:16:53,129][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 03:19:00,095][fairseq_cli.train][INFO] - end of epoch 2204 (average epoch stats below)
[2024-10-12 03:19:00,100][train][INFO] - {"epoch": 2204, "train_loss": "0.4", "train_ntokens": "260578", "train_nsentences": "1750.04", "train_wps": "98408.1", "train_ups": "0.38", "train_wpb": "260578", "train_bsz": "1750", "train_num_updates": "105748", "train_lr": "0.000399799", "train_gnorm": "0.362", "train_loss_scale": "2", "train_train_wall": "56", "train_gb_free": "39.8", "train_wall": "9261"}
[2024-10-12 03:19:00,220][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 03:19:00,234][fairseq.trainer][INFO] - begin training epoch 2205
[2024-10-12 03:19:00,235][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 03:21:06,626][fairseq_cli.train][INFO] - end of epoch 2205 (average epoch stats below)
[2024-10-12 03:21:06,630][train][INFO] - {"epoch": 2205, "train_loss": "0.39", "train_ntokens": "260276", "train_nsentences": "1750.04", "train_wps": "98739.8", "train_ups": "0.38", "train_wpb": "260276", "train_bsz": "1750", "train_num_updates": "105796", "train_lr": "0.000399734", "train_gnorm": "0.37", "train_loss_scale": "2", "train_train_wall": "34", "train_gb_free": "39.3", "train_wall": "9388"}
[2024-10-12 03:21:06,685][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 03:21:06,689][fairseq.trainer][INFO] - begin training epoch 2206
[2024-10-12 03:21:06,689][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 03:22:35,378][train_inner][INFO] - {"epoch": 2206, "update": 2205.083, "loss": "0.393", "ntokens": "260548", "nsentences": "1753.26", "wps": "87913.1", "ups": "0.34", "wpb": "260548", "bsz": "1753.3", "num_updates": "105800", "lr": "0.000399728", "gnorm": "0.366", "loss_scale": "2", "train_wall": "192", "gb_free": "40.1", "wall": "9477"}
[2024-10-12 03:23:12,949][fairseq_cli.train][INFO] - end of epoch 2206 (average epoch stats below)
[2024-10-12 03:23:12,951][train][INFO] - {"epoch": 2206, "train_loss": "0.394", "train_ntokens": "260241", "train_nsentences": "1750.04", "train_wps": "98889.3", "train_ups": "0.38", "train_wpb": "260241", "train_bsz": "1750", "train_num_updates": "105844", "train_lr": "0.000399668", "train_gnorm": "0.349", "train_loss_scale": "4", "train_train_wall": "39", "train_gb_free": "40.4", "train_wall": "9514"}
[2024-10-12 03:23:13,013][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 03:23:13,016][fairseq.trainer][INFO] - begin training epoch 2207
[2024-10-12 03:23:13,017][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 03:25:23,093][fairseq_cli.train][INFO] - end of epoch 2207 (average epoch stats below)
[2024-10-12 03:25:23,096][train][INFO] - {"epoch": 2207, "train_loss": "0.384", "train_ntokens": "260670", "train_nsentences": "1750.04", "train_wps": "96143", "train_ups": "0.37", "train_wpb": "260670", "train_bsz": "1750", "train_num_updates": "105892", "train_lr": "0.000399603", "train_gnorm": "0.359", "train_loss_scale": "4", "train_train_wall": "66", "train_gb_free": "39.1", "train_wall": "9644"}
[2024-10-12 03:25:23,204][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 03:25:23,208][fairseq.trainer][INFO] - begin training epoch 2208
[2024-10-12 03:25:23,208][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 03:27:28,976][fairseq_cli.train][INFO] - end of epoch 2208 (average epoch stats below)
[2024-10-12 03:27:28,980][train][INFO] - {"epoch": 2208, "train_loss": "0.392", "train_ntokens": "260462", "train_nsentences": "1750.04", "train_wps": "99317.2", "train_ups": "0.38", "train_wpb": "260462", "train_bsz": "1750", "train_num_updates": "105940", "train_lr": "0.000399538", "train_gnorm": "0.371", "train_loss_scale": "4", "train_train_wall": "46", "train_gb_free": "40.5", "train_wall": "9770"}
[2024-10-12 03:27:29,037][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 03:27:29,041][fairseq.trainer][INFO] - begin training epoch 2209
[2024-10-12 03:27:29,041][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 03:29:35,897][fairseq_cli.train][INFO] - end of epoch 2209 (average epoch stats below)
[2024-10-12 03:29:35,906][train][INFO] - {"epoch": 2209, "train_loss": "0.394", "train_ntokens": "260299", "train_nsentences": "1750.04", "train_wps": "98442.9", "train_ups": "0.38", "train_wpb": "260299", "train_bsz": "1750", "train_num_updates": "105988", "train_lr": "0.000399473", "train_gnorm": "0.364", "train_loss_scale": "4", "train_train_wall": "57", "train_gb_free": "39.2", "train_wall": "9897"}
[2024-10-12 03:29:36,025][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 03:29:36,029][fairseq.trainer][INFO] - begin training epoch 2210
[2024-10-12 03:29:36,029][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 03:31:10,581][train_inner][INFO] - {"epoch": 2210, "update": 2209.25, "loss": "0.39", "ntokens": "260622", "nsentences": "1735.36", "wps": "101175", "ups": "0.39", "wpb": "260622", "bsz": "1735.4", "num_updates": "106000", "lr": "0.000399457", "gnorm": "0.361", "loss_scale": "4", "train_wall": "219", "gb_free": "39.3", "wall": "9992"}
[2024-10-12 03:31:45,031][fairseq_cli.train][INFO] - end of epoch 2210 (average epoch stats below)
[2024-10-12 03:31:45,034][train][INFO] - {"epoch": 2210, "train_loss": "0.387", "train_ntokens": "260906", "train_nsentences": "1750.04", "train_wps": "96987.8", "train_ups": "0.37", "train_wpb": "260906", "train_bsz": "1750", "train_num_updates": "106036", "train_lr": "0.000399408", "train_gnorm": "0.375", "train_loss_scale": "4", "train_train_wall": "54", "train_gb_free": "39.7", "train_wall": "10026"}
[2024-10-12 03:31:45,139][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 03:31:45,151][fairseq.trainer][INFO] - begin training epoch 2211
[2024-10-12 03:31:45,152][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 03:33:50,313][fairseq_cli.train][INFO] - end of epoch 2211 (average epoch stats below)
[2024-10-12 03:33:50,316][train][INFO] - {"epoch": 2211, "train_loss": "0.383", "train_ntokens": "260278", "train_nsentences": "1750.04", "train_wps": "99724.4", "train_ups": "0.38", "train_wpb": "260278", "train_bsz": "1750", "train_num_updates": "106084", "train_lr": "0.000399342", "train_gnorm": "0.357", "train_loss_scale": "4", "train_train_wall": "47", "train_gb_free": "40.5", "train_wall": "10152"}
[2024-10-12 03:33:50,377][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 03:33:50,381][fairseq.trainer][INFO] - begin training epoch 2212
[2024-10-12 03:33:50,381][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 03:35:56,671][fairseq_cli.train][INFO] - end of epoch 2212 (average epoch stats below)
[2024-10-12 03:35:56,675][train][INFO] - {"epoch": 2212, "train_loss": "0.391", "train_ntokens": "260635", "train_nsentences": "1750.04", "train_wps": "99010", "train_ups": "0.38", "train_wpb": "260635", "train_bsz": "1750", "train_num_updates": "106132", "train_lr": "0.000399277", "train_gnorm": "0.354", "train_loss_scale": "4", "train_train_wall": "55", "train_gb_free": "39.2", "train_wall": "10278"}
[2024-10-12 03:35:56,780][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 03:35:56,794][fairseq.trainer][INFO] - begin training epoch 2213
[2024-10-12 03:35:56,794][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 03:38:01,854][fairseq_cli.train][INFO] - end of epoch 2213 (average epoch stats below)
[2024-10-12 03:38:01,861][train][INFO] - {"epoch": 2213, "train_loss": "0.403", "train_ntokens": "260703", "train_nsentences": "1750.04", "train_wps": "99963.9", "train_ups": "0.38", "train_wpb": "260703", "train_bsz": "1750", "train_num_updates": "106180", "train_lr": "0.000399212", "train_gnorm": "0.357", "train_loss_scale": "4", "train_train_wall": "54", "train_gb_free": "39.8", "train_wall": "10403"}
[2024-10-12 03:38:01,975][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 03:38:01,994][fairseq.trainer][INFO] - begin training epoch 2214
[2024-10-12 03:38:01,995][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 03:39:51,702][train_inner][INFO] - {"epoch": 2214, "update": 2213.417, "loss": "0.394", "ntokens": "260392", "nsentences": "1769.09", "wps": "99936.1", "ups": "0.38", "wpb": "260392", "bsz": "1769.1", "num_updates": "106200", "lr": "0.000399185", "gnorm": "0.359", "loss_scale": "4", "train_wall": "224", "gb_free": "39.7", "wall": "10513"}
[2024-10-12 03:40:10,558][fairseq_cli.train][INFO] - end of epoch 2214 (average epoch stats below)
[2024-10-12 03:40:10,562][train][INFO] - {"epoch": 2214, "train_loss": "0.392", "train_ntokens": "260483", "train_nsentences": "1750.04", "train_wps": "97152.4", "train_ups": "0.37", "train_wpb": "260483", "train_bsz": "1750", "train_num_updates": "106228", "train_lr": "0.000399147", "train_gnorm": "0.35", "train_loss_scale": "4", "train_train_wall": "53", "train_gb_free": "40.2", "train_wall": "10532"}
[2024-10-12 03:40:10,699][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 03:40:10,710][fairseq.trainer][INFO] - begin training epoch 2215
[2024-10-12 03:40:10,711][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 03:42:16,437][fairseq_cli.train][INFO] - end of epoch 2215 (average epoch stats below)
[2024-10-12 03:42:16,449][train][INFO] - {"epoch": 2215, "train_loss": "0.389", "train_ntokens": "260601", "train_nsentences": "1750.04", "train_wps": "99368.8", "train_ups": "0.38", "train_wpb": "260601", "train_bsz": "1750", "train_num_updates": "106276", "train_lr": "0.000399082", "train_gnorm": "0.365", "train_loss_scale": "4", "train_train_wall": "51", "train_gb_free": "39.2", "train_wall": "10658"}
[2024-10-12 03:42:16,569][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 03:42:16,582][fairseq.trainer][INFO] - begin training epoch 2216
[2024-10-12 03:42:16,583][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 03:44:21,131][fairseq_cli.train][INFO] - end of epoch 2216 (average epoch stats below)
[2024-10-12 03:44:21,143][train][INFO] - {"epoch": 2216, "train_loss": "0.393", "train_ntokens": "260683", "train_nsentences": "1750.04", "train_wps": "100352", "train_ups": "0.38", "train_wpb": "260683", "train_bsz": "1750", "train_num_updates": "106324", "train_lr": "0.000399016", "train_gnorm": "0.359", "train_loss_scale": "4", "train_train_wall": "52", "train_gb_free": "39.7", "train_wall": "10782"}
[2024-10-12 03:44:21,239][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 03:44:21,255][fairseq.trainer][INFO] - begin training epoch 2217
[2024-10-12 03:44:21,256][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 03:46:25,867][fairseq_cli.train][INFO] - end of epoch 2217 (average epoch stats below)
[2024-10-12 03:46:25,877][train][INFO] - {"epoch": 2217, "train_loss": "0.391", "train_ntokens": "260560", "train_nsentences": "1750.04", "train_wps": "100271", "train_ups": "0.38", "train_wpb": "260560", "train_bsz": "1750", "train_num_updates": "106372", "train_lr": "0.000398951", "train_gnorm": "0.347", "train_loss_scale": "4", "train_train_wall": "35", "train_gb_free": "39.3", "train_wall": "10907"}
[2024-10-12 03:46:25,992][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 03:46:25,996][fairseq.trainer][INFO] - begin training epoch 2218
[2024-10-12 03:46:25,997][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 03:48:17,012][train_inner][INFO] - {"epoch": 2218, "update": 2217.583, "loss": "0.39", "ntokens": "260628", "nsentences": "1742.52", "wps": "103157", "ups": "0.4", "wpb": "260628", "bsz": "1742.5", "num_updates": "106400", "lr": "0.000398913", "gnorm": "0.358", "loss_scale": "4", "train_wall": "186", "gb_free": "39.7", "wall": "11018"}
[2024-10-12 03:48:36,567][fairseq_cli.train][INFO] - end of epoch 2218 (average epoch stats below)
[2024-10-12 03:48:36,570][train][INFO] - {"epoch": 2218, "train_loss": "0.385", "train_ntokens": "260780", "train_nsentences": "1750.04", "train_wps": "95780.3", "train_ups": "0.37", "train_wpb": "260780", "train_bsz": "1750", "train_num_updates": "106420", "train_lr": "0.000398886", "train_gnorm": "0.384", "train_loss_scale": "4", "train_train_wall": "49", "train_gb_free": "39.7", "train_wall": "11038"}
[2024-10-12 03:48:36,677][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 03:48:36,682][fairseq.trainer][INFO] - begin training epoch 2219
[2024-10-12 03:48:36,682][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 03:50:43,073][fairseq_cli.train][INFO] - end of epoch 2219 (average epoch stats below)
[2024-10-12 03:50:43,107][train][INFO] - {"epoch": 2219, "train_loss": "0.393", "train_ntokens": "260754", "train_nsentences": "1750.04", "train_wps": "98918.9", "train_ups": "0.38", "train_wpb": "260754", "train_bsz": "1750", "train_num_updates": "106468", "train_lr": "0.000398821", "train_gnorm": "0.364", "train_loss_scale": "4", "train_train_wall": "51", "train_gb_free": "39.7", "train_wall": "11164"}
[2024-10-12 03:50:43,266][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 03:50:43,274][fairseq.trainer][INFO] - begin training epoch 2220
[2024-10-12 03:50:43,274][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 03:52:48,113][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 2220 @ 106516 updates
[2024-10-12 03:52:48,115][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-12 03:52:51,468][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-12 03:52:51,471][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 2220 @ 106516 updates, score None) (writing took 3.3573136813938618 seconds)
[2024-10-12 03:52:51,471][fairseq_cli.train][INFO] - end of epoch 2220 (average epoch stats below)
[2024-10-12 03:52:51,473][train][INFO] - {"epoch": 2220, "train_loss": "0.391", "train_ntokens": "260504", "train_nsentences": "1750.04", "train_wps": "97414.5", "train_ups": "0.37", "train_wpb": "260504", "train_bsz": "1750", "train_num_updates": "106516", "train_lr": "0.000398755", "train_gnorm": "0.362", "train_loss_scale": "4", "train_train_wall": "42", "train_gb_free": "40.8", "train_wall": "11293"}
[2024-10-12 03:52:51,526][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 03:52:51,530][fairseq.trainer][INFO] - begin training epoch 2221
[2024-10-12 03:52:51,530][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 03:54:55,613][fairseq_cli.train][INFO] - end of epoch 2221 (average epoch stats below)
[2024-10-12 03:54:55,618][train][INFO] - {"epoch": 2221, "train_loss": "0.391", "train_ntokens": "260804", "train_nsentences": "1750.04", "train_wps": "100841", "train_ups": "0.39", "train_wpb": "260804", "train_bsz": "1750", "train_num_updates": "106564", "train_lr": "0.00039869", "train_gnorm": "0.35", "train_loss_scale": "4", "train_train_wall": "32", "train_gb_free": "40.5", "train_wall": "11417"}
[2024-10-12 03:54:55,750][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 03:54:55,771][fairseq.trainer][INFO] - begin training epoch 2222
[2024-10-12 03:54:55,772][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 03:56:51,532][train_inner][INFO] - {"epoch": 2222, "update": 2221.75, "loss": "0.393", "ntokens": "260987", "nsentences": "1746.16", "wps": "101450", "ups": "0.39", "wpb": "260988", "bsz": "1746.2", "num_updates": "106600", "lr": "0.000398641", "gnorm": "0.365", "loss_scale": "4", "train_wall": "191", "gb_free": "39.6", "wall": "11533"}
[2024-10-12 03:57:02,822][fairseq_cli.train][INFO] - end of epoch 2222 (average epoch stats below)
[2024-10-12 03:57:02,827][train][INFO] - {"epoch": 2222, "train_loss": "0.401", "train_ntokens": "260725", "train_nsentences": "1750.04", "train_wps": "98384.9", "train_ups": "0.38", "train_wpb": "260725", "train_bsz": "1750", "train_num_updates": "106612", "train_lr": "0.000398625", "train_gnorm": "0.365", "train_loss_scale": "4", "train_train_wall": "58", "train_gb_free": "39.9", "train_wall": "11544"}
[2024-10-12 03:57:02,982][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 03:57:02,986][fairseq.trainer][INFO] - begin training epoch 2223
[2024-10-12 03:57:02,987][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 03:59:07,455][fairseq_cli.train][INFO] - end of epoch 2223 (average epoch stats below)
[2024-10-12 03:59:07,465][train][INFO] - {"epoch": 2223, "train_loss": "0.392", "train_ntokens": "260310", "train_nsentences": "1750.04", "train_wps": "100260", "train_ups": "0.39", "train_wpb": "260310", "train_bsz": "1750", "train_num_updates": "106660", "train_lr": "0.00039856", "train_gnorm": "0.351", "train_loss_scale": "4", "train_train_wall": "47", "train_gb_free": "40", "train_wall": "11669"}
[2024-10-12 03:59:07,602][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 03:59:07,607][fairseq.trainer][INFO] - begin training epoch 2224
[2024-10-12 03:59:07,608][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 04:01:13,902][fairseq_cli.train][INFO] - end of epoch 2224 (average epoch stats below)
[2024-10-12 04:01:13,917][train][INFO] - {"epoch": 2224, "train_loss": "0.391", "train_ntokens": "260602", "train_nsentences": "1750.04", "train_wps": "98924.9", "train_ups": "0.38", "train_wpb": "260602", "train_bsz": "1750", "train_num_updates": "106708", "train_lr": "0.000398495", "train_gnorm": "0.357", "train_loss_scale": "4", "train_train_wall": "38", "train_gb_free": "39.7", "train_wall": "11795"}
[2024-10-12 04:01:14,047][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 04:01:14,051][fairseq.trainer][INFO] - begin training epoch 2225
[2024-10-12 04:01:14,051][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 04:03:20,327][fairseq_cli.train][INFO] - end of epoch 2225 (average epoch stats below)
[2024-10-12 04:03:20,334][train][INFO] - {"epoch": 2225, "train_loss": "0.387", "train_ntokens": "260691", "train_nsentences": "1750.04", "train_wps": "98988", "train_ups": "0.38", "train_wpb": "260690", "train_bsz": "1750", "train_num_updates": "106756", "train_lr": "0.000398429", "train_gnorm": "0.375", "train_loss_scale": "4", "train_train_wall": "51", "train_gb_free": "39.2", "train_wall": "11922"}
[2024-10-12 04:03:20,408][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 04:03:20,411][fairseq.trainer][INFO] - begin training epoch 2226
[2024-10-12 04:03:20,412][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 04:05:27,746][train_inner][INFO] - {"epoch": 2226, "update": 2225.917, "loss": "0.392", "ntokens": "260402", "nsentences": "1754.38", "wps": "100892", "ups": "0.39", "wpb": "260402", "bsz": "1754.4", "num_updates": "106800", "lr": "0.00039837", "gnorm": "0.354", "loss_scale": "4", "train_wall": "205", "gb_free": "39.6", "wall": "12049"}
[2024-10-12 04:05:28,741][fairseq_cli.train][INFO] - end of epoch 2226 (average epoch stats below)
[2024-10-12 04:05:28,746][train][INFO] - {"epoch": 2226, "train_loss": "0.397", "train_ntokens": "260855", "train_nsentences": "1750.04", "train_wps": "97511.5", "train_ups": "0.37", "train_wpb": "260855", "train_bsz": "1750", "train_num_updates": "106804", "train_lr": "0.000398364", "train_gnorm": "0.328", "train_loss_scale": "4", "train_train_wall": "59", "train_gb_free": "39.6", "train_wall": "12050"}
[2024-10-12 04:05:28,871][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 04:05:28,885][fairseq.trainer][INFO] - begin training epoch 2227
[2024-10-12 04:05:28,886][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 04:07:38,450][fairseq_cli.train][INFO] - end of epoch 2227 (average epoch stats below)
[2024-10-12 04:07:38,453][train][INFO] - {"epoch": 2227, "train_loss": "0.388", "train_ntokens": "261185", "train_nsentences": "1750.04", "train_wps": "96657.9", "train_ups": "0.37", "train_wpb": "261185", "train_bsz": "1750", "train_num_updates": "106852", "train_lr": "0.000398299", "train_gnorm": "0.354", "train_loss_scale": "4", "train_train_wall": "57", "train_gb_free": "39.8", "train_wall": "12180"}
[2024-10-12 04:07:38,516][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 04:07:38,519][fairseq.trainer][INFO] - begin training epoch 2228
[2024-10-12 04:07:38,519][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 04:09:46,147][fairseq_cli.train][INFO] - end of epoch 2228 (average epoch stats below)
[2024-10-12 04:09:46,152][train][INFO] - {"epoch": 2228, "train_loss": "0.391", "train_ntokens": "260724", "train_nsentences": "1750.04", "train_wps": "98004.5", "train_ups": "0.38", "train_wpb": "260724", "train_bsz": "1750", "train_num_updates": "106900", "train_lr": "0.000398234", "train_gnorm": "0.355", "train_loss_scale": "4", "train_train_wall": "40", "train_gb_free": "41.1", "train_wall": "12307"}
[2024-10-12 04:09:46,296][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 04:09:46,310][fairseq.trainer][INFO] - begin training epoch 2229
[2024-10-12 04:09:46,310][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 04:11:55,571][fairseq_cli.train][INFO] - end of epoch 2229 (average epoch stats below)
[2024-10-12 04:11:55,591][train][INFO] - {"epoch": 2229, "train_loss": "0.391", "train_ntokens": "260439", "train_nsentences": "1750.04", "train_wps": "96591.2", "train_ups": "0.37", "train_wpb": "260439", "train_bsz": "1750", "train_num_updates": "106948", "train_lr": "0.000398168", "train_gnorm": "0.367", "train_loss_scale": "4", "train_train_wall": "57", "train_gb_free": "40.7", "train_wall": "12437"}
[2024-10-12 04:11:55,681][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 04:11:55,693][fairseq.trainer][INFO] - begin training epoch 2230
[2024-10-12 04:11:55,693][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 04:14:06,932][fairseq_cli.train][INFO] - end of epoch 2230 (average epoch stats below)
[2024-10-12 04:14:06,936][train][INFO] - {"epoch": 2230, "train_loss": "0.39", "train_ntokens": "260618", "train_nsentences": "1750.04", "train_wps": "95245.7", "train_ups": "0.37", "train_wpb": "260618", "train_bsz": "1750", "train_num_updates": "106996", "train_lr": "0.000398103", "train_gnorm": "0.353", "train_loss_scale": "4", "train_train_wall": "67", "train_gb_free": "39.7", "train_wall": "12568"}
[2024-10-12 04:14:06,996][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 04:14:07,000][fairseq.trainer][INFO] - begin training epoch 2231
[2024-10-12 04:14:07,000][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 04:15:34,243][train_inner][INFO] - {"epoch": 2231, "update": 2230.083, "loss": "0.391", "ntokens": "260707", "nsentences": "1745.82", "wps": "85972.1", "ups": "0.33", "wpb": "260707", "bsz": "1745.8", "num_updates": "107000", "lr": "0.000398098", "gnorm": "0.356", "loss_scale": "4", "train_wall": "232", "gb_free": "39.4", "wall": "12655"}
[2024-10-12 04:16:17,115][fairseq_cli.train][INFO] - end of epoch 2231 (average epoch stats below)
[2024-10-12 04:16:17,118][train][INFO] - {"epoch": 2231, "train_loss": "0.39", "train_ntokens": "260452", "train_nsentences": "1750.04", "train_wps": "96034.9", "train_ups": "0.37", "train_wpb": "260452", "train_bsz": "1750", "train_num_updates": "107044", "train_lr": "0.000398038", "train_gnorm": "0.359", "train_loss_scale": "4", "train_train_wall": "53", "train_gb_free": "40.1", "train_wall": "12698"}
[2024-10-12 04:16:17,228][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 04:16:17,240][fairseq.trainer][INFO] - begin training epoch 2232
[2024-10-12 04:16:17,241][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 04:18:28,573][fairseq_cli.train][INFO] - end of epoch 2232 (average epoch stats below)
[2024-10-12 04:18:28,588][train][INFO] - {"epoch": 2232, "train_loss": "0.387", "train_ntokens": "260577", "train_nsentences": "1750.04", "train_wps": "95139", "train_ups": "0.37", "train_wpb": "260577", "train_bsz": "1750", "train_num_updates": "107092", "train_lr": "0.000397973", "train_gnorm": "0.356", "train_loss_scale": "4", "train_train_wall": "43", "train_gb_free": "39.7", "train_wall": "12830"}
[2024-10-12 04:18:28,714][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 04:18:28,717][fairseq.trainer][INFO] - begin training epoch 2233
[2024-10-12 04:18:28,718][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 04:20:36,623][fairseq_cli.train][INFO] - end of epoch 2233 (average epoch stats below)
[2024-10-12 04:20:36,633][train][INFO] - {"epoch": 2233, "train_loss": "0.389", "train_ntokens": "260668", "train_nsentences": "1750.04", "train_wps": "97718.9", "train_ups": "0.37", "train_wpb": "260668", "train_bsz": "1750", "train_num_updates": "107140", "train_lr": "0.000397908", "train_gnorm": "0.373", "train_loss_scale": "4", "train_train_wall": "40", "train_gb_free": "39.7", "train_wall": "12958"}
[2024-10-12 04:20:36,760][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 04:20:36,768][fairseq.trainer][INFO] - begin training epoch 2234
[2024-10-12 04:20:36,768][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 04:22:42,868][fairseq_cli.train][INFO] - end of epoch 2234 (average epoch stats below)
[2024-10-12 04:22:42,872][train][INFO] - {"epoch": 2234, "train_loss": "0.392", "train_ntokens": "260739", "train_nsentences": "1750.04", "train_wps": "99144.2", "train_ups": "0.38", "train_wpb": "260739", "train_bsz": "1750", "train_num_updates": "107188", "train_lr": "0.000397842", "train_gnorm": "0.355", "train_loss_scale": "4", "train_train_wall": "49", "train_gb_free": "39.3", "train_wall": "13084"}
[2024-10-12 04:22:42,982][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 04:22:42,994][fairseq.trainer][INFO] - begin training epoch 2235
[2024-10-12 04:22:42,994][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 04:24:17,663][train_inner][INFO] - {"epoch": 2235, "update": 2234.25, "loss": "0.389", "ntokens": "260568", "nsentences": "1758.6", "wps": "99564.3", "ups": "0.38", "wpb": "260568", "bsz": "1758.6", "num_updates": "107200", "lr": "0.000397826", "gnorm": "0.362", "loss_scale": "4", "train_wall": "193", "gb_free": "39.9", "wall": "13179"}
[2024-10-12 04:24:46,055][fairseq_cli.train][INFO] - end of epoch 2235 (average epoch stats below)
[2024-10-12 04:24:46,057][train][INFO] - {"epoch": 2235, "train_loss": "0.388", "train_ntokens": "260969", "train_nsentences": "1750.04", "train_wps": "101691", "train_ups": "0.39", "train_wpb": "260969", "train_bsz": "1750", "train_num_updates": "107236", "train_lr": "0.000397777", "train_gnorm": "0.377", "train_loss_scale": "4", "train_train_wall": "46", "train_gb_free": "39.6", "train_wall": "13207"}
[2024-10-12 04:24:46,149][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 04:24:46,172][fairseq.trainer][INFO] - begin training epoch 2236
[2024-10-12 04:24:46,173][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 04:26:27,476][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2024-10-12 04:26:57,599][fairseq_cli.train][INFO] - end of epoch 2236 (average epoch stats below)
[2024-10-12 04:26:57,604][train][INFO] - {"epoch": 2236, "train_loss": "0.39", "train_ntokens": "260547", "train_nsentences": "1753.77", "train_wps": "93093.2", "train_ups": "0.36", "train_wpb": "260547", "train_bsz": "1753.8", "train_num_updates": "107283", "train_lr": "0.000397713", "train_gnorm": "0.377", "train_loss_scale": "2", "train_train_wall": "51", "train_gb_free": "40.1", "train_wall": "13339"}
[2024-10-12 04:26:57,679][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 04:26:57,684][fairseq.trainer][INFO] - begin training epoch 2237
[2024-10-12 04:26:57,685][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 04:29:03,449][fairseq_cli.train][INFO] - end of epoch 2237 (average epoch stats below)
[2024-10-12 04:29:03,454][train][INFO] - {"epoch": 2237, "train_loss": "0.393", "train_ntokens": "260621", "train_nsentences": "1750.04", "train_wps": "99405.2", "train_ups": "0.38", "train_wpb": "260621", "train_bsz": "1750", "train_num_updates": "107331", "train_lr": "0.000397648", "train_gnorm": "0.339", "train_loss_scale": "2", "train_train_wall": "50", "train_gb_free": "39.6", "train_wall": "13465"}
[2024-10-12 04:29:03,506][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 04:29:03,510][fairseq.trainer][INFO] - begin training epoch 2238
[2024-10-12 04:29:03,511][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 04:31:09,400][fairseq_cli.train][INFO] - end of epoch 2238 (average epoch stats below)
[2024-10-12 04:31:09,407][train][INFO] - {"epoch": 2238, "train_loss": "0.392", "train_ntokens": "260983", "train_nsentences": "1750.04", "train_wps": "99461.2", "train_ups": "0.38", "train_wpb": "260983", "train_bsz": "1750", "train_num_updates": "107379", "train_lr": "0.000397583", "train_gnorm": "0.379", "train_loss_scale": "2", "train_train_wall": "42", "train_gb_free": "39.8", "train_wall": "13591"}
[2024-10-12 04:31:09,512][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 04:31:09,518][fairseq.trainer][INFO] - begin training epoch 2239
[2024-10-12 04:31:09,519][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 04:32:55,162][train_inner][INFO] - {"epoch": 2239, "update": 2238.438, "loss": "0.39", "ntokens": "260663", "nsentences": "1757.26", "wps": "100740", "ups": "0.39", "wpb": "260663", "bsz": "1757.3", "num_updates": "107400", "lr": "0.000397554", "gnorm": "0.366", "loss_scale": "2", "train_wall": "208", "gb_free": "40", "wall": "13696"}
[2024-10-12 04:33:16,491][fairseq_cli.train][INFO] - end of epoch 2239 (average epoch stats below)
[2024-10-12 04:33:16,494][train][INFO] - {"epoch": 2239, "train_loss": "0.388", "train_ntokens": "260604", "train_nsentences": "1750.04", "train_wps": "98432.2", "train_ups": "0.38", "train_wpb": "260604", "train_bsz": "1750", "train_num_updates": "107427", "train_lr": "0.000397518", "train_gnorm": "0.344", "train_loss_scale": "2", "train_train_wall": "57", "train_gb_free": "39.7", "train_wall": "13718"}
[2024-10-12 04:33:16,553][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 04:33:16,558][fairseq.trainer][INFO] - begin training epoch 2240
[2024-10-12 04:33:16,558][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 04:35:21,880][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 2240 @ 107475 updates
[2024-10-12 04:35:21,881][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-12 04:35:25,762][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-12 04:35:25,764][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 2240 @ 107475 updates, score None) (writing took 3.884052002802491 seconds)
[2024-10-12 04:35:25,765][fairseq_cli.train][INFO] - end of epoch 2240 (average epoch stats below)
[2024-10-12 04:35:25,766][train][INFO] - {"epoch": 2240, "train_loss": "0.385", "train_ntokens": "260726", "train_nsentences": "1750.04", "train_wps": "96811.7", "train_ups": "0.37", "train_wpb": "260726", "train_bsz": "1750", "train_num_updates": "107475", "train_lr": "0.000397452", "train_gnorm": "0.35", "train_loss_scale": "2", "train_train_wall": "50", "train_gb_free": "41.1", "train_wall": "13847"}
[2024-10-12 04:35:25,839][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 04:35:25,878][fairseq.trainer][INFO] - begin training epoch 2241
[2024-10-12 04:35:25,878][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 04:37:31,069][fairseq_cli.train][INFO] - end of epoch 2241 (average epoch stats below)
[2024-10-12 04:37:31,072][train][INFO] - {"epoch": 2241, "train_loss": "0.393", "train_ntokens": "260503", "train_nsentences": "1750.04", "train_wps": "99791.8", "train_ups": "0.38", "train_wpb": "260503", "train_bsz": "1750", "train_num_updates": "107523", "train_lr": "0.000397387", "train_gnorm": "0.377", "train_loss_scale": "2", "train_train_wall": "55", "train_gb_free": "39.2", "train_wall": "13972"}
[2024-10-12 04:37:31,130][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 04:37:31,134][fairseq.trainer][INFO] - begin training epoch 2242
[2024-10-12 04:37:31,134][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 04:39:37,463][fairseq_cli.train][INFO] - end of epoch 2242 (average epoch stats below)
[2024-10-12 04:39:37,466][train][INFO] - {"epoch": 2242, "train_loss": "0.388", "train_ntokens": "260661", "train_nsentences": "1750.04", "train_wps": "98992.1", "train_ups": "0.38", "train_wpb": "260661", "train_bsz": "1750", "train_num_updates": "107571", "train_lr": "0.000397322", "train_gnorm": "0.35", "train_loss_scale": "2", "train_train_wall": "49", "train_gb_free": "39.6", "train_wall": "14099"}
[2024-10-12 04:39:37,523][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 04:39:37,528][fairseq.trainer][INFO] - begin training epoch 2243
[2024-10-12 04:39:37,529][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 04:41:28,202][train_inner][INFO] - {"epoch": 2243, "update": 2242.604, "loss": "0.388", "ntokens": "260966", "nsentences": "1737.85", "wps": "101734", "ups": "0.39", "wpb": "260966", "bsz": "1737.9", "num_updates": "107600", "lr": "0.000397283", "gnorm": "0.361", "loss_scale": "2", "train_wall": "217", "gb_free": "39.2", "wall": "14209"}
[2024-10-12 04:41:48,105][fairseq_cli.train][INFO] - end of epoch 2243 (average epoch stats below)
[2024-10-12 04:41:48,108][train][INFO] - {"epoch": 2243, "train_loss": "0.38", "train_ntokens": "260552", "train_nsentences": "1750.04", "train_wps": "95734", "train_ups": "0.37", "train_wpb": "260552", "train_bsz": "1750", "train_num_updates": "107619", "train_lr": "0.000397257", "train_gnorm": "0.373", "train_loss_scale": "2", "train_train_wall": "63", "train_gb_free": "39.6", "train_wall": "14229"}
[2024-10-12 04:41:48,165][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 04:41:48,169][fairseq.trainer][INFO] - begin training epoch 2244
[2024-10-12 04:41:48,169][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 04:43:54,364][fairseq_cli.train][INFO] - end of epoch 2244 (average epoch stats below)
[2024-10-12 04:43:54,387][train][INFO] - {"epoch": 2244, "train_loss": "0.385", "train_ntokens": "260380", "train_nsentences": "1750.04", "train_wps": "98985.9", "train_ups": "0.38", "train_wpb": "260380", "train_bsz": "1750", "train_num_updates": "107667", "train_lr": "0.000397192", "train_gnorm": "0.354", "train_loss_scale": "2", "train_train_wall": "49", "train_gb_free": "39.6", "train_wall": "14356"}
[2024-10-12 04:43:54,494][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 04:43:54,504][fairseq.trainer][INFO] - begin training epoch 2245
[2024-10-12 04:43:54,505][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 04:46:01,418][fairseq_cli.train][INFO] - end of epoch 2245 (average epoch stats below)
[2024-10-12 04:46:01,422][train][INFO] - {"epoch": 2245, "train_loss": "0.389", "train_ntokens": "260518", "train_nsentences": "1750.04", "train_wps": "98439.3", "train_ups": "0.38", "train_wpb": "260518", "train_bsz": "1750", "train_num_updates": "107715", "train_lr": "0.000397126", "train_gnorm": "0.357", "train_loss_scale": "2", "train_train_wall": "61", "train_gb_free": "39.6", "train_wall": "14483"}
[2024-10-12 04:46:01,488][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 04:46:01,492][fairseq.trainer][INFO] - begin training epoch 2246
[2024-10-12 04:46:01,493][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 04:48:14,013][fairseq_cli.train][INFO] - end of epoch 2246 (average epoch stats below)
[2024-10-12 04:48:14,019][train][INFO] - {"epoch": 2246, "train_loss": "0.387", "train_ntokens": "260559", "train_nsentences": "1750.04", "train_wps": "94325.2", "train_ups": "0.36", "train_wpb": "260558", "train_bsz": "1750", "train_num_updates": "107763", "train_lr": "0.000397061", "train_gnorm": "0.355", "train_loss_scale": "2", "train_train_wall": "63", "train_gb_free": "39.6", "train_wall": "14615"}
[2024-10-12 04:48:14,123][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 04:48:14,129][fairseq.trainer][INFO] - begin training epoch 2247
[2024-10-12 04:48:14,130][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 04:50:09,910][train_inner][INFO] - {"epoch": 2247, "update": 2246.771, "loss": "0.386", "ntokens": "260279", "nsentences": "1757.54", "wps": "99780.4", "ups": "0.38", "wpb": "260279", "bsz": "1757.5", "num_updates": "107800", "lr": "0.000397011", "gnorm": "0.359", "loss_scale": "2", "train_wall": "243", "gb_free": "39.8", "wall": "14731"}
[2024-10-12 04:50:20,768][fairseq_cli.train][INFO] - end of epoch 2247 (average epoch stats below)
[2024-10-12 04:50:20,771][train][INFO] - {"epoch": 2247, "train_loss": "0.387", "train_ntokens": "260754", "train_nsentences": "1750.04", "train_wps": "98748.6", "train_ups": "0.38", "train_wpb": "260754", "train_bsz": "1750", "train_num_updates": "107811", "train_lr": "0.000396996", "train_gnorm": "0.371", "train_loss_scale": "2", "train_train_wall": "60", "train_gb_free": "39.2", "train_wall": "14742"}
[2024-10-12 04:50:20,861][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 04:50:20,873][fairseq.trainer][INFO] - begin training epoch 2248
[2024-10-12 04:50:20,874][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 04:52:28,449][fairseq_cli.train][INFO] - end of epoch 2248 (average epoch stats below)
[2024-10-12 04:52:28,453][train][INFO] - {"epoch": 2248, "train_loss": "0.384", "train_ntokens": "260615", "train_nsentences": "1750.04", "train_wps": "97976.5", "train_ups": "0.38", "train_wpb": "260615", "train_bsz": "1750", "train_num_updates": "107859", "train_lr": "0.000396931", "train_gnorm": "0.368", "train_loss_scale": "2", "train_train_wall": "59", "train_gb_free": "40.1", "train_wall": "14870"}
[2024-10-12 04:52:28,550][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 04:52:28,561][fairseq.trainer][INFO] - begin training epoch 2249
[2024-10-12 04:52:28,561][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 04:54:34,964][fairseq_cli.train][INFO] - end of epoch 2249 (average epoch stats below)
[2024-10-12 04:54:34,979][train][INFO] - {"epoch": 2249, "train_loss": "0.394", "train_ntokens": "260517", "train_nsentences": "1750.04", "train_wps": "98835.2", "train_ups": "0.38", "train_wpb": "260517", "train_bsz": "1750", "train_num_updates": "107907", "train_lr": "0.000396865", "train_gnorm": "0.369", "train_loss_scale": "2", "train_train_wall": "43", "train_gb_free": "39.7", "train_wall": "14996"}
[2024-10-12 04:54:35,093][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 04:54:35,120][fairseq.trainer][INFO] - begin training epoch 2250
[2024-10-12 04:54:35,121][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 04:56:43,356][fairseq_cli.train][INFO] - end of epoch 2250 (average epoch stats below)
[2024-10-12 04:56:43,371][train][INFO] - {"epoch": 2250, "train_loss": "0.39", "train_ntokens": "260911", "train_nsentences": "1750.04", "train_wps": "97546.4", "train_ups": "0.37", "train_wpb": "260911", "train_bsz": "1750", "train_num_updates": "107955", "train_lr": "0.0003968", "train_gnorm": "0.361", "train_loss_scale": "2", "train_train_wall": "57", "train_gb_free": "39.8", "train_wall": "15125"}
[2024-10-12 04:56:43,486][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 04:56:43,489][fairseq.trainer][INFO] - begin training epoch 2251
[2024-10-12 04:56:43,489][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 04:58:49,775][train_inner][INFO] - {"epoch": 2251, "update": 2250.938, "loss": "0.389", "ntokens": "260807", "nsentences": "1743.58", "wps": "100340", "ups": "0.38", "wpb": "260807", "bsz": "1743.6", "num_updates": "108000", "lr": "0.000396739", "gnorm": "0.36", "loss_scale": "2", "train_wall": "221", "gb_free": "40.1", "wall": "15251"}
[2024-10-12 04:58:50,445][fairseq_cli.train][INFO] - end of epoch 2251 (average epoch stats below)
[2024-10-12 04:58:50,447][train][INFO] - {"epoch": 2251, "train_loss": "0.388", "train_ntokens": "260685", "train_nsentences": "1750.04", "train_wps": "98476.8", "train_ups": "0.38", "train_wpb": "260686", "train_bsz": "1750", "train_num_updates": "108003", "train_lr": "0.000396735", "train_gnorm": "0.343", "train_loss_scale": "2", "train_train_wall": "52", "train_gb_free": "39.8", "train_wall": "15252"}
[2024-10-12 04:58:50,507][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 04:58:50,513][fairseq.trainer][INFO] - begin training epoch 2252
[2024-10-12 04:58:50,514][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 05:00:55,383][fairseq_cli.train][INFO] - end of epoch 2252 (average epoch stats below)
[2024-10-12 05:00:55,387][train][INFO] - {"epoch": 2252, "train_loss": "0.388", "train_ntokens": "260735", "train_nsentences": "1750.04", "train_wps": "100173", "train_ups": "0.38", "train_wpb": "260735", "train_bsz": "1750", "train_num_updates": "108051", "train_lr": "0.00039667", "train_gnorm": "0.353", "train_loss_scale": "2", "train_train_wall": "54", "train_gb_free": "39.8", "train_wall": "15377"}
[2024-10-12 05:00:55,483][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 05:00:55,501][fairseq.trainer][INFO] - begin training epoch 2253
[2024-10-12 05:00:55,502][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 05:03:08,351][fairseq_cli.train][INFO] - end of epoch 2253 (average epoch stats below)
[2024-10-12 05:03:08,359][train][INFO] - {"epoch": 2253, "train_loss": "0.382", "train_ntokens": "260846", "train_nsentences": "1750.04", "train_wps": "94163.2", "train_ups": "0.36", "train_wpb": "260846", "train_bsz": "1750", "train_num_updates": "108099", "train_lr": "0.000396605", "train_gnorm": "0.374", "train_loss_scale": "2", "train_train_wall": "56", "train_gb_free": "40", "train_wall": "15510"}
[2024-10-12 05:03:08,439][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 05:03:08,442][fairseq.trainer][INFO] - begin training epoch 2254
[2024-10-12 05:03:08,443][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 05:05:14,603][fairseq_cli.train][INFO] - end of epoch 2254 (average epoch stats below)
[2024-10-12 05:05:14,609][train][INFO] - {"epoch": 2254, "train_loss": "0.388", "train_ntokens": "260792", "train_nsentences": "1750.04", "train_wps": "99157.3", "train_ups": "0.38", "train_wpb": "260792", "train_bsz": "1750", "train_num_updates": "108147", "train_lr": "0.000396539", "train_gnorm": "0.368", "train_loss_scale": "2", "train_train_wall": "49", "train_gb_free": "40", "train_wall": "15636"}
[2024-10-12 05:05:14,713][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 05:05:14,718][fairseq.trainer][INFO] - begin training epoch 2255
[2024-10-12 05:05:14,718][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 05:07:19,111][fairseq_cli.train][INFO] - end of epoch 2255 (average epoch stats below)
[2024-10-12 05:07:19,139][train][INFO] - {"epoch": 2255, "train_loss": "0.386", "train_ntokens": "260996", "train_nsentences": "1750.04", "train_wps": "100613", "train_ups": "0.39", "train_wpb": "260996", "train_bsz": "1750", "train_num_updates": "108195", "train_lr": "0.000396474", "train_gnorm": "0.348", "train_loss_scale": "2", "train_train_wall": "41", "train_gb_free": "39.7", "train_wall": "15760"}
[2024-10-12 05:07:19,234][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 05:07:19,250][fairseq.trainer][INFO] - begin training epoch 2256
[2024-10-12 05:07:19,250][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 05:08:47,836][train_inner][INFO] - {"epoch": 2256, "update": 2255.104, "loss": "0.386", "ntokens": "260755", "nsentences": "1746.2", "wps": "87201.8", "ups": "0.33", "wpb": "260755", "bsz": "1746.2", "num_updates": "108200", "lr": "0.000396467", "gnorm": "0.36", "loss_scale": "2", "train_wall": "207", "gb_free": "39.7", "wall": "15849"}
[2024-10-12 05:09:24,374][fairseq_cli.train][INFO] - end of epoch 2256 (average epoch stats below)
[2024-10-12 05:09:24,377][train][INFO] - {"epoch": 2256, "train_loss": "0.391", "train_ntokens": "260585", "train_nsentences": "1750.04", "train_wps": "99877.5", "train_ups": "0.38", "train_wpb": "260585", "train_bsz": "1750", "train_num_updates": "108243", "train_lr": "0.000396409", "train_gnorm": "0.37", "train_loss_scale": "2", "train_train_wall": "40", "train_gb_free": "40.2", "train_wall": "15886"}
[2024-10-12 05:09:24,478][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 05:09:24,482][fairseq.trainer][INFO] - begin training epoch 2257
[2024-10-12 05:09:24,482][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 05:11:30,756][fairseq_cli.train][INFO] - end of epoch 2257 (average epoch stats below)
[2024-10-12 05:11:30,768][train][INFO] - {"epoch": 2257, "train_loss": "0.388", "train_ntokens": "260620", "train_nsentences": "1750.04", "train_wps": "98984.6", "train_ups": "0.38", "train_wpb": "260620", "train_bsz": "1750", "train_num_updates": "108291", "train_lr": "0.000396344", "train_gnorm": "0.371", "train_loss_scale": "2", "train_train_wall": "52", "train_gb_free": "39.2", "train_wall": "16012"}
[2024-10-12 05:11:30,830][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 05:11:30,833][fairseq.trainer][INFO] - begin training epoch 2258
[2024-10-12 05:11:30,834][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 05:13:38,965][fairseq_cli.train][INFO] - end of epoch 2258 (average epoch stats below)
[2024-10-12 05:13:38,981][train][INFO] - {"epoch": 2258, "train_loss": "0.39", "train_ntokens": "260690", "train_nsentences": "1750.04", "train_wps": "97599.7", "train_ups": "0.37", "train_wpb": "260690", "train_bsz": "1750", "train_num_updates": "108339", "train_lr": "0.000396279", "train_gnorm": "0.368", "train_loss_scale": "2", "train_train_wall": "35", "train_gb_free": "39.7", "train_wall": "16140"}
[2024-10-12 05:13:39,087][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 05:13:39,091][fairseq.trainer][INFO] - begin training epoch 2259
[2024-10-12 05:13:39,091][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 05:15:48,787][fairseq_cli.train][INFO] - end of epoch 2259 (average epoch stats below)
[2024-10-12 05:15:48,790][train][INFO] - {"epoch": 2259, "train_loss": "0.394", "train_ntokens": "260990", "train_nsentences": "1750.04", "train_wps": "96510.3", "train_ups": "0.37", "train_wpb": "260990", "train_bsz": "1750", "train_num_updates": "108387", "train_lr": "0.000396213", "train_gnorm": "0.36", "train_loss_scale": "2", "train_train_wall": "60", "train_gb_free": "39.7", "train_wall": "16270"}
[2024-10-12 05:15:48,850][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 05:15:48,854][fairseq.trainer][INFO] - begin training epoch 2260
[2024-10-12 05:15:48,854][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 05:17:25,814][train_inner][INFO] - {"epoch": 2260, "update": 2259.271, "loss": "0.391", "ntokens": "260573", "nsentences": "1767.08", "wps": "100613", "ups": "0.39", "wpb": "260573", "bsz": "1767.1", "num_updates": "108400", "lr": "0.000396196", "gnorm": "0.366", "loss_scale": "2", "train_wall": "206", "gb_free": "39.5", "wall": "16367"}
[2024-10-12 05:17:58,310][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 2260 @ 108435 updates
[2024-10-12 05:17:58,311][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-12 05:18:01,462][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-12 05:18:01,464][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 2260 @ 108435 updates, score None) (writing took 3.154333060607314 seconds)
[2024-10-12 05:18:01,465][fairseq_cli.train][INFO] - end of epoch 2260 (average epoch stats below)
[2024-10-12 05:18:01,467][train][INFO] - {"epoch": 2260, "train_loss": "0.386", "train_ntokens": "260773", "train_nsentences": "1750.04", "train_wps": "94345.6", "train_ups": "0.36", "train_wpb": "260773", "train_bsz": "1750", "train_num_updates": "108435", "train_lr": "0.000396148", "train_gnorm": "0.348", "train_loss_scale": "2", "train_train_wall": "57", "train_gb_free": "40.3", "train_wall": "16403"}
[2024-10-12 05:18:01,517][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 05:18:01,528][fairseq.trainer][INFO] - begin training epoch 2261
[2024-10-12 05:18:01,528][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 05:20:12,457][fairseq_cli.train][INFO] - end of epoch 2261 (average epoch stats below)
[2024-10-12 05:20:12,466][train][INFO] - {"epoch": 2261, "train_loss": "0.398", "train_ntokens": "260456", "train_nsentences": "1750.04", "train_wps": "95437.6", "train_ups": "0.37", "train_wpb": "260456", "train_bsz": "1750", "train_num_updates": "108483", "train_lr": "0.000396083", "train_gnorm": "0.352", "train_loss_scale": "2", "train_train_wall": "60", "train_gb_free": "39.4", "train_wall": "16534"}
[2024-10-12 05:20:12,629][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 05:20:12,649][fairseq.trainer][INFO] - begin training epoch 2262
[2024-10-12 05:20:12,650][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 05:22:22,597][fairseq_cli.train][INFO] - end of epoch 2262 (average epoch stats below)
[2024-10-12 05:22:22,600][train][INFO] - {"epoch": 2262, "train_loss": "0.382", "train_ntokens": "260542", "train_nsentences": "1750.04", "train_wps": "96104.2", "train_ups": "0.37", "train_wpb": "260542", "train_bsz": "1750", "train_num_updates": "108531", "train_lr": "0.000396018", "train_gnorm": "0.353", "train_loss_scale": "2", "train_train_wall": "70", "train_gb_free": "40.3", "train_wall": "16664"}
[2024-10-12 05:22:22,729][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 05:22:22,735][fairseq.trainer][INFO] - begin training epoch 2263
[2024-10-12 05:22:22,735][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 05:24:30,307][fairseq_cli.train][INFO] - end of epoch 2263 (average epoch stats below)
[2024-10-12 05:24:30,310][train][INFO] - {"epoch": 2263, "train_loss": "0.386", "train_ntokens": "260738", "train_nsentences": "1750.04", "train_wps": "98001.7", "train_ups": "0.38", "train_wpb": "260738", "train_bsz": "1750", "train_num_updates": "108579", "train_lr": "0.000395952", "train_gnorm": "0.346", "train_loss_scale": "2", "train_train_wall": "63", "train_gb_free": "39.6", "train_wall": "16792"}
[2024-10-12 05:24:30,366][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 05:24:30,370][fairseq.trainer][INFO] - begin training epoch 2264
[2024-10-12 05:24:30,370][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 05:26:11,058][train_inner][INFO] - {"epoch": 2264, "update": 2263.438, "loss": "0.387", "ntokens": "260786", "nsentences": "1731.39", "wps": "99302.2", "ups": "0.38", "wpb": "260786", "bsz": "1731.4", "num_updates": "108600", "lr": "0.000395924", "gnorm": "0.351", "loss_scale": "2", "train_wall": "250", "gb_free": "39.9", "wall": "16892"}
[2024-10-12 05:26:37,018][fairseq_cli.train][INFO] - end of epoch 2264 (average epoch stats below)
[2024-10-12 05:26:37,021][train][INFO] - {"epoch": 2264, "train_loss": "0.391", "train_ntokens": "260628", "train_nsentences": "1750.04", "train_wps": "98732.8", "train_ups": "0.38", "train_wpb": "260628", "train_bsz": "1750", "train_num_updates": "108627", "train_lr": "0.000395887", "train_gnorm": "0.351", "train_loss_scale": "2", "train_train_wall": "51", "train_gb_free": "39.4", "train_wall": "16918"}
[2024-10-12 05:26:37,076][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 05:26:37,080][fairseq.trainer][INFO] - begin training epoch 2265
[2024-10-12 05:26:37,081][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 05:28:43,846][fairseq_cli.train][INFO] - end of epoch 2265 (average epoch stats below)
[2024-10-12 05:28:43,851][train][INFO] - {"epoch": 2265, "train_loss": "0.39", "train_ntokens": "260562", "train_nsentences": "1750.04", "train_wps": "98616.3", "train_ups": "0.38", "train_wpb": "260562", "train_bsz": "1750", "train_num_updates": "108675", "train_lr": "0.000395822", "train_gnorm": "0.352", "train_loss_scale": "2", "train_train_wall": "56", "train_gb_free": "39.6", "train_wall": "17045"}
[2024-10-12 05:28:43,908][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 05:28:43,912][fairseq.trainer][INFO] - begin training epoch 2266
[2024-10-12 05:28:43,912][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 05:30:54,205][fairseq_cli.train][INFO] - end of epoch 2266 (average epoch stats below)
[2024-10-12 05:30:54,209][train][INFO] - {"epoch": 2266, "train_loss": "0.39", "train_ntokens": "260785", "train_nsentences": "1750.04", "train_wps": "96028.7", "train_ups": "0.37", "train_wpb": "260785", "train_bsz": "1750", "train_num_updates": "108723", "train_lr": "0.000395757", "train_gnorm": "0.364", "train_loss_scale": "2", "train_train_wall": "43", "train_gb_free": "39.7", "train_wall": "17175"}
[2024-10-12 05:30:54,266][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 05:30:54,269][fairseq.trainer][INFO] - begin training epoch 2267
[2024-10-12 05:30:54,270][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 05:33:07,603][fairseq_cli.train][INFO] - end of epoch 2267 (average epoch stats below)
[2024-10-12 05:33:07,609][train][INFO] - {"epoch": 2267, "train_loss": "0.39", "train_ntokens": "260905", "train_nsentences": "1750.04", "train_wps": "93881.6", "train_ups": "0.36", "train_wpb": "260905", "train_bsz": "1750", "train_num_updates": "108771", "train_lr": "0.000395692", "train_gnorm": "0.384", "train_loss_scale": "2", "train_train_wall": "20", "train_gb_free": "40.1", "train_wall": "17309"}
[2024-10-12 05:33:07,702][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 05:33:07,719][fairseq.trainer][INFO] - begin training epoch 2268
[2024-10-12 05:33:07,719][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 05:34:56,430][train_inner][INFO] - {"epoch": 2268, "update": 2267.604, "loss": "0.391", "ntokens": "260774", "nsentences": "1746.38", "wps": "99273.1", "ups": "0.38", "wpb": "260774", "bsz": "1746.4", "num_updates": "108800", "lr": "0.000395652", "gnorm": "0.364", "loss_scale": "2", "train_wall": "180", "gb_free": "39.4", "wall": "17418"}
[2024-10-12 05:35:14,761][fairseq_cli.train][INFO] - end of epoch 2268 (average epoch stats below)
[2024-10-12 05:35:14,775][train][INFO] - {"epoch": 2268, "train_loss": "0.388", "train_ntokens": "260754", "train_nsentences": "1750.04", "train_wps": "98435.9", "train_ups": "0.38", "train_wpb": "260754", "train_bsz": "1750", "train_num_updates": "108819", "train_lr": "0.000395626", "train_gnorm": "0.361", "train_loss_scale": "2", "train_train_wall": "53", "train_gb_free": "39.8", "train_wall": "17436"}
[2024-10-12 05:35:14,878][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 05:35:14,885][fairseq.trainer][INFO] - begin training epoch 2269
[2024-10-12 05:35:14,886][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 05:37:24,120][fairseq_cli.train][INFO] - end of epoch 2269 (average epoch stats below)
[2024-10-12 05:37:24,140][train][INFO] - {"epoch": 2269, "train_loss": "0.379", "train_ntokens": "260510", "train_nsentences": "1750.04", "train_wps": "96663.8", "train_ups": "0.37", "train_wpb": "260510", "train_bsz": "1750", "train_num_updates": "108867", "train_lr": "0.000395561", "train_gnorm": "0.358", "train_loss_scale": "2", "train_train_wall": "54", "train_gb_free": "39.6", "train_wall": "17565"}
[2024-10-12 05:37:24,264][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 05:37:24,296][fairseq.trainer][INFO] - begin training epoch 2270
[2024-10-12 05:37:24,297][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 05:39:29,976][fairseq_cli.train][INFO] - end of epoch 2270 (average epoch stats below)
[2024-10-12 05:39:29,989][train][INFO] - {"epoch": 2270, "train_loss": "0.386", "train_ntokens": "260481", "train_nsentences": "1750.04", "train_wps": "99354.9", "train_ups": "0.38", "train_wpb": "260481", "train_bsz": "1750", "train_num_updates": "108915", "train_lr": "0.000395496", "train_gnorm": "0.357", "train_loss_scale": "2", "train_train_wall": "55", "train_gb_free": "39.7", "train_wall": "17691"}
[2024-10-12 05:39:30,088][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 05:39:30,095][fairseq.trainer][INFO] - begin training epoch 2271
[2024-10-12 05:39:30,096][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 05:41:37,462][fairseq_cli.train][INFO] - end of epoch 2271 (average epoch stats below)
[2024-10-12 05:41:37,466][train][INFO] - {"epoch": 2271, "train_loss": "0.389", "train_ntokens": "260516", "train_nsentences": "1750.04", "train_wps": "98097.8", "train_ups": "0.38", "train_wpb": "260516", "train_bsz": "1750", "train_num_updates": "108963", "train_lr": "0.000395431", "train_gnorm": "0.381", "train_loss_scale": "2", "train_train_wall": "54", "train_gb_free": "39.8", "train_wall": "17819"}
[2024-10-12 05:41:37,524][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 05:41:37,527][fairseq.trainer][INFO] - begin training epoch 2272
[2024-10-12 05:41:37,528][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 05:43:32,527][train_inner][INFO] - {"epoch": 2272, "update": 2271.771, "loss": "0.385", "ntokens": "260802", "nsentences": "1750.54", "wps": "101076", "ups": "0.39", "wpb": "260802", "bsz": "1750.5", "num_updates": "109000", "lr": "0.00039538", "gnorm": "0.361", "loss_scale": "2", "train_wall": "220", "gb_free": "39.8", "wall": "17934"}
[2024-10-12 05:43:49,290][fairseq_cli.train][INFO] - end of epoch 2272 (average epoch stats below)
[2024-10-12 05:43:49,292][train][INFO] - {"epoch": 2272, "train_loss": "0.387", "train_ntokens": "260819", "train_nsentences": "1750.04", "train_wps": "94970.7", "train_ups": "0.36", "train_wpb": "260819", "train_bsz": "1750", "train_num_updates": "109011", "train_lr": "0.000395365", "train_gnorm": "0.349", "train_loss_scale": "2", "train_train_wall": "43", "train_gb_free": "39.2", "train_wall": "17951"}
[2024-10-12 05:43:49,342][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 05:43:49,346][fairseq.trainer][INFO] - begin training epoch 2273
[2024-10-12 05:43:49,346][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 05:45:55,062][fairseq_cli.train][INFO] - end of epoch 2273 (average epoch stats below)
[2024-10-12 05:45:55,077][train][INFO] - {"epoch": 2273, "train_loss": "0.389", "train_ntokens": "260781", "train_nsentences": "1750.04", "train_wps": "99518.9", "train_ups": "0.38", "train_wpb": "260781", "train_bsz": "1750", "train_num_updates": "109059", "train_lr": "0.0003953", "train_gnorm": "0.371", "train_loss_scale": "2", "train_train_wall": "44", "train_gb_free": "39.9", "train_wall": "18076"}
[2024-10-12 05:45:55,179][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 05:45:55,186][fairseq.trainer][INFO] - begin training epoch 2274
[2024-10-12 05:45:55,187][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 05:48:01,718][fairseq_cli.train][INFO] - end of epoch 2274 (average epoch stats below)
[2024-10-12 05:48:01,721][train][INFO] - {"epoch": 2274, "train_loss": "0.383", "train_ntokens": "260826", "train_nsentences": "1750.04", "train_wps": "98859.7", "train_ups": "0.38", "train_wpb": "260826", "train_bsz": "1750", "train_num_updates": "109107", "train_lr": "0.000395235", "train_gnorm": "0.35", "train_loss_scale": "2", "train_train_wall": "58", "train_gb_free": "40.4", "train_wall": "18203"}
[2024-10-12 05:48:01,817][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 05:48:01,838][fairseq.trainer][INFO] - begin training epoch 2275
[2024-10-12 05:48:01,839][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 05:50:12,663][fairseq_cli.train][INFO] - end of epoch 2275 (average epoch stats below)
[2024-10-12 05:50:12,677][train][INFO] - {"epoch": 2275, "train_loss": "0.39", "train_ntokens": "260658", "train_nsentences": "1750.04", "train_wps": "95543.3", "train_ups": "0.37", "train_wpb": "260658", "train_bsz": "1750", "train_num_updates": "109155", "train_lr": "0.00039517", "train_gnorm": "0.361", "train_loss_scale": "2", "train_train_wall": "58", "train_gb_free": "39.4", "train_wall": "18334"}
[2024-10-12 05:50:12,751][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 05:50:12,761][fairseq.trainer][INFO] - begin training epoch 2276
[2024-10-12 05:50:12,762][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 05:52:18,191][train_inner][INFO] - {"epoch": 2276, "update": 2275.938, "loss": "0.388", "ntokens": "260530", "nsentences": "1759.44", "wps": "99135.1", "ups": "0.38", "wpb": "260530", "bsz": "1759.4", "num_updates": "109200", "lr": "0.000395109", "gnorm": "0.357", "loss_scale": "2", "train_wall": "229", "gb_free": "39.2", "wall": "18459"}
[2024-10-12 05:52:18,978][fairseq_cli.train][INFO] - end of epoch 2276 (average epoch stats below)
[2024-10-12 05:52:18,982][train][INFO] - {"epoch": 2276, "train_loss": "0.388", "train_ntokens": "260685", "train_nsentences": "1750.04", "train_wps": "99073.2", "train_ups": "0.38", "train_wpb": "260685", "train_bsz": "1750", "train_num_updates": "109203", "train_lr": "0.000395105", "train_gnorm": "0.35", "train_loss_scale": "2", "train_train_wall": "65", "train_gb_free": "40.3", "train_wall": "18460"}
[2024-10-12 05:52:19,095][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 05:52:19,105][fairseq.trainer][INFO] - begin training epoch 2277
[2024-10-12 05:52:19,106][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 05:54:27,108][fairseq_cli.train][INFO] - end of epoch 2277 (average epoch stats below)
[2024-10-12 05:54:27,113][train][INFO] - {"epoch": 2277, "train_loss": "0.386", "train_ntokens": "260552", "train_nsentences": "1750.04", "train_wps": "97612.4", "train_ups": "0.37", "train_wpb": "260552", "train_bsz": "1750", "train_num_updates": "109251", "train_lr": "0.000395039", "train_gnorm": "0.347", "train_loss_scale": "2", "train_train_wall": "35", "train_gb_free": "42", "train_wall": "18588"}
[2024-10-12 05:54:27,184][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 05:54:27,187][fairseq.trainer][INFO] - begin training epoch 2278
[2024-10-12 05:54:27,188][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 05:56:34,781][fairseq_cli.train][INFO] - end of epoch 2278 (average epoch stats below)
[2024-10-12 05:56:34,787][train][INFO] - {"epoch": 2278, "train_loss": "0.374", "train_ntokens": "260632", "train_nsentences": "1750.04", "train_wps": "97990.4", "train_ups": "0.38", "train_wpb": "260632", "train_bsz": "1750", "train_num_updates": "109299", "train_lr": "0.000394974", "train_gnorm": "0.358", "train_loss_scale": "2", "train_train_wall": "61", "train_gb_free": "40.1", "train_wall": "18716"}
[2024-10-12 05:56:34,887][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 05:56:34,913][fairseq.trainer][INFO] - begin training epoch 2279
[2024-10-12 05:56:34,913][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 05:58:42,544][fairseq_cli.train][INFO] - end of epoch 2279 (average epoch stats below)
[2024-10-12 05:58:42,547][train][INFO] - {"epoch": 2279, "train_loss": "0.387", "train_ntokens": "260792", "train_nsentences": "1750.04", "train_wps": "97983.7", "train_ups": "0.38", "train_wpb": "260792", "train_bsz": "1750", "train_num_updates": "109347", "train_lr": "0.000394909", "train_gnorm": "0.352", "train_loss_scale": "4", "train_train_wall": "48", "train_gb_free": "39.3", "train_wall": "18844"}
[2024-10-12 05:58:42,616][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 05:58:42,619][fairseq.trainer][INFO] - begin training epoch 2280
[2024-10-12 05:58:42,620][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 06:00:50,440][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 2280 @ 109395 updates
[2024-10-12 06:00:50,442][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-12 06:00:53,986][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-12 06:00:53,989][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 2280 @ 109395 updates, score None) (writing took 3.548482848331332 seconds)
[2024-10-12 06:00:53,989][fairseq_cli.train][INFO] - end of epoch 2280 (average epoch stats below)
[2024-10-12 06:00:53,992][train][INFO] - {"epoch": 2280, "train_loss": "0.382", "train_ntokens": "260378", "train_nsentences": "1750.04", "train_wps": "95090.7", "train_ups": "0.37", "train_wpb": "260378", "train_bsz": "1750", "train_num_updates": "109395", "train_lr": "0.000394844", "train_gnorm": "0.36", "train_loss_scale": "4", "train_train_wall": "50", "train_gb_free": "39.6", "train_wall": "18975"}
[2024-10-12 06:00:54,057][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 06:00:54,062][fairseq.trainer][INFO] - begin training epoch 2281
[2024-10-12 06:00:54,062][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 06:02:19,700][train_inner][INFO] - {"epoch": 2281, "update": 2280.104, "loss": "0.382", "ntokens": "260641", "nsentences": "1743.1", "wps": "86663.3", "ups": "0.33", "wpb": "260641", "bsz": "1743.1", "num_updates": "109400", "lr": "0.000394837", "gnorm": "0.354", "loss_scale": "4", "train_wall": "206", "gb_free": "39.3", "wall": "19061"}
[2024-10-12 06:02:59,195][fairseq_cli.train][INFO] - end of epoch 2281 (average epoch stats below)
[2024-10-12 06:02:59,198][train][INFO] - {"epoch": 2281, "train_loss": "0.385", "train_ntokens": "260837", "train_nsentences": "1750.04", "train_wps": "99999.5", "train_ups": "0.38", "train_wpb": "260837", "train_bsz": "1750", "train_num_updates": "109443", "train_lr": "0.000394779", "train_gnorm": "0.352", "train_loss_scale": "4", "train_train_wall": "38", "train_gb_free": "40.1", "train_wall": "19100"}
[2024-10-12 06:02:59,255][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 06:02:59,260][fairseq.trainer][INFO] - begin training epoch 2282
[2024-10-12 06:02:59,260][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 06:05:04,596][fairseq_cli.train][INFO] - end of epoch 2282 (average epoch stats below)
[2024-10-12 06:05:04,600][train][INFO] - {"epoch": 2282, "train_loss": "0.394", "train_ntokens": "260522", "train_nsentences": "1750.04", "train_wps": "99722.2", "train_ups": "0.38", "train_wpb": "260522", "train_bsz": "1750", "train_num_updates": "109491", "train_lr": "0.000394713", "train_gnorm": "0.378", "train_loss_scale": "4", "train_train_wall": "45", "train_gb_free": "39.6", "train_wall": "19226"}
[2024-10-12 06:05:04,665][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 06:05:04,672][fairseq.trainer][INFO] - begin training epoch 2283
[2024-10-12 06:05:04,673][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 06:07:09,417][fairseq_cli.train][INFO] - end of epoch 2283 (average epoch stats below)
[2024-10-12 06:07:09,440][train][INFO] - {"epoch": 2283, "train_loss": "0.391", "train_ntokens": "260671", "train_nsentences": "1750.04", "train_wps": "100242", "train_ups": "0.38", "train_wpb": "260671", "train_bsz": "1750", "train_num_updates": "109539", "train_lr": "0.000394648", "train_gnorm": "0.35", "train_loss_scale": "4", "train_train_wall": "46", "train_gb_free": "39.7", "train_wall": "19351"}
[2024-10-12 06:07:09,563][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 06:07:09,567][fairseq.trainer][INFO] - begin training epoch 2284
[2024-10-12 06:07:09,567][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 06:09:16,391][fairseq_cli.train][INFO] - end of epoch 2284 (average epoch stats below)
[2024-10-12 06:09:16,395][train][INFO] - {"epoch": 2284, "train_loss": "0.392", "train_ntokens": "260746", "train_nsentences": "1750.04", "train_wps": "98587.4", "train_ups": "0.38", "train_wpb": "260746", "train_bsz": "1750", "train_num_updates": "109587", "train_lr": "0.000394583", "train_gnorm": "0.359", "train_loss_scale": "4", "train_train_wall": "60", "train_gb_free": "39.9", "train_wall": "19478"}
[2024-10-12 06:09:16,478][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 06:09:16,482][fairseq.trainer][INFO] - begin training epoch 2285
[2024-10-12 06:09:16,483][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 06:10:52,556][train_inner][INFO] - {"epoch": 2285, "update": 2284.271, "loss": "0.391", "ntokens": "260682", "nsentences": "1755.52", "wps": "101660", "ups": "0.39", "wpb": "260682", "bsz": "1755.5", "num_updates": "109600", "lr": "0.000394565", "gnorm": "0.36", "loss_scale": "4", "train_wall": "205", "gb_free": "40.2", "wall": "19574"}
[2024-10-12 06:11:26,972][fairseq_cli.train][INFO] - end of epoch 2285 (average epoch stats below)
[2024-10-12 06:11:26,975][train][INFO] - {"epoch": 2285, "train_loss": "0.382", "train_ntokens": "260606", "train_nsentences": "1750.04", "train_wps": "95800.7", "train_ups": "0.37", "train_wpb": "260606", "train_bsz": "1750", "train_num_updates": "109635", "train_lr": "0.000394518", "train_gnorm": "0.347", "train_loss_scale": "4", "train_train_wall": "54", "train_gb_free": "39.6", "train_wall": "19608"}
[2024-10-12 06:11:27,082][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 06:11:27,085][fairseq.trainer][INFO] - begin training epoch 2286
[2024-10-12 06:11:27,086][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 06:13:35,151][fairseq_cli.train][INFO] - end of epoch 2286 (average epoch stats below)
[2024-10-12 06:13:35,156][train][INFO] - {"epoch": 2286, "train_loss": "0.386", "train_ntokens": "260629", "train_nsentences": "1750.04", "train_wps": "97603.4", "train_ups": "0.37", "train_wpb": "260629", "train_bsz": "1750", "train_num_updates": "109683", "train_lr": "0.000394452", "train_gnorm": "0.346", "train_loss_scale": "4", "train_train_wall": "57", "train_gb_free": "40.1", "train_wall": "19736"}
[2024-10-12 06:13:35,252][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 06:13:35,258][fairseq.trainer][INFO] - begin training epoch 2287
[2024-10-12 06:13:35,258][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 06:15:40,439][fairseq_cli.train][INFO] - end of epoch 2287 (average epoch stats below)
[2024-10-12 06:15:40,443][train][INFO] - {"epoch": 2287, "train_loss": "0.378", "train_ntokens": "260766", "train_nsentences": "1750.04", "train_wps": "99909.4", "train_ups": "0.38", "train_wpb": "260766", "train_bsz": "1750", "train_num_updates": "109731", "train_lr": "0.000394387", "train_gnorm": "0.361", "train_loss_scale": "4", "train_train_wall": "51", "train_gb_free": "39.3", "train_wall": "19862"}
[2024-10-12 06:15:40,582][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 06:15:40,593][fairseq.trainer][INFO] - begin training epoch 2288
[2024-10-12 06:15:40,594][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 06:17:49,636][fairseq_cli.train][INFO] - end of epoch 2288 (average epoch stats below)
[2024-10-12 06:17:49,639][train][INFO] - {"epoch": 2288, "train_loss": "0.385", "train_ntokens": "260746", "train_nsentences": "1750.04", "train_wps": "96877.1", "train_ups": "0.37", "train_wpb": "260746", "train_bsz": "1750", "train_num_updates": "109779", "train_lr": "0.000394322", "train_gnorm": "0.353", "train_loss_scale": "4", "train_train_wall": "49", "train_gb_free": "39.8", "train_wall": "19991"}
[2024-10-12 06:17:49,750][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 06:17:49,778][fairseq.trainer][INFO] - begin training epoch 2289
[2024-10-12 06:17:49,779][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 06:19:37,589][train_inner][INFO] - {"epoch": 2289, "update": 2288.438, "loss": "0.382", "ntokens": "260597", "nsentences": "1759.56", "wps": "99269.9", "ups": "0.38", "wpb": "260597", "bsz": "1759.6", "num_updates": "109800", "lr": "0.000394293", "gnorm": "0.351", "loss_scale": "4", "train_wall": "217", "gb_free": "39.6", "wall": "20099"}
[2024-10-12 06:19:58,805][fairseq_cli.train][INFO] - end of epoch 2289 (average epoch stats below)
[2024-10-12 06:19:58,821][train][INFO] - {"epoch": 2289, "train_loss": "0.376", "train_ntokens": "260529", "train_nsentences": "1750.04", "train_wps": "96817", "train_ups": "0.37", "train_wpb": "260529", "train_bsz": "1750", "train_num_updates": "109827", "train_lr": "0.000394257", "train_gnorm": "0.344", "train_loss_scale": "4", "train_train_wall": "55", "train_gb_free": "39.2", "train_wall": "20120"}
[2024-10-12 06:19:58,979][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 06:19:58,993][fairseq.trainer][INFO] - begin training epoch 2290
[2024-10-12 06:19:58,993][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 06:22:06,214][fairseq_cli.train][INFO] - end of epoch 2290 (average epoch stats below)
[2024-10-12 06:22:06,217][train][INFO] - {"epoch": 2290, "train_loss": "0.389", "train_ntokens": "260918", "train_nsentences": "1750.04", "train_wps": "98310.9", "train_ups": "0.38", "train_wpb": "260918", "train_bsz": "1750", "train_num_updates": "109875", "train_lr": "0.000394192", "train_gnorm": "0.367", "train_loss_scale": "4", "train_train_wall": "56", "train_gb_free": "39.3", "train_wall": "20247"}
[2024-10-12 06:22:06,304][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 06:22:06,315][fairseq.trainer][INFO] - begin training epoch 2291
[2024-10-12 06:22:06,316][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 06:24:09,708][fairseq_cli.train][INFO] - end of epoch 2291 (average epoch stats below)
[2024-10-12 06:24:09,726][train][INFO] - {"epoch": 2291, "train_loss": "0.391", "train_ntokens": "261040", "train_nsentences": "1750.04", "train_wps": "101453", "train_ups": "0.39", "train_wpb": "261040", "train_bsz": "1750", "train_num_updates": "109923", "train_lr": "0.000394126", "train_gnorm": "0.348", "train_loss_scale": "4", "train_train_wall": "49", "train_gb_free": "39.7", "train_wall": "20371"}
[2024-10-12 06:24:09,837][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 06:24:09,845][fairseq.trainer][INFO] - begin training epoch 2292
[2024-10-12 06:24:09,846][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 06:26:21,657][fairseq_cli.train][INFO] - end of epoch 2292 (average epoch stats below)
[2024-10-12 06:26:21,661][train][INFO] - {"epoch": 2292, "train_loss": "0.391", "train_ntokens": "260778", "train_nsentences": "1750.04", "train_wps": "94877.8", "train_ups": "0.36", "train_wpb": "260778", "train_bsz": "1750", "train_num_updates": "109971", "train_lr": "0.000394061", "train_gnorm": "0.374", "train_loss_scale": "4", "train_train_wall": "60", "train_gb_free": "39.3", "train_wall": "20503"}
[2024-10-12 06:26:21,717][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 06:26:21,721][fairseq.trainer][INFO] - begin training epoch 2293
[2024-10-12 06:26:21,722][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 06:28:13,872][train_inner][INFO] - {"epoch": 2293, "update": 2292.604, "loss": "0.388", "ntokens": "260718", "nsentences": "1746.11", "wps": "100999", "ups": "0.39", "wpb": "260718", "bsz": "1746.1", "num_updates": "110000", "lr": "0.000394022", "gnorm": "0.36", "loss_scale": "4", "train_wall": "226", "gb_free": "39.6", "wall": "20615"}
[2024-10-12 06:28:28,726][fairseq_cli.train][INFO] - end of epoch 2293 (average epoch stats below)
[2024-10-12 06:28:28,739][train][INFO] - {"epoch": 2293, "train_loss": "0.392", "train_ntokens": "260556", "train_nsentences": "1750.04", "train_wps": "98428.6", "train_ups": "0.38", "train_wpb": "260556", "train_bsz": "1750", "train_num_updates": "110019", "train_lr": "0.000393996", "train_gnorm": "0.362", "train_loss_scale": "4", "train_train_wall": "55", "train_gb_free": "39.3", "train_wall": "20630"}
[2024-10-12 06:28:28,833][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 06:28:28,846][fairseq.trainer][INFO] - begin training epoch 2294
[2024-10-12 06:28:28,847][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 06:30:34,396][fairseq_cli.train][INFO] - end of epoch 2294 (average epoch stats below)
[2024-10-12 06:30:34,403][train][INFO] - {"epoch": 2294, "train_loss": "0.387", "train_ntokens": "260712", "train_nsentences": "1750.04", "train_wps": "99589.2", "train_ups": "0.38", "train_wpb": "260712", "train_bsz": "1750", "train_num_updates": "110067", "train_lr": "0.000393931", "train_gnorm": "0.358", "train_loss_scale": "4", "train_train_wall": "53", "train_gb_free": "39.4", "train_wall": "20756"}
[2024-10-12 06:30:34,487][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 06:30:34,506][fairseq.trainer][INFO] - begin training epoch 2295
[2024-10-12 06:30:34,506][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 06:32:39,507][fairseq_cli.train][INFO] - end of epoch 2295 (average epoch stats below)
[2024-10-12 06:32:39,519][train][INFO] - {"epoch": 2295, "train_loss": "0.377", "train_ntokens": "260630", "train_nsentences": "1750.04", "train_wps": "99996.5", "train_ups": "0.38", "train_wpb": "260630", "train_bsz": "1750", "train_num_updates": "110115", "train_lr": "0.000393865", "train_gnorm": "0.352", "train_loss_scale": "4", "train_train_wall": "46", "train_gb_free": "40.3", "train_wall": "20881"}
[2024-10-12 06:32:39,640][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 06:32:39,644][fairseq.trainer][INFO] - begin training epoch 2296
[2024-10-12 06:32:39,644][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 06:34:48,478][fairseq_cli.train][INFO] - end of epoch 2296 (average epoch stats below)
[2024-10-12 06:34:48,486][train][INFO] - {"epoch": 2296, "train_loss": "0.38", "train_ntokens": "260857", "train_nsentences": "1750.04", "train_wps": "97090.5", "train_ups": "0.37", "train_wpb": "260857", "train_bsz": "1750", "train_num_updates": "110163", "train_lr": "0.0003938", "train_gnorm": "0.364", "train_loss_scale": "4", "train_train_wall": "56", "train_gb_free": "39.5", "train_wall": "21010"}
[2024-10-12 06:34:48,578][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 06:34:48,587][fairseq.trainer][INFO] - begin training epoch 2297
[2024-10-12 06:34:48,588][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 06:36:45,589][train_inner][INFO] - {"epoch": 2297, "update": 2296.771, "loss": "0.385", "ntokens": "260796", "nsentences": "1735.3", "wps": "101931", "ups": "0.39", "wpb": "260796", "bsz": "1735.3", "num_updates": "110200", "lr": "0.00039375", "gnorm": "0.357", "loss_scale": "4", "train_wall": "212", "gb_free": "39.8", "wall": "21127"}
[2024-10-12 06:36:56,193][fairseq_cli.train][INFO] - end of epoch 2297 (average epoch stats below)
[2024-10-12 06:36:56,198][train][INFO] - {"epoch": 2297, "train_loss": "0.39", "train_ntokens": "260266", "train_nsentences": "1750.04", "train_wps": "97824.6", "train_ups": "0.38", "train_wpb": "260266", "train_bsz": "1750", "train_num_updates": "110211", "train_lr": "0.000393735", "train_gnorm": "0.351", "train_loss_scale": "4", "train_train_wall": "53", "train_gb_free": "39.7", "train_wall": "21137"}
[2024-10-12 06:36:56,367][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 06:36:56,371][fairseq.trainer][INFO] - begin training epoch 2298
[2024-10-12 06:36:56,372][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 06:39:03,828][fairseq_cli.train][INFO] - end of epoch 2298 (average epoch stats below)
[2024-10-12 06:39:03,833][train][INFO] - {"epoch": 2298, "train_loss": "0.392", "train_ntokens": "261130", "train_nsentences": "1750.04", "train_wps": "98208", "train_ups": "0.38", "train_wpb": "261130", "train_bsz": "1750", "train_num_updates": "110259", "train_lr": "0.00039367", "train_gnorm": "0.378", "train_loss_scale": "4", "train_train_wall": "51", "train_gb_free": "39.2", "train_wall": "21265"}
[2024-10-12 06:39:03,916][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 06:39:03,935][fairseq.trainer][INFO] - begin training epoch 2299
[2024-10-12 06:39:03,936][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 06:41:10,843][fairseq_cli.train][INFO] - end of epoch 2299 (average epoch stats below)
[2024-10-12 06:41:10,851][train][INFO] - {"epoch": 2299, "train_loss": "0.4", "train_ntokens": "260391", "train_nsentences": "1750.04", "train_wps": "98406", "train_ups": "0.38", "train_wpb": "260391", "train_bsz": "1750", "train_num_updates": "110307", "train_lr": "0.000393605", "train_gnorm": "0.338", "train_loss_scale": "4", "train_train_wall": "33", "train_gb_free": "39.7", "train_wall": "21392"}
[2024-10-12 06:41:10,962][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 06:41:10,965][fairseq.trainer][INFO] - begin training epoch 2300
[2024-10-12 06:41:10,966][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 06:43:20,150][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 2300 @ 110355 updates
[2024-10-12 06:43:20,154][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-12 06:43:23,621][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-12 06:43:23,623][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 2300 @ 110355 updates, score None) (writing took 3.473537907935679 seconds)
[2024-10-12 06:43:23,624][fairseq_cli.train][INFO] - end of epoch 2300 (average epoch stats below)
[2024-10-12 06:43:23,626][train][INFO] - {"epoch": 2300, "train_loss": "0.389", "train_ntokens": "260446", "train_nsentences": "1750.04", "train_wps": "94157.5", "train_ups": "0.36", "train_wpb": "260446", "train_bsz": "1750", "train_num_updates": "110355", "train_lr": "0.000393539", "train_gnorm": "0.364", "train_loss_scale": "4", "train_train_wall": "53", "train_gb_free": "39.7", "train_wall": "21525"}
[2024-10-12 06:43:23,677][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 06:43:23,682][fairseq.trainer][INFO] - begin training epoch 2301
[2024-10-12 06:43:23,682][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 06:45:28,534][train_inner][INFO] - {"epoch": 2301, "update": 2300.938, "loss": "0.39", "ntokens": "260690", "nsentences": "1756.93", "wps": "99706.3", "ups": "0.38", "wpb": "260690", "bsz": "1756.9", "num_updates": "110400", "lr": "0.000393478", "gnorm": "0.364", "loss_scale": "4", "train_wall": "196", "gb_free": "39.1", "wall": "21650"}
[2024-10-12 06:45:29,325][fairseq_cli.train][INFO] - end of epoch 2301 (average epoch stats below)
[2024-10-12 06:45:29,331][train][INFO] - {"epoch": 2301, "train_loss": "0.378", "train_ntokens": "260701", "train_nsentences": "1750.04", "train_wps": "99552.6", "train_ups": "0.38", "train_wpb": "260701", "train_bsz": "1750", "train_num_updates": "110403", "train_lr": "0.000393474", "train_gnorm": "0.377", "train_loss_scale": "4", "train_train_wall": "49", "train_gb_free": "39.8", "train_wall": "21651"}
[2024-10-12 06:45:29,454][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 06:45:29,461][fairseq.trainer][INFO] - begin training epoch 2302
[2024-10-12 06:45:29,461][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 06:47:36,708][fairseq_cli.train][INFO] - end of epoch 2302 (average epoch stats below)
[2024-10-12 06:47:36,723][train][INFO] - {"epoch": 2302, "train_loss": "0.383", "train_ntokens": "260585", "train_nsentences": "1750.04", "train_wps": "98197.7", "train_ups": "0.38", "train_wpb": "260585", "train_bsz": "1750", "train_num_updates": "110451", "train_lr": "0.000393409", "train_gnorm": "0.392", "train_loss_scale": "4", "train_train_wall": "29", "train_gb_free": "39.8", "train_wall": "21778"}
[2024-10-12 06:47:36,835][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 06:47:36,846][fairseq.trainer][INFO] - begin training epoch 2303
[2024-10-12 06:47:36,846][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 06:49:47,678][fairseq_cli.train][INFO] - end of epoch 2303 (average epoch stats below)
[2024-10-12 06:49:47,687][train][INFO] - {"epoch": 2303, "train_loss": "0.385", "train_ntokens": "260784", "train_nsentences": "1750.04", "train_wps": "95585.9", "train_ups": "0.37", "train_wpb": "260784", "train_bsz": "1750", "train_num_updates": "110499", "train_lr": "0.000393344", "train_gnorm": "0.346", "train_loss_scale": "4", "train_train_wall": "63", "train_gb_free": "39.4", "train_wall": "21909"}
[2024-10-12 06:49:47,794][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 06:49:47,821][fairseq.trainer][INFO] - begin training epoch 2304
[2024-10-12 06:49:47,822][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 06:51:55,493][fairseq_cli.train][INFO] - end of epoch 2304 (average epoch stats below)
[2024-10-12 06:51:55,507][train][INFO] - {"epoch": 2304, "train_loss": "0.386", "train_ntokens": "260516", "train_nsentences": "1750.04", "train_wps": "97843", "train_ups": "0.38", "train_wpb": "260516", "train_bsz": "1750", "train_num_updates": "110547", "train_lr": "0.000393279", "train_gnorm": "0.353", "train_loss_scale": "4", "train_train_wall": "55", "train_gb_free": "39.2", "train_wall": "22037"}
[2024-10-12 06:51:55,597][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 06:51:55,606][fairseq.trainer][INFO] - begin training epoch 2305
[2024-10-12 06:51:55,606][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 06:54:02,704][fairseq_cli.train][INFO] - end of epoch 2305 (average epoch stats below)
[2024-10-12 06:54:02,708][train][INFO] - {"epoch": 2305, "train_loss": "0.385", "train_ntokens": "260804", "train_nsentences": "1750.04", "train_wps": "98419.3", "train_ups": "0.38", "train_wpb": "260804", "train_bsz": "1750", "train_num_updates": "110595", "train_lr": "0.000393213", "train_gnorm": "0.344", "train_loss_scale": "4", "train_train_wall": "48", "train_gb_free": "39.2", "train_wall": "22164"}
[2024-10-12 06:54:02,818][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 06:54:02,821][fairseq.trainer][INFO] - begin training epoch 2306
[2024-10-12 06:54:02,822][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 06:55:37,685][train_inner][INFO] - {"epoch": 2306, "update": 2305.104, "loss": "0.384", "ntokens": "260703", "nsentences": "1746.71", "wps": "85596.5", "ups": "0.33", "wpb": "260703", "bsz": "1746.7", "num_updates": "110600", "lr": "0.000393207", "gnorm": "0.359", "loss_scale": "4", "train_wall": "216", "gb_free": "40.3", "wall": "22259"}
[2024-10-12 06:56:16,036][fairseq_cli.train][INFO] - end of epoch 2306 (average epoch stats below)
[2024-10-12 06:56:16,039][train][INFO] - {"epoch": 2306, "train_loss": "0.379", "train_ntokens": "260912", "train_nsentences": "1750.04", "train_wps": "93934.9", "train_ups": "0.36", "train_wpb": "260912", "train_bsz": "1750", "train_num_updates": "110643", "train_lr": "0.000393148", "train_gnorm": "0.367", "train_loss_scale": "4", "train_train_wall": "58", "train_gb_free": "39.6", "train_wall": "22297"}
[2024-10-12 06:56:16,099][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 06:56:16,103][fairseq.trainer][INFO] - begin training epoch 2307
[2024-10-12 06:56:16,103][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 06:58:26,869][fairseq_cli.train][INFO] - end of epoch 2307 (average epoch stats below)
[2024-10-12 06:58:26,873][train][INFO] - {"epoch": 2307, "train_loss": "0.386", "train_ntokens": "260810", "train_nsentences": "1750.04", "train_wps": "95689", "train_ups": "0.37", "train_wpb": "260810", "train_bsz": "1750", "train_num_updates": "110691", "train_lr": "0.000393083", "train_gnorm": "0.365", "train_loss_scale": "4", "train_train_wall": "54", "train_gb_free": "39.8", "train_wall": "22428"}
[2024-10-12 06:58:27,024][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 06:58:27,039][fairseq.trainer][INFO] - begin training epoch 2308
[2024-10-12 06:58:27,040][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 07:00:34,937][fairseq_cli.train][INFO] - end of epoch 2308 (average epoch stats below)
[2024-10-12 07:00:34,941][train][INFO] - {"epoch": 2308, "train_loss": "0.383", "train_ntokens": "260383", "train_nsentences": "1750.04", "train_wps": "97605.8", "train_ups": "0.37", "train_wpb": "260383", "train_bsz": "1750", "train_num_updates": "110739", "train_lr": "0.000393018", "train_gnorm": "0.374", "train_loss_scale": "4", "train_train_wall": "40", "train_gb_free": "39.6", "train_wall": "22556"}
[2024-10-12 07:00:35,022][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 07:00:35,030][fairseq.trainer][INFO] - begin training epoch 2309
[2024-10-12 07:00:35,031][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 07:02:42,701][fairseq_cli.train][INFO] - end of epoch 2309 (average epoch stats below)
[2024-10-12 07:02:42,715][train][INFO] - {"epoch": 2309, "train_loss": "0.393", "train_ntokens": "260524", "train_nsentences": "1750.04", "train_wps": "97872.8", "train_ups": "0.38", "train_wpb": "260524", "train_bsz": "1750", "train_num_updates": "110787", "train_lr": "0.000392952", "train_gnorm": "0.36", "train_loss_scale": "4", "train_train_wall": "47", "train_gb_free": "39.6", "train_wall": "22684"}
[2024-10-12 07:02:42,826][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 07:02:42,839][fairseq.trainer][INFO] - begin training epoch 2310
[2024-10-12 07:02:42,840][fairseq_cli.train][INFO] - Start iterating over samples
