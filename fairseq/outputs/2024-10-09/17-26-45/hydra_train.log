[2024-10-09 17:27:24,804][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:13064', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-09 17:27:25,135][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:14180', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-09 17:27:26,383][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:19038', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-09 17:27:27,123][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:11800', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-09 17:27:27,850][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:19719', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-09 17:27:27,987][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:12779', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-09 17:27:28,451][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:14007', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-09 17:27:28,604][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:10382', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-09 17:27:29,398][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-09 17:27:29,400][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-09 17:27:29,400][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-09 17:27:29,400][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-09 17:27:29,401][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-09 17:27:29,402][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-09 17:27:29,790][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-09 17:27:31,652][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-09 17:27:31,654][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-09 17:27:31,654][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-09 17:27:31,654][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-09 17:27:31,655][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-09 17:27:31,656][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-09 17:27:31,809][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-09 17:27:31,823][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-09 17:27:31,823][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-09 17:27:31,823][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-09 17:27:31,824][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-09 17:27:31,825][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-09 17:27:32,092][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-09 17:27:34,113][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-09 17:27:35,656][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-09 17:27:35,658][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-09 17:27:35,658][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-09 17:27:35,658][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-09 17:27:35,783][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-09 17:27:35,784][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-09 17:27:35,910][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-09 17:27:35,913][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-09 17:27:35,913][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-09 17:27:35,913][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-09 17:27:35,914][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-09 17:27:35,914][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-09 17:27:36,196][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-09 17:27:36,199][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-09 17:27:36,199][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-09 17:27:36,199][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-09 17:27:36,200][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-09 17:27:36,201][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-09 17:27:36,736][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-09 17:27:37,117][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-09 17:27:37,752][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-09 17:27:37,763][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-09 17:27:37,763][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-09 17:27:37,763][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-09 17:27:37,764][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-09 17:27:37,765][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-09 17:27:38,105][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-09 17:27:39,817][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-09 17:27:39,828][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-09 17:27:39,839][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-09 17:27:39,839][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-09 17:27:39,840][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-09 17:27:39,841][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-09 17:27:40,201][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-09 17:27:42,720][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-09 17:28:52,981][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-09 17:28:53,049][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-09 17:28:53,049][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-09 17:28:53,049][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-09 17:28:53,049][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-09 17:28:53,049][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-09 17:28:53,049][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-09 17:28:53,049][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-09 17:28:53,049][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-09 17:28:53,049][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-09 17:28:53,049][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-09 17:28:53,055][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-09 17:28:53,056][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-09 17:29:35,568][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-09 17:29:35,575][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-09 17:29:35,575][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-09 17:29:35,575][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-09 17:29:35,575][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-09 17:29:35,575][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-09 17:29:35,575][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-09 17:29:35,575][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-09 17:29:35,575][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-09 17:29:35,575][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-09 17:29:35,576][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-09 17:29:35,576][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-09 17:29:35,577][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-09 17:30:01,267][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 1301 @ 62372 updates)
[2024-10-09 17:30:01,272][fairseq.trainer][INFO] - loading train data for epoch 1301
[2024-10-09 17:30:01,865][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-09 17:30:09,442][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 1301 @ 62372 updates)
[2024-10-09 17:30:09,469][fairseq.trainer][INFO] - loading train data for epoch 1301
[2024-10-09 17:30:10,491][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-09 17:30:14,504][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-09 17:30:14,505][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-09 17:30:14,505][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-09 17:30:14,505][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-09 17:30:14,505][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-09 17:30:14,505][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-09 17:30:14,505][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-09 17:30:14,505][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-09 17:30:14,505][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-09 17:30:14,505][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-09 17:30:14,505][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-09 17:30:14,505][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-09 17:30:14,511][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-09 17:30:15,364][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 17:30:15,368][fairseq.trainer][INFO] - begin training epoch 1301
[2024-10-09 17:30:15,368][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 17:30:19,833][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-09 17:30:19,834][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-09 17:30:19,836][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-09 17:30:19,836][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-09 17:30:19,836][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-09 17:30:19,836][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-09 17:30:19,836][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-09 17:30:19,836][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-09 17:30:19,836][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-09 17:30:19,837][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-09 17:30:19,837][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-09 17:30:19,837][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-09 17:30:19,839][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-09 17:30:42,352][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-09 17:30:42,353][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-09 17:30:42,353][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-09 17:30:42,353][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-09 17:30:42,353][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-09 17:30:42,353][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-09 17:30:42,353][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-09 17:30:42,353][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-09 17:30:42,354][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-09 17:30:42,354][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-09 17:30:42,354][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-09 17:30:42,354][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-09 17:30:42,355][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-09 17:30:46,864][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-09 17:30:46,865][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-09 17:30:46,865][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-09 17:30:46,865][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-09 17:30:46,865][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-09 17:30:46,865][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-09 17:30:46,865][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-09 17:30:46,865][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-09 17:30:46,865][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-09 17:30:46,865][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-09 17:30:46,865][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-09 17:30:46,865][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-09 17:30:46,866][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-09 17:30:52,008][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 1301 @ 62372 updates)
[2024-10-09 17:30:52,010][fairseq.trainer][INFO] - loading train data for epoch 1301
[2024-10-09 17:30:52,972][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-09 17:30:53,840][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 1301 @ 62372 updates)
[2024-10-09 17:30:53,842][fairseq.trainer][INFO] - loading train data for epoch 1301
[2024-10-09 17:30:55,439][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-09 17:31:00,859][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 17:31:00,872][fairseq.trainer][INFO] - begin training epoch 1301
[2024-10-09 17:31:00,873][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 17:31:05,956][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 17:31:06,085][fairseq.trainer][INFO] - begin training epoch 1301
[2024-10-09 17:31:06,085][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 17:31:08,200][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 17:31:08,210][fairseq.trainer][INFO] - begin training epoch 1301
[2024-10-09 17:31:08,210][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 17:31:14,361][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 1301 @ 62372 updates)
[2024-10-09 17:31:14,364][fairseq.trainer][INFO] - loading train data for epoch 1301
[2024-10-09 17:31:15,055][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-09 17:31:24,470][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 17:31:24,501][fairseq.trainer][INFO] - begin training epoch 1301
[2024-10-09 17:31:24,503][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 17:31:25,954][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 1301 @ 62372 updates)
[2024-10-09 17:31:25,996][fairseq.trainer][INFO] - loading train data for epoch 1301
[2024-10-09 17:31:26,492][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-09 17:31:30,621][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-09 17:31:30,625][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-09 17:31:30,625][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-09 17:31:30,630][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-09 17:31:30,631][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-09 17:31:30,631][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-09 17:31:30,631][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-09 17:31:30,631][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-09 17:31:30,631][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-09 17:31:30,631][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-09 17:31:30,631][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-09 17:31:30,632][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-09 17:31:30,632][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-09 17:31:35,461][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-09 17:31:35,462][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-09 17:31:35,462][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-09 17:31:35,462][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-09 17:31:35,462][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-09 17:31:35,462][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-09 17:31:35,462][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-09 17:31:35,462][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-09 17:31:35,462][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-09 17:31:35,462][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-09 17:31:35,491][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-09 17:31:35,491][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-09 17:31:35,493][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-09 17:31:42,379][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 17:31:42,397][fairseq.trainer][INFO] - begin training epoch 1301
[2024-10-09 17:31:42,398][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 17:31:48,347][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 1301 @ 62372 updates)
[2024-10-09 17:31:48,349][fairseq.trainer][INFO] - loading train data for epoch 1301
[2024-10-09 17:31:48,982][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-09 17:32:06,004][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 17:32:06,033][fairseq.trainer][INFO] - begin training epoch 1301
[2024-10-09 17:32:06,034][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 17:32:34,011][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 1301 @ 62372 updates)
[2024-10-09 17:32:34,013][fairseq.trainer][INFO] - loading train data for epoch 1301
[2024-10-09 17:32:34,236][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-09 17:32:39,790][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 17:32:39,824][fairseq.trainer][INFO] - begin training epoch 1301
[2024-10-09 17:32:39,854][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 17:40:40,688][train_inner][INFO] - {"epoch": 1301, "update": 1300.583, "loss": "0.478", "ntokens": "260036", "nsentences": "1767.07", "wps": "34242.4", "ups": "0.13", "wpb": "260036", "bsz": "1767.1", "num_updates": "62400", "lr": "0.000458696", "gnorm": "0.39", "loss_scale": "4", "train_wall": "324", "gb_free": "39.6", "wall": "665"}
[2024-10-09 17:41:21,168][fairseq.trainer][WARNING] - OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 180.00 MiB. GPU 
[2024-10-09 17:41:21,168][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 1            |        cudaMalloc retries: 5         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   7829 MiB |   8508 MiB | 327481 MiB | 319652 MiB |
|       from large pool |   7772 MiB |   8453 MiB | 325084 MiB | 317311 MiB |
|       from small pool |     56 MiB |    103 MiB |   2397 MiB |   2340 MiB |
|---------------------------------------------------------------------------|
| Active memory         |   7829 MiB |   8508 MiB | 327481 MiB | 319652 MiB |
|       from large pool |   7772 MiB |   8453 MiB | 325084 MiB | 317311 MiB |
|       from small pool |     56 MiB |    103 MiB |   2397 MiB |   2340 MiB |
|---------------------------------------------------------------------------|
| Requested memory      |   7820 MiB |   8499 MiB | 326970 MiB | 319150 MiB |
|       from large pool |   7764 MiB |   8444 MiB | 324578 MiB | 316814 MiB |
|       from small pool |     56 MiB |    102 MiB |   2392 MiB |   2335 MiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   8980 MiB |   9026 MiB |  24484 MiB |  15504 MiB |
|       from large pool |   8876 MiB |   8920 MiB |  24358 MiB |  15482 MiB |
|       from small pool |    104 MiB |    106 MiB |    126 MiB |     22 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    812 MiB |   1936 MiB | 208284 MiB | 207471 MiB |
|       from large pool |    765 MiB |   1908 MiB | 204949 MiB | 204184 MiB |
|       from small pool |     47 MiB |     67 MiB |   3334 MiB |   3286 MiB |
|---------------------------------------------------------------------------|
| Allocations           |    2785    |    3650    |   43210    |   40425    |
|       from large pool |     251    |     280    |    7810    |    7559    |
|       from small pool |    2534    |    3568    |   35400    |   32866    |
|---------------------------------------------------------------------------|
| Active allocs         |    2785    |    3650    |   43210    |   40425    |
|       from large pool |     251    |     280    |    7810    |    7559    |
|       from small pool |    2534    |    3568    |   35400    |   32866    |
|---------------------------------------------------------------------------|
| GPU reserved segments |     127    |     130    |     259    |     132    |
|       from large pool |      75    |      77    |     196    |     121    |
|       from small pool |      52    |      53    |      63    |      11    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     405    |     417    |   14730    |   14325    |
|       from large pool |      79    |      82    |    4842    |    4763    |
|       from small pool |     326    |     335    |    9888    |    9562    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-09 17:41:21,169][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-09 17:41:21,169][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-09 17:41:21,169][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-09 17:41:21,169][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 4                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-09 17:41:21,170][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 5                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-09 17:41:21,170][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 6                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-09 17:41:21,170][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 7                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-09 17:41:21,170][fairseq.trainer][WARNING] - attempting to recover from OOM in forward/backward pass
[2024-10-09 17:42:09,013][fairseq_cli.train][INFO] - end of epoch 1301 (average epoch stats below)
[2024-10-09 17:42:09,205][train][INFO] - {"epoch": 1301, "train_loss": "0.479", "train_ntokens": "260683", "train_nsentences": "1750.04", "train_wps": "41737.1", "train_ups": "0.16", "train_wpb": "260683", "train_bsz": "1750", "train_num_updates": "62420", "train_lr": "0.000458668", "train_gnorm": "0.381", "train_loss_scale": "4", "train_train_wall": "410", "train_gb_free": "39.8", "train_wall": "753"}
[2024-10-09 17:42:09,494][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 17:42:09,520][fairseq.trainer][INFO] - begin training epoch 1302
[2024-10-09 17:42:09,520][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 17:49:39,088][fairseq_cli.train][INFO] - end of epoch 1302 (average epoch stats below)
[2024-10-09 17:49:39,120][train][INFO] - {"epoch": 1302, "train_loss": "0.479", "train_ntokens": "260780", "train_nsentences": "1750.04", "train_wps": "27822.9", "train_ups": "0.11", "train_wpb": "260780", "train_bsz": "1750", "train_num_updates": "62468", "train_lr": "0.000458603", "train_gnorm": "0.385", "train_loss_scale": "4", "train_train_wall": "124", "train_gb_free": "39.2", "train_wall": "1204"}
[2024-10-09 17:49:39,209][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 17:49:39,221][fairseq.trainer][INFO] - begin training epoch 1303
[2024-10-09 17:49:39,221][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 17:52:16,121][fairseq_cli.train][INFO] - end of epoch 1303 (average epoch stats below)
[2024-10-09 17:52:16,125][train][INFO] - {"epoch": 1303, "train_loss": "0.471", "train_ntokens": "260708", "train_nsentences": "1750.04", "train_wps": "79705.7", "train_ups": "0.31", "train_wpb": "260708", "train_bsz": "1750", "train_num_updates": "62516", "train_lr": "0.000458538", "train_gnorm": "0.365", "train_loss_scale": "4", "train_train_wall": "53", "train_gb_free": "39.6", "train_wall": "1361"}
[2024-10-09 17:52:16,183][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 17:52:16,186][fairseq.trainer][INFO] - begin training epoch 1304
[2024-10-09 17:52:16,187][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 17:54:26,545][fairseq_cli.train][INFO] - end of epoch 1304 (average epoch stats below)
[2024-10-09 17:54:26,549][train][INFO] - {"epoch": 1304, "train_loss": "0.48", "train_ntokens": "260706", "train_nsentences": "1750.04", "train_wps": "95949.5", "train_ups": "0.37", "train_wpb": "260706", "train_bsz": "1750", "train_num_updates": "62564", "train_lr": "0.000458473", "train_gnorm": "0.376", "train_loss_scale": "4", "train_train_wall": "51", "train_gb_free": "39.6", "train_wall": "1491"}
[2024-10-09 17:54:26,602][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 17:54:26,605][fairseq.trainer][INFO] - begin training epoch 1305
[2024-10-09 17:54:26,606][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 17:56:21,308][train_inner][INFO] - {"epoch": 1305, "update": 1304.75, "loss": "0.476", "ntokens": "261096", "nsentences": "1737.73", "wps": "55516.8", "ups": "0.21", "wpb": "261096", "bsz": "1737.7", "num_updates": "62600", "lr": "0.000458424", "gnorm": "0.379", "loss_scale": "4", "train_wall": "359", "gb_free": "39.3", "wall": "1606"}
[2024-10-09 17:56:33,543][fairseq_cli.train][INFO] - end of epoch 1305 (average epoch stats below)
[2024-10-09 17:56:33,546][train][INFO] - {"epoch": 1305, "train_loss": "0.475", "train_ntokens": "260859", "train_nsentences": "1750.04", "train_wps": "98596", "train_ups": "0.38", "train_wpb": "260859", "train_bsz": "1750", "train_num_updates": "62612", "train_lr": "0.000458408", "train_gnorm": "0.394", "train_loss_scale": "4", "train_train_wall": "56", "train_gb_free": "39.8", "train_wall": "1618"}
[2024-10-09 17:56:33,615][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 17:56:33,619][fairseq.trainer][INFO] - begin training epoch 1306
[2024-10-09 17:56:33,620][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 17:58:42,128][fairseq_cli.train][INFO] - end of epoch 1306 (average epoch stats below)
[2024-10-09 17:58:42,132][train][INFO] - {"epoch": 1306, "train_loss": "0.487", "train_ntokens": "260473", "train_nsentences": "1750.04", "train_wps": "97235.2", "train_ups": "0.37", "train_wpb": "260473", "train_bsz": "1750", "train_num_updates": "62660", "train_lr": "0.000458342", "train_gnorm": "0.374", "train_loss_scale": "4", "train_train_wall": "52", "train_gb_free": "39.2", "train_wall": "1747"}
[2024-10-09 17:58:42,188][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 17:58:42,193][fairseq.trainer][INFO] - begin training epoch 1307
[2024-10-09 17:58:42,193][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 18:00:52,275][fairseq_cli.train][INFO] - end of epoch 1307 (average epoch stats below)
[2024-10-09 18:00:52,278][train][INFO] - {"epoch": 1307, "train_loss": "0.483", "train_ntokens": "260964", "train_nsentences": "1750.04", "train_wps": "96249.1", "train_ups": "0.37", "train_wpb": "260964", "train_bsz": "1750", "train_num_updates": "62708", "train_lr": "0.000458277", "train_gnorm": "0.414", "train_loss_scale": "4", "train_train_wall": "54", "train_gb_free": "39.9", "train_wall": "1877"}
[2024-10-09 18:00:52,366][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 18:00:52,370][fairseq.trainer][INFO] - begin training epoch 1308
[2024-10-09 18:00:52,371][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 18:02:57,794][fairseq_cli.train][INFO] - end of epoch 1308 (average epoch stats below)
[2024-10-09 18:02:57,797][train][INFO] - {"epoch": 1308, "train_loss": "0.475", "train_ntokens": "260620", "train_nsentences": "1750.04", "train_wps": "99666.5", "train_ups": "0.38", "train_wpb": "260620", "train_bsz": "1750", "train_num_updates": "62756", "train_lr": "0.000458212", "train_gnorm": "0.375", "train_loss_scale": "4", "train_train_wall": "53", "train_gb_free": "39.2", "train_wall": "2002"}
[2024-10-09 18:02:57,939][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 18:02:57,943][fairseq.trainer][INFO] - begin training epoch 1309
[2024-10-09 18:02:57,943][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 18:05:06,925][train_inner][INFO] - {"epoch": 1309, "update": 1308.917, "loss": "0.48", "ntokens": "260345", "nsentences": "1762.67", "wps": "99063.4", "ups": "0.38", "wpb": "260345", "bsz": "1762.7", "num_updates": "62800", "lr": "0.000458152", "gnorm": "0.384", "loss_scale": "4", "train_wall": "226", "gb_free": "42", "wall": "2131"}
[2024-10-09 18:05:08,023][fairseq_cli.train][INFO] - end of epoch 1309 (average epoch stats below)
[2024-10-09 18:05:08,025][train][INFO] - {"epoch": 1309, "train_loss": "0.473", "train_ntokens": "260498", "train_nsentences": "1750.04", "train_wps": "96017.4", "train_ups": "0.37", "train_wpb": "260498", "train_bsz": "1750", "train_num_updates": "62804", "train_lr": "0.000458147", "train_gnorm": "0.375", "train_loss_scale": "4", "train_train_wall": "56", "train_gb_free": "40.1", "train_wall": "2132"}
[2024-10-09 18:05:08,103][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 18:05:08,106][fairseq.trainer][INFO] - begin training epoch 1310
[2024-10-09 18:05:08,107][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 18:07:17,302][fairseq_cli.train][INFO] - end of epoch 1310 (average epoch stats below)
[2024-10-09 18:07:17,308][train][INFO] - {"epoch": 1310, "train_loss": "0.478", "train_ntokens": "260747", "train_nsentences": "1750.04", "train_wps": "96812.4", "train_ups": "0.37", "train_wpb": "260747", "train_bsz": "1750", "train_num_updates": "62852", "train_lr": "0.000458082", "train_gnorm": "0.361", "train_loss_scale": "4", "train_train_wall": "55", "train_gb_free": "39.7", "train_wall": "2262"}
[2024-10-09 18:07:17,410][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 18:07:17,414][fairseq.trainer][INFO] - begin training epoch 1311
[2024-10-09 18:07:17,414][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 18:09:27,054][fairseq_cli.train][INFO] - end of epoch 1311 (average epoch stats below)
[2024-10-09 18:09:27,076][train][INFO] - {"epoch": 1311, "train_loss": "0.488", "train_ntokens": "260494", "train_nsentences": "1750.04", "train_wps": "96356.1", "train_ups": "0.37", "train_wpb": "260494", "train_bsz": "1750", "train_num_updates": "62900", "train_lr": "0.000458016", "train_gnorm": "0.371", "train_loss_scale": "4", "train_train_wall": "59", "train_gb_free": "39.7", "train_wall": "2391"}
[2024-10-09 18:09:27,466][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 18:09:27,479][fairseq.trainer][INFO] - begin training epoch 1312
[2024-10-09 18:09:27,480][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 18:11:35,014][fairseq_cli.train][INFO] - end of epoch 1312 (average epoch stats below)
[2024-10-09 18:11:35,020][train][INFO] - {"epoch": 1312, "train_loss": "0.479", "train_ntokens": "260771", "train_nsentences": "1750.04", "train_wps": "97834.1", "train_ups": "0.38", "train_wpb": "260771", "train_bsz": "1750", "train_num_updates": "62948", "train_lr": "0.000457951", "train_gnorm": "0.379", "train_loss_scale": "4", "train_train_wall": "40", "train_gb_free": "39.6", "train_wall": "2519"}
[2024-10-09 18:11:35,125][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 18:11:35,129][fairseq.trainer][INFO] - begin training epoch 1313
[2024-10-09 18:11:35,129][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 18:13:50,744][fairseq_cli.train][INFO] - end of epoch 1313 (average epoch stats below)
[2024-10-09 18:13:50,747][train][INFO] - {"epoch": 1313, "train_loss": "0.487", "train_ntokens": "260814", "train_nsentences": "1750.04", "train_wps": "92238.3", "train_ups": "0.35", "train_wpb": "260814", "train_bsz": "1750", "train_num_updates": "62996", "train_lr": "0.000457886", "train_gnorm": "0.363", "train_loss_scale": "4", "train_train_wall": "72", "train_gb_free": "40.3", "train_wall": "2655"}
[2024-10-09 18:13:50,808][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 18:13:50,817][fairseq.trainer][INFO] - begin training epoch 1314
[2024-10-09 18:13:50,818][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 18:15:20,211][train_inner][INFO] - {"epoch": 1314, "update": 1313.083, "loss": "0.482", "ntokens": "260734", "nsentences": "1753.27", "wps": "85031.5", "ups": "0.33", "wpb": "260734", "bsz": "1753.3", "num_updates": "63000", "lr": "0.00045788", "gnorm": "0.368", "loss_scale": "4", "train_wall": "243", "gb_free": "39.7", "wall": "2745"}
[2024-10-09 18:15:57,993][fairseq_cli.train][INFO] - end of epoch 1314 (average epoch stats below)
[2024-10-09 18:15:58,007][train][INFO] - {"epoch": 1314, "train_loss": "0.486", "train_ntokens": "260964", "train_nsentences": "1750.04", "train_wps": "98441", "train_ups": "0.38", "train_wpb": "260964", "train_bsz": "1750", "train_num_updates": "63044", "train_lr": "0.000457821", "train_gnorm": "0.373", "train_loss_scale": "4", "train_train_wall": "50", "train_gb_free": "40.1", "train_wall": "2782"}
[2024-10-09 18:15:58,184][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 18:15:58,264][fairseq.trainer][INFO] - begin training epoch 1315
[2024-10-09 18:15:58,265][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 18:18:03,161][fairseq_cli.train][INFO] - end of epoch 1315 (average epoch stats below)
[2024-10-09 18:18:03,177][train][INFO] - {"epoch": 1315, "train_loss": "0.47", "train_ntokens": "260945", "train_nsentences": "1750.04", "train_wps": "100078", "train_ups": "0.38", "train_wpb": "260945", "train_bsz": "1750", "train_num_updates": "63092", "train_lr": "0.000457755", "train_gnorm": "0.373", "train_loss_scale": "4", "train_train_wall": "60", "train_gb_free": "39.2", "train_wall": "2908"}
[2024-10-09 18:18:03,265][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 18:18:03,268][fairseq.trainer][INFO] - begin training epoch 1316
[2024-10-09 18:18:03,269][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 18:20:08,820][fairseq_cli.train][INFO] - end of epoch 1316 (average epoch stats below)
[2024-10-09 18:20:08,835][train][INFO] - {"epoch": 1316, "train_loss": "0.482", "train_ntokens": "260916", "train_nsentences": "1750.04", "train_wps": "99676.5", "train_ups": "0.38", "train_wpb": "260916", "train_bsz": "1750", "train_num_updates": "63140", "train_lr": "0.00045769", "train_gnorm": "0.377", "train_loss_scale": "4", "train_train_wall": "40", "train_gb_free": "39.8", "train_wall": "3033"}
[2024-10-09 18:20:09,382][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 18:20:09,388][fairseq.trainer][INFO] - begin training epoch 1317
[2024-10-09 18:20:09,388][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 18:22:18,104][fairseq_cli.train][INFO] - end of epoch 1317 (average epoch stats below)
[2024-10-09 18:22:18,108][train][INFO] - {"epoch": 1317, "train_loss": "0.471", "train_ntokens": "260616", "train_nsentences": "1750.04", "train_wps": "96769.9", "train_ups": "0.37", "train_wpb": "260616", "train_bsz": "1750", "train_num_updates": "63188", "train_lr": "0.000457625", "train_gnorm": "0.383", "train_loss_scale": "4", "train_train_wall": "65", "train_gb_free": "39.7", "train_wall": "3163"}
[2024-10-09 18:22:18,219][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 18:22:18,240][fairseq.trainer][INFO] - begin training epoch 1318
[2024-10-09 18:22:18,240][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 18:23:52,119][train_inner][INFO] - {"epoch": 1318, "update": 1317.25, "loss": "0.477", "ntokens": "260997", "nsentences": "1744.91", "wps": "101972", "ups": "0.39", "wpb": "260998", "bsz": "1744.9", "num_updates": "63200", "lr": "0.000457609", "gnorm": "0.377", "loss_scale": "4", "train_wall": "221", "gb_free": "40.2", "wall": "3257"}
[2024-10-09 18:24:28,995][fairseq_cli.train][INFO] - end of epoch 1318 (average epoch stats below)
[2024-10-09 18:24:28,997][train][INFO] - {"epoch": 1318, "train_loss": "0.475", "train_ntokens": "261139", "train_nsentences": "1750.04", "train_wps": "95767.7", "train_ups": "0.37", "train_wpb": "261139", "train_bsz": "1750", "train_num_updates": "63236", "train_lr": "0.00045756", "train_gnorm": "0.352", "train_loss_scale": "4", "train_train_wall": "60", "train_gb_free": "39.6", "train_wall": "3293"}
[2024-10-09 18:24:29,051][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 18:24:29,056][fairseq.trainer][INFO] - begin training epoch 1319
[2024-10-09 18:24:29,056][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 18:26:37,439][fairseq_cli.train][INFO] - end of epoch 1319 (average epoch stats below)
[2024-10-09 18:26:37,443][train][INFO] - {"epoch": 1319, "train_loss": "0.479", "train_ntokens": "260789", "train_nsentences": "1750.04", "train_wps": "97458.3", "train_ups": "0.37", "train_wpb": "260789", "train_bsz": "1750", "train_num_updates": "63284", "train_lr": "0.000457495", "train_gnorm": "0.372", "train_loss_scale": "4", "train_train_wall": "58", "train_gb_free": "39.4", "train_wall": "3422"}
[2024-10-09 18:26:37,553][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 18:26:37,566][fairseq.trainer][INFO] - begin training epoch 1320
[2024-10-09 18:26:37,566][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 18:28:46,151][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 1320 @ 63332 updates
[2024-10-09 18:28:46,153][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-09 18:28:49,658][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-09 18:28:49,694][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 1320 @ 63332 updates, score None) (writing took 3.543314279988408 seconds)
[2024-10-09 18:28:49,695][fairseq_cli.train][INFO] - end of epoch 1320 (average epoch stats below)
[2024-10-09 18:28:49,699][train][INFO] - {"epoch": 1320, "train_loss": "0.477", "train_ntokens": "260925", "train_nsentences": "1750.04", "train_wps": "94700.9", "train_ups": "0.36", "train_wpb": "260925", "train_bsz": "1750", "train_num_updates": "63332", "train_lr": "0.000457429", "train_gnorm": "0.375", "train_loss_scale": "4", "train_train_wall": "54", "train_gb_free": "40.3", "train_wall": "3554"}
[2024-10-09 18:28:49,751][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 18:28:49,756][fairseq.trainer][INFO] - begin training epoch 1321
[2024-10-09 18:28:49,756][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 18:30:52,769][fairseq_cli.train][INFO] - end of epoch 1321 (average epoch stats below)
[2024-10-09 18:30:52,773][train][INFO] - {"epoch": 1321, "train_loss": "0.475", "train_ntokens": "261005", "train_nsentences": "1750.04", "train_wps": "101796", "train_ups": "0.39", "train_wpb": "261005", "train_bsz": "1750", "train_num_updates": "63380", "train_lr": "0.000457364", "train_gnorm": "0.358", "train_loss_scale": "4", "train_train_wall": "30", "train_gb_free": "39.3", "train_wall": "3677"}
[2024-10-09 18:30:52,875][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 18:30:52,880][fairseq.trainer][INFO] - begin training epoch 1322
[2024-10-09 18:30:52,881][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 18:32:40,698][train_inner][INFO] - {"epoch": 1322, "update": 1321.417, "loss": "0.476", "ntokens": "260825", "nsentences": "1751.88", "wps": "98690.1", "ups": "0.38", "wpb": "260825", "bsz": "1751.9", "num_updates": "63400", "lr": "0.000457337", "gnorm": "0.364", "loss_scale": "4", "train_wall": "207", "gb_free": "39.8", "wall": "3785"}
[2024-10-09 18:33:01,177][fairseq_cli.train][INFO] - end of epoch 1322 (average epoch stats below)
[2024-10-09 18:33:01,179][train][INFO] - {"epoch": 1322, "train_loss": "0.472", "train_ntokens": "260640", "train_nsentences": "1750.04", "train_wps": "97432.5", "train_ups": "0.37", "train_wpb": "260640", "train_bsz": "1750", "train_num_updates": "63428", "train_lr": "0.000457299", "train_gnorm": "0.369", "train_loss_scale": "4", "train_train_wall": "49", "train_gb_free": "40", "train_wall": "3806"}
[2024-10-09 18:33:01,256][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 18:33:01,260][fairseq.trainer][INFO] - begin training epoch 1323
[2024-10-09 18:33:01,261][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 18:35:05,226][fairseq_cli.train][INFO] - end of epoch 1323 (average epoch stats below)
[2024-10-09 18:35:05,234][train][INFO] - {"epoch": 1323, "train_loss": "0.482", "train_ntokens": "260602", "train_nsentences": "1750.04", "train_wps": "100836", "train_ups": "0.39", "train_wpb": "260602", "train_bsz": "1750", "train_num_updates": "63476", "train_lr": "0.000457234", "train_gnorm": "0.373", "train_loss_scale": "4", "train_train_wall": "35", "train_gb_free": "39.8", "train_wall": "3930"}
[2024-10-09 18:35:05,339][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 18:35:05,351][fairseq.trainer][INFO] - begin training epoch 1324
[2024-10-09 18:35:05,351][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 18:37:14,037][fairseq_cli.train][INFO] - end of epoch 1324 (average epoch stats below)
[2024-10-09 18:37:14,040][train][INFO] - {"epoch": 1324, "train_loss": "0.474", "train_ntokens": "260553", "train_nsentences": "1750.04", "train_wps": "97097.5", "train_ups": "0.37", "train_wpb": "260553", "train_bsz": "1750", "train_num_updates": "63524", "train_lr": "0.000457168", "train_gnorm": "0.372", "train_loss_scale": "4", "train_train_wall": "46", "train_gb_free": "39.3", "train_wall": "4058"}
[2024-10-09 18:37:14,111][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 18:37:14,115][fairseq.trainer][INFO] - begin training epoch 1325
[2024-10-09 18:37:14,115][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 18:39:21,142][fairseq_cli.train][INFO] - end of epoch 1325 (average epoch stats below)
[2024-10-09 18:39:21,146][train][INFO] - {"epoch": 1325, "train_loss": "0.476", "train_ntokens": "260751", "train_nsentences": "1750.04", "train_wps": "98472.1", "train_ups": "0.38", "train_wpb": "260751", "train_bsz": "1750", "train_num_updates": "63572", "train_lr": "0.000457103", "train_gnorm": "0.377", "train_loss_scale": "4", "train_train_wall": "59", "train_gb_free": "39.6", "train_wall": "4186"}
[2024-10-09 18:39:21,228][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 18:39:21,232][fairseq.trainer][INFO] - begin training epoch 1326
[2024-10-09 18:39:21,232][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 18:41:07,958][train_inner][INFO] - {"epoch": 1326, "update": 1325.583, "loss": "0.476", "ntokens": "260699", "nsentences": "1743.58", "wps": "102788", "ups": "0.39", "wpb": "260699", "bsz": "1743.6", "num_updates": "63600", "lr": "0.000457065", "gnorm": "0.373", "loss_scale": "4", "train_wall": "174", "gb_free": "39.4", "wall": "4292"}
[2024-10-09 18:41:27,132][fairseq_cli.train][INFO] - end of epoch 1326 (average epoch stats below)
[2024-10-09 18:41:27,136][train][INFO] - {"epoch": 1326, "train_loss": "0.475", "train_ntokens": "261044", "train_nsentences": "1750.04", "train_wps": "99456.1", "train_ups": "0.38", "train_wpb": "261044", "train_bsz": "1750", "train_num_updates": "63620", "train_lr": "0.000457038", "train_gnorm": "0.363", "train_loss_scale": "4", "train_train_wall": "30", "train_gb_free": "39.8", "train_wall": "4312"}
[2024-10-09 18:41:27,202][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 18:41:27,210][fairseq.trainer][INFO] - begin training epoch 1327
[2024-10-09 18:41:27,210][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 18:43:34,501][fairseq_cli.train][INFO] - end of epoch 1327 (average epoch stats below)
[2024-10-09 18:43:34,505][train][INFO] - {"epoch": 1327, "train_loss": "0.477", "train_ntokens": "261045", "train_nsentences": "1750.04", "train_wps": "98379.3", "train_ups": "0.38", "train_wpb": "261045", "train_bsz": "1750", "train_num_updates": "63668", "train_lr": "0.000456973", "train_gnorm": "0.369", "train_loss_scale": "4", "train_train_wall": "53", "train_gb_free": "39.6", "train_wall": "4439"}
[2024-10-09 18:43:34,564][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 18:43:34,572][fairseq.trainer][INFO] - begin training epoch 1328
[2024-10-09 18:43:34,572][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 18:45:43,983][fairseq_cli.train][INFO] - end of epoch 1328 (average epoch stats below)
[2024-10-09 18:45:43,986][train][INFO] - {"epoch": 1328, "train_loss": "0.478", "train_ntokens": "260860", "train_nsentences": "1750.04", "train_wps": "96705", "train_ups": "0.37", "train_wpb": "260860", "train_bsz": "1750", "train_num_updates": "63716", "train_lr": "0.000456908", "train_gnorm": "0.372", "train_loss_scale": "4", "train_train_wall": "64", "train_gb_free": "39.6", "train_wall": "4568"}
[2024-10-09 18:45:44,042][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 18:45:44,052][fairseq.trainer][INFO] - begin training epoch 1329
[2024-10-09 18:45:44,053][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 18:47:50,904][fairseq_cli.train][INFO] - end of epoch 1329 (average epoch stats below)
[2024-10-09 18:47:50,911][train][INFO] - {"epoch": 1329, "train_loss": "0.473", "train_ntokens": "260774", "train_nsentences": "1750.04", "train_wps": "98623.2", "train_ups": "0.38", "train_wpb": "260774", "train_bsz": "1750", "train_num_updates": "63764", "train_lr": "0.000456842", "train_gnorm": "0.37", "train_loss_scale": "4", "train_train_wall": "47", "train_gb_free": "39.6", "train_wall": "4695"}
[2024-10-09 18:47:50,999][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 18:47:51,003][fairseq.trainer][INFO] - begin training epoch 1330
[2024-10-09 18:47:51,004][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 18:49:46,315][train_inner][INFO] - {"epoch": 1330, "update": 1329.75, "loss": "0.476", "ntokens": "260668", "nsentences": "1768.05", "wps": "100575", "ups": "0.39", "wpb": "260668", "bsz": "1768", "num_updates": "63800", "lr": "0.000456793", "gnorm": "0.368", "loss_scale": "4", "train_wall": "232", "gb_free": "39.7", "wall": "4811"}
[2024-10-09 18:49:57,335][fairseq_cli.train][INFO] - end of epoch 1330 (average epoch stats below)
[2024-10-09 18:49:57,338][train][INFO] - {"epoch": 1330, "train_loss": "0.477", "train_ntokens": "260505", "train_nsentences": "1750.04", "train_wps": "98907.4", "train_ups": "0.38", "train_wpb": "260505", "train_bsz": "1750", "train_num_updates": "63812", "train_lr": "0.000456777", "train_gnorm": "0.374", "train_loss_scale": "4", "train_train_wall": "63", "train_gb_free": "39.7", "train_wall": "4822"}
[2024-10-09 18:49:57,501][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 18:49:57,506][fairseq.trainer][INFO] - begin training epoch 1331
[2024-10-09 18:49:57,507][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 18:52:03,815][fairseq_cli.train][INFO] - end of epoch 1331 (average epoch stats below)
[2024-10-09 18:52:03,828][train][INFO] - {"epoch": 1331, "train_loss": "0.479", "train_ntokens": "260585", "train_nsentences": "1750.04", "train_wps": "98894.7", "train_ups": "0.38", "train_wpb": "260585", "train_bsz": "1750", "train_num_updates": "63860", "train_lr": "0.000456712", "train_gnorm": "0.367", "train_loss_scale": "4", "train_train_wall": "36", "train_gb_free": "39.3", "train_wall": "4948"}
[2024-10-09 18:52:03,924][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 18:52:03,942][fairseq.trainer][INFO] - begin training epoch 1332
[2024-10-09 18:52:03,942][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 18:54:08,888][fairseq_cli.train][INFO] - end of epoch 1332 (average epoch stats below)
[2024-10-09 18:54:08,892][train][INFO] - {"epoch": 1332, "train_loss": "0.481", "train_ntokens": "260552", "train_nsentences": "1750.04", "train_wps": "100004", "train_ups": "0.38", "train_wpb": "260552", "train_bsz": "1750", "train_num_updates": "63908", "train_lr": "0.000456647", "train_gnorm": "0.387", "train_loss_scale": "4", "train_train_wall": "46", "train_gb_free": "40.1", "train_wall": "5073"}
[2024-10-09 18:54:09,024][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 18:54:09,028][fairseq.trainer][INFO] - begin training epoch 1333
[2024-10-09 18:54:09,028][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 18:56:16,028][fairseq_cli.train][INFO] - end of epoch 1333 (average epoch stats below)
[2024-10-09 18:56:16,031][train][INFO] - {"epoch": 1333, "train_loss": "0.473", "train_ntokens": "260637", "train_nsentences": "1750.04", "train_wps": "98403", "train_ups": "0.38", "train_wpb": "260637", "train_bsz": "1750", "train_num_updates": "63956", "train_lr": "0.000456582", "train_gnorm": "0.373", "train_loss_scale": "4", "train_train_wall": "62", "train_gb_free": "40.6", "train_wall": "5200"}
[2024-10-09 18:56:16,091][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 18:56:16,113][fairseq.trainer][INFO] - begin training epoch 1334
[2024-10-09 18:56:16,114][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 18:58:23,237][train_inner][INFO] - {"epoch": 1334, "update": 1333.917, "loss": "0.476", "ntokens": "260760", "nsentences": "1742.28", "wps": "100890", "ups": "0.39", "wpb": "260760", "bsz": "1742.3", "num_updates": "64000", "lr": "0.000456522", "gnorm": "0.376", "loss_scale": "4", "train_wall": "218", "gb_free": "39.3", "wall": "5328"}
[2024-10-09 18:58:24,230][fairseq_cli.train][INFO] - end of epoch 1334 (average epoch stats below)
[2024-10-09 18:58:24,233][train][INFO] - {"epoch": 1334, "train_loss": "0.472", "train_ntokens": "260962", "train_nsentences": "1750.04", "train_wps": "97707.9", "train_ups": "0.37", "train_wpb": "260962", "train_bsz": "1750", "train_num_updates": "64004", "train_lr": "0.000456516", "train_gnorm": "0.371", "train_loss_scale": "4", "train_train_wall": "65", "train_gb_free": "40", "train_wall": "5329"}
[2024-10-09 18:58:24,302][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 18:58:24,306][fairseq.trainer][INFO] - begin training epoch 1335
[2024-10-09 18:58:24,306][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 19:00:36,741][fairseq_cli.train][INFO] - end of epoch 1335 (average epoch stats below)
[2024-10-09 19:00:36,750][train][INFO] - {"epoch": 1335, "train_loss": "0.475", "train_ntokens": "260346", "train_nsentences": "1750.04", "train_wps": "94304.5", "train_ups": "0.36", "train_wpb": "260346", "train_bsz": "1750", "train_num_updates": "64052", "train_lr": "0.000456451", "train_gnorm": "0.364", "train_loss_scale": "4", "train_train_wall": "50", "train_gb_free": "39.7", "train_wall": "5461"}
[2024-10-09 19:00:36,909][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 19:00:36,927][fairseq.trainer][INFO] - begin training epoch 1336
[2024-10-09 19:00:36,928][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 19:02:46,466][fairseq_cli.train][INFO] - end of epoch 1336 (average epoch stats below)
[2024-10-09 19:02:46,470][train][INFO] - {"epoch": 1336, "train_loss": "0.48", "train_ntokens": "260671", "train_nsentences": "1750.04", "train_wps": "96457.3", "train_ups": "0.37", "train_wpb": "260671", "train_bsz": "1750", "train_num_updates": "64100", "train_lr": "0.000456386", "train_gnorm": "0.365", "train_loss_scale": "4", "train_train_wall": "54", "train_gb_free": "39.3", "train_wall": "5591"}
[2024-10-09 19:02:46,534][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 19:02:46,541][fairseq.trainer][INFO] - begin training epoch 1337
[2024-10-09 19:02:46,541][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 19:04:55,682][fairseq_cli.train][INFO] - end of epoch 1337 (average epoch stats below)
[2024-10-09 19:04:55,686][train][INFO] - {"epoch": 1337, "train_loss": "0.475", "train_ntokens": "260819", "train_nsentences": "1750.04", "train_wps": "96888.6", "train_ups": "0.37", "train_wpb": "260819", "train_bsz": "1750", "train_num_updates": "64148", "train_lr": "0.000456321", "train_gnorm": "0.385", "train_loss_scale": "4", "train_train_wall": "55", "train_gb_free": "39.3", "train_wall": "5720"}
[2024-10-09 19:04:55,748][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 19:04:55,762][fairseq.trainer][INFO] - begin training epoch 1338
[2024-10-09 19:04:55,763][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 19:07:03,079][fairseq_cli.train][INFO] - end of epoch 1338 (average epoch stats below)
[2024-10-09 19:07:03,097][train][INFO] - {"epoch": 1338, "train_loss": "0.474", "train_ntokens": "260832", "train_nsentences": "1750.04", "train_wps": "98267.2", "train_ups": "0.38", "train_wpb": "260832", "train_bsz": "1750", "train_num_updates": "64196", "train_lr": "0.000456255", "train_gnorm": "0.385", "train_loss_scale": "4", "train_train_wall": "31", "train_gb_free": "39.3", "train_wall": "5848"}
[2024-10-09 19:07:03,216][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 19:07:03,228][fairseq.trainer][INFO] - begin training epoch 1339
[2024-10-09 19:07:03,229][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 19:08:33,490][train_inner][INFO] - {"epoch": 1339, "update": 1338.083, "loss": "0.476", "ntokens": "260658", "nsentences": "1746.63", "wps": "85427.9", "ups": "0.33", "wpb": "260658", "bsz": "1746.6", "num_updates": "64200", "lr": "0.00045625", "gnorm": "0.375", "loss_scale": "4", "train_wall": "217", "gb_free": "39.3", "wall": "5938"}
[2024-10-09 19:09:12,755][fairseq_cli.train][INFO] - end of epoch 1339 (average epoch stats below)
[2024-10-09 19:09:12,759][train][INFO] - {"epoch": 1339, "train_loss": "0.472", "train_ntokens": "260734", "train_nsentences": "1750.04", "train_wps": "96525.2", "train_ups": "0.37", "train_wpb": "260734", "train_bsz": "1750", "train_num_updates": "64244", "train_lr": "0.00045619", "train_gnorm": "0.38", "train_loss_scale": "4", "train_train_wall": "65", "train_gb_free": "39.2", "train_wall": "5977"}
[2024-10-09 19:09:12,815][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 19:09:12,818][fairseq.trainer][INFO] - begin training epoch 1340
[2024-10-09 19:09:12,819][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 19:11:20,084][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 1340 @ 64292 updates
[2024-10-09 19:11:20,086][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-09 19:11:23,608][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-09 19:11:23,612][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 1340 @ 64292 updates, score None) (writing took 3.527074398472905 seconds)
[2024-10-09 19:11:23,612][fairseq_cli.train][INFO] - end of epoch 1340 (average epoch stats below)
[2024-10-09 19:11:23,614][train][INFO] - {"epoch": 1340, "train_loss": "0.471", "train_ntokens": "260870", "train_nsentences": "1750.04", "train_wps": "95693.5", "train_ups": "0.37", "train_wpb": "260870", "train_bsz": "1750", "train_num_updates": "64292", "train_lr": "0.000456125", "train_gnorm": "0.348", "train_loss_scale": "4", "train_train_wall": "64", "train_gb_free": "40.3", "train_wall": "6108"}
[2024-10-09 19:11:23,672][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 19:11:23,685][fairseq.trainer][INFO] - begin training epoch 1341
[2024-10-09 19:11:23,685][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 19:13:25,885][fairseq_cli.train][INFO] - end of epoch 1341 (average epoch stats below)
[2024-10-09 19:13:25,892][train][INFO] - {"epoch": 1341, "train_loss": "0.476", "train_ntokens": "260699", "train_nsentences": "1750.04", "train_wps": "102342", "train_ups": "0.39", "train_wpb": "260699", "train_bsz": "1750", "train_num_updates": "64340", "train_lr": "0.00045606", "train_gnorm": "0.37", "train_loss_scale": "4", "train_train_wall": "39", "train_gb_free": "39.7", "train_wall": "6230"}
[2024-10-09 19:13:26,012][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 19:13:26,029][fairseq.trainer][INFO] - begin training epoch 1342
[2024-10-09 19:13:26,029][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 19:15:34,059][fairseq_cli.train][INFO] - end of epoch 1342 (average epoch stats below)
[2024-10-09 19:15:34,063][train][INFO] - {"epoch": 1342, "train_loss": "0.478", "train_ntokens": "260835", "train_nsentences": "1750.04", "train_wps": "97687", "train_ups": "0.37", "train_wpb": "260835", "train_bsz": "1750", "train_num_updates": "64388", "train_lr": "0.000455995", "train_gnorm": "0.377", "train_loss_scale": "4", "train_train_wall": "52", "train_gb_free": "40.2", "train_wall": "6358"}
[2024-10-09 19:15:34,118][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 19:15:34,122][fairseq.trainer][INFO] - begin training epoch 1343
[2024-10-09 19:15:34,122][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 19:17:10,067][train_inner][INFO] - {"epoch": 1343, "update": 1342.25, "loss": "0.474", "ntokens": "260867", "nsentences": "1747.12", "wps": "100999", "ups": "0.39", "wpb": "260867", "bsz": "1747.1", "num_updates": "64400", "lr": "0.000455978", "gnorm": "0.37", "loss_scale": "4", "train_wall": "217", "gb_free": "39.6", "wall": "6454"}
[2024-10-09 19:17:42,053][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
[2024-10-09 19:17:42,751][fairseq_cli.train][INFO] - end of epoch 1343 (average epoch stats below)
[2024-10-09 19:17:42,756][train][INFO] - {"epoch": 1343, "train_loss": "0.47", "train_ntokens": "260962", "train_nsentences": "1745.02", "train_wps": "95309.8", "train_ups": "0.37", "train_wpb": "260962", "train_bsz": "1745", "train_num_updates": "64435", "train_lr": "0.000455931", "train_gnorm": "0.369", "train_loss_scale": "4", "train_train_wall": "55", "train_gb_free": "39.6", "train_wall": "6487"}
[2024-10-09 19:17:42,915][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 19:17:42,933][fairseq.trainer][INFO] - begin training epoch 1344
[2024-10-09 19:17:42,933][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 19:19:47,776][fairseq_cli.train][INFO] - end of epoch 1344 (average epoch stats below)
[2024-10-09 19:19:47,783][train][INFO] - {"epoch": 1344, "train_loss": "0.466", "train_ntokens": "260728", "train_nsentences": "1750.04", "train_wps": "100100", "train_ups": "0.38", "train_wpb": "260728", "train_bsz": "1750", "train_num_updates": "64483", "train_lr": "0.000455865", "train_gnorm": "0.381", "train_loss_scale": "4", "train_train_wall": "56", "train_gb_free": "39.6", "train_wall": "6612"}
[2024-10-09 19:19:47,935][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 19:19:47,939][fairseq.trainer][INFO] - begin training epoch 1345
[2024-10-09 19:19:47,940][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 19:21:57,952][fairseq_cli.train][INFO] - end of epoch 1345 (average epoch stats below)
[2024-10-09 19:21:57,969][train][INFO] - {"epoch": 1345, "train_loss": "0.479", "train_ntokens": "260723", "train_nsentences": "1750.04", "train_wps": "96132", "train_ups": "0.37", "train_wpb": "260723", "train_bsz": "1750", "train_num_updates": "64531", "train_lr": "0.0004558", "train_gnorm": "0.379", "train_loss_scale": "4", "train_train_wall": "50", "train_gb_free": "39.3", "train_wall": "6742"}
[2024-10-09 19:21:58,055][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 19:21:58,064][fairseq.trainer][INFO] - begin training epoch 1346
[2024-10-09 19:21:58,064][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 19:24:04,336][fairseq_cli.train][INFO] - end of epoch 1346 (average epoch stats below)
[2024-10-09 19:24:04,348][train][INFO] - {"epoch": 1346, "train_loss": "0.472", "train_ntokens": "260439", "train_nsentences": "1750.04", "train_wps": "98923.6", "train_ups": "0.38", "train_wpb": "260439", "train_bsz": "1750", "train_num_updates": "64579", "train_lr": "0.000455735", "train_gnorm": "0.384", "train_loss_scale": "4", "train_train_wall": "51", "train_gb_free": "40", "train_wall": "6869"}
[2024-10-09 19:24:04,514][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 19:24:04,525][fairseq.trainer][INFO] - begin training epoch 1347
[2024-10-09 19:24:04,526][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 19:25:51,073][train_inner][INFO] - {"epoch": 1347, "update": 1346.438, "loss": "0.47", "ntokens": "260618", "nsentences": "1751.1", "wps": "100045", "ups": "0.38", "wpb": "260618", "bsz": "1751.1", "num_updates": "64600", "lr": "0.000455707", "gnorm": "0.38", "loss_scale": "4", "train_wall": "219", "gb_free": "39.7", "wall": "6975"}
[2024-10-09 19:26:13,807][fairseq_cli.train][INFO] - end of epoch 1347 (average epoch stats below)
[2024-10-09 19:26:13,809][train][INFO] - {"epoch": 1347, "train_loss": "0.465", "train_ntokens": "260708", "train_nsentences": "1750.04", "train_wps": "96664.4", "train_ups": "0.37", "train_wpb": "260708", "train_bsz": "1750", "train_num_updates": "64627", "train_lr": "0.00045567", "train_gnorm": "0.38", "train_loss_scale": "4", "train_train_wall": "50", "train_gb_free": "39.8", "train_wall": "6998"}
[2024-10-09 19:26:13,868][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 19:26:13,871][fairseq.trainer][INFO] - begin training epoch 1348
[2024-10-09 19:26:13,872][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 19:28:22,842][fairseq_cli.train][INFO] - end of epoch 1348 (average epoch stats below)
[2024-10-09 19:28:22,848][train][INFO] - {"epoch": 1348, "train_loss": "0.475", "train_ntokens": "260661", "train_nsentences": "1750.04", "train_wps": "96964.5", "train_ups": "0.37", "train_wpb": "260660", "train_bsz": "1750", "train_num_updates": "64675", "train_lr": "0.000455605", "train_gnorm": "0.36", "train_loss_scale": "4", "train_train_wall": "58", "train_gb_free": "40.6", "train_wall": "7127"}
[2024-10-09 19:28:22,962][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 19:28:22,978][fairseq.trainer][INFO] - begin training epoch 1349
[2024-10-09 19:28:22,978][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 19:30:28,144][fairseq_cli.train][INFO] - end of epoch 1349 (average epoch stats below)
[2024-10-09 19:30:28,148][train][INFO] - {"epoch": 1349, "train_loss": "0.475", "train_ntokens": "260692", "train_nsentences": "1750.04", "train_wps": "99877.5", "train_ups": "0.38", "train_wpb": "260692", "train_bsz": "1750", "train_num_updates": "64723", "train_lr": "0.000455539", "train_gnorm": "0.359", "train_loss_scale": "4", "train_train_wall": "32", "train_gb_free": "39.8", "train_wall": "7253"}
[2024-10-09 19:30:28,281][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 19:30:28,301][fairseq.trainer][INFO] - begin training epoch 1350
[2024-10-09 19:30:28,302][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 19:32:33,845][fairseq_cli.train][INFO] - end of epoch 1350 (average epoch stats below)
[2024-10-09 19:32:33,879][train][INFO] - {"epoch": 1350, "train_loss": "0.47", "train_ntokens": "260528", "train_nsentences": "1750.04", "train_wps": "99475.4", "train_ups": "0.38", "train_wpb": "260528", "train_bsz": "1750", "train_num_updates": "64771", "train_lr": "0.000455474", "train_gnorm": "0.365", "train_loss_scale": "4", "train_train_wall": "59", "train_gb_free": "40.5", "train_wall": "7378"}
[2024-10-09 19:32:33,978][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 19:32:33,993][fairseq.trainer][INFO] - begin training epoch 1351
[2024-10-09 19:32:33,993][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 19:34:23,708][train_inner][INFO] - {"epoch": 1351, "update": 1350.604, "loss": "0.474", "ntokens": "260960", "nsentences": "1744.7", "wps": "101812", "ups": "0.39", "wpb": "260960", "bsz": "1744.7", "num_updates": "64800", "lr": "0.000455435", "gnorm": "0.363", "loss_scale": "4", "train_wall": "209", "gb_free": "39.8", "wall": "7488"}
[2024-10-09 19:34:41,288][fairseq_cli.train][INFO] - end of epoch 1351 (average epoch stats below)
[2024-10-09 19:34:41,291][train][INFO] - {"epoch": 1351, "train_loss": "0.475", "train_ntokens": "260913", "train_nsentences": "1750.04", "train_wps": "98296.9", "train_ups": "0.38", "train_wpb": "260913", "train_bsz": "1750", "train_num_updates": "64819", "train_lr": "0.000455409", "train_gnorm": "0.376", "train_loss_scale": "4", "train_train_wall": "58", "train_gb_free": "40.1", "train_wall": "7506"}
[2024-10-09 19:34:41,390][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 19:34:41,404][fairseq.trainer][INFO] - begin training epoch 1352
[2024-10-09 19:34:41,405][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 19:36:48,249][fairseq_cli.train][INFO] - end of epoch 1352 (average epoch stats below)
[2024-10-09 19:36:48,253][train][INFO] - {"epoch": 1352, "train_loss": "0.478", "train_ntokens": "260556", "train_nsentences": "1750.04", "train_wps": "98509.2", "train_ups": "0.38", "train_wpb": "260556", "train_bsz": "1750", "train_num_updates": "64867", "train_lr": "0.000455344", "train_gnorm": "0.366", "train_loss_scale": "4", "train_train_wall": "54", "train_gb_free": "40.1", "train_wall": "7633"}
[2024-10-09 19:36:48,347][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 19:36:48,368][fairseq.trainer][INFO] - begin training epoch 1353
[2024-10-09 19:36:48,369][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 19:38:58,930][fairseq_cli.train][INFO] - end of epoch 1353 (average epoch stats below)
[2024-10-09 19:38:58,937][train][INFO] - {"epoch": 1353, "train_loss": "0.471", "train_ntokens": "260798", "train_nsentences": "1750.04", "train_wps": "95794.1", "train_ups": "0.37", "train_wpb": "260798", "train_bsz": "1750", "train_num_updates": "64915", "train_lr": "0.000455279", "train_gnorm": "0.389", "train_loss_scale": "4", "train_train_wall": "61", "train_gb_free": "39.6", "train_wall": "7763"}
[2024-10-09 19:38:59,029][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 19:38:59,035][fairseq.trainer][INFO] - begin training epoch 1354
[2024-10-09 19:38:59,036][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 19:41:06,226][fairseq_cli.train][INFO] - end of epoch 1354 (average epoch stats below)
[2024-10-09 19:41:06,238][train][INFO] - {"epoch": 1354, "train_loss": "0.469", "train_ntokens": "260879", "train_nsentences": "1750.04", "train_wps": "98369.4", "train_ups": "0.38", "train_wpb": "260879", "train_bsz": "1750", "train_num_updates": "64963", "train_lr": "0.000455213", "train_gnorm": "0.37", "train_loss_scale": "4", "train_train_wall": "45", "train_gb_free": "39.4", "train_wall": "7891"}
[2024-10-09 19:41:06,359][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 19:41:06,365][fairseq.trainer][INFO] - begin training epoch 1355
[2024-10-09 19:41:06,366][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 19:43:06,748][train_inner][INFO] - {"epoch": 1355, "update": 1354.771, "loss": "0.473", "ntokens": "260497", "nsentences": "1751.65", "wps": "99609.6", "ups": "0.38", "wpb": "260498", "bsz": "1751.7", "num_updates": "65000", "lr": "0.000455163", "gnorm": "0.377", "loss_scale": "4", "train_wall": "222", "gb_free": "39.3", "wall": "8011"}
[2024-10-09 19:43:17,042][fairseq_cli.train][INFO] - end of epoch 1355 (average epoch stats below)
[2024-10-09 19:43:17,044][train][INFO] - {"epoch": 1355, "train_loss": "0.473", "train_ntokens": "260657", "train_nsentences": "1750.04", "train_wps": "95651.2", "train_ups": "0.37", "train_wpb": "260657", "train_bsz": "1750", "train_num_updates": "65011", "train_lr": "0.000455148", "train_gnorm": "0.378", "train_loss_scale": "4", "train_train_wall": "54", "train_gb_free": "39.7", "train_wall": "8021"}
[2024-10-09 19:43:17,291][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 19:43:17,295][fairseq.trainer][INFO] - begin training epoch 1356
[2024-10-09 19:43:17,296][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 19:46:12,855][fairseq_cli.train][INFO] - end of epoch 1356 (average epoch stats below)
[2024-10-09 19:46:12,860][train][INFO] - {"epoch": 1356, "train_loss": "0.465", "train_ntokens": "261009", "train_nsentences": "1750.04", "train_wps": "71259.9", "train_ups": "0.27", "train_wpb": "261009", "train_bsz": "1750", "train_num_updates": "65059", "train_lr": "0.000455083", "train_gnorm": "0.358", "train_loss_scale": "4", "train_train_wall": "58", "train_gb_free": "39.2", "train_wall": "8197"}
[2024-10-09 19:46:13,126][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 19:46:13,143][fairseq.trainer][INFO] - begin training epoch 1357
[2024-10-09 19:46:13,144][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 19:49:14,803][fairseq_cli.train][INFO] - end of epoch 1357 (average epoch stats below)
[2024-10-09 19:49:14,806][train][INFO] - {"epoch": 1357, "train_loss": "0.467", "train_ntokens": "260247", "train_nsentences": "1750.04", "train_wps": "68657.8", "train_ups": "0.26", "train_wpb": "260247", "train_bsz": "1750", "train_num_updates": "65107", "train_lr": "0.000455018", "train_gnorm": "0.356", "train_loss_scale": "4", "train_train_wall": "65", "train_gb_free": "39.8", "train_wall": "8379"}
[2024-10-09 19:49:15,436][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 19:49:15,456][fairseq.trainer][INFO] - begin training epoch 1358
[2024-10-09 19:49:15,456][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 19:53:03,778][fairseq_cli.train][INFO] - end of epoch 1358 (average epoch stats below)
[2024-10-09 19:53:03,792][train][INFO] - {"epoch": 1358, "train_loss": "0.471", "train_ntokens": "260360", "train_nsentences": "1750.04", "train_wps": "54577.1", "train_ups": "0.21", "train_wpb": "260360", "train_bsz": "1750", "train_num_updates": "65155", "train_lr": "0.000454952", "train_gnorm": "0.381", "train_loss_scale": "4", "train_train_wall": "115", "train_gb_free": "39.7", "train_wall": "8608"}
[2024-10-09 19:53:04,044][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 19:53:04,050][fairseq.trainer][INFO] - begin training epoch 1359
[2024-10-09 19:53:04,051][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 19:56:30,899][train_inner][INFO] - {"epoch": 1359, "update": 1358.938, "loss": "0.468", "ntokens": "260674", "nsentences": "1745.36", "wps": "64834.7", "ups": "0.25", "wpb": "260674", "bsz": "1745.4", "num_updates": "65200", "lr": "0.000454891", "gnorm": "0.368", "loss_scale": "4", "train_wall": "344", "gb_free": "39.6", "wall": "8815"}
[2024-10-09 19:56:34,580][fairseq_cli.train][INFO] - end of epoch 1359 (average epoch stats below)
[2024-10-09 19:56:34,582][train][INFO] - {"epoch": 1359, "train_loss": "0.467", "train_ntokens": "260320", "train_nsentences": "1750.04", "train_wps": "59279.6", "train_ups": "0.23", "train_wpb": "260320", "train_bsz": "1750", "train_num_updates": "65203", "train_lr": "0.000454887", "train_gnorm": "0.382", "train_loss_scale": "4", "train_train_wall": "99", "train_gb_free": "39.6", "train_wall": "8819"}
[2024-10-09 19:56:34,776][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 19:56:34,795][fairseq.trainer][INFO] - begin training epoch 1360
[2024-10-09 19:56:34,795][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 20:00:32,094][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 1360 @ 65251 updates
[2024-10-09 20:00:32,096][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-09 20:00:36,687][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-09 20:00:36,704][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 1360 @ 65251 updates, score None) (writing took 4.609180807135999 seconds)
[2024-10-09 20:00:36,704][fairseq_cli.train][INFO] - end of epoch 1360 (average epoch stats below)
[2024-10-09 20:00:36,707][train][INFO] - {"epoch": 1360, "train_loss": "0.464", "train_ntokens": "261063", "train_nsentences": "1750.04", "train_wps": "51755.3", "train_ups": "0.2", "train_wpb": "261063", "train_bsz": "1750", "train_num_updates": "65251", "train_lr": "0.000454822", "train_gnorm": "0.377", "train_loss_scale": "4", "train_train_wall": "122", "train_gb_free": "40.2", "train_wall": "9061"}
[2024-10-09 20:00:36,935][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 20:00:36,975][fairseq.trainer][INFO] - begin training epoch 1361
[2024-10-09 20:00:36,976][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 20:04:05,829][fairseq_cli.train][INFO] - end of epoch 1361 (average epoch stats below)
[2024-10-09 20:04:05,836][train][INFO] - {"epoch": 1361, "train_loss": "0.469", "train_ntokens": "260490", "train_nsentences": "1750.04", "train_wps": "59789.6", "train_ups": "0.23", "train_wpb": "260490", "train_bsz": "1750", "train_num_updates": "65299", "train_lr": "0.000454757", "train_gnorm": "0.376", "train_loss_scale": "4", "train_train_wall": "116", "train_gb_free": "39.8", "train_wall": "9270"}
[2024-10-09 20:04:06,052][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 20:04:06,076][fairseq.trainer][INFO] - begin training epoch 1362
[2024-10-09 20:04:06,077][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 20:07:47,683][fairseq_cli.train][INFO] - end of epoch 1362 (average epoch stats below)
[2024-10-09 20:07:47,691][train][INFO] - {"epoch": 1362, "train_loss": "0.462", "train_ntokens": "260870", "train_nsentences": "1750.04", "train_wps": "56442", "train_ups": "0.22", "train_wpb": "260870", "train_bsz": "1750", "train_num_updates": "65347", "train_lr": "0.000454692", "train_gnorm": "0.373", "train_loss_scale": "4", "train_train_wall": "124", "train_gb_free": "39.7", "train_wall": "9492"}
[2024-10-09 20:07:47,912][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 20:07:47,916][fairseq.trainer][INFO] - begin training epoch 1363
[2024-10-09 20:07:47,916][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 20:11:19,138][fairseq_cli.train][INFO] - end of epoch 1363 (average epoch stats below)
[2024-10-09 20:11:19,141][train][INFO] - {"epoch": 1363, "train_loss": "0.473", "train_ntokens": "260604", "train_nsentences": "1750.04", "train_wps": "59158.8", "train_ups": "0.23", "train_wpb": "260604", "train_bsz": "1750", "train_num_updates": "65395", "train_lr": "0.000454626", "train_gnorm": "0.366", "train_loss_scale": "4", "train_train_wall": "125", "train_gb_free": "39.6", "train_wall": "9704"}
[2024-10-09 20:11:19,318][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 20:11:19,336][fairseq.trainer][INFO] - begin training epoch 1364
[2024-10-09 20:11:19,336][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 20:13:49,556][train_inner][INFO] - {"epoch": 1364, "update": 1363.104, "loss": "0.467", "ntokens": "260669", "nsentences": "1750.53", "wps": "50193.8", "ups": "0.19", "wpb": "260669", "bsz": "1750.5", "num_updates": "65400", "lr": "0.00045462", "gnorm": "0.374", "loss_scale": "4", "train_wall": "549", "gb_free": "39.3", "wall": "9854"}
[2024-10-09 20:14:25,431][fairseq_cli.train][INFO] - end of epoch 1364 (average epoch stats below)
[2024-10-09 20:14:25,434][train][INFO] - {"epoch": 1364, "train_loss": "0.462", "train_ntokens": "260416", "train_nsentences": "1750.04", "train_wps": "67099.4", "train_ups": "0.26", "train_wpb": "260416", "train_bsz": "1750", "train_num_updates": "65443", "train_lr": "0.000454561", "train_gnorm": "0.387", "train_loss_scale": "4", "train_train_wall": "93", "train_gb_free": "39.6", "train_wall": "9890"}
[2024-10-09 20:14:25,503][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 20:14:25,515][fairseq.trainer][INFO] - begin training epoch 1365
[2024-10-09 20:14:25,516][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 20:17:54,181][fairseq_cli.train][INFO] - end of epoch 1365 (average epoch stats below)
[2024-10-09 20:17:54,195][train][INFO] - {"epoch": 1365, "train_loss": "0.47", "train_ntokens": "260552", "train_nsentences": "1750.04", "train_wps": "59909", "train_ups": "0.23", "train_wpb": "260552", "train_bsz": "1750", "train_num_updates": "65491", "train_lr": "0.000454496", "train_gnorm": "0.363", "train_loss_scale": "4", "train_train_wall": "105", "train_gb_free": "39.7", "train_wall": "10099"}
[2024-10-09 20:17:54,444][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 20:17:54,457][fairseq.trainer][INFO] - begin training epoch 1366
[2024-10-09 20:17:54,460][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 20:21:11,858][fairseq_cli.train][INFO] - end of epoch 1366 (average epoch stats below)
[2024-10-09 20:21:11,863][train][INFO] - {"epoch": 1366, "train_loss": "0.479", "train_ntokens": "260720", "train_nsentences": "1750.04", "train_wps": "63312.3", "train_ups": "0.24", "train_wpb": "260720", "train_bsz": "1750", "train_num_updates": "65539", "train_lr": "0.000454431", "train_gnorm": "0.4", "train_loss_scale": "4", "train_train_wall": "102", "train_gb_free": "39.2", "train_wall": "10296"}
[2024-10-09 20:21:12,083][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 20:21:12,124][fairseq.trainer][INFO] - begin training epoch 1367
[2024-10-09 20:21:12,124][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 20:23:24,033][fairseq_cli.train][INFO] - end of epoch 1367 (average epoch stats below)
[2024-10-09 20:23:24,042][train][INFO] - {"epoch": 1367, "train_loss": "0.471", "train_ntokens": "260953", "train_nsentences": "1750.04", "train_wps": "94765.7", "train_ups": "0.36", "train_wpb": "260953", "train_bsz": "1750", "train_num_updates": "65587", "train_lr": "0.000454365", "train_gnorm": "0.38", "train_loss_scale": "4", "train_train_wall": "50", "train_gb_free": "40.1", "train_wall": "10428"}
[2024-10-09 20:23:24,311][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 20:23:24,320][fairseq.trainer][INFO] - begin training epoch 1368
[2024-10-09 20:23:24,320][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 20:26:08,453][train_inner][INFO] - {"epoch": 1368, "update": 1367.271, "loss": "0.471", "ntokens": "260513", "nsentences": "1752.92", "wps": "70517.6", "ups": "0.27", "wpb": "260513", "bsz": "1752.9", "num_updates": "65600", "lr": "0.000454348", "gnorm": "0.384", "loss_scale": "4", "train_wall": "342", "gb_free": "40.6", "wall": "10593"}
[2024-10-09 20:26:34,148][fairseq_cli.train][INFO] - end of epoch 1368 (average epoch stats below)
[2024-10-09 20:26:34,150][train][INFO] - {"epoch": 1368, "train_loss": "0.469", "train_ntokens": "260838", "train_nsentences": "1750.04", "train_wps": "65859.5", "train_ups": "0.25", "train_wpb": "260838", "train_bsz": "1750", "train_num_updates": "65635", "train_lr": "0.0004543", "train_gnorm": "0.384", "train_loss_scale": "4", "train_train_wall": "75", "train_gb_free": "39.3", "train_wall": "10619"}
[2024-10-09 20:26:34,362][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 20:26:34,393][fairseq.trainer][INFO] - begin training epoch 1369
[2024-10-09 20:26:34,393][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 20:29:48,533][fairseq_cli.train][INFO] - end of epoch 1369 (average epoch stats below)
[2024-10-09 20:29:48,539][train][INFO] - {"epoch": 1369, "train_loss": "0.464", "train_ntokens": "261141", "train_nsentences": "1750.04", "train_wps": "64483.8", "train_ups": "0.25", "train_wpb": "261140", "train_bsz": "1750", "train_num_updates": "65683", "train_lr": "0.000454235", "train_gnorm": "0.375", "train_loss_scale": "4", "train_train_wall": "109", "train_gb_free": "39.8", "train_wall": "10813"}
[2024-10-09 20:29:48,724][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 20:29:48,754][fairseq.trainer][INFO] - begin training epoch 1370
[2024-10-09 20:29:48,754][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 20:33:03,388][fairseq_cli.train][INFO] - end of epoch 1370 (average epoch stats below)
[2024-10-09 20:33:03,396][train][INFO] - {"epoch": 1370, "train_loss": "0.471", "train_ntokens": "260989", "train_nsentences": "1750.04", "train_wps": "64291.5", "train_ups": "0.25", "train_wpb": "260989", "train_bsz": "1750", "train_num_updates": "65731", "train_lr": "0.00045417", "train_gnorm": "0.368", "train_loss_scale": "4", "train_train_wall": "72", "train_gb_free": "39.7", "train_wall": "11008"}
[2024-10-09 20:33:03,511][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 20:33:03,525][fairseq.trainer][INFO] - begin training epoch 1371
[2024-10-09 20:33:03,525][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 20:36:18,165][fairseq_cli.train][INFO] - end of epoch 1371 (average epoch stats below)
[2024-10-09 20:36:18,173][train][INFO] - {"epoch": 1371, "train_loss": "0.465", "train_ntokens": "261026", "train_nsentences": "1750.04", "train_wps": "64327.3", "train_ups": "0.25", "train_wpb": "261026", "train_bsz": "1750", "train_num_updates": "65779", "train_lr": "0.000454105", "train_gnorm": "0.363", "train_loss_scale": "4", "train_train_wall": "42", "train_gb_free": "39.3", "train_wall": "11203"}
[2024-10-09 20:36:18,440][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 20:36:18,464][fairseq.trainer][INFO] - begin training epoch 1372
[2024-10-09 20:36:18,465][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 20:39:03,298][train_inner][INFO] - {"epoch": 1372, "update": 1371.438, "loss": "0.467", "ntokens": "261164", "nsentences": "1757.78", "wps": "67413.1", "ups": "0.26", "wpb": "261164", "bsz": "1757.8", "num_updates": "65800", "lr": "0.000454076", "gnorm": "0.371", "loss_scale": "4", "train_wall": "320", "gb_free": "39.8", "wall": "11368"}
[2024-10-09 20:39:18,185][fairseq_cli.train][INFO] - end of epoch 1372 (average epoch stats below)
[2024-10-09 20:39:18,192][train][INFO] - {"epoch": 1372, "train_loss": "0.467", "train_ntokens": "260925", "train_nsentences": "1750.04", "train_wps": "69576.1", "train_ups": "0.27", "train_wpb": "260925", "train_bsz": "1750", "train_num_updates": "65827", "train_lr": "0.000454039", "train_gnorm": "0.366", "train_loss_scale": "4", "train_train_wall": "86", "train_gb_free": "40.3", "train_wall": "11383"}
[2024-10-09 20:39:18,398][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 20:39:18,404][fairseq.trainer][INFO] - begin training epoch 1373
[2024-10-09 20:39:18,405][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 20:42:16,695][fairseq_cli.train][INFO] - end of epoch 1373 (average epoch stats below)
[2024-10-09 20:42:16,702][train][INFO] - {"epoch": 1373, "train_loss": "0.476", "train_ntokens": "260965", "train_nsentences": "1750.04", "train_wps": "70174.4", "train_ups": "0.27", "train_wpb": "260965", "train_bsz": "1750", "train_num_updates": "65875", "train_lr": "0.000453974", "train_gnorm": "0.381", "train_loss_scale": "4", "train_train_wall": "84", "train_gb_free": "39.8", "train_wall": "11561"}
[2024-10-09 20:42:16,901][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 20:42:16,912][fairseq.trainer][INFO] - begin training epoch 1374
[2024-10-09 20:42:16,913][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 20:45:13,299][fairseq_cli.train][INFO] - end of epoch 1374 (average epoch stats below)
[2024-10-09 20:45:13,303][train][INFO] - {"epoch": 1374, "train_loss": "0.471", "train_ntokens": "261123", "train_nsentences": "1750.04", "train_wps": "70974.8", "train_ups": "0.27", "train_wpb": "261123", "train_bsz": "1750", "train_num_updates": "65923", "train_lr": "0.000453909", "train_gnorm": "0.369", "train_loss_scale": "4", "train_train_wall": "78", "train_gb_free": "40.2", "train_wall": "11738"}
[2024-10-09 20:45:13,550][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 20:45:13,569][fairseq.trainer][INFO] - begin training epoch 1375
[2024-10-09 20:45:13,570][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 20:48:12,085][fairseq_cli.train][INFO] - end of epoch 1375 (average epoch stats below)
[2024-10-09 20:48:12,090][train][INFO] - {"epoch": 1375, "train_loss": "0.471", "train_ntokens": "260526", "train_nsentences": "1750.04", "train_wps": "69945.8", "train_ups": "0.27", "train_wpb": "260526", "train_bsz": "1750", "train_num_updates": "65971", "train_lr": "0.000453844", "train_gnorm": "0.368", "train_loss_scale": "4", "train_train_wall": "92", "train_gb_free": "39.7", "train_wall": "11917"}
[2024-10-09 20:48:12,261][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 20:48:12,265][fairseq.trainer][INFO] - begin training epoch 1376
[2024-10-09 20:48:12,266][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 20:51:09,170][train_inner][INFO] - {"epoch": 1376, "update": 1375.604, "loss": "0.473", "ntokens": "260828", "nsentences": "1751.61", "wps": "71866.6", "ups": "0.28", "wpb": "260828", "bsz": "1751.6", "num_updates": "66000", "lr": "0.000453804", "gnorm": "0.372", "loss_scale": "4", "train_wall": "322", "gb_free": "40.6", "wall": "12094"}
[2024-10-09 20:51:22,086][fairseq_cli.train][INFO] - end of epoch 1376 (average epoch stats below)
[2024-10-09 20:51:22,089][train][INFO] - {"epoch": 1376, "train_loss": "0.475", "train_ntokens": "260716", "train_nsentences": "1750.04", "train_wps": "65866.6", "train_ups": "0.25", "train_wpb": "260716", "train_bsz": "1750", "train_num_updates": "66019", "train_lr": "0.000453779", "train_gnorm": "0.381", "train_loss_scale": "4", "train_train_wall": "67", "train_gb_free": "40.1", "train_wall": "12107"}
[2024-10-09 20:51:22,303][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 20:51:22,317][fairseq.trainer][INFO] - begin training epoch 1377
[2024-10-09 20:51:22,318][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 20:54:44,083][fairseq_cli.train][INFO] - end of epoch 1377 (average epoch stats below)
[2024-10-09 20:54:44,094][train][INFO] - {"epoch": 1377, "train_loss": "0.47", "train_ntokens": "261133", "train_nsentences": "1750.04", "train_wps": "62051", "train_ups": "0.24", "train_wpb": "261133", "train_bsz": "1750", "train_num_updates": "66067", "train_lr": "0.000453713", "train_gnorm": "0.371", "train_loss_scale": "4", "train_train_wall": "122", "train_gb_free": "40.5", "train_wall": "12309"}
[2024-10-09 20:54:44,312][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 20:54:44,325][fairseq.trainer][INFO] - begin training epoch 1378
[2024-10-09 20:54:44,326][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 20:58:00,329][fairseq_cli.train][INFO] - end of epoch 1378 (average epoch stats below)
[2024-10-09 20:58:00,333][train][INFO] - {"epoch": 1378, "train_loss": "0.473", "train_ntokens": "260723", "train_nsentences": "1750.04", "train_wps": "63773.6", "train_ups": "0.24", "train_wpb": "260723", "train_bsz": "1750", "train_num_updates": "66115", "train_lr": "0.000453648", "train_gnorm": "0.358", "train_loss_scale": "4", "train_train_wall": "54", "train_gb_free": "39.6", "train_wall": "12505"}
[2024-10-09 20:58:00,506][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 20:58:00,513][fairseq.trainer][INFO] - begin training epoch 1379
[2024-10-09 20:58:00,514][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 21:00:09,651][fairseq_cli.train][INFO] - end of epoch 1379 (average epoch stats below)
[2024-10-09 21:00:09,659][train][INFO] - {"epoch": 1379, "train_loss": "0.471", "train_ntokens": "260700", "train_nsentences": "1750.04", "train_wps": "96766.2", "train_ups": "0.37", "train_wpb": "260700", "train_bsz": "1750", "train_num_updates": "66163", "train_lr": "0.000453583", "train_gnorm": "0.373", "train_loss_scale": "4", "train_train_wall": "49", "train_gb_free": "40.5", "train_wall": "12634"}
[2024-10-09 21:00:09,727][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 21:00:09,730][fairseq.trainer][INFO] - begin training epoch 1380
[2024-10-09 21:00:09,731][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 21:02:08,409][train_inner][INFO] - {"epoch": 1380, "update": 1379.771, "loss": "0.47", "ntokens": "260802", "nsentences": "1735.39", "wps": "79122.6", "ups": "0.3", "wpb": "260802", "bsz": "1735.4", "num_updates": "66200", "lr": "0.000453533", "gnorm": "0.376", "loss_scale": "4", "train_wall": "282", "gb_free": "40.1", "wall": "12753"}
[2024-10-09 21:02:20,066][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 1380 @ 66211 updates
[2024-10-09 21:02:20,069][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-09 21:02:23,921][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-09 21:02:23,925][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 1380 @ 66211 updates, score None) (writing took 3.8590724049136043 seconds)
[2024-10-09 21:02:23,926][fairseq_cli.train][INFO] - end of epoch 1380 (average epoch stats below)
[2024-10-09 21:02:23,928][train][INFO] - {"epoch": 1380, "train_loss": "0.466", "train_ntokens": "260425", "train_nsentences": "1750.04", "train_wps": "93107.2", "train_ups": "0.36", "train_wpb": "260425", "train_bsz": "1750", "train_num_updates": "66211", "train_lr": "0.000453518", "train_gnorm": "0.4", "train_loss_scale": "4", "train_train_wall": "55", "train_gb_free": "42.2", "train_wall": "12768"}
[2024-10-09 21:02:23,979][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 21:02:24,000][fairseq.trainer][INFO] - begin training epoch 1381
[2024-10-09 21:02:24,001][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 21:04:28,297][fairseq_cli.train][INFO] - end of epoch 1381 (average epoch stats below)
[2024-10-09 21:04:28,302][train][INFO] - {"epoch": 1381, "train_loss": "0.469", "train_ntokens": "260632", "train_nsentences": "1750.04", "train_wps": "100588", "train_ups": "0.39", "train_wpb": "260632", "train_bsz": "1750", "train_num_updates": "66259", "train_lr": "0.000453452", "train_gnorm": "0.344", "train_loss_scale": "4", "train_train_wall": "29", "train_gb_free": "39.6", "train_wall": "12893"}
[2024-10-09 21:04:28,411][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 21:04:28,429][fairseq.trainer][INFO] - begin training epoch 1382
[2024-10-09 21:04:28,429][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 21:07:20,364][fairseq_cli.train][INFO] - end of epoch 1382 (average epoch stats below)
[2024-10-09 21:07:20,373][train][INFO] - {"epoch": 1382, "train_loss": "0.474", "train_ntokens": "260805", "train_nsentences": "1750.04", "train_wps": "72758", "train_ups": "0.28", "train_wpb": "260805", "train_bsz": "1750", "train_num_updates": "66307", "train_lr": "0.000453387", "train_gnorm": "0.382", "train_loss_scale": "4", "train_train_wall": "77", "train_gb_free": "40.1", "train_wall": "13065"}
[2024-10-09 21:07:20,530][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 21:07:20,556][fairseq.trainer][INFO] - begin training epoch 1383
[2024-10-09 21:07:20,556][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 21:09:47,988][fairseq_cli.train][INFO] - end of epoch 1383 (average epoch stats below)
[2024-10-09 21:09:48,003][train][INFO] - {"epoch": 1383, "train_loss": "0.468", "train_ntokens": "260681", "train_nsentences": "1750.04", "train_wps": "84759.8", "train_ups": "0.33", "train_wpb": "260681", "train_bsz": "1750", "train_num_updates": "66355", "train_lr": "0.000453322", "train_gnorm": "0.376", "train_loss_scale": "4", "train_train_wall": "52", "train_gb_free": "39.8", "train_wall": "13212"}
[2024-10-09 21:09:48,184][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 21:09:48,191][fairseq.trainer][INFO] - begin training epoch 1384
[2024-10-09 21:09:48,192][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 21:12:09,584][train_inner][INFO] - {"epoch": 1384, "update": 1383.938, "loss": "0.47", "ntokens": "260641", "nsentences": "1753.35", "wps": "86711.4", "ups": "0.33", "wpb": "260641", "bsz": "1753.4", "num_updates": "66400", "lr": "0.000453261", "gnorm": "0.367", "loss_scale": "4", "train_wall": "232", "gb_free": "40", "wall": "13354"}
[2024-10-09 21:12:10,348][fairseq_cli.train][INFO] - end of epoch 1384 (average epoch stats below)
[2024-10-09 21:12:10,350][train][INFO] - {"epoch": 1384, "train_loss": "0.469", "train_ntokens": "260668", "train_nsentences": "1750.04", "train_wps": "87901.2", "train_ups": "0.34", "train_wpb": "260668", "train_bsz": "1750", "train_num_updates": "66403", "train_lr": "0.000453257", "train_gnorm": "0.366", "train_loss_scale": "4", "train_train_wall": "63", "train_gb_free": "39.8", "train_wall": "13355"}
[2024-10-09 21:12:10,513][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 21:12:10,517][fairseq.trainer][INFO] - begin training epoch 1385
[2024-10-09 21:12:10,517][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 21:14:28,831][fairseq_cli.train][INFO] - end of epoch 1385 (average epoch stats below)
[2024-10-09 21:14:28,838][train][INFO] - {"epoch": 1385, "train_loss": "0.469", "train_ntokens": "260738", "train_nsentences": "1750.04", "train_wps": "90373.8", "train_ups": "0.35", "train_wpb": "260738", "train_bsz": "1750", "train_num_updates": "66451", "train_lr": "0.000453192", "train_gnorm": "0.374", "train_loss_scale": "4", "train_train_wall": "65", "train_gb_free": "40.3", "train_wall": "13493"}
[2024-10-09 21:14:28,992][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 21:14:29,021][fairseq.trainer][INFO] - begin training epoch 1386
[2024-10-09 21:14:29,022][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 21:16:38,227][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
[2024-10-09 21:16:42,811][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2024-10-09 21:16:51,365][fairseq_cli.train][INFO] - end of epoch 1386 (average epoch stats below)
[2024-10-09 21:16:51,368][train][INFO] - {"epoch": 1386, "train_loss": "0.464", "train_ntokens": "260686", "train_nsentences": "1744.43", "train_wps": "84135.6", "train_ups": "0.32", "train_wpb": "260686", "train_bsz": "1744.4", "train_num_updates": "66497", "train_lr": "0.000453129", "train_gnorm": "0.382", "train_loss_scale": "2", "train_train_wall": "54", "train_gb_free": "39.6", "train_wall": "13636"}
[2024-10-09 21:16:51,519][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 21:16:51,527][fairseq.trainer][INFO] - begin training epoch 1387
[2024-10-09 21:16:51,527][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 21:19:10,158][fairseq_cli.train][INFO] - end of epoch 1387 (average epoch stats below)
[2024-10-09 21:19:10,166][train][INFO] - {"epoch": 1387, "train_loss": "0.47", "train_ntokens": "260722", "train_nsentences": "1750.04", "train_wps": "90166", "train_ups": "0.35", "train_wpb": "260722", "train_bsz": "1750", "train_num_updates": "66545", "train_lr": "0.000453064", "train_gnorm": "0.357", "train_loss_scale": "2", "train_train_wall": "54", "train_gb_free": "40.1", "train_wall": "13775"}
[2024-10-09 21:19:10,357][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 21:19:10,367][fairseq.trainer][INFO] - begin training epoch 1388
[2024-10-09 21:19:10,368][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 21:21:42,184][fairseq_cli.train][INFO] - end of epoch 1388 (average epoch stats below)
[2024-10-09 21:21:42,189][train][INFO] - {"epoch": 1388, "train_loss": "0.477", "train_ntokens": "260524", "train_nsentences": "1750.04", "train_wps": "82260.7", "train_ups": "0.32", "train_wpb": "260524", "train_bsz": "1750", "train_num_updates": "66593", "train_lr": "0.000452999", "train_gnorm": "0.384", "train_loss_scale": "2", "train_train_wall": "45", "train_gb_free": "39.6", "train_wall": "13927"}
[2024-10-09 21:21:42,350][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 21:21:42,377][fairseq.trainer][INFO] - begin training epoch 1389
[2024-10-09 21:21:42,378][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 21:23:39,129][train_inner][INFO] - {"epoch": 1389, "update": 1388.146, "loss": "0.469", "ntokens": "260783", "nsentences": "1743.8", "wps": "75639.7", "ups": "0.29", "wpb": "260783", "bsz": "1743.8", "num_updates": "66600", "lr": "0.000452989", "gnorm": "0.374", "loss_scale": "2", "train_wall": "249", "gb_free": "39.7", "wall": "14044"}
[2024-10-09 21:24:09,178][fairseq_cli.train][INFO] - end of epoch 1389 (average epoch stats below)
[2024-10-09 21:24:09,180][train][INFO] - {"epoch": 1389, "train_loss": "0.466", "train_ntokens": "260492", "train_nsentences": "1750.04", "train_wps": "85065.1", "train_ups": "0.33", "train_wpb": "260492", "train_bsz": "1750", "train_num_updates": "66641", "train_lr": "0.000452933", "train_gnorm": "0.414", "train_loss_scale": "2", "train_train_wall": "59", "train_gb_free": "39.2", "train_wall": "14074"}
[2024-10-09 21:24:09,319][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 21:24:09,323][fairseq.trainer][INFO] - begin training epoch 1390
[2024-10-09 21:24:09,323][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 21:26:25,291][fairseq_cli.train][INFO] - end of epoch 1390 (average epoch stats below)
[2024-10-09 21:26:25,294][train][INFO] - {"epoch": 1390, "train_loss": "0.466", "train_ntokens": "260783", "train_nsentences": "1750.04", "train_wps": "91966.1", "train_ups": "0.35", "train_wpb": "260783", "train_bsz": "1750", "train_num_updates": "66689", "train_lr": "0.000452868", "train_gnorm": "0.361", "train_loss_scale": "2", "train_train_wall": "50", "train_gb_free": "40.1", "train_wall": "14210"}
[2024-10-09 21:26:25,417][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 21:26:25,448][fairseq.trainer][INFO] - begin training epoch 1391
[2024-10-09 21:26:25,448][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 21:28:46,454][fairseq_cli.train][INFO] - end of epoch 1391 (average epoch stats below)
[2024-10-09 21:28:46,460][train][INFO] - {"epoch": 1391, "train_loss": "0.458", "train_ntokens": "260746", "train_nsentences": "1750.04", "train_wps": "88667.3", "train_ups": "0.34", "train_wpb": "260746", "train_bsz": "1750", "train_num_updates": "66737", "train_lr": "0.000452803", "train_gnorm": "0.355", "train_loss_scale": "2", "train_train_wall": "63", "train_gb_free": "39.6", "train_wall": "14351"}
[2024-10-09 21:28:46,609][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 21:28:46,660][fairseq.trainer][INFO] - begin training epoch 1392
[2024-10-09 21:28:46,660][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 21:31:00,608][fairseq_cli.train][INFO] - end of epoch 1392 (average epoch stats below)
[2024-10-09 21:31:00,615][train][INFO] - {"epoch": 1392, "train_loss": "0.46", "train_ntokens": "260790", "train_nsentences": "1750.04", "train_wps": "93311.8", "train_ups": "0.36", "train_wpb": "260790", "train_bsz": "1750", "train_num_updates": "66785", "train_lr": "0.000452738", "train_gnorm": "0.382", "train_loss_scale": "2", "train_train_wall": "51", "train_gb_free": "42.2", "train_wall": "14485"}
[2024-10-09 21:31:00,838][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 21:31:00,842][fairseq.trainer][INFO] - begin training epoch 1393
[2024-10-09 21:31:00,842][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 21:33:02,559][train_inner][INFO] - {"epoch": 1393, "update": 1392.312, "loss": "0.463", "ntokens": "260432", "nsentences": "1772.79", "wps": "92449.9", "ups": "0.35", "wpb": "260432", "bsz": "1772.8", "num_updates": "66800", "lr": "0.000452717", "gnorm": "0.376", "loss_scale": "2", "train_wall": "231", "gb_free": "39.4", "wall": "14607"}
[2024-10-09 21:33:21,223][fairseq_cli.train][INFO] - end of epoch 1393 (average epoch stats below)
[2024-10-09 21:33:21,226][train][INFO] - {"epoch": 1393, "train_loss": "0.473", "train_ntokens": "260560", "train_nsentences": "1750.04", "train_wps": "88950", "train_ups": "0.34", "train_wpb": "260560", "train_bsz": "1750", "train_num_updates": "66833", "train_lr": "0.000452673", "train_gnorm": "0.368", "train_loss_scale": "2", "train_train_wall": "56", "train_gb_free": "39.3", "train_wall": "14626"}
[2024-10-09 21:33:21,359][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 21:33:21,369][fairseq.trainer][INFO] - begin training epoch 1394
[2024-10-09 21:33:21,370][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 21:36:05,121][fairseq_cli.train][INFO] - end of epoch 1394 (average epoch stats below)
[2024-10-09 21:36:05,123][train][INFO] - {"epoch": 1394, "train_loss": "0.461", "train_ntokens": "260376", "train_nsentences": "1750.04", "train_wps": "76256.8", "train_ups": "0.29", "train_wpb": "260376", "train_bsz": "1750", "train_num_updates": "66881", "train_lr": "0.000452607", "train_gnorm": "0.399", "train_loss_scale": "2", "train_train_wall": "56", "train_gb_free": "39.3", "train_wall": "14790"}
[2024-10-09 21:36:05,235][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 21:36:05,244][fairseq.trainer][INFO] - begin training epoch 1395
[2024-10-09 21:36:05,245][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 21:38:37,857][fairseq_cli.train][INFO] - end of epoch 1395 (average epoch stats below)
[2024-10-09 21:38:37,872][train][INFO] - {"epoch": 1395, "train_loss": "0.464", "train_ntokens": "260993", "train_nsentences": "1750.04", "train_wps": "82017.3", "train_ups": "0.31", "train_wpb": "260993", "train_bsz": "1750", "train_num_updates": "66929", "train_lr": "0.000452542", "train_gnorm": "0.377", "train_loss_scale": "2", "train_train_wall": "74", "train_gb_free": "39.3", "train_wall": "14942"}
[2024-10-09 21:38:38,077][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 21:38:38,083][fairseq.trainer][INFO] - begin training epoch 1396
[2024-10-09 21:38:38,083][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 21:40:58,842][fairseq_cli.train][INFO] - end of epoch 1396 (average epoch stats below)
[2024-10-09 21:40:58,851][train][INFO] - {"epoch": 1396, "train_loss": "0.471", "train_ntokens": "260761", "train_nsentences": "1750.04", "train_wps": "88786", "train_ups": "0.34", "train_wpb": "260761", "train_bsz": "1750", "train_num_updates": "66977", "train_lr": "0.000452477", "train_gnorm": "0.366", "train_loss_scale": "2", "train_train_wall": "55", "train_gb_free": "39.4", "train_wall": "15083"}
[2024-10-09 21:40:58,976][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 21:40:58,994][fairseq.trainer][INFO] - begin training epoch 1397
[2024-10-09 21:40:58,994][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 21:42:43,836][train_inner][INFO] - {"epoch": 1397, "update": 1396.479, "loss": "0.467", "ntokens": "260819", "nsentences": "1736.81", "wps": "89740.9", "ups": "0.34", "wpb": "260819", "bsz": "1736.8", "num_updates": "67000", "lr": "0.000452446", "gnorm": "0.38", "loss_scale": "2", "train_wall": "244", "gb_free": "39.3", "wall": "15188"}
[2024-10-09 21:43:05,132][fairseq_cli.train][INFO] - end of epoch 1397 (average epoch stats below)
[2024-10-09 21:43:05,136][train][INFO] - {"epoch": 1397, "train_loss": "0.468", "train_ntokens": "260758", "train_nsentences": "1750.04", "train_wps": "99116.8", "train_ups": "0.38", "train_wpb": "260758", "train_bsz": "1750", "train_num_updates": "67025", "train_lr": "0.000452412", "train_gnorm": "0.376", "train_loss_scale": "2", "train_train_wall": "62", "train_gb_free": "39.3", "train_wall": "15210"}
[2024-10-09 21:43:05,209][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 21:43:05,217][fairseq.trainer][INFO] - begin training epoch 1398
[2024-10-09 21:43:05,218][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 21:45:12,192][fairseq_cli.train][INFO] - end of epoch 1398 (average epoch stats below)
[2024-10-09 21:45:12,198][train][INFO] - {"epoch": 1398, "train_loss": "0.475", "train_ntokens": "260810", "train_nsentences": "1750.04", "train_wps": "98529.6", "train_ups": "0.38", "train_wpb": "260810", "train_bsz": "1750", "train_num_updates": "67073", "train_lr": "0.000452346", "train_gnorm": "0.371", "train_loss_scale": "2", "train_train_wall": "49", "train_gb_free": "42", "train_wall": "15337"}
[2024-10-09 21:45:12,290][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 21:45:12,308][fairseq.trainer][INFO] - begin training epoch 1399
[2024-10-09 21:45:12,309][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 21:47:17,654][fairseq_cli.train][INFO] - end of epoch 1399 (average epoch stats below)
[2024-10-09 21:47:17,657][train][INFO] - {"epoch": 1399, "train_loss": "0.47", "train_ntokens": "260991", "train_nsentences": "1750.04", "train_wps": "99855.8", "train_ups": "0.38", "train_wpb": "260991", "train_bsz": "1750", "train_num_updates": "67121", "train_lr": "0.000452281", "train_gnorm": "0.363", "train_loss_scale": "2", "train_train_wall": "63", "train_gb_free": "40.1", "train_wall": "15462"}
[2024-10-09 21:47:17,750][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 21:47:17,759][fairseq.trainer][INFO] - begin training epoch 1400
[2024-10-09 21:47:17,759][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 21:49:26,914][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 1400 @ 67169 updates
[2024-10-09 21:49:26,915][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-09 21:49:30,470][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-09 21:49:30,486][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 1400 @ 67169 updates, score None) (writing took 3.57281278911978 seconds)
[2024-10-09 21:49:30,487][fairseq_cli.train][INFO] - end of epoch 1400 (average epoch stats below)
[2024-10-09 21:49:30,489][train][INFO] - {"epoch": 1400, "train_loss": "0.46", "train_ntokens": "260544", "train_nsentences": "1750.04", "train_wps": "94152.7", "train_ups": "0.36", "train_wpb": "260544", "train_bsz": "1750", "train_num_updates": "67169", "train_lr": "0.000452216", "train_gnorm": "0.373", "train_loss_scale": "2", "train_train_wall": "62", "train_gb_free": "39.8", "train_wall": "15595"}
[2024-10-09 21:49:30,576][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 21:49:30,581][fairseq.trainer][INFO] - begin training epoch 1401
[2024-10-09 21:49:30,581][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 21:51:17,129][train_inner][INFO] - {"epoch": 1401, "update": 1400.646, "loss": "0.469", "ntokens": "260736", "nsentences": "1746.35", "wps": "101594", "ups": "0.39", "wpb": "260736", "bsz": "1746.3", "num_updates": "67200", "lr": "0.000452174", "gnorm": "0.375", "loss_scale": "2", "train_wall": "222", "gb_free": "40.7", "wall": "15702"}
[2024-10-09 21:51:33,218][fairseq_cli.train][INFO] - end of epoch 1401 (average epoch stats below)
[2024-10-09 21:51:33,225][train][INFO] - {"epoch": 1401, "train_loss": "0.469", "train_ntokens": "260613", "train_nsentences": "1750.04", "train_wps": "101928", "train_ups": "0.39", "train_wpb": "260613", "train_bsz": "1750", "train_num_updates": "67217", "train_lr": "0.000452151", "train_gnorm": "0.392", "train_loss_scale": "2", "train_train_wall": "43", "train_gb_free": "39.3", "train_wall": "15718"}
[2024-10-09 21:51:33,379][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 21:51:33,391][fairseq.trainer][INFO] - begin training epoch 1402
[2024-10-09 21:51:33,392][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 21:53:42,663][fairseq_cli.train][INFO] - end of epoch 1402 (average epoch stats below)
[2024-10-09 21:53:42,668][train][INFO] - {"epoch": 1402, "train_loss": "0.459", "train_ntokens": "260709", "train_nsentences": "1750.04", "train_wps": "96679.7", "train_ups": "0.37", "train_wpb": "260709", "train_bsz": "1750", "train_num_updates": "67265", "train_lr": "0.000452086", "train_gnorm": "0.346", "train_loss_scale": "2", "train_train_wall": "59", "train_gb_free": "39.3", "train_wall": "15847"}
[2024-10-09 21:53:42,731][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 21:53:42,735][fairseq.trainer][INFO] - begin training epoch 1403
[2024-10-09 21:53:42,735][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 21:55:59,407][fairseq_cli.train][INFO] - end of epoch 1403 (average epoch stats below)
[2024-10-09 21:55:59,422][train][INFO] - {"epoch": 1403, "train_loss": "0.459", "train_ntokens": "260752", "train_nsentences": "1750.04", "train_wps": "91532.5", "train_ups": "0.35", "train_wpb": "260752", "train_bsz": "1750", "train_num_updates": "67313", "train_lr": "0.00045202", "train_gnorm": "0.375", "train_loss_scale": "2", "train_train_wall": "64", "train_gb_free": "39.3", "train_wall": "15984"}
[2024-10-09 21:55:59,538][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 21:55:59,542][fairseq.trainer][INFO] - begin training epoch 1404
[2024-10-09 21:55:59,542][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 21:58:14,285][fairseq_cli.train][INFO] - end of epoch 1404 (average epoch stats below)
[2024-10-09 21:58:14,297][train][INFO] - {"epoch": 1404, "train_loss": "0.46", "train_ntokens": "260538", "train_nsentences": "1750.04", "train_wps": "92724.7", "train_ups": "0.36", "train_wpb": "260538", "train_bsz": "1750", "train_num_updates": "67361", "train_lr": "0.000451955", "train_gnorm": "0.378", "train_loss_scale": "2", "train_train_wall": "44", "train_gb_free": "40", "train_wall": "16119"}
[2024-10-09 21:58:14,441][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 21:58:14,449][fairseq.trainer][INFO] - begin training epoch 1405
[2024-10-09 21:58:14,450][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 22:00:28,432][train_inner][INFO] - {"epoch": 1405, "update": 1404.812, "loss": "0.461", "ntokens": "260774", "nsentences": "1748.17", "wps": "94604.7", "ups": "0.36", "wpb": "260774", "bsz": "1748.2", "num_updates": "67400", "lr": "0.000451902", "gnorm": "0.366", "loss_scale": "2", "train_wall": "244", "gb_free": "40", "wall": "16253"}
[2024-10-09 22:00:34,397][fairseq_cli.train][INFO] - end of epoch 1405 (average epoch stats below)
[2024-10-09 22:00:34,399][train][INFO] - {"epoch": 1405, "train_loss": "0.47", "train_ntokens": "260810", "train_nsentences": "1750.04", "train_wps": "89357.2", "train_ups": "0.34", "train_wpb": "260810", "train_bsz": "1750", "train_num_updates": "67409", "train_lr": "0.00045189", "train_gnorm": "0.377", "train_loss_scale": "2", "train_train_wall": "66", "train_gb_free": "40.2", "train_wall": "16259"}
[2024-10-09 22:00:34,570][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 22:00:34,578][fairseq.trainer][INFO] - begin training epoch 1406
[2024-10-09 22:00:34,579][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 22:02:48,642][fairseq_cli.train][INFO] - end of epoch 1406 (average epoch stats below)
[2024-10-09 22:02:48,660][train][INFO] - {"epoch": 1406, "train_loss": "0.475", "train_ntokens": "260474", "train_nsentences": "1750.04", "train_wps": "93125.2", "train_ups": "0.36", "train_wpb": "260474", "train_bsz": "1750", "train_num_updates": "67457", "train_lr": "0.000451825", "train_gnorm": "0.373", "train_loss_scale": "2", "train_train_wall": "37", "train_gb_free": "39.8", "train_wall": "16393"}
[2024-10-09 22:02:48,758][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 22:02:48,781][fairseq.trainer][INFO] - begin training epoch 1407
[2024-10-09 22:02:48,781][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 22:04:57,852][fairseq_cli.train][INFO] - end of epoch 1407 (average epoch stats below)
[2024-10-09 22:04:57,857][train][INFO] - {"epoch": 1407, "train_loss": "0.465", "train_ntokens": "260998", "train_nsentences": "1750.04", "train_wps": "96970", "train_ups": "0.37", "train_wpb": "260998", "train_bsz": "1750", "train_num_updates": "67505", "train_lr": "0.00045176", "train_gnorm": "0.353", "train_loss_scale": "2", "train_train_wall": "49", "train_gb_free": "39.7", "train_wall": "16522"}
[2024-10-09 22:04:57,987][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 22:04:57,991][fairseq.trainer][INFO] - begin training epoch 1408
[2024-10-09 22:04:57,991][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 22:07:02,626][fairseq_cli.train][INFO] - end of epoch 1408 (average epoch stats below)
[2024-10-09 22:07:02,631][train][INFO] - {"epoch": 1408, "train_loss": "0.46", "train_ntokens": "260456", "train_nsentences": "1750.04", "train_wps": "100201", "train_ups": "0.38", "train_wpb": "260456", "train_bsz": "1750", "train_num_updates": "67553", "train_lr": "0.000451694", "train_gnorm": "0.372", "train_loss_scale": "2", "train_train_wall": "37", "train_gb_free": "40.6", "train_wall": "16647"}
[2024-10-09 22:07:02,746][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 22:07:02,765][fairseq.trainer][INFO] - begin training epoch 1409
[2024-10-09 22:07:02,765][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 22:09:10,227][train_inner][INFO] - {"epoch": 1409, "update": 1408.979, "loss": "0.469", "ntokens": "260559", "nsentences": "1753.34", "wps": "99876.5", "ups": "0.38", "wpb": "260559", "bsz": "1753.3", "num_updates": "67600", "lr": "0.00045163", "gnorm": "0.363", "loss_scale": "2", "train_wall": "189", "gb_free": "39.2", "wall": "16775"}
[2024-10-09 22:09:10,477][fairseq_cli.train][INFO] - end of epoch 1409 (average epoch stats below)
[2024-10-09 22:09:10,483][train][INFO] - {"epoch": 1409, "train_loss": "0.472", "train_ntokens": "260426", "train_nsentences": "1750.04", "train_wps": "97777.1", "train_ups": "0.38", "train_wpb": "260426", "train_bsz": "1750", "train_num_updates": "67601", "train_lr": "0.000451629", "train_gnorm": "0.352", "train_loss_scale": "2", "train_train_wall": "61", "train_gb_free": "39.7", "train_wall": "16775"}
[2024-10-09 22:09:10,587][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 22:09:10,596][fairseq.trainer][INFO] - begin training epoch 1410
[2024-10-09 22:09:10,597][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 22:11:23,600][fairseq_cli.train][INFO] - end of epoch 1410 (average epoch stats below)
[2024-10-09 22:11:23,606][train][INFO] - {"epoch": 1410, "train_loss": "0.47", "train_ntokens": "260477", "train_nsentences": "1750.04", "train_wps": "93922.1", "train_ups": "0.36", "train_wpb": "260476", "train_bsz": "1750", "train_num_updates": "67649", "train_lr": "0.000451564", "train_gnorm": "0.37", "train_loss_scale": "2", "train_train_wall": "56", "train_gb_free": "40.2", "train_wall": "16908"}
[2024-10-09 22:11:23,719][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 22:11:23,723][fairseq.trainer][INFO] - begin training epoch 1411
[2024-10-09 22:11:23,724][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 22:13:41,356][fairseq_cli.train][INFO] - end of epoch 1411 (average epoch stats below)
[2024-10-09 22:13:41,373][train][INFO] - {"epoch": 1411, "train_loss": "0.466", "train_ntokens": "261004", "train_nsentences": "1750.04", "train_wps": "90940.3", "train_ups": "0.35", "train_wpb": "261004", "train_bsz": "1750", "train_num_updates": "67697", "train_lr": "0.000451499", "train_gnorm": "0.385", "train_loss_scale": "2", "train_train_wall": "64", "train_gb_free": "39.2", "train_wall": "17046"}
[2024-10-09 22:13:41,508][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 22:13:41,513][fairseq.trainer][INFO] - begin training epoch 1412
[2024-10-09 22:13:41,514][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 22:15:52,494][fairseq_cli.train][INFO] - end of epoch 1412 (average epoch stats below)
[2024-10-09 22:15:52,524][train][INFO] - {"epoch": 1412, "train_loss": "0.47", "train_ntokens": "260594", "train_nsentences": "1750.04", "train_wps": "95377.3", "train_ups": "0.37", "train_wpb": "260594", "train_bsz": "1750", "train_num_updates": "67745", "train_lr": "0.000451433", "train_gnorm": "0.379", "train_loss_scale": "2", "train_train_wall": "45", "train_gb_free": "40.5", "train_wall": "17177"}
[2024-10-09 22:15:52,753][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 22:15:52,781][fairseq.trainer][INFO] - begin training epoch 1413
[2024-10-09 22:15:52,782][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 22:18:04,158][fairseq_cli.train][INFO] - end of epoch 1413 (average epoch stats below)
[2024-10-09 22:18:04,173][train][INFO] - {"epoch": 1413, "train_loss": "0.464", "train_ntokens": "260938", "train_nsentences": "1750.04", "train_wps": "95142.3", "train_ups": "0.36", "train_wpb": "260938", "train_bsz": "1750", "train_num_updates": "67793", "train_lr": "0.000451368", "train_gnorm": "0.396", "train_loss_scale": "2", "train_train_wall": "47", "train_gb_free": "39.7", "train_wall": "17309"}
[2024-10-09 22:18:04,362][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 22:18:04,368][fairseq.trainer][INFO] - begin training epoch 1414
[2024-10-09 22:18:04,368][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 22:19:47,947][train_inner][INFO] - {"epoch": 1414, "update": 1413.146, "loss": "0.467", "ntokens": "260878", "nsentences": "1744.58", "wps": "81816.8", "ups": "0.31", "wpb": "260878", "bsz": "1744.6", "num_updates": "67800", "lr": "0.000451359", "gnorm": "0.383", "loss_scale": "2", "train_wall": "241", "gb_free": "39.1", "wall": "17412"}
[2024-10-09 22:20:17,414][fairseq_cli.train][INFO] - end of epoch 1414 (average epoch stats below)
[2024-10-09 22:20:17,416][train][INFO] - {"epoch": 1414, "train_loss": "0.464", "train_ntokens": "260910", "train_nsentences": "1750.04", "train_wps": "93993.8", "train_ups": "0.36", "train_wpb": "260910", "train_bsz": "1750", "train_num_updates": "67841", "train_lr": "0.000451303", "train_gnorm": "0.356", "train_loss_scale": "2", "train_train_wall": "58", "train_gb_free": "39.3", "train_wall": "17442"}
[2024-10-09 22:20:17,536][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 22:20:17,541][fairseq.trainer][INFO] - begin training epoch 1415
[2024-10-09 22:20:17,547][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 22:22:27,669][fairseq_cli.train][INFO] - end of epoch 1415 (average epoch stats below)
[2024-10-09 22:22:27,677][train][INFO] - {"epoch": 1415, "train_loss": "0.461", "train_ntokens": "260738", "train_nsentences": "1750.04", "train_wps": "96085.2", "train_ups": "0.37", "train_wpb": "260738", "train_bsz": "1750", "train_num_updates": "67889", "train_lr": "0.000451238", "train_gnorm": "0.368", "train_loss_scale": "2", "train_train_wall": "31", "train_gb_free": "39.7", "train_wall": "17572"}
[2024-10-09 22:22:27,833][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 22:22:27,837][fairseq.trainer][INFO] - begin training epoch 1416
[2024-10-09 22:22:27,838][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 22:24:39,114][fairseq_cli.train][INFO] - end of epoch 1416 (average epoch stats below)
[2024-10-09 22:24:39,129][train][INFO] - {"epoch": 1416, "train_loss": "0.452", "train_ntokens": "260594", "train_nsentences": "1750.04", "train_wps": "95158.5", "train_ups": "0.37", "train_wpb": "260594", "train_bsz": "1750", "train_num_updates": "67937", "train_lr": "0.000451173", "train_gnorm": "0.378", "train_loss_scale": "2", "train_train_wall": "63", "train_gb_free": "40.1", "train_wall": "17704"}
[2024-10-09 22:24:39,281][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 22:24:39,294][fairseq.trainer][INFO] - begin training epoch 1417
[2024-10-09 22:24:39,295][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 22:26:55,251][fairseq_cli.train][INFO] - end of epoch 1417 (average epoch stats below)
[2024-10-09 22:26:55,255][train][INFO] - {"epoch": 1417, "train_loss": "0.466", "train_ntokens": "260487", "train_nsentences": "1750.04", "train_wps": "91854.2", "train_ups": "0.35", "train_wpb": "260487", "train_bsz": "1750", "train_num_updates": "67985", "train_lr": "0.000451107", "train_gnorm": "0.381", "train_loss_scale": "2", "train_train_wall": "55", "train_gb_free": "40.1", "train_wall": "17840"}
[2024-10-09 22:26:55,321][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 22:26:55,327][fairseq.trainer][INFO] - begin training epoch 1418
[2024-10-09 22:26:55,327][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 22:28:28,593][train_inner][INFO] - {"epoch": 1418, "update": 1417.312, "loss": "0.46", "ntokens": "260527", "nsentences": "1758.2", "wps": "100080", "ups": "0.38", "wpb": "260527", "bsz": "1758.2", "num_updates": "68000", "lr": "0.000451087", "gnorm": "0.369", "loss_scale": "2", "train_wall": "198", "gb_free": "40.1", "wall": "17933"}
[2024-10-09 22:29:02,011][fairseq_cli.train][INFO] - end of epoch 1418 (average epoch stats below)
[2024-10-09 22:29:02,013][train][INFO] - {"epoch": 1418, "train_loss": "0.46", "train_ntokens": "260788", "train_nsentences": "1750.04", "train_wps": "98756.2", "train_ups": "0.38", "train_wpb": "260788", "train_bsz": "1750", "train_num_updates": "68033", "train_lr": "0.000451042", "train_gnorm": "0.385", "train_loss_scale": "2", "train_train_wall": "53", "train_gb_free": "39.6", "train_wall": "17966"}
[2024-10-09 22:29:02,069][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 22:29:02,072][fairseq.trainer][INFO] - begin training epoch 1419
[2024-10-09 22:29:02,072][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 22:31:08,455][fairseq_cli.train][INFO] - end of epoch 1419 (average epoch stats below)
[2024-10-09 22:31:08,458][train][INFO] - {"epoch": 1419, "train_loss": "0.465", "train_ntokens": "260638", "train_nsentences": "1750.04", "train_wps": "98943.9", "train_ups": "0.38", "train_wpb": "260638", "train_bsz": "1750", "train_num_updates": "68081", "train_lr": "0.000450977", "train_gnorm": "0.355", "train_loss_scale": "2", "train_train_wall": "37", "train_gb_free": "39.2", "train_wall": "18093"}
[2024-10-09 22:31:08,524][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 22:31:08,528][fairseq.trainer][INFO] - begin training epoch 1420
[2024-10-09 22:31:08,528][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 22:33:20,446][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 1420 @ 68129 updates
[2024-10-09 22:33:20,447][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-09 22:33:23,702][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-09 22:33:23,704][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 1420 @ 68129 updates, score None) (writing took 3.2580911461263895 seconds)
[2024-10-09 22:33:23,705][fairseq_cli.train][INFO] - end of epoch 1420 (average epoch stats below)
[2024-10-09 22:33:23,708][train][INFO] - {"epoch": 1420, "train_loss": "0.467", "train_ntokens": "260776", "train_nsentences": "1750.04", "train_wps": "92552.6", "train_ups": "0.35", "train_wpb": "260776", "train_bsz": "1750", "train_num_updates": "68129", "train_lr": "0.000450912", "train_gnorm": "0.351", "train_loss_scale": "2", "train_train_wall": "37", "train_gb_free": "40.7", "train_wall": "18228"}
[2024-10-09 22:33:23,780][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 22:33:23,784][fairseq.trainer][INFO] - begin training epoch 1421
[2024-10-09 22:33:23,785][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 22:35:26,568][fairseq_cli.train][INFO] - end of epoch 1421 (average epoch stats below)
[2024-10-09 22:35:26,575][train][INFO] - {"epoch": 1421, "train_loss": "0.465", "train_ntokens": "260811", "train_nsentences": "1750.04", "train_wps": "101897", "train_ups": "0.39", "train_wpb": "260811", "train_bsz": "1750", "train_num_updates": "68177", "train_lr": "0.000450846", "train_gnorm": "0.389", "train_loss_scale": "2", "train_train_wall": "44", "train_gb_free": "39.8", "train_wall": "18351"}
[2024-10-09 22:35:26,685][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 22:35:26,694][fairseq.trainer][INFO] - begin training epoch 1422
[2024-10-09 22:35:26,695][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 22:37:15,194][train_inner][INFO] - {"epoch": 1422, "update": 1421.479, "loss": "0.464", "ntokens": "260741", "nsentences": "1757.67", "wps": "99029.2", "ups": "0.38", "wpb": "260741", "bsz": "1757.7", "num_updates": "68200", "lr": "0.000450815", "gnorm": "0.37", "loss_scale": "2", "train_wall": "168", "gb_free": "39.8", "wall": "18460"}
[2024-10-09 22:37:32,986][fairseq_cli.train][INFO] - end of epoch 1422 (average epoch stats below)
[2024-10-09 22:37:32,989][train][INFO] - {"epoch": 1422, "train_loss": "0.463", "train_ntokens": "260699", "train_nsentences": "1750.04", "train_wps": "98991.7", "train_ups": "0.38", "train_wpb": "260699", "train_bsz": "1750", "train_num_updates": "68225", "train_lr": "0.000450781", "train_gnorm": "0.366", "train_loss_scale": "2", "train_train_wall": "34", "train_gb_free": "39.3", "train_wall": "18477"}
[2024-10-09 22:37:33,125][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 22:37:33,140][fairseq.trainer][INFO] - begin training epoch 1423
[2024-10-09 22:37:33,140][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 22:39:42,383][fairseq_cli.train][INFO] - end of epoch 1423 (average epoch stats below)
[2024-10-09 22:39:42,385][train][INFO] - {"epoch": 1423, "train_loss": "0.462", "train_ntokens": "260821", "train_nsentences": "1750.04", "train_wps": "96755.2", "train_ups": "0.37", "train_wpb": "260821", "train_bsz": "1750", "train_num_updates": "68273", "train_lr": "0.000450716", "train_gnorm": "0.387", "train_loss_scale": "2", "train_train_wall": "54", "train_gb_free": "39.3", "train_wall": "18607"}
[2024-10-09 22:39:42,440][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 22:39:42,444][fairseq.trainer][INFO] - begin training epoch 1424
[2024-10-09 22:39:42,444][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 22:41:50,723][fairseq_cli.train][INFO] - end of epoch 1424 (average epoch stats below)
[2024-10-09 22:41:50,726][train][INFO] - {"epoch": 1424, "train_loss": "0.467", "train_ntokens": "260716", "train_nsentences": "1750.04", "train_wps": "97511.3", "train_ups": "0.37", "train_wpb": "260716", "train_bsz": "1750", "train_num_updates": "68321", "train_lr": "0.000450651", "train_gnorm": "0.388", "train_loss_scale": "2", "train_train_wall": "58", "train_gb_free": "40.4", "train_wall": "18735"}
[2024-10-09 22:41:50,797][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 22:41:50,816][fairseq.trainer][INFO] - begin training epoch 1425
[2024-10-09 22:41:50,817][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 22:43:58,458][fairseq_cli.train][INFO] - end of epoch 1425 (average epoch stats below)
[2024-10-09 22:43:58,462][train][INFO] - {"epoch": 1425, "train_loss": "0.462", "train_ntokens": "260241", "train_nsentences": "1750.04", "train_wps": "97795.4", "train_ups": "0.38", "train_wpb": "260241", "train_bsz": "1750", "train_num_updates": "68369", "train_lr": "0.000450586", "train_gnorm": "0.361", "train_loss_scale": "2", "train_train_wall": "57", "train_gb_free": "39.8", "train_wall": "18863"}
[2024-10-09 22:43:58,535][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 22:43:58,544][fairseq.trainer][INFO] - begin training epoch 1426
[2024-10-09 22:43:58,545][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 22:45:47,879][train_inner][INFO] - {"epoch": 1426, "update": 1425.646, "loss": "0.465", "ntokens": "260588", "nsentences": "1740.34", "wps": "101657", "ups": "0.39", "wpb": "260588", "bsz": "1740.3", "num_updates": "68400", "lr": "0.000450543", "gnorm": "0.372", "loss_scale": "2", "train_wall": "205", "gb_free": "40.1", "wall": "18972"}
[2024-10-09 22:46:06,304][fairseq_cli.train][INFO] - end of epoch 1426 (average epoch stats below)
[2024-10-09 22:46:06,306][train][INFO] - {"epoch": 1426, "train_loss": "0.462", "train_ntokens": "261058", "train_nsentences": "1750.04", "train_wps": "98018.3", "train_ups": "0.38", "train_wpb": "261058", "train_bsz": "1750", "train_num_updates": "68417", "train_lr": "0.00045052", "train_gnorm": "0.344", "train_loss_scale": "2", "train_train_wall": "33", "train_gb_free": "40.1", "train_wall": "18991"}
[2024-10-09 22:46:06,366][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 22:46:06,370][fairseq.trainer][INFO] - begin training epoch 1427
[2024-10-09 22:46:06,370][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 22:48:12,156][fairseq_cli.train][INFO] - end of epoch 1427 (average epoch stats below)
[2024-10-09 22:48:12,163][train][INFO] - {"epoch": 1427, "train_loss": "0.461", "train_ntokens": "260876", "train_nsentences": "1750.04", "train_wps": "99498.5", "train_ups": "0.38", "train_wpb": "260876", "train_bsz": "1750", "train_num_updates": "68465", "train_lr": "0.000450455", "train_gnorm": "0.374", "train_loss_scale": "2", "train_train_wall": "52", "train_gb_free": "41", "train_wall": "19117"}
[2024-10-09 22:48:12,282][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 22:48:12,286][fairseq.trainer][INFO] - begin training epoch 1428
[2024-10-09 22:48:12,286][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 22:50:23,299][fairseq_cli.train][INFO] - end of epoch 1428 (average epoch stats below)
[2024-10-09 22:50:23,302][train][INFO] - {"epoch": 1428, "train_loss": "0.462", "train_ntokens": "260795", "train_nsentences": "1750.04", "train_wps": "95459.7", "train_ups": "0.37", "train_wpb": "260795", "train_bsz": "1750", "train_num_updates": "68513", "train_lr": "0.00045039", "train_gnorm": "0.366", "train_loss_scale": "2", "train_train_wall": "51", "train_gb_free": "39.7", "train_wall": "19248"}
[2024-10-09 22:50:23,359][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 22:50:23,363][fairseq.trainer][INFO] - begin training epoch 1429
[2024-10-09 22:50:23,363][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 22:52:31,210][fairseq_cli.train][INFO] - end of epoch 1429 (average epoch stats below)
[2024-10-09 22:52:31,215][train][INFO] - {"epoch": 1429, "train_loss": "0.468", "train_ntokens": "260746", "train_nsentences": "1750.04", "train_wps": "97848.8", "train_ups": "0.38", "train_wpb": "260746", "train_bsz": "1750", "train_num_updates": "68561", "train_lr": "0.000450325", "train_gnorm": "0.37", "train_loss_scale": "4", "train_train_wall": "57", "train_gb_free": "40.1", "train_wall": "19376"}
[2024-10-09 22:52:31,322][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 22:52:31,333][fairseq.trainer][INFO] - begin training epoch 1430
[2024-10-09 22:52:31,334][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 22:54:33,151][train_inner][INFO] - {"epoch": 1430, "update": 1429.812, "loss": "0.463", "ntokens": "260890", "nsentences": "1752.98", "wps": "99347.3", "ups": "0.38", "wpb": "260890", "bsz": "1753", "num_updates": "68600", "lr": "0.000450272", "gnorm": "0.367", "loss_scale": "4", "train_wall": "209", "gb_free": "39.3", "wall": "19498"}
[2024-10-09 22:54:38,713][fairseq_cli.train][INFO] - end of epoch 1430 (average epoch stats below)
[2024-10-09 22:54:38,725][train][INFO] - {"epoch": 1430, "train_loss": "0.46", "train_ntokens": "260814", "train_nsentences": "1750.04", "train_wps": "98191.7", "train_ups": "0.38", "train_wpb": "260814", "train_bsz": "1750", "train_num_updates": "68609", "train_lr": "0.00045026", "train_gnorm": "0.375", "train_loss_scale": "4", "train_train_wall": "42", "train_gb_free": "40.6", "train_wall": "19503"}
[2024-10-09 22:54:38,849][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 22:54:38,875][fairseq.trainer][INFO] - begin training epoch 1431
[2024-10-09 22:54:38,875][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 22:56:48,845][fairseq_cli.train][INFO] - end of epoch 1431 (average epoch stats below)
[2024-10-09 22:56:48,848][train][INFO] - {"epoch": 1431, "train_loss": "0.458", "train_ntokens": "260776", "train_nsentences": "1750.04", "train_wps": "96197.8", "train_ups": "0.37", "train_wpb": "260776", "train_bsz": "1750", "train_num_updates": "68657", "train_lr": "0.000450194", "train_gnorm": "0.364", "train_loss_scale": "4", "train_train_wall": "41", "train_gb_free": "39.3", "train_wall": "19633"}
[2024-10-09 22:56:49,054][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 22:56:49,060][fairseq.trainer][INFO] - begin training epoch 1432
[2024-10-09 22:56:49,061][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 22:59:02,975][fairseq_cli.train][INFO] - end of epoch 1432 (average epoch stats below)
[2024-10-09 22:59:02,978][train][INFO] - {"epoch": 1432, "train_loss": "0.465", "train_ntokens": "260517", "train_nsentences": "1750.04", "train_wps": "93231.9", "train_ups": "0.36", "train_wpb": "260517", "train_bsz": "1750", "train_num_updates": "68705", "train_lr": "0.000450129", "train_gnorm": "0.391", "train_loss_scale": "4", "train_train_wall": "65", "train_gb_free": "39.2", "train_wall": "19767"}
[2024-10-09 22:59:03,086][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 22:59:03,093][fairseq.trainer][INFO] - begin training epoch 1433
[2024-10-09 22:59:03,094][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 23:01:17,628][fairseq_cli.train][INFO] - end of epoch 1433 (average epoch stats below)
[2024-10-09 23:01:17,634][train][INFO] - {"epoch": 1433, "train_loss": "0.458", "train_ntokens": "260827", "train_nsentences": "1750.04", "train_wps": "92978.3", "train_ups": "0.36", "train_wpb": "260827", "train_bsz": "1750", "train_num_updates": "68753", "train_lr": "0.000450064", "train_gnorm": "0.401", "train_loss_scale": "4", "train_train_wall": "59", "train_gb_free": "40.1", "train_wall": "19902"}
[2024-10-09 23:01:17,731][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 23:01:17,737][fairseq.trainer][INFO] - begin training epoch 1434
[2024-10-09 23:01:17,737][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 23:03:21,429][train_inner][INFO] - {"epoch": 1434, "update": 1433.979, "loss": "0.459", "ntokens": "260758", "nsentences": "1749.83", "wps": "98721.8", "ups": "0.38", "wpb": "260758", "bsz": "1749.8", "num_updates": "68800", "lr": "0.00045", "gnorm": "0.383", "loss_scale": "4", "train_wall": "220", "gb_free": "39.6", "wall": "20026"}
[2024-10-09 23:03:21,746][fairseq_cli.train][INFO] - end of epoch 1434 (average epoch stats below)
[2024-10-09 23:03:21,763][train][INFO] - {"epoch": 1434, "train_loss": "0.454", "train_ntokens": "260773", "train_nsentences": "1750.04", "train_wps": "100855", "train_ups": "0.39", "train_wpb": "260773", "train_bsz": "1750", "train_num_updates": "68801", "train_lr": "0.000449999", "train_gnorm": "0.374", "train_loss_scale": "4", "train_train_wall": "50", "train_gb_free": "39.6", "train_wall": "20026"}
[2024-10-09 23:03:21,856][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 23:03:21,863][fairseq.trainer][INFO] - begin training epoch 1435
[2024-10-09 23:03:21,864][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 23:05:26,621][fairseq_cli.train][INFO] - end of epoch 1435 (average epoch stats below)
[2024-10-09 23:05:26,636][train][INFO] - {"epoch": 1435, "train_loss": "0.458", "train_ntokens": "260613", "train_nsentences": "1750.04", "train_wps": "100181", "train_ups": "0.38", "train_wpb": "260613", "train_bsz": "1750", "train_num_updates": "68849", "train_lr": "0.000449933", "train_gnorm": "0.366", "train_loss_scale": "4", "train_train_wall": "49", "train_gb_free": "40.3", "train_wall": "20151"}
[2024-10-09 23:05:26,774][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 23:05:26,789][fairseq.trainer][INFO] - begin training epoch 1436
[2024-10-09 23:05:26,789][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 23:07:34,554][fairseq_cli.train][INFO] - end of epoch 1436 (average epoch stats below)
[2024-10-09 23:07:34,557][train][INFO] - {"epoch": 1436, "train_loss": "0.467", "train_ntokens": "260724", "train_nsentences": "1750.04", "train_wps": "97835.4", "train_ups": "0.38", "train_wpb": "260724", "train_bsz": "1750", "train_num_updates": "68897", "train_lr": "0.000449868", "train_gnorm": "0.375", "train_loss_scale": "4", "train_train_wall": "54", "train_gb_free": "39.9", "train_wall": "20279"}
[2024-10-09 23:07:34,611][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 23:07:34,615][fairseq.trainer][INFO] - begin training epoch 1437
[2024-10-09 23:07:34,615][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 23:09:44,871][fairseq_cli.train][INFO] - end of epoch 1437 (average epoch stats below)
[2024-10-09 23:09:44,878][train][INFO] - {"epoch": 1437, "train_loss": "0.464", "train_ntokens": "260450", "train_nsentences": "1750.04", "train_wps": "95933.8", "train_ups": "0.37", "train_wpb": "260450", "train_bsz": "1750", "train_num_updates": "68945", "train_lr": "0.000449803", "train_gnorm": "0.374", "train_loss_scale": "4", "train_train_wall": "31", "train_gb_free": "40.5", "train_wall": "20409"}
[2024-10-09 23:09:44,980][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 23:09:44,986][fairseq.trainer][INFO] - begin training epoch 1438
[2024-10-09 23:09:44,986][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 23:11:54,742][fairseq_cli.train][INFO] - end of epoch 1438 (average epoch stats below)
[2024-10-09 23:11:54,745][train][INFO] - {"epoch": 1438, "train_loss": "0.461", "train_ntokens": "260441", "train_nsentences": "1750.04", "train_wps": "96263.1", "train_ups": "0.37", "train_wpb": "260441", "train_bsz": "1750", "train_num_updates": "68993", "train_lr": "0.000449738", "train_gnorm": "0.363", "train_loss_scale": "4", "train_train_wall": "46", "train_gb_free": "39.6", "train_wall": "20539"}
[2024-10-09 23:11:54,797][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 23:11:54,801][fairseq.trainer][INFO] - begin training epoch 1439
[2024-10-09 23:11:54,801][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 23:13:27,470][train_inner][INFO] - {"epoch": 1439, "update": 1438.146, "loss": "0.462", "ntokens": "260456", "nsentences": "1750.83", "wps": "85954", "ups": "0.33", "wpb": "260456", "bsz": "1750.8", "num_updates": "69000", "lr": "0.000449728", "gnorm": "0.368", "loss_scale": "4", "train_wall": "202", "gb_free": "39.6", "wall": "20632"}
[2024-10-09 23:13:59,761][fairseq_cli.train][INFO] - end of epoch 1439 (average epoch stats below)
[2024-10-09 23:13:59,775][train][INFO] - {"epoch": 1439, "train_loss": "0.463", "train_ntokens": "260773", "train_nsentences": "1750.04", "train_wps": "100126", "train_ups": "0.38", "train_wpb": "260773", "train_bsz": "1750", "train_num_updates": "69041", "train_lr": "0.000449673", "train_gnorm": "0.36", "train_loss_scale": "4", "train_train_wall": "54", "train_gb_free": "39.3", "train_wall": "20664"}
[2024-10-09 23:13:59,878][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 23:13:59,890][fairseq.trainer][INFO] - begin training epoch 1440
[2024-10-09 23:13:59,891][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 23:16:03,952][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 1440 @ 69089 updates
[2024-10-09 23:16:03,954][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-09 23:16:07,739][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-09 23:16:07,742][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 1440 @ 69089 updates, score None) (writing took 3.7905270513147116 seconds)
[2024-10-09 23:16:07,743][fairseq_cli.train][INFO] - end of epoch 1440 (average epoch stats below)
[2024-10-09 23:16:07,745][train][INFO] - {"epoch": 1440, "train_loss": "0.458", "train_ntokens": "260674", "train_nsentences": "1750.04", "train_wps": "97778.2", "train_ups": "0.38", "train_wpb": "260674", "train_bsz": "1750", "train_num_updates": "69089", "train_lr": "0.000449607", "train_gnorm": "0.381", "train_loss_scale": "4", "train_train_wall": "35", "train_gb_free": "39.7", "train_wall": "20792"}
[2024-10-09 23:16:07,821][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 23:16:07,826][fairseq.trainer][INFO] - begin training epoch 1441
[2024-10-09 23:16:07,826][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 23:18:13,537][fairseq_cli.train][INFO] - end of epoch 1441 (average epoch stats below)
[2024-10-09 23:18:13,542][train][INFO] - {"epoch": 1441, "train_loss": "0.462", "train_ntokens": "260694", "train_nsentences": "1750.04", "train_wps": "99475.9", "train_ups": "0.38", "train_wpb": "260694", "train_bsz": "1750", "train_num_updates": "69137", "train_lr": "0.000449542", "train_gnorm": "0.374", "train_loss_scale": "4", "train_train_wall": "33", "train_gb_free": "39.3", "train_wall": "20918"}
[2024-10-09 23:18:13,697][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 23:18:13,700][fairseq.trainer][INFO] - begin training epoch 1442
[2024-10-09 23:18:13,701][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 23:20:20,585][fairseq_cli.train][INFO] - end of epoch 1442 (average epoch stats below)
[2024-10-09 23:20:20,595][train][INFO] - {"epoch": 1442, "train_loss": "0.453", "train_ntokens": "260830", "train_nsentences": "1750.04", "train_wps": "98544.8", "train_ups": "0.38", "train_wpb": "260830", "train_bsz": "1750", "train_num_updates": "69185", "train_lr": "0.000449477", "train_gnorm": "0.383", "train_loss_scale": "4", "train_train_wall": "48", "train_gb_free": "39.6", "train_wall": "21045"}
[2024-10-09 23:20:20,719][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 23:20:20,735][fairseq.trainer][INFO] - begin training epoch 1443
[2024-10-09 23:20:20,735][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 23:22:01,920][train_inner][INFO] - {"epoch": 1443, "update": 1442.312, "loss": "0.459", "ntokens": "260818", "nsentences": "1758.14", "wps": "101398", "ups": "0.39", "wpb": "260818", "bsz": "1758.1", "num_updates": "69200", "lr": "0.000449457", "gnorm": "0.376", "loss_scale": "4", "train_wall": "164", "gb_free": "39.6", "wall": "21146"}
[2024-10-09 23:22:27,522][fairseq_cli.train][INFO] - end of epoch 1443 (average epoch stats below)
[2024-10-09 23:22:27,526][train][INFO] - {"epoch": 1443, "train_loss": "0.463", "train_ntokens": "261144", "train_nsentences": "1750.04", "train_wps": "98757.4", "train_ups": "0.38", "train_wpb": "261144", "train_bsz": "1750", "train_num_updates": "69233", "train_lr": "0.000449412", "train_gnorm": "0.357", "train_loss_scale": "4", "train_train_wall": "42", "train_gb_free": "39.3", "train_wall": "21172"}
[2024-10-09 23:22:27,737][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 23:22:27,753][fairseq.trainer][INFO] - begin training epoch 1444
[2024-10-09 23:22:27,754][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 23:24:34,314][fairseq_cli.train][INFO] - end of epoch 1444 (average epoch stats below)
[2024-10-09 23:24:34,318][train][INFO] - {"epoch": 1444, "train_loss": "0.468", "train_ntokens": "260416", "train_nsentences": "1750.04", "train_wps": "98589.9", "train_ups": "0.38", "train_wpb": "260416", "train_bsz": "1750", "train_num_updates": "69281", "train_lr": "0.000449346", "train_gnorm": "0.355", "train_loss_scale": "4", "train_train_wall": "55", "train_gb_free": "39.6", "train_wall": "21299"}
[2024-10-09 23:24:34,403][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 23:24:34,416][fairseq.trainer][INFO] - begin training epoch 1445
[2024-10-09 23:24:34,416][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 23:26:42,267][fairseq_cli.train][INFO] - end of epoch 1445 (average epoch stats below)
[2024-10-09 23:26:42,271][train][INFO] - {"epoch": 1445, "train_loss": "0.463", "train_ntokens": "260501", "train_nsentences": "1750.04", "train_wps": "97726.3", "train_ups": "0.38", "train_wpb": "260501", "train_bsz": "1750", "train_num_updates": "69329", "train_lr": "0.000449281", "train_gnorm": "0.362", "train_loss_scale": "4", "train_train_wall": "53", "train_gb_free": "39.8", "train_wall": "21427"}
[2024-10-09 23:26:42,385][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 23:26:42,392][fairseq.trainer][INFO] - begin training epoch 1446
[2024-10-09 23:26:42,392][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 23:28:54,718][fairseq_cli.train][INFO] - end of epoch 1446 (average epoch stats below)
[2024-10-09 23:28:54,729][train][INFO] - {"epoch": 1446, "train_loss": "0.457", "train_ntokens": "260650", "train_nsentences": "1750.04", "train_wps": "94456.6", "train_ups": "0.36", "train_wpb": "260650", "train_bsz": "1750", "train_num_updates": "69377", "train_lr": "0.000449216", "train_gnorm": "0.379", "train_loss_scale": "4", "train_train_wall": "54", "train_gb_free": "40.1", "train_wall": "21559"}
[2024-10-09 23:28:54,886][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 23:28:54,900][fairseq.trainer][INFO] - begin training epoch 1447
[2024-10-09 23:28:54,900][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 23:30:51,526][train_inner][INFO] - {"epoch": 1447, "update": 1446.479, "loss": "0.463", "ntokens": "260559", "nsentences": "1752.54", "wps": "98398.6", "ups": "0.38", "wpb": "260559", "bsz": "1752.5", "num_updates": "69400", "lr": "0.000449185", "gnorm": "0.361", "loss_scale": "4", "train_wall": "222", "gb_free": "39.8", "wall": "21676"}
[2024-10-09 23:31:10,152][fairseq_cli.train][INFO] - end of epoch 1447 (average epoch stats below)
[2024-10-09 23:31:10,155][train][INFO] - {"epoch": 1447, "train_loss": "0.459", "train_ntokens": "260616", "train_nsentences": "1750.04", "train_wps": "92375.2", "train_ups": "0.35", "train_wpb": "260616", "train_bsz": "1750", "train_num_updates": "69425", "train_lr": "0.000449151", "train_gnorm": "0.348", "train_loss_scale": "4", "train_train_wall": "53", "train_gb_free": "39.2", "train_wall": "21695"}
[2024-10-09 23:31:10,280][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 23:31:10,284][fairseq.trainer][INFO] - begin training epoch 1448
[2024-10-09 23:31:10,285][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 23:33:17,253][fairseq_cli.train][INFO] - end of epoch 1448 (average epoch stats below)
[2024-10-09 23:33:17,256][train][INFO] - {"epoch": 1448, "train_loss": "0.464", "train_ntokens": "260845", "train_nsentences": "1750.04", "train_wps": "98511.6", "train_ups": "0.38", "train_wpb": "260845", "train_bsz": "1750", "train_num_updates": "69473", "train_lr": "0.000449086", "train_gnorm": "0.362", "train_loss_scale": "4", "train_train_wall": "50", "train_gb_free": "39.7", "train_wall": "21822"}
[2024-10-09 23:33:17,305][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 23:33:17,309][fairseq.trainer][INFO] - begin training epoch 1449
[2024-10-09 23:33:17,309][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 23:35:22,121][fairseq_cli.train][INFO] - end of epoch 1449 (average epoch stats below)
[2024-10-09 23:35:22,141][train][INFO] - {"epoch": 1449, "train_loss": "0.455", "train_ntokens": "260870", "train_nsentences": "1750.04", "train_wps": "100280", "train_ups": "0.38", "train_wpb": "260870", "train_bsz": "1750", "train_num_updates": "69521", "train_lr": "0.00044902", "train_gnorm": "0.371", "train_loss_scale": "4", "train_train_wall": "39", "train_gb_free": "39.2", "train_wall": "21947"}
[2024-10-09 23:35:22,281][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 23:35:22,297][fairseq.trainer][INFO] - begin training epoch 1450
[2024-10-09 23:35:22,298][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 23:37:30,773][fairseq_cli.train][INFO] - end of epoch 1450 (average epoch stats below)
[2024-10-09 23:37:30,781][train][INFO] - {"epoch": 1450, "train_loss": "0.459", "train_ntokens": "260167", "train_nsentences": "1750.04", "train_wps": "97080.3", "train_ups": "0.37", "train_wpb": "260167", "train_bsz": "1750", "train_num_updates": "69569", "train_lr": "0.000448955", "train_gnorm": "0.372", "train_loss_scale": "4", "train_train_wall": "53", "train_gb_free": "40.3", "train_wall": "22075"}
[2024-10-09 23:37:30,981][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 23:37:30,985][fairseq.trainer][INFO] - begin training epoch 1451
[2024-10-09 23:37:30,986][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 23:39:26,174][train_inner][INFO] - {"epoch": 1451, "update": 1450.646, "loss": "0.458", "ntokens": "260609", "nsentences": "1753.62", "wps": "101277", "ups": "0.39", "wpb": "260609", "bsz": "1753.6", "num_updates": "69600", "lr": "0.000448913", "gnorm": "0.363", "loss_scale": "4", "train_wall": "197", "gb_free": "40.2", "wall": "22191"}
[2024-10-09 23:39:43,088][fairseq_cli.train][INFO] - end of epoch 1451 (average epoch stats below)
[2024-10-09 23:39:43,095][train][INFO] - {"epoch": 1451, "train_loss": "0.456", "train_ntokens": "260893", "train_nsentences": "1750.04", "train_wps": "94651.2", "train_ups": "0.36", "train_wpb": "260893", "train_bsz": "1750", "train_num_updates": "69617", "train_lr": "0.00044889", "train_gnorm": "0.345", "train_loss_scale": "4", "train_train_wall": "55", "train_gb_free": "39.7", "train_wall": "22208"}
[2024-10-09 23:39:43,246][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 23:39:43,252][fairseq.trainer][INFO] - begin training epoch 1452
[2024-10-09 23:39:43,252][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 23:41:55,354][fairseq_cli.train][INFO] - end of epoch 1452 (average epoch stats below)
[2024-10-09 23:41:55,357][train][INFO] - {"epoch": 1452, "train_loss": "0.469", "train_ntokens": "260760", "train_nsentences": "1750.04", "train_wps": "94639.2", "train_ups": "0.36", "train_wpb": "260760", "train_bsz": "1750", "train_num_updates": "69665", "train_lr": "0.000448825", "train_gnorm": "0.368", "train_loss_scale": "4", "train_train_wall": "56", "train_gb_free": "39.8", "train_wall": "22340"}
[2024-10-09 23:41:55,483][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 23:41:55,487][fairseq.trainer][INFO] - begin training epoch 1453
[2024-10-09 23:41:55,488][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 23:44:12,029][fairseq_cli.train][INFO] - end of epoch 1453 (average epoch stats below)
[2024-10-09 23:44:12,034][train][INFO] - {"epoch": 1453, "train_loss": "0.455", "train_ntokens": "260458", "train_nsentences": "1750.04", "train_wps": "91474.7", "train_ups": "0.35", "train_wpb": "260458", "train_bsz": "1750", "train_num_updates": "69713", "train_lr": "0.00044876", "train_gnorm": "0.372", "train_loss_scale": "4", "train_train_wall": "66", "train_gb_free": "39.8", "train_wall": "22476"}
[2024-10-09 23:44:12,213][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 23:44:12,222][fairseq.trainer][INFO] - begin training epoch 1454
[2024-10-09 23:44:12,222][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 23:46:27,439][fairseq_cli.train][INFO] - end of epoch 1454 (average epoch stats below)
[2024-10-09 23:46:27,450][train][INFO] - {"epoch": 1454, "train_loss": "0.466", "train_ntokens": "260354", "train_nsentences": "1750.04", "train_wps": "92287.7", "train_ups": "0.35", "train_wpb": "260354", "train_bsz": "1750", "train_num_updates": "69761", "train_lr": "0.000448694", "train_gnorm": "0.362", "train_loss_scale": "4", "train_train_wall": "65", "train_gb_free": "40", "train_wall": "22612"}
[2024-10-09 23:46:27,545][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 23:46:27,561][fairseq.trainer][INFO] - begin training epoch 1455
[2024-10-09 23:46:27,562][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 23:48:29,052][train_inner][INFO] - {"epoch": 1455, "update": 1454.812, "loss": "0.461", "ntokens": "260775", "nsentences": "1730.51", "wps": "96075.6", "ups": "0.37", "wpb": "260775", "bsz": "1730.5", "num_updates": "69800", "lr": "0.000448641", "gnorm": "0.364", "loss_scale": "4", "train_wall": "253", "gb_free": "39.6", "wall": "22733"}
[2024-10-09 23:48:36,559][fairseq_cli.train][INFO] - end of epoch 1455 (average epoch stats below)
[2024-10-09 23:48:36,564][train][INFO] - {"epoch": 1455, "train_loss": "0.459", "train_ntokens": "260651", "train_nsentences": "1750.04", "train_wps": "96904.3", "train_ups": "0.37", "train_wpb": "260651", "train_bsz": "1750", "train_num_updates": "69809", "train_lr": "0.000448629", "train_gnorm": "0.359", "train_loss_scale": "4", "train_train_wall": "58", "train_gb_free": "39.6", "train_wall": "22741"}
[2024-10-09 23:48:36,618][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 23:48:36,622][fairseq.trainer][INFO] - begin training epoch 1456
[2024-10-09 23:48:36,622][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 23:50:48,213][fairseq_cli.train][INFO] - end of epoch 1456 (average epoch stats below)
[2024-10-09 23:50:48,230][train][INFO] - {"epoch": 1456, "train_loss": "0.469", "train_ntokens": "261007", "train_nsentences": "1750.04", "train_wps": "95156.7", "train_ups": "0.36", "train_wpb": "261007", "train_bsz": "1750", "train_num_updates": "69857", "train_lr": "0.000448564", "train_gnorm": "0.38", "train_loss_scale": "4", "train_train_wall": "51", "train_gb_free": "39.8", "train_wall": "22873"}
[2024-10-09 23:50:48,356][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 23:50:48,372][fairseq.trainer][INFO] - begin training epoch 1457
[2024-10-09 23:50:48,372][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 23:53:03,272][fairseq_cli.train][INFO] - end of epoch 1457 (average epoch stats below)
[2024-10-09 23:53:03,275][train][INFO] - {"epoch": 1457, "train_loss": "0.465", "train_ntokens": "260949", "train_nsentences": "1750.04", "train_wps": "92759.3", "train_ups": "0.36", "train_wpb": "260949", "train_bsz": "1750", "train_num_updates": "69905", "train_lr": "0.000448499", "train_gnorm": "0.364", "train_loss_scale": "4", "train_train_wall": "63", "train_gb_free": "39.6", "train_wall": "23008"}
[2024-10-09 23:53:03,329][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 23:53:03,348][fairseq.trainer][INFO] - begin training epoch 1458
[2024-10-09 23:53:03,348][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 23:55:12,420][fairseq_cli.train][INFO] - end of epoch 1458 (average epoch stats below)
[2024-10-09 23:55:12,425][train][INFO] - {"epoch": 1458, "train_loss": "0.456", "train_ntokens": "260480", "train_nsentences": "1750.04", "train_wps": "96813.5", "train_ups": "0.37", "train_wpb": "260480", "train_bsz": "1750", "train_num_updates": "69953", "train_lr": "0.000448433", "train_gnorm": "0.364", "train_loss_scale": "4", "train_train_wall": "62", "train_gb_free": "40.6", "train_wall": "23137"}
[2024-10-09 23:55:12,501][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 23:55:12,507][fairseq.trainer][INFO] - begin training epoch 1459
[2024-10-09 23:55:12,508][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 23:57:21,075][train_inner][INFO] - {"epoch": 1459, "update": 1458.979, "loss": "0.465", "ntokens": "260762", "nsentences": "1754.59", "wps": "98032.1", "ups": "0.38", "wpb": "260762", "bsz": "1754.6", "num_updates": "70000", "lr": "0.00044837", "gnorm": "0.369", "loss_scale": "4", "train_wall": "233", "gb_free": "39.3", "wall": "23265"}
[2024-10-09 23:57:21,341][fairseq_cli.train][INFO] - end of epoch 1459 (average epoch stats below)
[2024-10-09 23:57:21,343][train][INFO] - {"epoch": 1459, "train_loss": "0.467", "train_ntokens": "260648", "train_nsentences": "1750.04", "train_wps": "97049.3", "train_ups": "0.37", "train_wpb": "260648", "train_bsz": "1750", "train_num_updates": "70001", "train_lr": "0.000448368", "train_gnorm": "0.374", "train_loss_scale": "4", "train_train_wall": "50", "train_gb_free": "39.3", "train_wall": "23266"}
[2024-10-09 23:57:21,437][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 23:57:21,449][fairseq.trainer][INFO] - begin training epoch 1460
[2024-10-09 23:57:21,451][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 23:59:28,718][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 1460 @ 70049 updates
[2024-10-09 23:59:28,720][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-09 23:59:32,477][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-09 23:59:32,479][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 1460 @ 70049 updates, score None) (writing took 3.7612572833895683 seconds)
[2024-10-09 23:59:32,480][fairseq_cli.train][INFO] - end of epoch 1460 (average epoch stats below)
[2024-10-09 23:59:32,482][train][INFO] - {"epoch": 1460, "train_loss": "0.457", "train_ntokens": "260472", "train_nsentences": "1750.04", "train_wps": "95341.9", "train_ups": "0.37", "train_wpb": "260472", "train_bsz": "1750", "train_num_updates": "70049", "train_lr": "0.000448303", "train_gnorm": "0.384", "train_loss_scale": "4", "train_train_wall": "42", "train_gb_free": "39.6", "train_wall": "23397"}
[2024-10-09 23:59:32,545][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-09 23:59:32,549][fairseq.trainer][INFO] - begin training epoch 1461
[2024-10-09 23:59:32,550][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 00:01:39,978][fairseq_cli.train][INFO] - end of epoch 1461 (average epoch stats below)
[2024-10-10 00:01:39,983][train][INFO] - {"epoch": 1461, "train_loss": "0.466", "train_ntokens": "260949", "train_nsentences": "1750.04", "train_wps": "98242.7", "train_ups": "0.38", "train_wpb": "260949", "train_bsz": "1750", "train_num_updates": "70097", "train_lr": "0.000448238", "train_gnorm": "0.378", "train_loss_scale": "4", "train_train_wall": "55", "train_gb_free": "39.6", "train_wall": "23524"}
[2024-10-10 00:01:40,041][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 00:01:40,045][fairseq.trainer][INFO] - begin training epoch 1462
[2024-10-10 00:01:40,045][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 00:03:49,007][fairseq_cli.train][INFO] - end of epoch 1462 (average epoch stats below)
[2024-10-10 00:03:49,010][train][INFO] - {"epoch": 1462, "train_loss": "0.464", "train_ntokens": "260933", "train_nsentences": "1750.04", "train_wps": "97075.6", "train_ups": "0.37", "train_wpb": "260933", "train_bsz": "1750", "train_num_updates": "70145", "train_lr": "0.000448173", "train_gnorm": "0.38", "train_loss_scale": "4", "train_train_wall": "46", "train_gb_free": "40.5", "train_wall": "23653"}
[2024-10-10 00:03:49,076][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 00:03:49,080][fairseq.trainer][INFO] - begin training epoch 1463
[2024-10-10 00:03:49,081][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 00:05:53,257][fairseq_cli.train][INFO] - end of epoch 1463 (average epoch stats below)
[2024-10-10 00:05:53,261][train][INFO] - {"epoch": 1463, "train_loss": "0.447", "train_ntokens": "260655", "train_nsentences": "1750.04", "train_wps": "100698", "train_ups": "0.39", "train_wpb": "260655", "train_bsz": "1750", "train_num_updates": "70193", "train_lr": "0.000448107", "train_gnorm": "0.374", "train_loss_scale": "4", "train_train_wall": "49", "train_gb_free": "39.7", "train_wall": "23778"}
[2024-10-10 00:05:53,361][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 00:05:53,380][fairseq.trainer][INFO] - begin training epoch 1464
[2024-10-10 00:05:53,381][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 00:07:27,109][train_inner][INFO] - {"epoch": 1464, "update": 1463.146, "loss": "0.458", "ntokens": "260934", "nsentences": "1739.27", "wps": "86115", "ups": "0.33", "wpb": "260934", "bsz": "1739.3", "num_updates": "70200", "lr": "0.000448098", "gnorm": "0.379", "loss_scale": "4", "train_wall": "198", "gb_free": "40", "wall": "23872"}
[2024-10-10 00:08:01,957][fairseq_cli.train][INFO] - end of epoch 1464 (average epoch stats below)
[2024-10-10 00:08:01,960][train][INFO] - {"epoch": 1464, "train_loss": "0.461", "train_ntokens": "260589", "train_nsentences": "1750.04", "train_wps": "97194.8", "train_ups": "0.37", "train_wpb": "260589", "train_bsz": "1750", "train_num_updates": "70241", "train_lr": "0.000448042", "train_gnorm": "0.4", "train_loss_scale": "4", "train_train_wall": "29", "train_gb_free": "39.7", "train_wall": "23906"}
[2024-10-10 00:08:02,092][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 00:08:02,096][fairseq.trainer][INFO] - begin training epoch 1465
[2024-10-10 00:08:02,096][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 00:10:11,599][fairseq_cli.train][INFO] - end of epoch 1465 (average epoch stats below)
[2024-10-10 00:10:11,603][train][INFO] - {"epoch": 1465, "train_loss": "0.461", "train_ntokens": "260897", "train_nsentences": "1750.04", "train_wps": "96599", "train_ups": "0.37", "train_wpb": "260897", "train_bsz": "1750", "train_num_updates": "70289", "train_lr": "0.000447977", "train_gnorm": "0.388", "train_loss_scale": "4", "train_train_wall": "39", "train_gb_free": "39.6", "train_wall": "24036"}
[2024-10-10 00:10:11,700][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 00:10:11,703][fairseq.trainer][INFO] - begin training epoch 1466
[2024-10-10 00:10:11,703][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 00:12:22,224][fairseq_cli.train][INFO] - end of epoch 1466 (average epoch stats below)
[2024-10-10 00:12:22,239][train][INFO] - {"epoch": 1466, "train_loss": "0.462", "train_ntokens": "260701", "train_nsentences": "1750.04", "train_wps": "95797.4", "train_ups": "0.37", "train_wpb": "260701", "train_bsz": "1750", "train_num_updates": "70337", "train_lr": "0.000447912", "train_gnorm": "0.352", "train_loss_scale": "4", "train_train_wall": "54", "train_gb_free": "39.7", "train_wall": "24167"}
[2024-10-10 00:12:22,375][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 00:12:22,381][fairseq.trainer][INFO] - begin training epoch 1467
[2024-10-10 00:12:22,382][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 00:14:29,389][fairseq_cli.train][INFO] - end of epoch 1467 (average epoch stats below)
[2024-10-10 00:14:29,394][train][INFO] - {"epoch": 1467, "train_loss": "0.462", "train_ntokens": "260941", "train_nsentences": "1750.04", "train_wps": "98505.8", "train_ups": "0.38", "train_wpb": "260941", "train_bsz": "1750", "train_num_updates": "70385", "train_lr": "0.000447846", "train_gnorm": "0.354", "train_loss_scale": "4", "train_train_wall": "54", "train_gb_free": "39.3", "train_wall": "24294"}
[2024-10-10 00:14:29,533][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 00:14:29,543][fairseq.trainer][INFO] - begin training epoch 1468
[2024-10-10 00:14:29,543][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 00:16:04,928][train_inner][INFO] - {"epoch": 1468, "update": 1467.312, "loss": "0.461", "ntokens": "260713", "nsentences": "1751.42", "wps": "100699", "ups": "0.39", "wpb": "260713", "bsz": "1751.4", "num_updates": "70400", "lr": "0.000447826", "gnorm": "0.373", "loss_scale": "4", "train_wall": "185", "gb_free": "39.2", "wall": "24389"}
[2024-10-10 00:16:40,023][fairseq_cli.train][INFO] - end of epoch 1468 (average epoch stats below)
[2024-10-10 00:16:40,031][train][INFO] - {"epoch": 1468, "train_loss": "0.459", "train_ntokens": "260468", "train_nsentences": "1750.04", "train_wps": "95710.9", "train_ups": "0.37", "train_wpb": "260468", "train_bsz": "1750", "train_num_updates": "70433", "train_lr": "0.000447781", "train_gnorm": "0.38", "train_loss_scale": "4", "train_train_wall": "45", "train_gb_free": "39.7", "train_wall": "24424"}
[2024-10-10 00:16:40,136][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 00:16:40,149][fairseq.trainer][INFO] - begin training epoch 1469
[2024-10-10 00:16:40,150][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 00:18:48,190][fairseq_cli.train][INFO] - end of epoch 1469 (average epoch stats below)
[2024-10-10 00:18:48,193][train][INFO] - {"epoch": 1469, "train_loss": "0.448", "train_ntokens": "260310", "train_nsentences": "1750.04", "train_wps": "97505.8", "train_ups": "0.37", "train_wpb": "260310", "train_bsz": "1750", "train_num_updates": "70481", "train_lr": "0.000447716", "train_gnorm": "0.369", "train_loss_scale": "4", "train_train_wall": "51", "train_gb_free": "39.6", "train_wall": "24553"}
[2024-10-10 00:18:48,256][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 00:18:48,260][fairseq.trainer][INFO] - begin training epoch 1470
[2024-10-10 00:18:48,260][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 00:20:57,050][fairseq_cli.train][INFO] - end of epoch 1470 (average epoch stats below)
[2024-10-10 00:20:57,053][train][INFO] - {"epoch": 1470, "train_loss": "0.458", "train_ntokens": "260757", "train_nsentences": "1750.04", "train_wps": "97138.1", "train_ups": "0.37", "train_wpb": "260758", "train_bsz": "1750", "train_num_updates": "70529", "train_lr": "0.000447651", "train_gnorm": "0.351", "train_loss_scale": "4", "train_train_wall": "63", "train_gb_free": "39.8", "train_wall": "24681"}
[2024-10-10 00:20:57,112][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 00:20:57,118][fairseq.trainer][INFO] - begin training epoch 1471
[2024-10-10 00:20:57,118][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 00:23:04,616][fairseq_cli.train][INFO] - end of epoch 1471 (average epoch stats below)
[2024-10-10 00:23:04,624][train][INFO] - {"epoch": 1471, "train_loss": "0.458", "train_ntokens": "260968", "train_nsentences": "1750.04", "train_wps": "98194.9", "train_ups": "0.38", "train_wpb": "260968", "train_bsz": "1750", "train_num_updates": "70577", "train_lr": "0.000447586", "train_gnorm": "0.363", "train_loss_scale": "4", "train_train_wall": "53", "train_gb_free": "39.3", "train_wall": "24809"}
[2024-10-10 00:23:04,723][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 00:23:04,728][fairseq.trainer][INFO] - begin training epoch 1472
[2024-10-10 00:23:04,728][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 00:24:42,435][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
[2024-10-10 00:24:51,336][train_inner][INFO] - {"epoch": 1472, "update": 1471.5, "loss": "0.456", "ntokens": "260514", "nsentences": "1756.68", "wps": "98979.3", "ups": "0.38", "wpb": "260514", "bsz": "1756.7", "num_updates": "70600", "lr": "0.000447554", "gnorm": "0.367", "loss_scale": "4", "train_wall": "225", "gb_free": "39.6", "wall": "24916"}
[2024-10-10 00:25:12,951][fairseq_cli.train][INFO] - end of epoch 1472 (average epoch stats below)
[2024-10-10 00:25:12,963][train][INFO] - {"epoch": 1472, "train_loss": "0.463", "train_ntokens": "260848", "train_nsentences": "1750.57", "train_wps": "95536.2", "train_ups": "0.37", "train_wpb": "260848", "train_bsz": "1750.6", "train_num_updates": "70624", "train_lr": "0.000447522", "train_gnorm": "0.403", "train_loss_scale": "4", "train_train_wall": "49", "train_gb_free": "39.1", "train_wall": "24937"}
[2024-10-10 00:25:13,058][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 00:25:13,064][fairseq.trainer][INFO] - begin training epoch 1473
[2024-10-10 00:25:13,065][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 00:27:18,747][fairseq_cli.train][INFO] - end of epoch 1473 (average epoch stats below)
[2024-10-10 00:27:18,761][train][INFO] - {"epoch": 1473, "train_loss": "0.454", "train_ntokens": "260379", "train_nsentences": "1750.04", "train_wps": "99363", "train_ups": "0.38", "train_wpb": "260379", "train_bsz": "1750", "train_num_updates": "70672", "train_lr": "0.000447457", "train_gnorm": "0.371", "train_loss_scale": "4", "train_train_wall": "57", "train_gb_free": "39.8", "train_wall": "25063"}
[2024-10-10 00:27:18,901][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 00:27:18,909][fairseq.trainer][INFO] - begin training epoch 1474
[2024-10-10 00:27:18,909][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 00:29:26,255][fairseq_cli.train][INFO] - end of epoch 1474 (average epoch stats below)
[2024-10-10 00:29:26,274][train][INFO] - {"epoch": 1474, "train_loss": "0.453", "train_ntokens": "260764", "train_nsentences": "1750.04", "train_wps": "98166.5", "train_ups": "0.38", "train_wpb": "260764", "train_bsz": "1750", "train_num_updates": "70720", "train_lr": "0.000447391", "train_gnorm": "0.366", "train_loss_scale": "4", "train_train_wall": "29", "train_gb_free": "40.1", "train_wall": "25191"}
[2024-10-10 00:29:26,380][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 00:29:26,384][fairseq.trainer][INFO] - begin training epoch 1475
[2024-10-10 00:29:26,384][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 00:31:36,039][fairseq_cli.train][INFO] - end of epoch 1475 (average epoch stats below)
[2024-10-10 00:31:36,042][train][INFO] - {"epoch": 1475, "train_loss": "0.458", "train_ntokens": "260890", "train_nsentences": "1750.04", "train_wps": "96503.4", "train_ups": "0.37", "train_wpb": "260890", "train_bsz": "1750", "train_num_updates": "70768", "train_lr": "0.000447326", "train_gnorm": "0.363", "train_loss_scale": "4", "train_train_wall": "56", "train_gb_free": "39.6", "train_wall": "25320"}
[2024-10-10 00:31:36,099][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 00:31:36,103][fairseq.trainer][INFO] - begin training epoch 1476
[2024-10-10 00:31:36,103][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 00:33:31,632][train_inner][INFO] - {"epoch": 1476, "update": 1475.667, "loss": "0.457", "ntokens": "260512", "nsentences": "1759.7", "wps": "100141", "ups": "0.38", "wpb": "260512", "bsz": "1759.7", "num_updates": "70800", "lr": "0.000447283", "gnorm": "0.372", "loss_scale": "4", "train_wall": "213", "gb_free": "39.8", "wall": "25436"}
[2024-10-10 00:33:47,930][fairseq_cli.train][INFO] - end of epoch 1476 (average epoch stats below)
[2024-10-10 00:33:47,933][train][INFO] - {"epoch": 1476, "train_loss": "0.458", "train_ntokens": "260735", "train_nsentences": "1750.04", "train_wps": "94893.5", "train_ups": "0.36", "train_wpb": "260736", "train_bsz": "1750", "train_num_updates": "70816", "train_lr": "0.000447261", "train_gnorm": "0.361", "train_loss_scale": "4", "train_train_wall": "65", "train_gb_free": "39.7", "train_wall": "25452"}
[2024-10-10 00:33:48,035][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 00:33:48,039][fairseq.trainer][INFO] - begin training epoch 1477
[2024-10-10 00:33:48,040][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 00:35:58,468][fairseq_cli.train][INFO] - end of epoch 1477 (average epoch stats below)
[2024-10-10 00:35:58,476][train][INFO] - {"epoch": 1477, "train_loss": "0.454", "train_ntokens": "261413", "train_nsentences": "1750.04", "train_wps": "96124.9", "train_ups": "0.37", "train_wpb": "261413", "train_bsz": "1750", "train_num_updates": "70864", "train_lr": "0.000447196", "train_gnorm": "0.392", "train_loss_scale": "4", "train_train_wall": "66", "train_gb_free": "40", "train_wall": "25583"}
[2024-10-10 00:35:58,602][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 00:35:58,606][fairseq.trainer][INFO] - begin training epoch 1478
[2024-10-10 00:35:58,607][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 00:38:01,139][fairseq_cli.train][INFO] - end of epoch 1478 (average epoch stats below)
[2024-10-10 00:38:01,156][train][INFO] - {"epoch": 1478, "train_loss": "0.453", "train_ntokens": "260238", "train_nsentences": "1750.04", "train_wps": "101839", "train_ups": "0.39", "train_wpb": "260238", "train_bsz": "1750", "train_num_updates": "70912", "train_lr": "0.00044713", "train_gnorm": "0.365", "train_loss_scale": "4", "train_train_wall": "42", "train_gb_free": "40.1", "train_wall": "25706"}
[2024-10-10 00:38:01,256][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 00:38:01,266][fairseq.trainer][INFO] - begin training epoch 1479
[2024-10-10 00:38:01,266][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 00:40:09,757][fairseq_cli.train][INFO] - end of epoch 1479 (average epoch stats below)
[2024-10-10 00:40:09,763][train][INFO] - {"epoch": 1479, "train_loss": "0.45", "train_ntokens": "260864", "train_nsentences": "1750.04", "train_wps": "97367", "train_ups": "0.37", "train_wpb": "260864", "train_bsz": "1750", "train_num_updates": "70960", "train_lr": "0.000447065", "train_gnorm": "0.359", "train_loss_scale": "4", "train_train_wall": "47", "train_gb_free": "39.2", "train_wall": "25834"}
[2024-10-10 00:40:09,859][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 00:40:09,864][fairseq.trainer][INFO] - begin training epoch 1480
[2024-10-10 00:40:09,865][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 00:42:14,627][train_inner][INFO] - {"epoch": 1480, "update": 1479.833, "loss": "0.454", "ntokens": "261114", "nsentences": "1735.8", "wps": "99859.2", "ups": "0.38", "wpb": "261114", "bsz": "1735.8", "num_updates": "71000", "lr": "0.000447011", "gnorm": "0.371", "loss_scale": "4", "train_wall": "230", "gb_free": "40.1", "wall": "25959"}
[2024-10-10 00:42:18,034][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 1480 @ 71008 updates
[2024-10-10 00:42:18,034][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-10 00:42:21,462][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-10 00:42:21,465][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 1480 @ 71008 updates, score None) (writing took 3.4310124153271317 seconds)
[2024-10-10 00:42:21,465][fairseq_cli.train][INFO] - end of epoch 1480 (average epoch stats below)
[2024-10-10 00:42:21,467][train][INFO] - {"epoch": 1480, "train_loss": "0.456", "train_ntokens": "260868", "train_nsentences": "1750.04", "train_wps": "95077.8", "train_ups": "0.36", "train_wpb": "260868", "train_bsz": "1750", "train_num_updates": "71008", "train_lr": "0.000447", "train_gnorm": "0.363", "train_loss_scale": "4", "train_train_wall": "63", "train_gb_free": "39.1", "train_wall": "25966"}
[2024-10-10 00:42:21,544][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 00:42:21,549][fairseq.trainer][INFO] - begin training epoch 1481
[2024-10-10 00:42:21,549][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 00:44:28,660][fairseq_cli.train][INFO] - end of epoch 1481 (average epoch stats below)
[2024-10-10 00:44:28,674][train][INFO] - {"epoch": 1481, "train_loss": "0.453", "train_ntokens": "260791", "train_nsentences": "1750.04", "train_wps": "98410.3", "train_ups": "0.38", "train_wpb": "260791", "train_bsz": "1750", "train_num_updates": "71056", "train_lr": "0.000446935", "train_gnorm": "0.379", "train_loss_scale": "4", "train_train_wall": "46", "train_gb_free": "39.6", "train_wall": "26093"}
[2024-10-10 00:44:28,818][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 00:44:28,837][fairseq.trainer][INFO] - begin training epoch 1482
[2024-10-10 00:44:28,838][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 00:46:36,401][fairseq_cli.train][INFO] - end of epoch 1482 (average epoch stats below)
[2024-10-10 00:46:36,419][train][INFO] - {"epoch": 1482, "train_loss": "0.455", "train_ntokens": "260635", "train_nsentences": "1750.04", "train_wps": "97947.8", "train_ups": "0.38", "train_wpb": "260635", "train_bsz": "1750", "train_num_updates": "71104", "train_lr": "0.00044687", "train_gnorm": "0.356", "train_loss_scale": "4", "train_train_wall": "53", "train_gb_free": "39.7", "train_wall": "26221"}
[2024-10-10 00:46:36,542][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 00:46:36,570][fairseq.trainer][INFO] - begin training epoch 1483
[2024-10-10 00:46:36,571][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 00:48:42,378][fairseq_cli.train][INFO] - end of epoch 1483 (average epoch stats below)
[2024-10-10 00:48:42,383][train][INFO] - {"epoch": 1483, "train_loss": "0.457", "train_ntokens": "261016", "train_nsentences": "1750.04", "train_wps": "99466.6", "train_ups": "0.38", "train_wpb": "261016", "train_bsz": "1750", "train_num_updates": "71152", "train_lr": "0.000446804", "train_gnorm": "0.37", "train_loss_scale": "4", "train_train_wall": "39", "train_gb_free": "40.1", "train_wall": "26347"}
[2024-10-10 00:48:42,502][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 00:48:42,519][fairseq.trainer][INFO] - begin training epoch 1484
[2024-10-10 00:48:42,519][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 00:50:52,223][train_inner][INFO] - {"epoch": 1484, "update": 1484.0, "loss": "0.454", "ntokens": "260695", "nsentences": "1756.7", "wps": "100734", "ups": "0.39", "wpb": "260694", "bsz": "1756.7", "num_updates": "71200", "lr": "0.000446739", "gnorm": "0.365", "loss_scale": "4", "train_wall": "197", "gb_free": "39.9", "wall": "26477"}
[2024-10-10 00:50:52,227][fairseq_cli.train][INFO] - end of epoch 1484 (average epoch stats below)
[2024-10-10 00:50:52,228][train][INFO] - {"epoch": 1484, "train_loss": "0.448", "train_ntokens": "260526", "train_nsentences": "1750.04", "train_wps": "96311.7", "train_ups": "0.37", "train_wpb": "260526", "train_bsz": "1750", "train_num_updates": "71200", "train_lr": "0.000446739", "train_gnorm": "0.358", "train_loss_scale": "4", "train_train_wall": "55", "train_gb_free": "39.9", "train_wall": "26477"}
[2024-10-10 00:50:52,284][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 00:50:52,287][fairseq.trainer][INFO] - begin training epoch 1485
[2024-10-10 00:50:52,288][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 00:53:02,722][fairseq_cli.train][INFO] - end of epoch 1485 (average epoch stats below)
[2024-10-10 00:53:02,725][train][INFO] - {"epoch": 1485, "train_loss": "0.449", "train_ntokens": "261148", "train_nsentences": "1750.04", "train_wps": "96059.9", "train_ups": "0.37", "train_wpb": "261148", "train_bsz": "1750", "train_num_updates": "71248", "train_lr": "0.000446674", "train_gnorm": "0.351", "train_loss_scale": "4", "train_train_wall": "47", "train_gb_free": "39.6", "train_wall": "26607"}
[2024-10-10 00:53:02,776][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 00:53:02,782][fairseq.trainer][INFO] - begin training epoch 1486
[2024-10-10 00:53:02,782][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 00:55:10,200][fairseq_cli.train][INFO] - end of epoch 1486 (average epoch stats below)
[2024-10-10 00:55:10,215][train][INFO] - {"epoch": 1486, "train_loss": "0.459", "train_ntokens": "260768", "train_nsentences": "1750.04", "train_wps": "98192.6", "train_ups": "0.38", "train_wpb": "260768", "train_bsz": "1750", "train_num_updates": "71296", "train_lr": "0.000446609", "train_gnorm": "0.354", "train_loss_scale": "4", "train_train_wall": "48", "train_gb_free": "39.9", "train_wall": "26735"}
[2024-10-10 00:55:10,334][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 00:55:10,337][fairseq.trainer][INFO] - begin training epoch 1487
[2024-10-10 00:55:10,338][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 00:57:20,953][fairseq_cli.train][INFO] - end of epoch 1487 (average epoch stats below)
[2024-10-10 00:57:20,975][train][INFO] - {"epoch": 1487, "train_loss": "0.453", "train_ntokens": "260478", "train_nsentences": "1750.04", "train_wps": "95628.8", "train_ups": "0.37", "train_wpb": "260478", "train_bsz": "1750", "train_num_updates": "71344", "train_lr": "0.000446543", "train_gnorm": "0.357", "train_loss_scale": "4", "train_train_wall": "60", "train_gb_free": "39.7", "train_wall": "26865"}
[2024-10-10 00:57:21,078][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 00:57:21,093][fairseq.trainer][INFO] - begin training epoch 1488
[2024-10-10 00:57:21,093][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 00:59:26,869][fairseq_cli.train][INFO] - end of epoch 1488 (average epoch stats below)
[2024-10-10 00:59:26,873][train][INFO] - {"epoch": 1488, "train_loss": "0.46", "train_ntokens": "260431", "train_nsentences": "1750.04", "train_wps": "99294.5", "train_ups": "0.38", "train_wpb": "260431", "train_bsz": "1750", "train_num_updates": "71392", "train_lr": "0.000446478", "train_gnorm": "0.372", "train_loss_scale": "4", "train_train_wall": "52", "train_gb_free": "40.1", "train_wall": "26991"}
[2024-10-10 00:59:26,958][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 00:59:26,975][fairseq.trainer][INFO] - begin training epoch 1489
[2024-10-10 00:59:26,975][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 01:01:01,066][train_inner][INFO] - {"epoch": 1489, "update": 1488.167, "loss": "0.455", "ntokens": "260699", "nsentences": "1757.04", "wps": "85638.2", "ups": "0.33", "wpb": "260699", "bsz": "1757", "num_updates": "71400", "lr": "0.000446467", "gnorm": "0.362", "loss_scale": "4", "train_wall": "227", "gb_free": "40.1", "wall": "27085"}
[2024-10-10 01:01:09,957][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2024-10-10 01:01:35,418][fairseq_cli.train][INFO] - end of epoch 1489 (average epoch stats below)
[2024-10-10 01:01:35,421][train][INFO] - {"epoch": 1489, "train_loss": "0.454", "train_ntokens": "260816", "train_nsentences": "1733.43", "train_wps": "95363.6", "train_ups": "0.37", "train_wpb": "260816", "train_bsz": "1733.4", "train_num_updates": "71439", "train_lr": "0.000446414", "train_gnorm": "0.393", "train_loss_scale": "2", "train_train_wall": "53", "train_gb_free": "39.7", "train_wall": "27120"}
[2024-10-10 01:01:35,530][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 01:01:35,533][fairseq.trainer][INFO] - begin training epoch 1490
[2024-10-10 01:01:35,534][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 01:03:43,492][fairseq_cli.train][INFO] - end of epoch 1490 (average epoch stats below)
[2024-10-10 01:03:43,495][train][INFO] - {"epoch": 1490, "train_loss": "0.445", "train_ntokens": "260572", "train_nsentences": "1750.04", "train_wps": "97661.5", "train_ups": "0.37", "train_wpb": "260572", "train_bsz": "1750", "train_num_updates": "71487", "train_lr": "0.000446349", "train_gnorm": "0.359", "train_loss_scale": "2", "train_train_wall": "60", "train_gb_free": "39.6", "train_wall": "27248"}
[2024-10-10 01:03:43,553][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 01:03:43,557][fairseq.trainer][INFO] - begin training epoch 1491
[2024-10-10 01:03:43,557][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 01:05:48,877][fairseq_cli.train][INFO] - end of epoch 1491 (average epoch stats below)
[2024-10-10 01:05:48,889][train][INFO] - {"epoch": 1491, "train_loss": "0.463", "train_ntokens": "260282", "train_nsentences": "1750.04", "train_wps": "99641.6", "train_ups": "0.38", "train_wpb": "260282", "train_bsz": "1750", "train_num_updates": "71535", "train_lr": "0.000446284", "train_gnorm": "0.342", "train_loss_scale": "2", "train_train_wall": "51", "train_gb_free": "39.6", "train_wall": "27373"}
[2024-10-10 01:05:48,980][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 01:05:48,993][fairseq.trainer][INFO] - begin training epoch 1492
[2024-10-10 01:05:48,994][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 01:07:54,942][fairseq_cli.train][INFO] - end of epoch 1492 (average epoch stats below)
[2024-10-10 01:07:54,945][train][INFO] - {"epoch": 1492, "train_loss": "0.452", "train_ntokens": "260853", "train_nsentences": "1750.04", "train_wps": "99331.2", "train_ups": "0.38", "train_wpb": "260852", "train_bsz": "1750", "train_num_updates": "71583", "train_lr": "0.000446219", "train_gnorm": "0.36", "train_loss_scale": "2", "train_train_wall": "55", "train_gb_free": "39.7", "train_wall": "27499"}
[2024-10-10 01:07:55,044][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 01:07:55,048][fairseq.trainer][INFO] - begin training epoch 1493
[2024-10-10 01:07:55,048][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 01:09:32,126][train_inner][INFO] - {"epoch": 1493, "update": 1492.354, "loss": "0.453", "ntokens": "260509", "nsentences": "1743.73", "wps": "101951", "ups": "0.39", "wpb": "260509", "bsz": "1743.7", "num_updates": "71600", "lr": "0.000446196", "gnorm": "0.362", "loss_scale": "2", "train_wall": "222", "gb_free": "39.6", "wall": "27597"}
[2024-10-10 01:10:00,698][fairseq_cli.train][INFO] - end of epoch 1493 (average epoch stats below)
[2024-10-10 01:10:00,700][train][INFO] - {"epoch": 1493, "train_loss": "0.455", "train_ntokens": "260691", "train_nsentences": "1750.04", "train_wps": "99508.1", "train_ups": "0.38", "train_wpb": "260691", "train_bsz": "1750", "train_num_updates": "71631", "train_lr": "0.000446154", "train_gnorm": "0.397", "train_loss_scale": "2", "train_train_wall": "45", "train_gb_free": "39.7", "train_wall": "27625"}
[2024-10-10 01:10:00,825][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 01:10:00,830][fairseq.trainer][INFO] - begin training epoch 1494
[2024-10-10 01:10:00,830][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 01:12:10,146][fairseq_cli.train][INFO] - end of epoch 1494 (average epoch stats below)
[2024-10-10 01:12:10,154][train][INFO] - {"epoch": 1494, "train_loss": "0.451", "train_ntokens": "260808", "train_nsentences": "1750.04", "train_wps": "96708.3", "train_ups": "0.37", "train_wpb": "260808", "train_bsz": "1750", "train_num_updates": "71679", "train_lr": "0.000446088", "train_gnorm": "0.364", "train_loss_scale": "2", "train_train_wall": "59", "train_gb_free": "39.3", "train_wall": "27755"}
[2024-10-10 01:12:10,209][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 01:12:10,213][fairseq.trainer][INFO] - begin training epoch 1495
[2024-10-10 01:12:10,213][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 01:14:22,006][fairseq_cli.train][INFO] - end of epoch 1495 (average epoch stats below)
[2024-10-10 01:14:22,011][train][INFO] - {"epoch": 1495, "train_loss": "0.451", "train_ntokens": "260697", "train_nsentences": "1750.04", "train_wps": "94904.5", "train_ups": "0.36", "train_wpb": "260697", "train_bsz": "1750", "train_num_updates": "71727", "train_lr": "0.000446023", "train_gnorm": "0.398", "train_loss_scale": "2", "train_train_wall": "65", "train_gb_free": "40.3", "train_wall": "27886"}
[2024-10-10 01:14:22,073][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 01:14:22,078][fairseq.trainer][INFO] - begin training epoch 1496
[2024-10-10 01:14:22,078][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 01:16:25,800][fairseq_cli.train][INFO] - end of epoch 1496 (average epoch stats below)
[2024-10-10 01:16:25,804][train][INFO] - {"epoch": 1496, "train_loss": "0.455", "train_ntokens": "260702", "train_nsentences": "1750.04", "train_wps": "101089", "train_ups": "0.39", "train_wpb": "260702", "train_bsz": "1750", "train_num_updates": "71775", "train_lr": "0.000445958", "train_gnorm": "0.349", "train_loss_scale": "2", "train_train_wall": "36", "train_gb_free": "40.5", "train_wall": "28010"}
[2024-10-10 01:16:25,937][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 01:16:25,949][fairseq.trainer][INFO] - begin training epoch 1497
[2024-10-10 01:16:25,950][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 01:18:16,202][train_inner][INFO] - {"epoch": 1497, "update": 1496.521, "loss": "0.452", "ntokens": "261026", "nsentences": "1737.22", "wps": "99615.8", "ups": "0.38", "wpb": "261026", "bsz": "1737.2", "num_updates": "71800", "lr": "0.000445924", "gnorm": "0.372", "loss_scale": "2", "train_wall": "225", "gb_free": "39.2", "wall": "28121"}
[2024-10-10 01:18:33,669][fairseq_cli.train][INFO] - end of epoch 1497 (average epoch stats below)
[2024-10-10 01:18:33,675][train][INFO] - {"epoch": 1497, "train_loss": "0.454", "train_ntokens": "261112", "train_nsentences": "1750.04", "train_wps": "98021.1", "train_ups": "0.38", "train_wpb": "261112", "train_bsz": "1750", "train_num_updates": "71823", "train_lr": "0.000445893", "train_gnorm": "0.354", "train_loss_scale": "2", "train_train_wall": "61", "train_gb_free": "39.6", "train_wall": "28138"}
[2024-10-10 01:18:33,784][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 01:18:33,801][fairseq.trainer][INFO] - begin training epoch 1498
[2024-10-10 01:18:33,801][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 01:20:38,672][fairseq_cli.train][INFO] - end of epoch 1498 (average epoch stats below)
[2024-10-10 01:20:38,695][train][INFO] - {"epoch": 1498, "train_loss": "0.458", "train_ntokens": "260540", "train_nsentences": "1750.04", "train_wps": "100046", "train_ups": "0.38", "train_wpb": "260540", "train_bsz": "1750", "train_num_updates": "71871", "train_lr": "0.000445827", "train_gnorm": "0.365", "train_loss_scale": "2", "train_train_wall": "39", "train_gb_free": "39.7", "train_wall": "28263"}
[2024-10-10 01:20:38,808][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 01:20:38,844][fairseq.trainer][INFO] - begin training epoch 1499
[2024-10-10 01:20:38,844][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 01:22:43,510][fairseq_cli.train][INFO] - end of epoch 1499 (average epoch stats below)
[2024-10-10 01:22:43,527][train][INFO] - {"epoch": 1499, "train_loss": "0.457", "train_ntokens": "260724", "train_nsentences": "1750.04", "train_wps": "100266", "train_ups": "0.38", "train_wpb": "260724", "train_bsz": "1750", "train_num_updates": "71919", "train_lr": "0.000445762", "train_gnorm": "0.378", "train_loss_scale": "2", "train_train_wall": "51", "train_gb_free": "39.2", "train_wall": "28388"}
[2024-10-10 01:22:43,638][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 01:22:43,648][fairseq.trainer][INFO] - begin training epoch 1500
[2024-10-10 01:22:43,648][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 01:24:50,717][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 1500 @ 71967 updates
[2024-10-10 01:24:50,718][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-10 01:24:54,304][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-10 01:24:54,308][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 1500 @ 71967 updates, score None) (writing took 3.5909912763163447 seconds)
[2024-10-10 01:24:54,308][fairseq_cli.train][INFO] - end of epoch 1500 (average epoch stats below)
[2024-10-10 01:24:54,312][train][INFO] - {"epoch": 1500, "train_loss": "0.448", "train_ntokens": "260776", "train_nsentences": "1750.04", "train_wps": "95712.7", "train_ups": "0.37", "train_wpb": "260776", "train_bsz": "1750", "train_num_updates": "71967", "train_lr": "0.000445697", "train_gnorm": "0.37", "train_loss_scale": "2", "train_train_wall": "54", "train_gb_free": "39.3", "train_wall": "28519"}
[2024-10-10 01:24:54,369][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-10 01:24:54,397][fairseq.trainer][INFO] - begin training epoch 1501
[2024-10-10 01:24:54,397][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-10 01:26:41,998][train_inner][INFO] - {"epoch": 1501, "update": 1500.688, "loss": "0.454", "ntokens": "260571", "nsentences": "1763.25", "wps": "103036", "ups": "0.4", "wpb": "260571", "bsz": "1763.2", "num_updates": "72000", "lr": "0.000445652", "gnorm": "0.369", "loss_scale": "2", "train_wall": "196", "gb_free": "39.4", "wall": "28626"}
