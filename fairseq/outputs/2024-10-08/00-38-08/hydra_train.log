[2024-10-08 00:38:54,235][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:15777', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-08 00:38:54,600][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:19368', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-08 00:38:59,412][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-08 00:38:59,414][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-08 00:38:59,414][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-08 00:38:59,414][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-08 00:38:59,415][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-08 00:38:59,416][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-08 00:38:59,935][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-08 00:39:01,475][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:13979', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-08 00:39:01,603][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:11990', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-08 00:39:02,111][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:10042', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-08 00:39:02,170][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:12823', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-08 00:39:02,405][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:13508', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-08 00:39:03,777][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:10046', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-08 00:39:04,894][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-08 00:39:04,900][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-08 00:39:04,900][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-08 00:39:04,900][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-08 00:39:04,901][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-08 00:39:04,909][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-08 00:39:05,347][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-08 00:39:06,295][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-08 00:39:06,304][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-08 00:39:06,304][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-08 00:39:06,305][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-08 00:39:06,305][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-08 00:39:06,306][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-08 00:39:06,792][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-08 00:39:07,111][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-08 00:39:07,123][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-08 00:39:07,123][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-08 00:39:07,123][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-08 00:39:07,124][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-08 00:39:07,125][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-08 00:39:07,607][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-08 00:39:07,680][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-08 00:39:07,687][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-08 00:39:07,687][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-08 00:39:07,687][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-08 00:39:07,688][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-08 00:39:07,689][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-08 00:39:07,984][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-08 00:39:09,285][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-08 00:39:09,291][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-08 00:39:09,291][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-08 00:39:09,291][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-08 00:39:09,292][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-08 00:39:09,299][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-08 00:39:09,958][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-08 00:39:14,841][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-08 00:39:14,857][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-08 00:39:14,862][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-08 00:39:14,862][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-08 00:39:14,863][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-08 00:39:14,864][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-08 00:39:15,368][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-08 00:39:46,799][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-08 00:39:46,823][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-08 00:39:46,823][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-08 00:39:46,823][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-08 00:39:46,824][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-08 00:39:46,825][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-08 00:39:48,836][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-08 00:40:12,972][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-08 00:40:12,973][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-08 00:40:12,973][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-08 00:40:12,973][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-08 00:40:12,973][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-08 00:40:12,973][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-08 00:40:12,973][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-08 00:40:12,973][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-08 00:40:12,973][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-08 00:40:12,973][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-08 00:40:12,974][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-08 00:40:12,974][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-08 00:40:12,976][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-08 00:40:15,886][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-08 00:40:15,895][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-08 00:40:15,895][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-08 00:40:15,895][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-08 00:40:15,895][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-08 00:40:15,895][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-08 00:40:15,896][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-08 00:40:15,896][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-08 00:40:15,896][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-08 00:40:15,896][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-08 00:40:15,896][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-08 00:40:15,896][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-08 00:40:15,897][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-08 00:40:48,241][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 741 @ 35502 updates)
[2024-10-08 00:40:48,287][fairseq.trainer][INFO] - loading train data for epoch 741
[2024-10-08 00:40:49,639][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-08 00:40:51,471][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 741 @ 35502 updates)
[2024-10-08 00:40:51,473][fairseq.trainer][INFO] - loading train data for epoch 741
[2024-10-08 00:40:52,468][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-08 00:41:00,627][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 00:41:00,636][fairseq.trainer][INFO] - begin training epoch 741
[2024-10-08 00:41:00,637][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 00:41:05,684][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-08 00:41:05,685][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-08 00:41:05,685][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-08 00:41:05,685][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-08 00:41:05,685][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-08 00:41:05,685][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-08 00:41:05,685][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-08 00:41:05,685][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-08 00:41:05,685][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-08 00:41:05,686][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-08 00:41:05,686][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-08 00:41:05,686][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-08 00:41:05,687][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-08 00:41:16,786][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-08 00:41:16,786][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-08 00:41:16,786][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-08 00:41:16,786][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-08 00:41:16,786][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-08 00:41:16,786][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-08 00:41:16,786][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-08 00:41:16,786][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-08 00:41:16,786][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-08 00:41:16,786][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-08 00:41:16,786][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-08 00:41:16,787][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-08 00:41:16,788][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-08 00:41:18,708][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-08 00:41:18,719][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-08 00:41:18,720][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-08 00:41:18,720][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-08 00:41:18,720][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-08 00:41:18,721][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-08 00:41:18,721][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-08 00:41:18,731][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-08 00:41:18,731][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-08 00:41:18,731][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-08 00:41:18,732][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-08 00:41:18,732][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-08 00:41:18,741][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-08 00:41:29,268][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 00:41:29,285][fairseq.trainer][INFO] - begin training epoch 741
[2024-10-08 00:41:29,285][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 00:41:34,917][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 741 @ 35502 updates)
[2024-10-08 00:41:34,919][fairseq.trainer][INFO] - loading train data for epoch 741
[2024-10-08 00:41:35,699][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-08 00:41:46,603][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 00:41:46,610][fairseq.trainer][INFO] - begin training epoch 741
[2024-10-08 00:41:46,612][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 00:41:57,904][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 741 @ 35502 updates)
[2024-10-08 00:41:57,934][fairseq.trainer][INFO] - loading train data for epoch 741
[2024-10-08 00:41:58,373][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-08 00:42:04,971][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 00:42:04,981][fairseq.trainer][INFO] - begin training epoch 741
[2024-10-08 00:42:04,982][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 00:42:06,878][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-08 00:42:06,878][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-08 00:42:06,878][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-08 00:42:06,878][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-08 00:42:06,879][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-08 00:42:06,879][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-08 00:42:06,879][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-08 00:42:06,879][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-08 00:42:06,879][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-08 00:42:06,879][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-08 00:42:06,879][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-08 00:42:06,879][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-08 00:42:06,880][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-08 00:42:24,665][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 741 @ 35502 updates)
[2024-10-08 00:42:24,687][fairseq.trainer][INFO] - loading train data for epoch 741
[2024-10-08 00:42:26,179][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-08 00:42:27,749][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 741 @ 35502 updates)
[2024-10-08 00:42:27,751][fairseq.trainer][INFO] - loading train data for epoch 741
[2024-10-08 00:42:28,762][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-08 00:42:38,598][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 00:42:38,608][fairseq.trainer][INFO] - begin training epoch 741
[2024-10-08 00:42:38,609][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 00:42:41,371][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-08 00:42:41,379][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-08 00:42:41,379][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-08 00:42:41,379][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-08 00:42:41,379][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-08 00:42:41,379][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-08 00:42:41,379][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-08 00:42:41,379][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-08 00:42:41,379][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-08 00:42:41,379][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-08 00:42:41,380][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-08 00:42:41,380][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-08 00:42:41,381][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-08 00:42:45,127][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 00:42:45,145][fairseq.trainer][INFO] - begin training epoch 741
[2024-10-08 00:42:45,180][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 00:42:48,414][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-08 00:42:48,414][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-08 00:42:48,414][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-08 00:42:48,414][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-08 00:42:48,414][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-08 00:42:48,414][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-08 00:42:48,414][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-08 00:42:48,414][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-08 00:42:48,414][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-08 00:42:48,414][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-08 00:42:48,415][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-08 00:42:48,415][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-08 00:42:48,416][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-08 00:43:16,989][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 741 @ 35502 updates)
[2024-10-08 00:43:16,997][fairseq.trainer][INFO] - loading train data for epoch 741
[2024-10-08 00:43:17,965][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-08 00:43:19,358][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 741 @ 35502 updates)
[2024-10-08 00:43:19,361][fairseq.trainer][INFO] - loading train data for epoch 741
[2024-10-08 00:43:19,984][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-08 00:43:25,381][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 00:43:25,385][fairseq.trainer][INFO] - begin training epoch 741
[2024-10-08 00:43:25,386][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 00:43:25,894][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 00:43:25,923][fairseq.trainer][INFO] - begin training epoch 741
[2024-10-08 00:43:25,924][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 00:48:40,796][fairseq.trainer][WARNING] - OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 
[2024-10-08 00:48:40,844][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 1            |        cudaMalloc retries: 2         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   6775 MiB |   7128 MiB |  10509 MiB |   3733 MiB |
|       from large pool |   6724 MiB |   7077 MiB |  10329 MiB |   3605 MiB |
|       from small pool |     51 MiB |    102 MiB |    179 MiB |    128 MiB |
|---------------------------------------------------------------------------|
| Active memory         |   6775 MiB |   7128 MiB |  10509 MiB |   3733 MiB |
|       from large pool |   6724 MiB |   7077 MiB |  10329 MiB |   3605 MiB |
|       from small pool |     51 MiB |    102 MiB |    179 MiB |    128 MiB |
|---------------------------------------------------------------------------|
| Requested memory      |   6764 MiB |   7117 MiB |  10496 MiB |   3731 MiB |
|       from large pool |   6713 MiB |   7066 MiB |  10317 MiB |   3603 MiB |
|       from small pool |     50 MiB |    102 MiB |    179 MiB |    128 MiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   7332 MiB |   7332 MiB |   7382 MiB |  51200 KiB |
|       from large pool |   7278 MiB |   7278 MiB |   7278 MiB |      0 KiB |
|       from small pool |     54 MiB |    104 MiB |    104 MiB |  51200 KiB |
|---------------------------------------------------------------------------|
| Non-releasable memory | 204918 KiB | 305800 KiB |   3306 MiB |   3106 MiB |
|       from large pool | 201962 KiB | 301664 KiB |   3085 MiB |   2888 MiB |
|       from small pool |   2956 KiB |  16429 KiB |    221 MiB |    218 MiB |
|---------------------------------------------------------------------------|
| Allocations           |    1261    |    1421    |    1799    |     538    |
|       from large pool |     237    |     239    |     294    |      57    |
|       from small pool |    1024    |    1343    |    1505    |     481    |
|---------------------------------------------------------------------------|
| Active allocs         |    1261    |    1421    |    1799    |     538    |
|       from large pool |     237    |     239    |     294    |      57    |
|       from small pool |    1024    |    1343    |    1505    |     481    |
|---------------------------------------------------------------------------|
| GPU reserved segments |     105    |     129    |     130    |      25    |
|       from large pool |      78    |      78    |      78    |       0    |
|       from small pool |      27    |      52    |      52    |      25    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     160    |     162    |     339    |     179    |
|       from large pool |      62    |      64    |     111    |      49    |
|       from small pool |      98    |      99    |     228    |     130    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-08 00:48:40,845][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-08 00:48:40,855][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-08 00:48:40,856][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-08 00:48:40,856][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 4                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-08 00:48:40,856][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 5                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-08 00:48:40,856][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 6                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-08 00:48:40,857][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 7                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-08 00:48:40,857][fairseq.trainer][WARNING] - attempting to recover from OOM in forward/backward pass
[2024-10-08 00:51:15,685][fairseq_cli.train][INFO] - end of epoch 741 (average epoch stats below)
[2024-10-08 00:51:15,821][train][INFO] - {"epoch": 741, "train_loss": "0.608", "train_ntokens": "260399", "train_nsentences": "1750.04", "train_wps": "58093.5", "train_ups": "0.22", "train_wpb": "260399", "train_bsz": "1750", "train_num_updates": "35550", "train_lr": "0.000495177", "train_gnorm": "0.391", "train_loss_scale": "2", "train_train_wall": "157", "train_gb_free": "39.6", "train_wall": "663"}
[2024-10-08 00:51:16,292][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 00:51:16,306][fairseq.trainer][INFO] - begin training epoch 742
[2024-10-08 00:51:16,308][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 00:52:31,233][fairseq.trainer][WARNING] - OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 178.00 MiB. GPU 
[2024-10-08 00:52:31,250][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 1            |        cudaMalloc retries: 1         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   6267 MiB |   6268 MiB |   9311 MiB |   3044 MiB |
|       from large pool |   6217 MiB |   6217 MiB |   9139 MiB |   2921 MiB |
|       from small pool |     49 MiB |    102 MiB |    172 MiB |    122 MiB |
|---------------------------------------------------------------------------|
| Active memory         |   6267 MiB |   6268 MiB |   9311 MiB |   3044 MiB |
|       from large pool |   6217 MiB |   6217 MiB |   9139 MiB |   2921 MiB |
|       from small pool |     49 MiB |    102 MiB |    172 MiB |    122 MiB |
|---------------------------------------------------------------------------|
| Requested memory      |   6256 MiB |   6257 MiB |   9298 MiB |   3042 MiB |
|       from large pool |   6206 MiB |   6206 MiB |   9126 MiB |   2920 MiB |
|       from small pool |     49 MiB |    102 MiB |    171 MiB |    122 MiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   6440 MiB |   6490 MiB |   6490 MiB |  51200 KiB |
|       from large pool |   6386 MiB |   6386 MiB |   6386 MiB |      0 KiB |
|       from small pool |     54 MiB |    104 MiB |    104 MiB |  51200 KiB |
|---------------------------------------------------------------------------|
| Non-releasable memory | 176542 KiB | 298971 KiB |   3000 MiB |   2827 MiB |
|       from large pool | 172254 KiB | 294528 KiB |   2785 MiB |   2616 MiB |
|       from small pool |   4288 KiB |  16429 KiB |    215 MiB |    210 MiB |
|---------------------------------------------------------------------------|
| Allocations           |    1241    |    1421    |    1764    |     523    |
|       from large pool |     223    |     223    |     273    |      50    |
|       from small pool |    1018    |    1343    |    1491    |     473    |
|---------------------------------------------------------------------------|
| Active allocs         |    1241    |    1421    |    1764    |     523    |
|       from large pool |     223    |     223    |     273    |      50    |
|       from small pool |    1018    |    1343    |    1491    |     473    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      99    |     124    |     124    |      25    |
|       from large pool |      72    |      72    |      72    |       0    |
|       from small pool |      27    |      52    |      52    |      25    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     156    |     156    |     327    |     171    |
|       from large pool |      58    |      58    |     101    |      43    |
|       from small pool |      98    |      99    |     226    |     128    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-08 00:52:31,251][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-08 00:52:31,251][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-08 00:52:31,251][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-08 00:52:31,252][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 4                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-08 00:52:31,252][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 5                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-08 00:52:31,252][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 6                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-08 00:52:31,252][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 7                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-08 00:52:31,252][fairseq.trainer][WARNING] - attempting to recover from OOM in forward/backward pass
[2024-10-08 01:01:05,476][fairseq_cli.train][INFO] - end of epoch 742 (average epoch stats below)
[2024-10-08 01:01:05,504][train][INFO] - {"epoch": 742, "train_loss": "0.609", "train_ntokens": "260748", "train_nsentences": "1750.04", "train_wps": "21225.5", "train_ups": "0.08", "train_wpb": "260748", "train_bsz": "1750", "train_num_updates": "35598", "train_lr": "0.000495111", "train_gnorm": "0.392", "train_loss_scale": "2", "train_train_wall": "204", "train_gb_free": "40.5", "train_wall": "1253"}
[2024-10-08 01:01:05,672][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 01:01:05,712][fairseq.trainer][INFO] - begin training epoch 743
[2024-10-08 01:01:05,712][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 01:03:19,172][train_inner][INFO] - {"epoch": 743, "update": 742.042, "loss": "0.608", "ntokens": "260572", "nsentences": "1754.9", "wps": "27058.3", "ups": "0.1", "wpb": "260572", "bsz": "1754.9", "num_updates": "35600", "lr": "0.000495109", "gnorm": "0.391", "loss_scale": "2", "train_wall": "394", "gb_free": "40.3", "wall": "1386"}
[2024-10-08 01:04:24,092][fairseq_cli.train][INFO] - end of epoch 743 (average epoch stats below)
[2024-10-08 01:04:24,094][train][INFO] - {"epoch": 743, "train_loss": "0.612", "train_ntokens": "260737", "train_nsentences": "1750.04", "train_wps": "63021.7", "train_ups": "0.24", "train_wpb": "260737", "train_bsz": "1750", "train_num_updates": "35646", "train_lr": "0.000495046", "train_gnorm": "0.404", "train_loss_scale": "2", "train_train_wall": "97", "train_gb_free": "39.8", "train_wall": "1451"}
[2024-10-08 01:04:24,329][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 01:04:24,338][fairseq.trainer][INFO] - begin training epoch 744
[2024-10-08 01:04:24,338][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 01:06:49,726][fairseq_cli.train][INFO] - end of epoch 744 (average epoch stats below)
[2024-10-08 01:06:49,731][train][INFO] - {"epoch": 744, "train_loss": "0.603", "train_ntokens": "260578", "train_nsentences": "1750.04", "train_wps": "85884.7", "train_ups": "0.33", "train_wpb": "260578", "train_bsz": "1750", "train_num_updates": "35694", "train_lr": "0.000494981", "train_gnorm": "0.406", "train_loss_scale": "2", "train_train_wall": "67", "train_gb_free": "39.8", "train_wall": "1597"}
[2024-10-08 01:06:49,918][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 01:06:49,942][fairseq.trainer][INFO] - begin training epoch 745
[2024-10-08 01:06:49,943][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 01:09:24,784][fairseq_cli.train][INFO] - end of epoch 745 (average epoch stats below)
[2024-10-08 01:09:24,788][train][INFO] - {"epoch": 745, "train_loss": "0.605", "train_ntokens": "260786", "train_nsentences": "1750.04", "train_wps": "80730.7", "train_ups": "0.31", "train_wpb": "260786", "train_bsz": "1750", "train_num_updates": "35742", "train_lr": "0.000494916", "train_gnorm": "0.407", "train_loss_scale": "2", "train_train_wall": "58", "train_gb_free": "39.3", "train_wall": "1752"}
[2024-10-08 01:09:24,975][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 01:09:24,980][fairseq.trainer][INFO] - begin training epoch 746
[2024-10-08 01:09:24,981][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 01:12:00,172][fairseq_cli.train][INFO] - end of epoch 746 (average epoch stats below)
[2024-10-08 01:12:00,178][train][INFO] - {"epoch": 746, "train_loss": "0.605", "train_ntokens": "260770", "train_nsentences": "1750.04", "train_wps": "80553.2", "train_ups": "0.31", "train_wpb": "260770", "train_bsz": "1750", "train_num_updates": "35790", "train_lr": "0.000494851", "train_gnorm": "0.382", "train_loss_scale": "2", "train_train_wall": "70", "train_gb_free": "39.3", "train_wall": "1907"}
[2024-10-08 01:12:00,376][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 01:12:00,383][fairseq.trainer][INFO] - begin training epoch 747
[2024-10-08 01:12:00,384][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 01:14:29,470][train_inner][INFO] - {"epoch": 747, "update": 746.208, "loss": "0.605", "ntokens": "260723", "nsentences": "1747.78", "wps": "77793.5", "ups": "0.3", "wpb": "260723", "bsz": "1747.8", "num_updates": "35800", "lr": "0.000494837", "gnorm": "0.399", "loss_scale": "2", "train_wall": "304", "gb_free": "42.3", "wall": "2056"}
[2024-10-08 01:14:48,781][fairseq_cli.train][INFO] - end of epoch 747 (average epoch stats below)
[2024-10-08 01:14:48,790][train][INFO] - {"epoch": 747, "train_loss": "0.6", "train_ntokens": "260714", "train_nsentences": "1750.04", "train_wps": "74222.7", "train_ups": "0.28", "train_wpb": "260714", "train_bsz": "1750", "train_num_updates": "35838", "train_lr": "0.000494785", "train_gnorm": "0.424", "train_loss_scale": "2", "train_train_wall": "63", "train_gb_free": "41.5", "train_wall": "2076"}
[2024-10-08 01:14:49,033][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 01:14:49,038][fairseq.trainer][INFO] - begin training epoch 748
[2024-10-08 01:14:49,039][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 01:17:31,872][fairseq_cli.train][INFO] - end of epoch 748 (average epoch stats below)
[2024-10-08 01:17:31,877][train][INFO] - {"epoch": 748, "train_loss": "0.605", "train_ntokens": "260364", "train_nsentences": "1750.04", "train_wps": "76632.3", "train_ups": "0.29", "train_wpb": "260364", "train_bsz": "1750", "train_num_updates": "35886", "train_lr": "0.00049472", "train_gnorm": "0.38", "train_loss_scale": "2", "train_train_wall": "28", "train_gb_free": "39.6", "train_wall": "2239"}
[2024-10-08 01:17:32,012][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 01:17:32,017][fairseq.trainer][INFO] - begin training epoch 749
[2024-10-08 01:17:32,017][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 01:20:14,514][fairseq_cli.train][INFO] - end of epoch 749 (average epoch stats below)
[2024-10-08 01:20:14,519][train][INFO] - {"epoch": 749, "train_loss": "0.606", "train_ntokens": "260641", "train_nsentences": "1750.04", "train_wps": "76923.6", "train_ups": "0.3", "train_wpb": "260640", "train_bsz": "1750", "train_num_updates": "35934", "train_lr": "0.000494655", "train_gnorm": "0.396", "train_loss_scale": "2", "train_train_wall": "60", "train_gb_free": "40.3", "train_wall": "2402"}
[2024-10-08 01:20:14,700][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 01:20:14,705][fairseq.trainer][INFO] - begin training epoch 750
[2024-10-08 01:20:14,705][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 01:23:01,317][fairseq_cli.train][INFO] - end of epoch 750 (average epoch stats below)
[2024-10-08 01:23:01,321][train][INFO] - {"epoch": 750, "train_loss": "0.606", "train_ntokens": "260881", "train_nsentences": "1750.04", "train_wps": "75073.9", "train_ups": "0.29", "train_wpb": "260881", "train_bsz": "1750", "train_num_updates": "35982", "train_lr": "0.00049459", "train_gnorm": "0.432", "train_loss_scale": "2", "train_train_wall": "85", "train_gb_free": "39.8", "train_wall": "2568"}
[2024-10-08 01:23:01,497][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 01:23:01,503][fairseq.trainer][INFO] - begin training epoch 751
[2024-10-08 01:23:01,504][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 01:24:44,380][train_inner][INFO] - {"epoch": 751, "update": 750.375, "loss": "0.605", "ntokens": "260675", "nsentences": "1741.61", "wps": "84786.8", "ups": "0.33", "wpb": "260675", "bsz": "1741.6", "num_updates": "36000", "lr": "0.000494565", "gnorm": "0.409", "loss_scale": "2", "train_wall": "205", "gb_free": "39.3", "wall": "2671"}
[2024-10-08 01:25:14,793][fairseq_cli.train][INFO] - end of epoch 751 (average epoch stats below)
[2024-10-08 01:25:14,797][train][INFO] - {"epoch": 751, "train_loss": "0.607", "train_ntokens": "261037", "train_nsentences": "1750.04", "train_wps": "93876", "train_ups": "0.36", "train_wpb": "261037", "train_bsz": "1750", "train_num_updates": "36030", "train_lr": "0.000494524", "train_gnorm": "0.379", "train_loss_scale": "2", "train_train_wall": "44", "train_gb_free": "39.3", "train_wall": "2702"}
[2024-10-08 01:25:14,926][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 01:25:14,981][fairseq.trainer][INFO] - begin training epoch 752
[2024-10-08 01:25:14,982][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 01:27:23,766][fairseq_cli.train][INFO] - end of epoch 752 (average epoch stats below)
[2024-10-08 01:27:23,769][train][INFO] - {"epoch": 752, "train_loss": "0.602", "train_ntokens": "260793", "train_nsentences": "1750.04", "train_wps": "97077.3", "train_ups": "0.37", "train_wpb": "260793", "train_bsz": "1750", "train_num_updates": "36078", "train_lr": "0.000494459", "train_gnorm": "0.495", "train_loss_scale": "2", "train_train_wall": "56", "train_gb_free": "39.4", "train_wall": "2831"}
[2024-10-08 01:27:23,844][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 01:27:23,848][fairseq.trainer][INFO] - begin training epoch 753
[2024-10-08 01:27:23,848][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 01:29:34,064][fairseq_cli.train][INFO] - end of epoch 753 (average epoch stats below)
[2024-10-08 01:29:34,096][train][INFO] - {"epoch": 753, "train_loss": "0.599", "train_ntokens": "260674", "train_nsentences": "1750.04", "train_wps": "96019.9", "train_ups": "0.37", "train_wpb": "260674", "train_bsz": "1750", "train_num_updates": "36126", "train_lr": "0.000494394", "train_gnorm": "0.389", "train_loss_scale": "2", "train_train_wall": "47", "train_gb_free": "40.2", "train_wall": "2961"}
[2024-10-08 01:29:34,224][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 01:29:34,241][fairseq.trainer][INFO] - begin training epoch 754
[2024-10-08 01:29:34,242][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 01:31:41,530][fairseq_cli.train][INFO] - end of epoch 754 (average epoch stats below)
[2024-10-08 01:31:41,547][train][INFO] - {"epoch": 754, "train_loss": "0.601", "train_ntokens": "260830", "train_nsentences": "1750.04", "train_wps": "98233.4", "train_ups": "0.38", "train_wpb": "260830", "train_bsz": "1750", "train_num_updates": "36174", "train_lr": "0.000494329", "train_gnorm": "0.392", "train_loss_scale": "2", "train_train_wall": "51", "train_gb_free": "40", "train_wall": "3089"}
[2024-10-08 01:31:41,704][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 01:31:41,716][fairseq.trainer][INFO] - begin training epoch 755
[2024-10-08 01:31:41,716][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 01:33:36,922][train_inner][INFO] - {"epoch": 755, "update": 754.542, "loss": "0.601", "ntokens": "260956", "nsentences": "1750.47", "wps": "98004.5", "ups": "0.38", "wpb": "260956", "bsz": "1750.5", "num_updates": "36200", "lr": "0.000494293", "gnorm": "0.407", "loss_scale": "2", "train_wall": "222", "gb_free": "40.1", "wall": "3204"}
[2024-10-08 01:33:56,143][fairseq_cli.train][INFO] - end of epoch 755 (average epoch stats below)
[2024-10-08 01:33:56,146][train][INFO] - {"epoch": 755, "train_loss": "0.598", "train_ntokens": "260948", "train_nsentences": "1750.04", "train_wps": "93060.1", "train_ups": "0.36", "train_wpb": "260948", "train_bsz": "1750", "train_num_updates": "36222", "train_lr": "0.000494264", "train_gnorm": "0.397", "train_loss_scale": "2", "train_train_wall": "56", "train_gb_free": "39.6", "train_wall": "3223"}
[2024-10-08 01:33:56,269][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 01:33:56,273][fairseq.trainer][INFO] - begin training epoch 756
[2024-10-08 01:33:56,273][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 01:36:08,795][fairseq_cli.train][INFO] - end of epoch 756 (average epoch stats below)
[2024-10-08 01:36:08,805][train][INFO] - {"epoch": 756, "train_loss": "0.598", "train_ntokens": "260744", "train_nsentences": "1750.04", "train_wps": "94346.5", "train_ups": "0.36", "train_wpb": "260744", "train_bsz": "1750", "train_num_updates": "36270", "train_lr": "0.000494198", "train_gnorm": "0.396", "train_loss_scale": "2", "train_train_wall": "33", "train_gb_free": "39.2", "train_wall": "3356"}
[2024-10-08 01:36:08,969][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 01:36:08,978][fairseq.trainer][INFO] - begin training epoch 757
[2024-10-08 01:36:08,978][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 01:38:31,317][fairseq_cli.train][INFO] - end of epoch 757 (average epoch stats below)
[2024-10-08 01:38:31,323][train][INFO] - {"epoch": 757, "train_loss": "0.604", "train_ntokens": "260318", "train_nsentences": "1750.04", "train_wps": "87676.9", "train_ups": "0.34", "train_wpb": "260318", "train_bsz": "1750", "train_num_updates": "36318", "train_lr": "0.000494133", "train_gnorm": "0.414", "train_loss_scale": "2", "train_train_wall": "47", "train_gb_free": "40", "train_wall": "3498"}
[2024-10-08 01:38:31,454][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 01:38:31,459][fairseq.trainer][INFO] - begin training epoch 758
[2024-10-08 01:38:31,460][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 01:40:49,410][fairseq_cli.train][INFO] - end of epoch 758 (average epoch stats below)
[2024-10-08 01:40:49,414][train][INFO] - {"epoch": 758, "train_loss": "0.598", "train_ntokens": "260850", "train_nsentences": "1750.04", "train_wps": "90671.9", "train_ups": "0.35", "train_wpb": "260850", "train_bsz": "1750", "train_num_updates": "36366", "train_lr": "0.000494068", "train_gnorm": "0.387", "train_loss_scale": "2", "train_train_wall": "57", "train_gb_free": "39.2", "train_wall": "3636"}
[2024-10-08 01:40:49,524][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 01:40:49,531][fairseq.trainer][INFO] - begin training epoch 759
[2024-10-08 01:40:49,531][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 01:42:49,785][train_inner][INFO] - {"epoch": 759, "update": 758.708, "loss": "0.599", "ntokens": "260807", "nsentences": "1753.6", "wps": "94348.6", "ups": "0.36", "wpb": "260807", "bsz": "1753.6", "num_updates": "36400", "lr": "0.000494022", "gnorm": "0.398", "loss_scale": "2", "train_wall": "182", "gb_free": "40.5", "wall": "3757"}
[2024-10-08 01:43:03,312][fairseq_cli.train][INFO] - end of epoch 759 (average epoch stats below)
[2024-10-08 01:43:03,315][train][INFO] - {"epoch": 759, "train_loss": "0.599", "train_ntokens": "260810", "train_nsentences": "1750.04", "train_wps": "93495.4", "train_ups": "0.36", "train_wpb": "260810", "train_bsz": "1750", "train_num_updates": "36414", "train_lr": "0.000494003", "train_gnorm": "0.402", "train_loss_scale": "2", "train_train_wall": "36", "train_gb_free": "39.7", "train_wall": "3770"}
[2024-10-08 01:43:03,499][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 01:43:03,528][fairseq.trainer][INFO] - begin training epoch 760
[2024-10-08 01:43:03,528][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 01:45:20,499][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 760 @ 36462 updates
[2024-10-08 01:45:20,501][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-08 01:45:25,356][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-08 01:45:25,358][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 760 @ 36462 updates, score None) (writing took 4.859600414521992 seconds)
[2024-10-08 01:45:25,359][fairseq_cli.train][INFO] - end of epoch 760 (average epoch stats below)
[2024-10-08 01:45:25,361][train][INFO] - {"epoch": 760, "train_loss": "0.598", "train_ntokens": "260663", "train_nsentences": "1750.04", "train_wps": "88084.3", "train_ups": "0.34", "train_wpb": "260663", "train_bsz": "1750", "train_num_updates": "36462", "train_lr": "0.000493938", "train_gnorm": "0.375", "train_loss_scale": "2", "train_train_wall": "53", "train_gb_free": "39.7", "train_wall": "3912"}
[2024-10-08 01:45:25,490][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 01:45:25,520][fairseq.trainer][INFO] - begin training epoch 761
[2024-10-08 01:45:25,521][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 01:47:40,328][fairseq_cli.train][INFO] - end of epoch 761 (average epoch stats below)
[2024-10-08 01:47:40,341][train][INFO] - {"epoch": 761, "train_loss": "0.601", "train_ntokens": "260754", "train_nsentences": "1750.04", "train_wps": "92728.4", "train_ups": "0.36", "train_wpb": "260754", "train_bsz": "1750", "train_num_updates": "36510", "train_lr": "0.000493872", "train_gnorm": "0.425", "train_loss_scale": "2", "train_train_wall": "43", "train_gb_free": "39.6", "train_wall": "4047"}
[2024-10-08 01:47:40,494][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 01:47:40,510][fairseq.trainer][INFO] - begin training epoch 762
[2024-10-08 01:47:40,512][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 01:50:16,358][fairseq_cli.train][INFO] - end of epoch 762 (average epoch stats below)
[2024-10-08 01:50:16,363][train][INFO] - {"epoch": 762, "train_loss": "0.601", "train_ntokens": "260840", "train_nsentences": "1750.04", "train_wps": "80248.4", "train_ups": "0.31", "train_wpb": "260840", "train_bsz": "1750", "train_num_updates": "36558", "train_lr": "0.000493807", "train_gnorm": "0.43", "train_loss_scale": "2", "train_train_wall": "74", "train_gb_free": "39.7", "train_wall": "4203"}
[2024-10-08 01:50:16,650][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 01:50:16,663][fairseq.trainer][INFO] - begin training epoch 763
[2024-10-08 01:50:16,664][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 01:53:43,331][train_inner][INFO] - {"epoch": 763, "update": 762.875, "loss": "0.599", "ntokens": "260428", "nsentences": "1756.21", "wps": "79699.6", "ups": "0.31", "wpb": "260428", "bsz": "1756.2", "num_updates": "36600", "lr": "0.00049375", "gnorm": "0.413", "loss_scale": "2", "train_wall": "277", "gb_free": "40.1", "wall": "4410"}
[2024-10-08 01:53:45,345][fairseq_cli.train][INFO] - end of epoch 763 (average epoch stats below)
[2024-10-08 01:53:45,347][train][INFO] - {"epoch": 763, "train_loss": "0.598", "train_ntokens": "260673", "train_nsentences": "1750.04", "train_wps": "59872.4", "train_ups": "0.23", "train_wpb": "260673", "train_bsz": "1750", "train_num_updates": "36606", "train_lr": "0.000493742", "train_gnorm": "0.401", "train_loss_scale": "2", "train_train_wall": "100", "train_gb_free": "39.7", "train_wall": "4412"}
[2024-10-08 01:53:45,568][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 01:53:45,585][fairseq.trainer][INFO] - begin training epoch 764
[2024-10-08 01:53:45,586][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 01:57:25,218][fairseq_cli.train][INFO] - end of epoch 764 (average epoch stats below)
[2024-10-08 01:57:25,236][train][INFO] - {"epoch": 764, "train_loss": "0.598", "train_ntokens": "260993", "train_nsentences": "1750.04", "train_wps": "56973.4", "train_ups": "0.22", "train_wpb": "260993", "train_bsz": "1750", "train_num_updates": "36654", "train_lr": "0.000493677", "train_gnorm": "0.385", "train_loss_scale": "2", "train_train_wall": "123", "train_gb_free": "39.2", "train_wall": "4632"}
[2024-10-08 01:57:25,489][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 01:57:25,525][fairseq.trainer][INFO] - begin training epoch 765
[2024-10-08 01:57:25,525][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 02:01:08,361][fairseq_cli.train][INFO] - end of epoch 765 (average epoch stats below)
[2024-10-08 02:01:08,365][train][INFO] - {"epoch": 765, "train_loss": "0.6", "train_ntokens": "260611", "train_nsentences": "1750.04", "train_wps": "56063.7", "train_ups": "0.22", "train_wpb": "260611", "train_bsz": "1750", "train_num_updates": "36702", "train_lr": "0.000493611", "train_gnorm": "0.404", "train_loss_scale": "2", "train_train_wall": "143", "train_gb_free": "39.7", "train_wall": "4855"}
[2024-10-08 02:01:08,608][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 02:01:08,615][fairseq.trainer][INFO] - begin training epoch 766
[2024-10-08 02:01:08,616][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 02:04:54,821][fairseq_cli.train][INFO] - end of epoch 766 (average epoch stats below)
[2024-10-08 02:04:54,840][train][INFO] - {"epoch": 766, "train_loss": "0.593", "train_ntokens": "260882", "train_nsentences": "1750.04", "train_wps": "55293.3", "train_ups": "0.21", "train_wpb": "260882", "train_bsz": "1750", "train_num_updates": "36750", "train_lr": "0.000493546", "train_gnorm": "0.401", "train_loss_scale": "2", "train_train_wall": "21", "train_gb_free": "39.3", "train_wall": "5082"}
[2024-10-08 02:04:55,093][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 02:04:55,097][fairseq.trainer][INFO] - begin training epoch 767
[2024-10-08 02:04:55,097][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 02:08:41,007][fairseq_cli.train][INFO] - end of epoch 767 (average epoch stats below)
[2024-10-08 02:08:41,015][train][INFO] - {"epoch": 767, "train_loss": "0.6", "train_ntokens": "260669", "train_nsentences": "1750.04", "train_wps": "55321.3", "train_ups": "0.21", "train_wpb": "260669", "train_bsz": "1750", "train_num_updates": "36798", "train_lr": "0.000493481", "train_gnorm": "0.418", "train_loss_scale": "2", "train_train_wall": "117", "train_gb_free": "39.1", "train_wall": "5308"}
[2024-10-08 02:08:41,269][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 02:08:41,315][fairseq.trainer][INFO] - begin training epoch 768
[2024-10-08 02:08:41,316][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 02:11:44,699][train_inner][INFO] - {"epoch": 768, "update": 767.042, "loss": "0.598", "ntokens": "260869", "nsentences": "1747.72", "wps": "48248.9", "ups": "0.18", "wpb": "260869", "bsz": "1747.7", "num_updates": "36800", "lr": "0.000493478", "gnorm": "0.401", "loss_scale": "2", "train_wall": "471", "gb_free": "39.3", "wall": "5492"}
[2024-10-08 02:12:37,613][fairseq_cli.train][INFO] - end of epoch 768 (average epoch stats below)
[2024-10-08 02:12:37,635][train][INFO] - {"epoch": 768, "train_loss": "0.593", "train_ntokens": "260368", "train_nsentences": "1750.04", "train_wps": "52821.7", "train_ups": "0.2", "train_wpb": "260368", "train_bsz": "1750", "train_num_updates": "36846", "train_lr": "0.000493416", "train_gnorm": "0.39", "train_loss_scale": "2", "train_train_wall": "92", "train_gb_free": "40.2", "train_wall": "5545"}
[2024-10-08 02:12:37,853][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 02:12:37,859][fairseq.trainer][INFO] - begin training epoch 769
[2024-10-08 02:12:37,859][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 02:15:22,651][fairseq_cli.train][INFO] - end of epoch 769 (average epoch stats below)
[2024-10-08 02:15:22,657][train][INFO] - {"epoch": 769, "train_loss": "0.595", "train_ntokens": "260827", "train_nsentences": "1750.04", "train_wps": "75868.1", "train_ups": "0.29", "train_wpb": "260827", "train_bsz": "1750", "train_num_updates": "36894", "train_lr": "0.000493351", "train_gnorm": "0.381", "train_loss_scale": "2", "train_train_wall": "48", "train_gb_free": "40.1", "train_wall": "5710"}
[2024-10-08 02:15:22,897][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 02:15:22,925][fairseq.trainer][INFO] - begin training epoch 770
[2024-10-08 02:15:22,925][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 02:18:06,403][fairseq_cli.train][INFO] - end of epoch 770 (average epoch stats below)
[2024-10-08 02:18:06,422][train][INFO] - {"epoch": 770, "train_loss": "0.594", "train_ntokens": "261067", "train_nsentences": "1750.04", "train_wps": "76521.1", "train_ups": "0.29", "train_wpb": "261067", "train_bsz": "1750", "train_num_updates": "36942", "train_lr": "0.000493285", "train_gnorm": "0.381", "train_loss_scale": "2", "train_train_wall": "79", "train_gb_free": "39.6", "train_wall": "5873"}
[2024-10-08 02:18:06,593][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 02:18:06,604][fairseq.trainer][INFO] - begin training epoch 771
[2024-10-08 02:18:06,604][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 02:20:42,393][fairseq_cli.train][INFO] - end of epoch 771 (average epoch stats below)
[2024-10-08 02:20:42,407][train][INFO] - {"epoch": 771, "train_loss": "0.593", "train_ntokens": "260639", "train_nsentences": "1750.04", "train_wps": "80205.7", "train_ups": "0.31", "train_wpb": "260639", "train_bsz": "1750", "train_num_updates": "36990", "train_lr": "0.00049322", "train_gnorm": "0.379", "train_loss_scale": "2", "train_train_wall": "44", "train_gb_free": "39.7", "train_wall": "6029"}
[2024-10-08 02:20:42,611][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 02:20:42,618][fairseq.trainer][INFO] - begin training epoch 772
[2024-10-08 02:20:42,618][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 02:22:27,102][train_inner][INFO] - {"epoch": 772, "update": 771.208, "loss": "0.593", "ntokens": "260640", "nsentences": "1756.17", "wps": "81155.1", "ups": "0.31", "wpb": "260640", "bsz": "1756.2", "num_updates": "37000", "lr": "0.000493207", "gnorm": "0.384", "loss_scale": "2", "train_wall": "223", "gb_free": "39.8", "wall": "6134"}
[2024-10-08 02:22:57,273][fairseq_cli.train][INFO] - end of epoch 772 (average epoch stats below)
[2024-10-08 02:22:57,282][train][INFO] - {"epoch": 772, "train_loss": "0.593", "train_ntokens": "260742", "train_nsentences": "1750.04", "train_wps": "92802.3", "train_ups": "0.36", "train_wpb": "260742", "train_bsz": "1750", "train_num_updates": "37038", "train_lr": "0.000493155", "train_gnorm": "0.421", "train_loss_scale": "2", "train_train_wall": "53", "train_gb_free": "40.7", "train_wall": "6164"}
[2024-10-08 02:22:57,528][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 02:22:57,534][fairseq.trainer][INFO] - begin training epoch 773
[2024-10-08 02:22:57,535][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 02:26:09,617][fairseq_cli.train][INFO] - end of epoch 773 (average epoch stats below)
[2024-10-08 02:26:09,628][train][INFO] - {"epoch": 773, "train_loss": "0.595", "train_ntokens": "260758", "train_nsentences": "1750.04", "train_wps": "65073.3", "train_ups": "0.25", "train_wpb": "260758", "train_bsz": "1750", "train_num_updates": "37086", "train_lr": "0.00049309", "train_gnorm": "0.426", "train_loss_scale": "2", "train_train_wall": "63", "train_gb_free": "39.1", "train_wall": "6357"}
[2024-10-08 02:26:09,822][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 02:26:09,836][fairseq.trainer][INFO] - begin training epoch 774
[2024-10-08 02:26:09,836][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 02:28:55,630][fairseq_cli.train][INFO] - end of epoch 774 (average epoch stats below)
[2024-10-08 02:28:55,638][train][INFO] - {"epoch": 774, "train_loss": "0.591", "train_ntokens": "260679", "train_nsentences": "1750.04", "train_wps": "75374.4", "train_ups": "0.29", "train_wpb": "260679", "train_bsz": "1750", "train_num_updates": "37134", "train_lr": "0.000493024", "train_gnorm": "0.431", "train_loss_scale": "2", "train_train_wall": "45", "train_gb_free": "39.3", "train_wall": "6523"}
[2024-10-08 02:28:55,850][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 02:28:55,854][fairseq.trainer][INFO] - begin training epoch 775
[2024-10-08 02:28:55,855][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 02:31:41,166][fairseq_cli.train][INFO] - end of epoch 775 (average epoch stats below)
[2024-10-08 02:31:41,173][train][INFO] - {"epoch": 775, "train_loss": "0.596", "train_ntokens": "260606", "train_nsentences": "1750.04", "train_wps": "75568.9", "train_ups": "0.29", "train_wpb": "260606", "train_bsz": "1750", "train_num_updates": "37182", "train_lr": "0.000492959", "train_gnorm": "0.381", "train_loss_scale": "2", "train_train_wall": "55", "train_gb_free": "40", "train_wall": "6688"}
[2024-10-08 02:31:41,395][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 02:31:41,404][fairseq.trainer][INFO] - begin training epoch 776
[2024-10-08 02:31:41,404][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 02:34:10,236][train_inner][INFO] - {"epoch": 776, "update": 775.375, "loss": "0.594", "ntokens": "260915", "nsentences": "1742.77", "wps": "74223.4", "ups": "0.28", "wpb": "260915", "bsz": "1742.8", "num_updates": "37200", "lr": "0.000492935", "gnorm": "0.41", "loss_scale": "2", "train_wall": "213", "gb_free": "40.6", "wall": "6837"}
[2024-10-08 02:34:26,425][fairseq_cli.train][INFO] - end of epoch 776 (average epoch stats below)
[2024-10-08 02:34:26,429][train][INFO] - {"epoch": 776, "train_loss": "0.597", "train_ntokens": "260898", "train_nsentences": "1750.04", "train_wps": "75782", "train_ups": "0.29", "train_wpb": "260898", "train_bsz": "1750", "train_num_updates": "37230", "train_lr": "0.000492894", "train_gnorm": "0.41", "train_loss_scale": "2", "train_train_wall": "36", "train_gb_free": "39.3", "train_wall": "6853"}
[2024-10-08 02:34:26,736][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 02:34:26,753][fairseq.trainer][INFO] - begin training epoch 777
[2024-10-08 02:34:26,753][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 02:37:37,727][fairseq_cli.train][INFO] - end of epoch 777 (average epoch stats below)
[2024-10-08 02:37:37,735][train][INFO] - {"epoch": 777, "train_loss": "0.592", "train_ntokens": "260683", "train_nsentences": "1750.04", "train_wps": "65410.1", "train_ups": "0.25", "train_wpb": "260683", "train_bsz": "1750", "train_num_updates": "37278", "train_lr": "0.000492829", "train_gnorm": "0.406", "train_loss_scale": "2", "train_train_wall": "80", "train_gb_free": "39.9", "train_wall": "7045"}
[2024-10-08 02:37:37,907][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 02:37:37,911][fairseq.trainer][INFO] - begin training epoch 778
[2024-10-08 02:37:37,912][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 02:40:09,347][fairseq_cli.train][INFO] - end of epoch 778 (average epoch stats below)
[2024-10-08 02:40:09,358][train][INFO] - {"epoch": 778, "train_loss": "0.591", "train_ntokens": "260943", "train_nsentences": "1750.04", "train_wps": "82609.3", "train_ups": "0.32", "train_wpb": "260943", "train_bsz": "1750", "train_num_updates": "37326", "train_lr": "0.000492764", "train_gnorm": "0.403", "train_loss_scale": "2", "train_train_wall": "71", "train_gb_free": "39.3", "train_wall": "7196"}
[2024-10-08 02:40:09,533][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 02:40:09,550][fairseq.trainer][INFO] - begin training epoch 779
[2024-10-08 02:40:09,550][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 02:43:26,108][fairseq_cli.train][INFO] - end of epoch 779 (average epoch stats below)
[2024-10-08 02:43:26,116][train][INFO] - {"epoch": 779, "train_loss": "0.595", "train_ntokens": "260847", "train_nsentences": "1750.04", "train_wps": "63635.8", "train_ups": "0.24", "train_wpb": "260847", "train_bsz": "1750", "train_num_updates": "37374", "train_lr": "0.000492698", "train_gnorm": "0.383", "train_loss_scale": "2", "train_train_wall": "84", "train_gb_free": "40.1", "train_wall": "7393"}
[2024-10-08 02:43:26,352][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 02:43:26,357][fairseq.trainer][INFO] - begin training epoch 780
[2024-10-08 02:43:26,357][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 02:46:22,946][train_inner][INFO] - {"epoch": 780, "update": 779.542, "loss": "0.594", "ntokens": "260658", "nsentences": "1754.55", "wps": "71149.5", "ups": "0.27", "wpb": "260658", "bsz": "1754.5", "num_updates": "37400", "lr": "0.000492663", "gnorm": "0.403", "loss_scale": "2", "train_wall": "328", "gb_free": "39.4", "wall": "7570"}
[2024-10-08 02:46:44,344][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 780 @ 37422 updates
[2024-10-08 02:46:44,344][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-08 02:46:48,179][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-08 02:46:48,195][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 780 @ 37422 updates, score None) (writing took 3.8512530270963907 seconds)
[2024-10-08 02:46:48,195][fairseq_cli.train][INFO] - end of epoch 780 (average epoch stats below)
[2024-10-08 02:46:48,198][train][INFO] - {"epoch": 780, "train_loss": "0.598", "train_ntokens": "260517", "train_nsentences": "1750.04", "train_wps": "61881", "train_ups": "0.24", "train_wpb": "260517", "train_bsz": "1750", "train_num_updates": "37422", "train_lr": "0.000492633", "train_gnorm": "0.419", "train_loss_scale": "2", "train_train_wall": "96", "train_gb_free": "40.4", "train_wall": "7595"}
[2024-10-08 02:46:48,443][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 02:46:48,452][fairseq.trainer][INFO] - begin training epoch 781
[2024-10-08 02:46:48,453][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 02:49:57,793][fairseq_cli.train][INFO] - end of epoch 781 (average epoch stats below)
[2024-10-08 02:49:57,801][train][INFO] - {"epoch": 781, "train_loss": "0.589", "train_ntokens": "260594", "train_nsentences": "1750.04", "train_wps": "65973.3", "train_ups": "0.25", "train_wpb": "260594", "train_bsz": "1750", "train_num_updates": "37470", "train_lr": "0.000492568", "train_gnorm": "0.42", "train_loss_scale": "2", "train_train_wall": "89", "train_gb_free": "39.3", "train_wall": "7785"}
[2024-10-08 02:49:58,059][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 02:49:58,072][fairseq.trainer][INFO] - begin training epoch 782
[2024-10-08 02:49:58,073][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 02:52:56,304][fairseq_cli.train][INFO] - end of epoch 782 (average epoch stats below)
[2024-10-08 02:52:56,320][train][INFO] - {"epoch": 782, "train_loss": "0.592", "train_ntokens": "260436", "train_nsentences": "1750.04", "train_wps": "70026.9", "train_ups": "0.27", "train_wpb": "260436", "train_bsz": "1750", "train_num_updates": "37518", "train_lr": "0.000492503", "train_gnorm": "0.371", "train_loss_scale": "2", "train_train_wall": "79", "train_gb_free": "39.9", "train_wall": "7963"}
[2024-10-08 02:52:56,506][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 02:52:56,515][fairseq.trainer][INFO] - begin training epoch 783
[2024-10-08 02:52:56,516][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 02:56:15,352][fairseq_cli.train][INFO] - end of epoch 783 (average epoch stats below)
[2024-10-08 02:56:15,369][train][INFO] - {"epoch": 783, "train_loss": "0.588", "train_ntokens": "260777", "train_nsentences": "1750.04", "train_wps": "62886.6", "train_ups": "0.24", "train_wpb": "260777", "train_bsz": "1750", "train_num_updates": "37566", "train_lr": "0.000492438", "train_gnorm": "0.387", "train_loss_scale": "4", "train_train_wall": "79", "train_gb_free": "39.6", "train_wall": "8162"}
[2024-10-08 02:56:15,610][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 02:56:15,627][fairseq.trainer][INFO] - begin training epoch 784
[2024-10-08 02:56:15,628][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 02:59:25,501][train_inner][INFO] - {"epoch": 784, "update": 783.708, "loss": "0.59", "ntokens": "260851", "nsentences": "1736.58", "wps": "66668.8", "ups": "0.26", "wpb": "260851", "bsz": "1736.6", "num_updates": "37600", "lr": "0.000492391", "gnorm": "0.402", "loss_scale": "4", "train_wall": "330", "gb_free": "39.6", "wall": "8353"}
[2024-10-08 02:59:37,073][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2024-10-08 02:59:38,253][fairseq_cli.train][INFO] - end of epoch 784 (average epoch stats below)
[2024-10-08 02:59:38,259][train][INFO] - {"epoch": 784, "train_loss": "0.59", "train_ntokens": "260545", "train_nsentences": "1757.68", "train_wps": "60357.8", "train_ups": "0.23", "train_wpb": "260545", "train_bsz": "1757.7", "train_num_updates": "37613", "train_lr": "0.000492374", "train_gnorm": "0.44", "train_loss_scale": "2", "train_train_wall": "76", "train_gb_free": "39.8", "train_wall": "8365"}
[2024-10-08 02:59:38,428][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 02:59:38,435][fairseq.trainer][INFO] - begin training epoch 785
[2024-10-08 02:59:38,435][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 03:03:07,429][fairseq_cli.train][INFO] - end of epoch 785 (average epoch stats below)
[2024-10-08 03:03:07,436][train][INFO] - {"epoch": 785, "train_loss": "0.592", "train_ntokens": "260685", "train_nsentences": "1750.04", "train_wps": "59820.2", "train_ups": "0.23", "train_wpb": "260685", "train_bsz": "1750", "train_num_updates": "37661", "train_lr": "0.000492308", "train_gnorm": "0.486", "train_loss_scale": "2", "train_train_wall": "112", "train_gb_free": "45.1", "train_wall": "8574"}
[2024-10-08 03:03:07,707][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 03:03:07,719][fairseq.trainer][INFO] - begin training epoch 786
[2024-10-08 03:03:07,720][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 03:06:45,607][fairseq_cli.train][INFO] - end of epoch 786 (average epoch stats below)
[2024-10-08 03:06:45,619][train][INFO] - {"epoch": 786, "train_loss": "0.586", "train_ntokens": "260708", "train_nsentences": "1750.04", "train_wps": "57356.4", "train_ups": "0.22", "train_wpb": "260708", "train_bsz": "1750", "train_num_updates": "37709", "train_lr": "0.000492243", "train_gnorm": "0.381", "train_loss_scale": "2", "train_train_wall": "118", "train_gb_free": "39.4", "train_wall": "8793"}
[2024-10-08 03:06:45,857][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 03:06:45,878][fairseq.trainer][INFO] - begin training epoch 787
[2024-10-08 03:06:45,878][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 03:10:31,792][fairseq_cli.train][INFO] - end of epoch 787 (average epoch stats below)
[2024-10-08 03:10:31,812][train][INFO] - {"epoch": 787, "train_loss": "0.59", "train_ntokens": "260854", "train_nsentences": "1750.04", "train_wps": "55356.1", "train_ups": "0.21", "train_wpb": "260854", "train_bsz": "1750", "train_num_updates": "37757", "train_lr": "0.000492178", "train_gnorm": "0.394", "train_loss_scale": "2", "train_train_wall": "134", "train_gb_free": "40.1", "train_wall": "9019"}
[2024-10-08 03:10:32,035][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 03:10:32,062][fairseq.trainer][INFO] - begin training epoch 788
[2024-10-08 03:10:32,063][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 03:14:07,096][train_inner][INFO] - {"epoch": 788, "update": 787.896, "loss": "0.59", "ntokens": "260549", "nsentences": "1766.38", "wps": "59110.2", "ups": "0.23", "wpb": "260549", "bsz": "1766.4", "num_updates": "37800", "lr": "0.00049212", "gnorm": "0.42", "loss_scale": "2", "train_wall": "447", "gb_free": "40.5", "wall": "9234"}
[2024-10-08 03:14:08,614][fairseq_cli.train][INFO] - end of epoch 788 (average epoch stats below)
[2024-10-08 03:14:08,617][train][INFO] - {"epoch": 788, "train_loss": "0.589", "train_ntokens": "260710", "train_nsentences": "1750.04", "train_wps": "57721.2", "train_ups": "0.22", "train_wpb": "260710", "train_bsz": "1750", "train_num_updates": "37805", "train_lr": "0.000492113", "train_gnorm": "0.394", "train_loss_scale": "2", "train_train_wall": "72", "train_gb_free": "39.7", "train_wall": "9236"}
[2024-10-08 03:14:08,873][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 03:14:08,888][fairseq.trainer][INFO] - begin training epoch 789
[2024-10-08 03:14:08,889][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 03:16:49,279][fairseq_cli.train][INFO] - end of epoch 789 (average epoch stats below)
[2024-10-08 03:16:49,299][train][INFO] - {"epoch": 789, "train_loss": "0.59", "train_ntokens": "260791", "train_nsentences": "1750.04", "train_wps": "77911.5", "train_ups": "0.3", "train_wpb": "260791", "train_bsz": "1750", "train_num_updates": "37853", "train_lr": "0.000492048", "train_gnorm": "0.414", "train_loss_scale": "2", "train_train_wall": "34", "train_gb_free": "39.5", "train_wall": "9396"}
[2024-10-08 03:16:49,459][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 03:16:49,472][fairseq.trainer][INFO] - begin training epoch 790
[2024-10-08 03:16:49,473][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 03:19:16,505][fairseq_cli.train][INFO] - end of epoch 790 (average epoch stats below)
[2024-10-08 03:19:16,519][train][INFO] - {"epoch": 790, "train_loss": "0.594", "train_ntokens": "260707", "train_nsentences": "1750.04", "train_wps": "85004", "train_ups": "0.33", "train_wpb": "260707", "train_bsz": "1750", "train_num_updates": "37901", "train_lr": "0.000491982", "train_gnorm": "0.449", "train_loss_scale": "2", "train_train_wall": "69", "train_gb_free": "39.8", "train_wall": "9544"}
[2024-10-08 03:19:16,678][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 03:19:16,690][fairseq.trainer][INFO] - begin training epoch 791
[2024-10-08 03:19:16,690][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 03:21:37,684][fairseq_cli.train][INFO] - end of epoch 791 (average epoch stats below)
[2024-10-08 03:21:37,697][train][INFO] - {"epoch": 791, "train_loss": "0.58", "train_ntokens": "260575", "train_nsentences": "1750.04", "train_wps": "88601", "train_ups": "0.34", "train_wpb": "260575", "train_bsz": "1750", "train_num_updates": "37949", "train_lr": "0.000491917", "train_gnorm": "0.408", "train_loss_scale": "2", "train_train_wall": "48", "train_gb_free": "39.1", "train_wall": "9685"}
[2024-10-08 03:21:37,907][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 03:21:37,918][fairseq.trainer][INFO] - begin training epoch 792
[2024-10-08 03:21:37,918][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 03:23:49,043][fairseq_cli.train][INFO] - end of epoch 792 (average epoch stats below)
[2024-10-08 03:23:49,057][train][INFO] - {"epoch": 792, "train_loss": "0.589", "train_ntokens": "260717", "train_nsentences": "1750.04", "train_wps": "95270.3", "train_ups": "0.37", "train_wpb": "260717", "train_bsz": "1750", "train_num_updates": "37997", "train_lr": "0.000491852", "train_gnorm": "0.403", "train_loss_scale": "2", "train_train_wall": "54", "train_gb_free": "40.2", "train_wall": "9816"}
[2024-10-08 03:23:49,173][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 03:23:49,188][fairseq.trainer][INFO] - begin training epoch 793
[2024-10-08 03:23:49,189][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 03:25:16,989][train_inner][INFO] - {"epoch": 793, "update": 792.062, "loss": "0.588", "ntokens": "260480", "nsentences": "1746.86", "wps": "77768.7", "ups": "0.3", "wpb": "260480", "bsz": "1746.9", "num_updates": "38000", "lr": "0.000491848", "gnorm": "0.419", "loss_scale": "2", "train_wall": "222", "gb_free": "39.7", "wall": "9904"}
[2024-10-08 03:25:58,708][fairseq_cli.train][INFO] - end of epoch 793 (average epoch stats below)
[2024-10-08 03:25:58,711][train][INFO] - {"epoch": 793, "train_loss": "0.595", "train_ntokens": "260890", "train_nsentences": "1750.04", "train_wps": "96587.8", "train_ups": "0.37", "train_wpb": "260890", "train_bsz": "1750", "train_num_updates": "38045", "train_lr": "0.000491787", "train_gnorm": "0.418", "train_loss_scale": "2", "train_train_wall": "54", "train_gb_free": "39.6", "train_wall": "9946"}
[2024-10-08 03:25:58,894][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 03:25:58,901][fairseq.trainer][INFO] - begin training epoch 794
[2024-10-08 03:25:58,901][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 03:28:29,873][fairseq_cli.train][INFO] - end of epoch 794 (average epoch stats below)
[2024-10-08 03:28:29,883][train][INFO] - {"epoch": 794, "train_loss": "0.588", "train_ntokens": "260576", "train_nsentences": "1750.04", "train_wps": "82739.7", "train_ups": "0.32", "train_wpb": "260576", "train_bsz": "1750", "train_num_updates": "38093", "train_lr": "0.000491721", "train_gnorm": "0.419", "train_loss_scale": "2", "train_train_wall": "52", "train_gb_free": "39.6", "train_wall": "10097"}
[2024-10-08 03:28:30,052][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 03:28:30,061][fairseq.trainer][INFO] - begin training epoch 795
[2024-10-08 03:28:30,062][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 03:31:38,486][fairseq_cli.train][INFO] - end of epoch 795 (average epoch stats below)
[2024-10-08 03:31:38,496][train][INFO] - {"epoch": 795, "train_loss": "0.587", "train_ntokens": "260663", "train_nsentences": "1750.04", "train_wps": "66336.7", "train_ups": "0.25", "train_wpb": "260663", "train_bsz": "1750", "train_num_updates": "38141", "train_lr": "0.000491656", "train_gnorm": "0.398", "train_loss_scale": "2", "train_train_wall": "96", "train_gb_free": "40.2", "train_wall": "10286"}
[2024-10-08 03:31:38,739][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 03:31:38,745][fairseq.trainer][INFO] - begin training epoch 796
[2024-10-08 03:31:38,746][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 03:34:40,328][fairseq_cli.train][INFO] - end of epoch 796 (average epoch stats below)
[2024-10-08 03:34:40,337][train][INFO] - {"epoch": 796, "train_loss": "0.585", "train_ntokens": "261032", "train_nsentences": "1750.04", "train_wps": "68904.8", "train_ups": "0.26", "train_wpb": "261032", "train_bsz": "1750", "train_num_updates": "38189", "train_lr": "0.000491591", "train_gnorm": "0.376", "train_loss_scale": "2", "train_train_wall": "98", "train_gb_free": "39.7", "train_wall": "10467"}
[2024-10-08 03:34:40,511][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 03:34:40,515][fairseq.trainer][INFO] - begin training epoch 797
[2024-10-08 03:34:40,515][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 03:37:27,616][train_inner][INFO] - {"epoch": 797, "update": 796.229, "loss": "0.588", "ntokens": "260738", "nsentences": "1756.08", "wps": "71377.8", "ups": "0.27", "wpb": "260738", "bsz": "1756.1", "num_updates": "38200", "lr": "0.000491576", "gnorm": "0.402", "loss_scale": "2", "train_wall": "326", "gb_free": "40.1", "wall": "10635"}
[2024-10-08 03:37:49,284][fairseq_cli.train][INFO] - end of epoch 797 (average epoch stats below)
[2024-10-08 03:37:49,286][train][INFO] - {"epoch": 797, "train_loss": "0.588", "train_ntokens": "260729", "train_nsentences": "1750.04", "train_wps": "66235.7", "train_ups": "0.25", "train_wpb": "260730", "train_bsz": "1750", "train_num_updates": "38237", "train_lr": "0.000491526", "train_gnorm": "0.394", "train_loss_scale": "2", "train_train_wall": "63", "train_gb_free": "39.2", "train_wall": "10656"}
[2024-10-08 03:37:49,509][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 03:37:49,516][fairseq.trainer][INFO] - begin training epoch 798
[2024-10-08 03:37:49,517][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 03:40:14,063][fairseq_cli.train][INFO] - end of epoch 798 (average epoch stats below)
[2024-10-08 03:40:14,072][train][INFO] - {"epoch": 798, "train_loss": "0.588", "train_ntokens": "260735", "train_nsentences": "1750.04", "train_wps": "86442.2", "train_ups": "0.33", "train_wpb": "260735", "train_bsz": "1750", "train_num_updates": "38285", "train_lr": "0.000491461", "train_gnorm": "0.426", "train_loss_scale": "2", "train_train_wall": "55", "train_gb_free": "39.8", "train_wall": "10801"}
[2024-10-08 03:40:14,312][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 03:40:14,316][fairseq.trainer][INFO] - begin training epoch 799
[2024-10-08 03:40:14,317][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 03:43:09,217][fairseq_cli.train][INFO] - end of epoch 799 (average epoch stats below)
[2024-10-08 03:43:09,232][train][INFO] - {"epoch": 799, "train_loss": "0.587", "train_ntokens": "260908", "train_nsentences": "1750.04", "train_wps": "71499.6", "train_ups": "0.27", "train_wpb": "260908", "train_bsz": "1750", "train_num_updates": "38333", "train_lr": "0.000491395", "train_gnorm": "0.399", "train_loss_scale": "2", "train_train_wall": "76", "train_gb_free": "39.8", "train_wall": "10976"}
[2024-10-08 03:43:09,391][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 03:43:09,417][fairseq.trainer][INFO] - begin training epoch 800
[2024-10-08 03:43:09,417][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 03:46:19,441][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 800 @ 38381 updates
[2024-10-08 03:46:19,445][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-08 03:46:23,414][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-08 03:46:23,430][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 800 @ 38381 updates, score None) (writing took 3.9885423285886645 seconds)
[2024-10-08 03:46:23,430][fairseq_cli.train][INFO] - end of epoch 800 (average epoch stats below)
[2024-10-08 03:46:23,433][train][INFO] - {"epoch": 800, "train_loss": "0.585", "train_ntokens": "260410", "train_nsentences": "1750.04", "train_wps": "64365.8", "train_ups": "0.25", "train_wpb": "260410", "train_bsz": "1750", "train_num_updates": "38381", "train_lr": "0.00049133", "train_gnorm": "0.403", "train_loss_scale": "2", "train_train_wall": "86", "train_gb_free": "39.8", "train_wall": "11170"}
[2024-10-08 03:46:23,619][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 03:46:23,637][fairseq.trainer][INFO] - begin training epoch 801
[2024-10-08 03:46:23,637][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 03:48:33,340][train_inner][INFO] - {"epoch": 801, "update": 800.396, "loss": "0.587", "ntokens": "260898", "nsentences": "1753.53", "wps": "78383.1", "ups": "0.3", "wpb": "260898", "bsz": "1753.5", "num_updates": "38400", "lr": "0.000491304", "gnorm": "0.405", "loss_scale": "2", "train_wall": "262", "gb_free": "40.7", "wall": "11300"}
[2024-10-08 03:48:53,037][fairseq_cli.train][INFO] - end of epoch 801 (average epoch stats below)
[2024-10-08 03:48:53,041][train][INFO] - {"epoch": 801, "train_loss": "0.587", "train_ntokens": "260722", "train_nsentences": "1750.04", "train_wps": "83651.4", "train_ups": "0.32", "train_wpb": "260722", "train_bsz": "1750", "train_num_updates": "38429", "train_lr": "0.000491265", "train_gnorm": "0.39", "train_loss_scale": "2", "train_train_wall": "39", "train_gb_free": "39.3", "train_wall": "11320"}
[2024-10-08 03:48:53,259][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 03:48:53,279][fairseq.trainer][INFO] - begin training epoch 802
[2024-10-08 03:48:53,279][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 03:51:23,087][fairseq_cli.train][INFO] - end of epoch 802 (average epoch stats below)
[2024-10-08 03:51:23,113][train][INFO] - {"epoch": 802, "train_loss": "0.585", "train_ntokens": "260747", "train_nsentences": "1750.04", "train_wps": "83405.9", "train_ups": "0.32", "train_wpb": "260747", "train_bsz": "1750", "train_num_updates": "38477", "train_lr": "0.0004912", "train_gnorm": "0.428", "train_loss_scale": "2", "train_train_wall": "47", "train_gb_free": "39.3", "train_wall": "11470"}
[2024-10-08 03:51:23,245][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 03:51:23,250][fairseq.trainer][INFO] - begin training epoch 803
[2024-10-08 03:51:23,250][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 03:53:56,848][fairseq_cli.train][INFO] - end of epoch 803 (average epoch stats below)
[2024-10-08 03:53:56,855][train][INFO] - {"epoch": 803, "train_loss": "0.578", "train_ntokens": "260797", "train_nsentences": "1750.04", "train_wps": "81425.5", "train_ups": "0.31", "train_wpb": "260797", "train_bsz": "1750", "train_num_updates": "38525", "train_lr": "0.000491135", "train_gnorm": "0.38", "train_loss_scale": "2", "train_train_wall": "74", "train_gb_free": "39.6", "train_wall": "11624"}
[2024-10-08 03:53:57,001][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 03:53:57,013][fairseq.trainer][INFO] - begin training epoch 804
[2024-10-08 03:53:57,013][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 03:56:23,642][fairseq_cli.train][INFO] - end of epoch 804 (average epoch stats below)
[2024-10-08 03:56:23,649][train][INFO] - {"epoch": 804, "train_loss": "0.582", "train_ntokens": "260396", "train_nsentences": "1750.04", "train_wps": "85148.6", "train_ups": "0.33", "train_wpb": "260396", "train_bsz": "1750", "train_num_updates": "38573", "train_lr": "0.000491069", "train_gnorm": "0.404", "train_loss_scale": "2", "train_train_wall": "64", "train_gb_free": "39.4", "train_wall": "11771"}
[2024-10-08 03:56:23,787][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 03:56:23,800][fairseq.trainer][INFO] - begin training epoch 805
[2024-10-08 03:56:23,800][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 03:58:48,435][train_inner][INFO] - {"epoch": 805, "update": 804.562, "loss": "0.583", "ntokens": "260515", "nsentences": "1752.62", "wps": "84708.7", "ups": "0.33", "wpb": "260515", "bsz": "1752.6", "num_updates": "38600", "lr": "0.000491033", "gnorm": "0.4", "loss_scale": "2", "train_wall": "266", "gb_free": "40.8", "wall": "11915"}
[2024-10-08 03:59:03,708][fairseq_cli.train][INFO] - end of epoch 805 (average epoch stats below)
[2024-10-08 03:59:03,710][train][INFO] - {"epoch": 805, "train_loss": "0.585", "train_ntokens": "260858", "train_nsentences": "1750.04", "train_wps": "78229.3", "train_ups": "0.3", "train_wpb": "260858", "train_bsz": "1750", "train_num_updates": "38621", "train_lr": "0.000491004", "train_gnorm": "0.369", "train_loss_scale": "2", "train_train_wall": "80", "train_gb_free": "39.6", "train_wall": "11931"}
[2024-10-08 03:59:03,851][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 03:59:03,857][fairseq.trainer][INFO] - begin training epoch 806
[2024-10-08 03:59:03,858][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 04:02:12,503][fairseq_cli.train][INFO] - end of epoch 806 (average epoch stats below)
[2024-10-08 04:02:12,509][train][INFO] - {"epoch": 806, "train_loss": "0.588", "train_ntokens": "260894", "train_nsentences": "1750.04", "train_wps": "66330.2", "train_ups": "0.25", "train_wpb": "260894", "train_bsz": "1750", "train_num_updates": "38669", "train_lr": "0.000490939", "train_gnorm": "0.371", "train_loss_scale": "2", "train_train_wall": "43", "train_gb_free": "40.5", "train_wall": "12120"}
[2024-10-08 04:02:12,753][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 04:02:12,765][fairseq.trainer][INFO] - begin training epoch 807
[2024-10-08 04:02:12,766][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 04:05:30,204][fairseq_cli.train][INFO] - end of epoch 807 (average epoch stats below)
[2024-10-08 04:05:30,220][train][INFO] - {"epoch": 807, "train_loss": "0.584", "train_ntokens": "260584", "train_nsentences": "1750.04", "train_wps": "63265.4", "train_ups": "0.24", "train_wpb": "260584", "train_bsz": "1750", "train_num_updates": "38717", "train_lr": "0.000490874", "train_gnorm": "0.404", "train_loss_scale": "2", "train_train_wall": "65", "train_gb_free": "40.1", "train_wall": "12317"}
[2024-10-08 04:05:30,422][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 04:05:30,440][fairseq.trainer][INFO] - begin training epoch 808
[2024-10-08 04:05:30,441][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 04:08:43,653][fairseq_cli.train][INFO] - end of epoch 808 (average epoch stats below)
[2024-10-08 04:08:43,661][train][INFO] - {"epoch": 808, "train_loss": "0.589", "train_ntokens": "260598", "train_nsentences": "1750.04", "train_wps": "64665", "train_ups": "0.25", "train_wpb": "260598", "train_bsz": "1750", "train_num_updates": "38765", "train_lr": "0.000490808", "train_gnorm": "0.406", "train_loss_scale": "2", "train_train_wall": "59", "train_gb_free": "39.7", "train_wall": "12511"}
[2024-10-08 04:08:43,861][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 04:08:43,897][fairseq.trainer][INFO] - begin training epoch 809
[2024-10-08 04:08:43,898][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 04:11:26,175][train_inner][INFO] - {"epoch": 809, "update": 808.729, "loss": "0.587", "ntokens": "260774", "nsentences": "1739.62", "wps": "68830.1", "ups": "0.26", "wpb": "260774", "bsz": "1739.6", "num_updates": "38800", "lr": "0.000490761", "gnorm": "0.393", "loss_scale": "2", "train_wall": "219", "gb_free": "39.8", "wall": "12673"}
[2024-10-08 04:11:38,896][fairseq_cli.train][INFO] - end of epoch 809 (average epoch stats below)
[2024-10-08 04:11:38,898][train][INFO] - {"epoch": 809, "train_loss": "0.589", "train_ntokens": "260533", "train_nsentences": "1750.04", "train_wps": "71365.2", "train_ups": "0.27", "train_wpb": "260533", "train_bsz": "1750", "train_num_updates": "38813", "train_lr": "0.000490743", "train_gnorm": "0.4", "train_loss_scale": "2", "train_train_wall": "50", "train_gb_free": "39.7", "train_wall": "12686"}
[2024-10-08 04:11:39,111][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 04:11:39,119][fairseq.trainer][INFO] - begin training epoch 810
[2024-10-08 04:11:39,119][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 04:14:42,257][fairseq_cli.train][INFO] - end of epoch 810 (average epoch stats below)
[2024-10-08 04:14:42,262][train][INFO] - {"epoch": 810, "train_loss": "0.58", "train_ntokens": "260344", "train_nsentences": "1750.04", "train_wps": "68153.1", "train_ups": "0.26", "train_wpb": "260344", "train_bsz": "1750", "train_num_updates": "38861", "train_lr": "0.000490678", "train_gnorm": "0.385", "train_loss_scale": "2", "train_train_wall": "57", "train_gb_free": "39.4", "train_wall": "12869"}
[2024-10-08 04:14:42,509][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 04:14:42,512][fairseq.trainer][INFO] - begin training epoch 811
[2024-10-08 04:14:42,513][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 04:18:14,495][fairseq_cli.train][INFO] - end of epoch 811 (average epoch stats below)
[2024-10-08 04:18:14,499][train][INFO] - {"epoch": 811, "train_loss": "0.58", "train_ntokens": "260493", "train_nsentences": "1750.04", "train_wps": "58914.6", "train_ups": "0.23", "train_wpb": "260494", "train_bsz": "1750", "train_num_updates": "38909", "train_lr": "0.000490613", "train_gnorm": "0.386", "train_loss_scale": "2", "train_train_wall": "122", "train_gb_free": "39.3", "train_wall": "13082"}
[2024-10-08 04:18:14,705][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 04:18:14,744][fairseq.trainer][INFO] - begin training epoch 812
[2024-10-08 04:18:14,744][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 04:21:40,159][fairseq_cli.train][INFO] - end of epoch 812 (average epoch stats below)
[2024-10-08 04:21:40,166][train][INFO] - {"epoch": 812, "train_loss": "0.584", "train_ntokens": "260872", "train_nsentences": "1750.04", "train_wps": "60884.9", "train_ups": "0.23", "train_wpb": "260872", "train_bsz": "1750", "train_num_updates": "38957", "train_lr": "0.000490548", "train_gnorm": "0.432", "train_loss_scale": "2", "train_train_wall": "84", "train_gb_free": "40", "train_wall": "13287"}
[2024-10-08 04:21:40,382][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 04:21:40,398][fairseq.trainer][INFO] - begin training epoch 813
[2024-10-08 04:21:40,398][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 04:24:15,919][train_inner][INFO] - {"epoch": 813, "update": 812.896, "loss": "0.581", "ntokens": "260642", "nsentences": "1749.46", "wps": "67723.7", "ups": "0.26", "wpb": "260642", "bsz": "1749.5", "num_updates": "39000", "lr": "0.000490489", "gnorm": "0.403", "loss_scale": "2", "train_wall": "332", "gb_free": "39.4", "wall": "13443"}
[2024-10-08 04:24:17,307][fairseq_cli.train][INFO] - end of epoch 813 (average epoch stats below)
[2024-10-08 04:24:17,309][train][INFO] - {"epoch": 813, "train_loss": "0.578", "train_ntokens": "260690", "train_nsentences": "1750.04", "train_wps": "79630.5", "train_ups": "0.31", "train_wpb": "260690", "train_bsz": "1750", "train_num_updates": "39005", "train_lr": "0.000490482", "train_gnorm": "0.419", "train_loss_scale": "2", "train_train_wall": "59", "train_gb_free": "40.3", "train_wall": "13444"}
[2024-10-08 04:24:17,468][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 04:24:17,479][fairseq.trainer][INFO] - begin training epoch 814
[2024-10-08 04:24:17,479][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 04:26:34,841][fairseq_cli.train][INFO] - end of epoch 814 (average epoch stats below)
[2024-10-08 04:26:34,857][train][INFO] - {"epoch": 814, "train_loss": "0.574", "train_ntokens": "260977", "train_nsentences": "1750.04", "train_wps": "91074.9", "train_ups": "0.35", "train_wpb": "260977", "train_bsz": "1750", "train_num_updates": "39053", "train_lr": "0.000490417", "train_gnorm": "0.412", "train_loss_scale": "2", "train_train_wall": "31", "train_gb_free": "39.7", "train_wall": "13582"}
[2024-10-08 04:26:34,964][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 04:26:34,976][fairseq.trainer][INFO] - begin training epoch 815
[2024-10-08 04:26:34,976][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 04:28:51,596][fairseq_cli.train][INFO] - end of epoch 815 (average epoch stats below)
[2024-10-08 04:28:51,605][train][INFO] - {"epoch": 815, "train_loss": "0.575", "train_ntokens": "260920", "train_nsentences": "1750.04", "train_wps": "91587.6", "train_ups": "0.35", "train_wpb": "260920", "train_bsz": "1750", "train_num_updates": "39101", "train_lr": "0.000490352", "train_gnorm": "0.367", "train_loss_scale": "2", "train_train_wall": "51", "train_gb_free": "40.1", "train_wall": "13719"}
[2024-10-08 04:28:51,778][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 04:28:51,809][fairseq.trainer][INFO] - begin training epoch 816
[2024-10-08 04:28:51,810][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 04:31:10,779][fairseq_cli.train][INFO] - end of epoch 816 (average epoch stats below)
[2024-10-08 04:31:10,789][train][INFO] - {"epoch": 816, "train_loss": "0.587", "train_ntokens": "260829", "train_nsentences": "1750.04", "train_wps": "89953.5", "train_ups": "0.34", "train_wpb": "260829", "train_bsz": "1750", "train_num_updates": "39149", "train_lr": "0.000490287", "train_gnorm": "0.363", "train_loss_scale": "2", "train_train_wall": "61", "train_gb_free": "41.5", "train_wall": "13858"}
[2024-10-08 04:31:10,960][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 04:31:10,973][fairseq.trainer][INFO] - begin training epoch 817
[2024-10-08 04:31:10,974][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 04:33:36,755][fairseq_cli.train][INFO] - end of epoch 817 (average epoch stats below)
[2024-10-08 04:33:36,768][train][INFO] - {"epoch": 817, "train_loss": "0.581", "train_ntokens": "260823", "train_nsentences": "1750.04", "train_wps": "85764.4", "train_ups": "0.33", "train_wpb": "260823", "train_bsz": "1750", "train_num_updates": "39197", "train_lr": "0.000490221", "train_gnorm": "0.395", "train_loss_scale": "2", "train_train_wall": "54", "train_gb_free": "39.7", "train_wall": "14004"}
[2024-10-08 04:33:36,894][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 04:33:36,898][fairseq.trainer][INFO] - begin training epoch 818
[2024-10-08 04:33:36,899][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 04:35:17,500][train_inner][INFO] - {"epoch": 818, "update": 817.062, "loss": "0.58", "ntokens": "260780", "nsentences": "1754.04", "wps": "78836.6", "ups": "0.3", "wpb": "260780", "bsz": "1754", "num_updates": "39200", "lr": "0.000490217", "gnorm": "0.386", "loss_scale": "2", "train_wall": "222", "gb_free": "39.3", "wall": "14105"}
[2024-10-08 04:35:47,968][fairseq_cli.train][INFO] - end of epoch 818 (average epoch stats below)
[2024-10-08 04:35:47,980][train][INFO] - {"epoch": 818, "train_loss": "0.584", "train_ntokens": "260617", "train_nsentences": "1750.04", "train_wps": "95348.6", "train_ups": "0.37", "train_wpb": "260617", "train_bsz": "1750", "train_num_updates": "39245", "train_lr": "0.000490156", "train_gnorm": "0.408", "train_loss_scale": "2", "train_train_wall": "49", "train_gb_free": "39.6", "train_wall": "14135"}
[2024-10-08 04:35:48,045][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 04:35:48,060][fairseq.trainer][INFO] - begin training epoch 819
[2024-10-08 04:35:48,061][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 04:38:01,690][fairseq_cli.train][INFO] - end of epoch 819 (average epoch stats below)
[2024-10-08 04:38:01,694][train][INFO] - {"epoch": 819, "train_loss": "0.577", "train_ntokens": "260583", "train_nsentences": "1750.04", "train_wps": "93544.6", "train_ups": "0.36", "train_wpb": "260583", "train_bsz": "1750", "train_num_updates": "39293", "train_lr": "0.000490091", "train_gnorm": "0.414", "train_loss_scale": "2", "train_train_wall": "40", "train_gb_free": "39.6", "train_wall": "14269"}
[2024-10-08 04:38:01,792][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 04:38:01,798][fairseq.trainer][INFO] - begin training epoch 820
[2024-10-08 04:38:01,798][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 04:40:11,254][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 820 @ 39341 updates
[2024-10-08 04:40:11,255][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-08 04:40:15,729][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-08 04:40:15,731][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 820 @ 39341 updates, score None) (writing took 4.477111672051251 seconds)
[2024-10-08 04:40:15,731][fairseq_cli.train][INFO] - end of epoch 820 (average epoch stats below)
[2024-10-08 04:40:15,734][train][INFO] - {"epoch": 820, "train_loss": "0.575", "train_ntokens": "260822", "train_nsentences": "1750.04", "train_wps": "93403.8", "train_ups": "0.36", "train_wpb": "260822", "train_bsz": "1750", "train_num_updates": "39341", "train_lr": "0.000490026", "train_gnorm": "0.4", "train_loss_scale": "2", "train_train_wall": "53", "train_gb_free": "39.4", "train_wall": "14403"}
[2024-10-08 04:40:15,851][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 04:40:15,855][fairseq.trainer][INFO] - begin training epoch 821
[2024-10-08 04:40:15,856][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 04:42:25,022][fairseq_cli.train][INFO] - end of epoch 821 (average epoch stats below)
[2024-10-08 04:42:25,033][train][INFO] - {"epoch": 821, "train_loss": "0.577", "train_ntokens": "260740", "train_nsentences": "1750.04", "train_wps": "96797.7", "train_ups": "0.37", "train_wpb": "260740", "train_bsz": "1750", "train_num_updates": "39389", "train_lr": "0.000489961", "train_gnorm": "0.46", "train_loss_scale": "2", "train_train_wall": "49", "train_gb_free": "39.7", "train_wall": "14532"}
[2024-10-08 04:42:25,305][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 04:42:25,326][fairseq.trainer][INFO] - begin training epoch 822
[2024-10-08 04:42:25,327][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 04:44:54,400][train_inner][INFO] - {"epoch": 822, "update": 821.229, "loss": "0.579", "ntokens": "260907", "nsentences": "1746.24", "wps": "90452.5", "ups": "0.35", "wpb": "260907", "bsz": "1746.2", "num_updates": "39400", "lr": "0.000489946", "gnorm": "0.421", "loss_scale": "2", "train_wall": "208", "gb_free": "39.9", "wall": "14681"}
[2024-10-08 04:45:18,059][fairseq_cli.train][INFO] - end of epoch 822 (average epoch stats below)
[2024-10-08 04:45:18,062][train][INFO] - {"epoch": 822, "train_loss": "0.581", "train_ntokens": "261040", "train_nsentences": "1750.04", "train_wps": "72416.7", "train_ups": "0.28", "train_wpb": "261040", "train_bsz": "1750", "train_num_updates": "39437", "train_lr": "0.000489895", "train_gnorm": "0.432", "train_loss_scale": "2", "train_train_wall": "63", "train_gb_free": "39.6", "train_wall": "14705"}
[2024-10-08 04:45:18,235][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 04:45:18,239][fairseq.trainer][INFO] - begin training epoch 823
[2024-10-08 04:45:18,240][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 04:47:58,061][fairseq_cli.train][INFO] - end of epoch 823 (average epoch stats below)
[2024-10-08 04:47:58,070][train][INFO] - {"epoch": 823, "train_loss": "0.577", "train_ntokens": "260578", "train_nsentences": "1750.04", "train_wps": "78172.3", "train_ups": "0.3", "train_wpb": "260578", "train_bsz": "1750", "train_num_updates": "39485", "train_lr": "0.00048983", "train_gnorm": "0.369", "train_loss_scale": "2", "train_train_wall": "64", "train_gb_free": "39.3", "train_wall": "14865"}
[2024-10-08 04:47:58,242][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 04:47:58,251][fairseq.trainer][INFO] - begin training epoch 824
[2024-10-08 04:47:58,251][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 04:50:38,852][fairseq_cli.train][INFO] - end of epoch 824 (average epoch stats below)
[2024-10-08 04:50:38,869][train][INFO] - {"epoch": 824, "train_loss": "0.581", "train_ntokens": "260523", "train_nsentences": "1750.04", "train_wps": "77770", "train_ups": "0.3", "train_wpb": "260523", "train_bsz": "1750", "train_num_updates": "39533", "train_lr": "0.000489765", "train_gnorm": "0.381", "train_loss_scale": "2", "train_train_wall": "54", "train_gb_free": "39.4", "train_wall": "15026"}
[2024-10-08 04:50:39,013][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 04:50:39,028][fairseq.trainer][INFO] - begin training epoch 825
[2024-10-08 04:50:39,029][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 04:53:01,156][fairseq_cli.train][INFO] - end of epoch 825 (average epoch stats below)
[2024-10-08 04:53:01,165][train][INFO] - {"epoch": 825, "train_loss": "0.584", "train_ntokens": "260893", "train_nsentences": "1750.04", "train_wps": "88009", "train_ups": "0.34", "train_wpb": "260893", "train_bsz": "1750", "train_num_updates": "39581", "train_lr": "0.0004897", "train_gnorm": "0.377", "train_loss_scale": "2", "train_train_wall": "48", "train_gb_free": "39.8", "train_wall": "15168"}
[2024-10-08 04:53:01,333][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 04:53:01,349][fairseq.trainer][INFO] - begin training epoch 826
[2024-10-08 04:53:01,350][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 04:55:59,017][train_inner][INFO] - {"epoch": 826, "update": 825.396, "loss": "0.579", "ntokens": "260802", "nsentences": "1738.41", "wps": "78483.3", "ups": "0.3", "wpb": "260802", "bsz": "1738.4", "num_updates": "39600", "lr": "0.000489674", "gnorm": "0.389", "loss_scale": "2", "train_wall": "263", "gb_free": "39.7", "wall": "15346"}
[2024-10-08 04:56:20,704][fairseq_cli.train][INFO] - end of epoch 826 (average epoch stats below)
[2024-10-08 04:56:20,707][train][INFO] - {"epoch": 826, "train_loss": "0.583", "train_ntokens": "260775", "train_nsentences": "1750.04", "train_wps": "62730.7", "train_ups": "0.24", "train_wpb": "260775", "train_bsz": "1750", "train_num_updates": "39629", "train_lr": "0.000489635", "train_gnorm": "0.397", "train_loss_scale": "2", "train_train_wall": "95", "train_gb_free": "40.5", "train_wall": "15368"}
[2024-10-08 04:56:20,898][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 04:56:20,905][fairseq.trainer][INFO] - begin training epoch 827
[2024-10-08 04:56:20,906][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 04:59:26,090][fairseq_cli.train][INFO] - end of epoch 827 (average epoch stats below)
[2024-10-08 04:59:26,095][train][INFO] - {"epoch": 827, "train_loss": "0.578", "train_ntokens": "260968", "train_nsentences": "1750.04", "train_wps": "67570", "train_ups": "0.26", "train_wpb": "260968", "train_bsz": "1750", "train_num_updates": "39677", "train_lr": "0.000489569", "train_gnorm": "0.386", "train_loss_scale": "4", "train_train_wall": "85", "train_gb_free": "40.1", "train_wall": "15553"}
[2024-10-08 04:59:26,299][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 04:59:26,310][fairseq.trainer][INFO] - begin training epoch 828
[2024-10-08 04:59:26,311][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 05:02:24,443][fairseq_cli.train][INFO] - end of epoch 828 (average epoch stats below)
[2024-10-08 05:02:24,450][train][INFO] - {"epoch": 828, "train_loss": "0.57", "train_ntokens": "260619", "train_nsentences": "1750.04", "train_wps": "70141.2", "train_ups": "0.27", "train_wpb": "260619", "train_bsz": "1750", "train_num_updates": "39725", "train_lr": "0.000489504", "train_gnorm": "0.39", "train_loss_scale": "4", "train_train_wall": "69", "train_gb_free": "45.1", "train_wall": "15731"}
[2024-10-08 05:02:24,606][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 05:02:24,612][fairseq.trainer][INFO] - begin training epoch 829
[2024-10-08 05:02:24,612][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 05:04:33,170][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2024-10-08 05:04:50,648][fairseq_cli.train][INFO] - end of epoch 829 (average epoch stats below)
[2024-10-08 05:04:50,651][train][INFO] - {"epoch": 829, "train_loss": "0.576", "train_ntokens": "260209", "train_nsentences": "1764.21", "train_wps": "83652.3", "train_ups": "0.32", "train_wpb": "260209", "train_bsz": "1764.2", "train_num_updates": "39772", "train_lr": "0.00048944", "train_gnorm": "0.422", "train_loss_scale": "2", "train_train_wall": "53", "train_gb_free": "39.3", "train_wall": "15878"}
[2024-10-08 05:04:50,817][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 05:04:50,834][fairseq.trainer][INFO] - begin training epoch 830
[2024-10-08 05:04:50,834][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 05:07:07,770][train_inner][INFO] - {"epoch": 830, "update": 829.583, "loss": "0.576", "ntokens": "260487", "nsentences": "1757.54", "wps": "77903.1", "ups": "0.3", "wpb": "260486", "bsz": "1757.5", "num_updates": "39800", "lr": "0.000489402", "gnorm": "0.392", "loss_scale": "2", "train_wall": "278", "gb_free": "39.8", "wall": "16015"}
[2024-10-08 05:07:21,353][fairseq_cli.train][INFO] - end of epoch 830 (average epoch stats below)
[2024-10-08 05:07:21,357][train][INFO] - {"epoch": 830, "train_loss": "0.566", "train_ntokens": "260581", "train_nsentences": "1750.04", "train_wps": "82997.4", "train_ups": "0.32", "train_wpb": "260581", "train_bsz": "1750", "train_num_updates": "39820", "train_lr": "0.000489375", "train_gnorm": "0.375", "train_loss_scale": "2", "train_train_wall": "63", "train_gb_free": "39.2", "train_wall": "16028"}
[2024-10-08 05:07:21,499][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 05:07:21,503][fairseq.trainer][INFO] - begin training epoch 831
[2024-10-08 05:07:21,503][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 05:09:33,974][fairseq_cli.train][INFO] - end of epoch 831 (average epoch stats below)
[2024-10-08 05:09:33,987][train][INFO] - {"epoch": 831, "train_loss": "0.584", "train_ntokens": "260708", "train_nsentences": "1750.04", "train_wps": "94356", "train_ups": "0.36", "train_wpb": "260708", "train_bsz": "1750", "train_num_updates": "39868", "train_lr": "0.00048931", "train_gnorm": "0.444", "train_loss_scale": "2", "train_train_wall": "53", "train_gb_free": "39.3", "train_wall": "16161"}
[2024-10-08 05:09:34,106][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 05:09:34,117][fairseq.trainer][INFO] - begin training epoch 832
[2024-10-08 05:09:34,117][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 05:11:48,680][fairseq_cli.train][INFO] - end of epoch 832 (average epoch stats below)
[2024-10-08 05:11:48,684][train][INFO] - {"epoch": 832, "train_loss": "0.571", "train_ntokens": "260668", "train_nsentences": "1750.04", "train_wps": "92892.5", "train_ups": "0.36", "train_wpb": "260668", "train_bsz": "1750", "train_num_updates": "39916", "train_lr": "0.000489245", "train_gnorm": "0.367", "train_loss_scale": "2", "train_train_wall": "57", "train_gb_free": "39.8", "train_wall": "16296"}
[2024-10-08 05:11:48,750][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 05:11:48,766][fairseq.trainer][INFO] - begin training epoch 833
[2024-10-08 05:11:48,766][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 05:14:00,639][fairseq_cli.train][INFO] - end of epoch 833 (average epoch stats below)
[2024-10-08 05:14:00,649][train][INFO] - {"epoch": 833, "train_loss": "0.578", "train_ntokens": "260569", "train_nsentences": "1750.04", "train_wps": "94783.6", "train_ups": "0.36", "train_wpb": "260569", "train_bsz": "1750", "train_num_updates": "39964", "train_lr": "0.000489179", "train_gnorm": "0.411", "train_loss_scale": "2", "train_train_wall": "53", "train_gb_free": "40.5", "train_wall": "16428"}
[2024-10-08 05:14:00,833][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 05:14:00,845][fairseq.trainer][INFO] - begin training epoch 834
[2024-10-08 05:14:00,846][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 05:16:39,278][train_inner][INFO] - {"epoch": 834, "update": 833.75, "loss": "0.576", "ntokens": "260685", "nsentences": "1747.69", "wps": "91227.7", "ups": "0.35", "wpb": "260685", "bsz": "1747.7", "num_updates": "40000", "lr": "0.00048913", "gnorm": "0.403", "loss_scale": "2", "train_wall": "238", "gb_free": "40.1", "wall": "16586"}
[2024-10-08 05:16:50,172][fairseq_cli.train][INFO] - end of epoch 834 (average epoch stats below)
[2024-10-08 05:16:50,176][train][INFO] - {"epoch": 834, "train_loss": "0.573", "train_ntokens": "260802", "train_nsentences": "1750.04", "train_wps": "73845.7", "train_ups": "0.28", "train_wpb": "260802", "train_bsz": "1750", "train_num_updates": "40012", "train_lr": "0.000489114", "train_gnorm": "0.381", "train_loss_scale": "2", "train_train_wall": "73", "train_gb_free": "40.1", "train_wall": "16597"}
[2024-10-08 05:16:50,393][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 05:16:50,408][fairseq.trainer][INFO] - begin training epoch 835
[2024-10-08 05:16:50,409][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 05:19:54,257][fairseq_cli.train][INFO] - end of epoch 835 (average epoch stats below)
[2024-10-08 05:19:54,272][train][INFO] - {"epoch": 835, "train_loss": "0.581", "train_ntokens": "260532", "train_nsentences": "1750.04", "train_wps": "67930.7", "train_ups": "0.26", "train_wpb": "260532", "train_bsz": "1750", "train_num_updates": "40060", "train_lr": "0.000489049", "train_gnorm": "0.45", "train_loss_scale": "2", "train_train_wall": "45", "train_gb_free": "39.6", "train_wall": "16781"}
[2024-10-08 05:19:54,482][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 05:19:54,497][fairseq.trainer][INFO] - begin training epoch 836
[2024-10-08 05:19:54,498][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 05:22:33,051][fairseq_cli.train][INFO] - end of epoch 836 (average epoch stats below)
[2024-10-08 05:22:33,056][train][INFO] - {"epoch": 836, "train_loss": "0.575", "train_ntokens": "260788", "train_nsentences": "1750.04", "train_wps": "78837.4", "train_ups": "0.3", "train_wpb": "260788", "train_bsz": "1750", "train_num_updates": "40108", "train_lr": "0.000488984", "train_gnorm": "0.448", "train_loss_scale": "2", "train_train_wall": "68", "train_gb_free": "39.4", "train_wall": "16940"}
[2024-10-08 05:22:33,218][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 05:22:33,232][fairseq.trainer][INFO] - begin training epoch 837
[2024-10-08 05:22:33,233][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 05:25:07,372][fairseq_cli.train][INFO] - end of epoch 837 (average epoch stats below)
[2024-10-08 05:25:07,377][train][INFO] - {"epoch": 837, "train_loss": "0.571", "train_ntokens": "260642", "train_nsentences": "1750.04", "train_wps": "81072", "train_ups": "0.31", "train_wpb": "260642", "train_bsz": "1750", "train_num_updates": "40156", "train_lr": "0.000488918", "train_gnorm": "0.362", "train_loss_scale": "2", "train_train_wall": "75", "train_gb_free": "39.8", "train_wall": "17094"}
[2024-10-08 05:25:07,551][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 05:25:07,562][fairseq.trainer][INFO] - begin training epoch 838
[2024-10-08 05:25:07,564][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 05:27:30,592][train_inner][INFO] - {"epoch": 838, "update": 837.917, "loss": "0.577", "ntokens": "260684", "nsentences": "1759.88", "wps": "80049.5", "ups": "0.31", "wpb": "260684", "bsz": "1759.9", "num_updates": "40200", "lr": "0.000488859", "gnorm": "0.414", "loss_scale": "2", "train_wall": "253", "gb_free": "42.2", "wall": "17238"}
[2024-10-08 05:27:31,622][fairseq_cli.train][INFO] - end of epoch 838 (average epoch stats below)
[2024-10-08 05:27:31,628][train][INFO] - {"epoch": 838, "train_loss": "0.581", "train_ntokens": "260854", "train_nsentences": "1750.04", "train_wps": "86803.9", "train_ups": "0.33", "train_wpb": "260854", "train_bsz": "1750", "train_num_updates": "40204", "train_lr": "0.000488853", "train_gnorm": "0.403", "train_loss_scale": "2", "train_train_wall": "56", "train_gb_free": "39.7", "train_wall": "17239"}
[2024-10-08 05:27:31,785][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 05:27:31,791][fairseq.trainer][INFO] - begin training epoch 839
[2024-10-08 05:27:31,792][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 05:29:57,551][fairseq_cli.train][INFO] - end of epoch 839 (average epoch stats below)
[2024-10-08 05:29:57,559][train][INFO] - {"epoch": 839, "train_loss": "0.57", "train_ntokens": "260443", "train_nsentences": "1750.04", "train_wps": "85667.2", "train_ups": "0.33", "train_wpb": "260443", "train_bsz": "1750", "train_num_updates": "40252", "train_lr": "0.000488788", "train_gnorm": "0.394", "train_loss_scale": "2", "train_train_wall": "33", "train_gb_free": "39.8", "train_wall": "17385"}
[2024-10-08 05:29:57,743][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 05:29:57,748][fairseq.trainer][INFO] - begin training epoch 840
[2024-10-08 05:29:57,749][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 05:32:12,500][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 840 @ 40300 updates
[2024-10-08 05:32:12,502][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-08 05:32:17,082][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-08 05:32:17,099][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 840 @ 40300 updates, score None) (writing took 4.598798027262092 seconds)
[2024-10-08 05:32:17,099][fairseq_cli.train][INFO] - end of epoch 840 (average epoch stats below)
[2024-10-08 05:32:17,102][train][INFO] - {"epoch": 840, "train_loss": "0.574", "train_ntokens": "260905", "train_nsentences": "1750.04", "train_wps": "89750.1", "train_ups": "0.34", "train_wpb": "260905", "train_bsz": "1750", "train_num_updates": "40300", "train_lr": "0.000488723", "train_gnorm": "0.416", "train_loss_scale": "2", "train_train_wall": "62", "train_gb_free": "39.6", "train_wall": "17524"}
[2024-10-08 05:32:17,344][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 05:32:17,369][fairseq.trainer][INFO] - begin training epoch 841
[2024-10-08 05:32:17,369][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 05:34:42,112][fairseq_cli.train][INFO] - end of epoch 841 (average epoch stats below)
[2024-10-08 05:34:42,120][train][INFO] - {"epoch": 841, "train_loss": "0.574", "train_ntokens": "260541", "train_nsentences": "1750.04", "train_wps": "86239.4", "train_ups": "0.33", "train_wpb": "260541", "train_bsz": "1750", "train_num_updates": "40348", "train_lr": "0.000488658", "train_gnorm": "0.391", "train_loss_scale": "2", "train_train_wall": "44", "train_gb_free": "40.1", "train_wall": "17669"}
[2024-10-08 05:34:42,259][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 05:34:42,283][fairseq.trainer][INFO] - begin training epoch 842
[2024-10-08 05:34:42,284][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 05:37:21,212][fairseq_cli.train][INFO] - end of epoch 842 (average epoch stats below)
[2024-10-08 05:37:21,223][train][INFO] - {"epoch": 842, "train_loss": "0.58", "train_ntokens": "260709", "train_nsentences": "1750.04", "train_wps": "78655.9", "train_ups": "0.3", "train_wpb": "260710", "train_bsz": "1750", "train_num_updates": "40396", "train_lr": "0.000488592", "train_gnorm": "0.409", "train_loss_scale": "2", "train_train_wall": "59", "train_gb_free": "39.6", "train_wall": "17828"}
[2024-10-08 05:37:21,337][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 05:37:21,367][fairseq.trainer][INFO] - begin training epoch 843
[2024-10-08 05:37:21,368][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 05:39:15,118][train_inner][INFO] - {"epoch": 843, "update": 842.083, "loss": "0.574", "ntokens": "260614", "nsentences": "1752.6", "wps": "73983.8", "ups": "0.28", "wpb": "260614", "bsz": "1752.6", "num_updates": "40400", "lr": "0.000488587", "gnorm": "0.402", "loss_scale": "2", "train_wall": "215", "gb_free": "40.5", "wall": "17942"}
[2024-10-08 05:39:43,924][fairseq_cli.train][INFO] - end of epoch 843 (average epoch stats below)
[2024-10-08 05:39:43,935][train][INFO] - {"epoch": 843, "train_loss": "0.576", "train_ntokens": "260514", "train_nsentences": "1750.04", "train_wps": "87629", "train_ups": "0.34", "train_wpb": "260514", "train_bsz": "1750", "train_num_updates": "40444", "train_lr": "0.000488527", "train_gnorm": "0.388", "train_loss_scale": "2", "train_train_wall": "43", "train_gb_free": "39.6", "train_wall": "17971"}
[2024-10-08 05:39:44,200][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 05:39:44,208][fairseq.trainer][INFO] - begin training epoch 844
[2024-10-08 05:39:44,209][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 05:42:00,026][fairseq_cli.train][INFO] - end of epoch 844 (average epoch stats below)
[2024-10-08 05:42:00,037][train][INFO] - {"epoch": 844, "train_loss": "0.566", "train_ntokens": "260888", "train_nsentences": "1750.04", "train_wps": "92012.4", "train_ups": "0.35", "train_wpb": "260888", "train_bsz": "1750", "train_num_updates": "40492", "train_lr": "0.000488462", "train_gnorm": "0.375", "train_loss_scale": "2", "train_train_wall": "52", "train_gb_free": "39.6", "train_wall": "18107"}
[2024-10-08 05:42:00,189][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 05:42:00,217][fairseq.trainer][INFO] - begin training epoch 845
[2024-10-08 05:42:00,217][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 05:44:14,576][fairseq_cli.train][INFO] - end of epoch 845 (average epoch stats below)
[2024-10-08 05:44:14,584][train][INFO] - {"epoch": 845, "train_loss": "0.567", "train_ntokens": "260927", "train_nsentences": "1750.04", "train_wps": "93088.1", "train_ups": "0.36", "train_wpb": "260927", "train_bsz": "1750", "train_num_updates": "40540", "train_lr": "0.000488397", "train_gnorm": "0.414", "train_loss_scale": "2", "train_train_wall": "51", "train_gb_free": "39.9", "train_wall": "18242"}
[2024-10-08 05:44:14,803][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 05:44:14,814][fairseq.trainer][INFO] - begin training epoch 846
[2024-10-08 05:44:14,814][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 05:46:43,371][fairseq_cli.train][INFO] - end of epoch 846 (average epoch stats below)
[2024-10-08 05:46:43,376][train][INFO] - {"epoch": 846, "train_loss": "0.576", "train_ntokens": "260840", "train_nsentences": "1750.04", "train_wps": "84148.7", "train_ups": "0.32", "train_wpb": "260840", "train_bsz": "1750", "train_num_updates": "40588", "train_lr": "0.000488332", "train_gnorm": "0.397", "train_loss_scale": "2", "train_train_wall": "47", "train_gb_free": "39.8", "train_wall": "18390"}
[2024-10-08 05:46:43,554][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 05:46:43,561][fairseq.trainer][INFO] - begin training epoch 847
[2024-10-08 05:46:43,562][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 05:49:46,592][train_inner][INFO] - {"epoch": 847, "update": 846.25, "loss": "0.571", "ntokens": "260957", "nsentences": "1740.86", "wps": "82651.5", "ups": "0.32", "wpb": "260957", "bsz": "1740.9", "num_updates": "40600", "lr": "0.000488315", "gnorm": "0.399", "loss_scale": "2", "train_wall": "255", "gb_free": "40", "wall": "18574"}
[2024-10-08 05:50:21,558][fairseq_cli.train][INFO] - end of epoch 847 (average epoch stats below)
[2024-10-08 05:50:21,563][train][INFO] - {"epoch": 847, "train_loss": "0.57", "train_ntokens": "260733", "train_nsentences": "1750.04", "train_wps": "57361.4", "train_ups": "0.22", "train_wpb": "260734", "train_bsz": "1750", "train_num_updates": "40636", "train_lr": "0.000488266", "train_gnorm": "0.409", "train_loss_scale": "2", "train_train_wall": "111", "train_gb_free": "39.7", "train_wall": "18609"}
[2024-10-08 05:50:21,780][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 05:50:21,791][fairseq.trainer][INFO] - begin training epoch 848
[2024-10-08 05:50:21,791][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 05:54:03,687][fairseq_cli.train][INFO] - end of epoch 848 (average epoch stats below)
[2024-10-08 05:54:03,704][train][INFO] - {"epoch": 848, "train_loss": "0.57", "train_ntokens": "260645", "train_nsentences": "1750.04", "train_wps": "56320.6", "train_ups": "0.22", "train_wpb": "260645", "train_bsz": "1750", "train_num_updates": "40684", "train_lr": "0.000488201", "train_gnorm": "0.375", "train_loss_scale": "2", "train_train_wall": "96", "train_gb_free": "39.6", "train_wall": "18831"}
[2024-10-08 05:54:03,833][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 05:54:03,836][fairseq.trainer][INFO] - begin training epoch 849
[2024-10-08 05:54:03,837][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 05:57:42,879][fairseq_cli.train][INFO] - end of epoch 849 (average epoch stats below)
[2024-10-08 05:57:42,890][train][INFO] - {"epoch": 849, "train_loss": "0.567", "train_ntokens": "260565", "train_nsentences": "1750.04", "train_wps": "57062.9", "train_ups": "0.22", "train_wpb": "260565", "train_bsz": "1750", "train_num_updates": "40732", "train_lr": "0.000488136", "train_gnorm": "0.377", "train_loss_scale": "2", "train_train_wall": "103", "train_gb_free": "39.1", "train_wall": "19050"}
[2024-10-08 05:57:43,173][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 05:57:43,195][fairseq.trainer][INFO] - begin training epoch 850
[2024-10-08 05:57:43,196][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 06:01:45,374][fairseq_cli.train][INFO] - end of epoch 850 (average epoch stats below)
[2024-10-08 06:01:45,380][train][INFO] - {"epoch": 850, "train_loss": "0.575", "train_ntokens": "261037", "train_nsentences": "1750.04", "train_wps": "51671.9", "train_ups": "0.2", "train_wpb": "261037", "train_bsz": "1750", "train_num_updates": "40780", "train_lr": "0.000488071", "train_gnorm": "0.45", "train_loss_scale": "2", "train_train_wall": "57", "train_gb_free": "39.7", "train_wall": "19292"}
[2024-10-08 06:01:45,656][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 06:01:45,678][fairseq.trainer][INFO] - begin training epoch 851
[2024-10-08 06:01:45,678][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 06:05:35,443][train_inner][INFO] - {"epoch": 851, "update": 850.417, "loss": "0.57", "ntokens": "260527", "nsentences": "1768.6", "wps": "54916.8", "ups": "0.21", "wpb": "260527", "bsz": "1768.6", "num_updates": "40800", "lr": "0.000488043", "gnorm": "0.395", "loss_scale": "2", "train_wall": "339", "gb_free": "40.3", "wall": "19522"}
[2024-10-08 06:05:51,759][fairseq_cli.train][INFO] - end of epoch 851 (average epoch stats below)
[2024-10-08 06:05:51,762][train][INFO] - {"epoch": 851, "train_loss": "0.568", "train_ntokens": "260749", "train_nsentences": "1750.04", "train_wps": "50799.7", "train_ups": "0.19", "train_wpb": "260749", "train_bsz": "1750", "train_num_updates": "40828", "train_lr": "0.000488005", "train_gnorm": "0.387", "train_loss_scale": "2", "train_train_wall": "65", "train_gb_free": "39.4", "train_wall": "19539"}
[2024-10-08 06:05:51,995][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 06:05:52,008][fairseq.trainer][INFO] - begin training epoch 852
[2024-10-08 06:05:52,009][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 06:08:38,961][fairseq_cli.train][INFO] - end of epoch 852 (average epoch stats below)
[2024-10-08 06:08:38,964][train][INFO] - {"epoch": 852, "train_loss": "0.568", "train_ntokens": "260656", "train_nsentences": "1750.04", "train_wps": "74830.1", "train_ups": "0.29", "train_wpb": "260656", "train_bsz": "1750", "train_num_updates": "40876", "train_lr": "0.00048794", "train_gnorm": "0.4", "train_loss_scale": "2", "train_train_wall": "63", "train_gb_free": "39.6", "train_wall": "19706"}
[2024-10-08 06:08:39,191][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 06:08:39,221][fairseq.trainer][INFO] - begin training epoch 853
[2024-10-08 06:08:39,222][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 06:11:10,875][fairseq_cli.train][INFO] - end of epoch 853 (average epoch stats below)
[2024-10-08 06:11:10,879][train][INFO] - {"epoch": 853, "train_loss": "0.566", "train_ntokens": "260452", "train_nsentences": "1750.04", "train_wps": "82295.6", "train_ups": "0.32", "train_wpb": "260452", "train_bsz": "1750", "train_num_updates": "40924", "train_lr": "0.000487875", "train_gnorm": "0.387", "train_loss_scale": "2", "train_train_wall": "62", "train_gb_free": "40", "train_wall": "19858"}
[2024-10-08 06:11:11,093][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 06:11:11,107][fairseq.trainer][INFO] - begin training epoch 854
[2024-10-08 06:11:11,107][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 06:13:27,207][fairseq_cli.train][INFO] - end of epoch 854 (average epoch stats below)
[2024-10-08 06:13:27,210][train][INFO] - {"epoch": 854, "train_loss": "0.567", "train_ntokens": "260708", "train_nsentences": "1750.04", "train_wps": "91793.4", "train_ups": "0.35", "train_wpb": "260708", "train_bsz": "1750", "train_num_updates": "40972", "train_lr": "0.00048781", "train_gnorm": "0.396", "train_loss_scale": "2", "train_train_wall": "59", "train_gb_free": "39.3", "train_wall": "19994"}
[2024-10-08 06:13:27,276][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 06:13:27,285][fairseq.trainer][INFO] - begin training epoch 855
[2024-10-08 06:13:27,285][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 06:16:12,188][train_inner][INFO] - {"epoch": 855, "update": 854.583, "loss": "0.569", "ntokens": "260757", "nsentences": "1739.56", "wps": "81903.8", "ups": "0.31", "wpb": "260757", "bsz": "1739.6", "num_updates": "41000", "lr": "0.000487772", "gnorm": "0.394", "loss_scale": "2", "train_wall": "281", "gb_free": "39.4", "wall": "20159"}
[2024-10-08 06:16:25,862][fairseq_cli.train][INFO] - end of epoch 855 (average epoch stats below)
[2024-10-08 06:16:25,866][train][INFO] - {"epoch": 855, "train_loss": "0.572", "train_ntokens": "260737", "train_nsentences": "1750.04", "train_wps": "70054.6", "train_ups": "0.27", "train_wpb": "260738", "train_bsz": "1750", "train_num_updates": "41020", "train_lr": "0.000487745", "train_gnorm": "0.405", "train_loss_scale": "2", "train_train_wall": "95", "train_gb_free": "39.6", "train_wall": "20173"}
[2024-10-08 06:16:26,091][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 06:16:26,101][fairseq.trainer][INFO] - begin training epoch 856
[2024-10-08 06:16:26,101][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 06:19:11,185][fairseq_cli.train][INFO] - end of epoch 856 (average epoch stats below)
[2024-10-08 06:19:11,200][train][INFO] - {"epoch": 856, "train_loss": "0.563", "train_ntokens": "260581", "train_nsentences": "1750.04", "train_wps": "75654.1", "train_ups": "0.29", "train_wpb": "260581", "train_bsz": "1750", "train_num_updates": "41068", "train_lr": "0.000487679", "train_gnorm": "0.352", "train_loss_scale": "2", "train_train_wall": "74", "train_gb_free": "39.7", "train_wall": "20338"}
[2024-10-08 06:19:11,460][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 06:19:11,466][fairseq.trainer][INFO] - begin training epoch 857
[2024-10-08 06:19:11,466][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 06:22:38,522][fairseq_cli.train][INFO] - end of epoch 857 (average epoch stats below)
[2024-10-08 06:22:38,529][train][INFO] - {"epoch": 857, "train_loss": "0.565", "train_ntokens": "261004", "train_nsentences": "1750.04", "train_wps": "60427.8", "train_ups": "0.23", "train_wpb": "261004", "train_bsz": "1750", "train_num_updates": "41116", "train_lr": "0.000487614", "train_gnorm": "0.416", "train_loss_scale": "2", "train_train_wall": "120", "train_gb_free": "39.3", "train_wall": "20546"}
[2024-10-08 06:22:38,696][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 06:22:38,724][fairseq.trainer][INFO] - begin training epoch 858
[2024-10-08 06:22:38,725][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 06:25:40,842][fairseq_cli.train][INFO] - end of epoch 858 (average epoch stats below)
[2024-10-08 06:25:40,852][train][INFO] - {"epoch": 858, "train_loss": "0.568", "train_ntokens": "260508", "train_nsentences": "1750.04", "train_wps": "68584.9", "train_ups": "0.26", "train_wpb": "260508", "train_bsz": "1750", "train_num_updates": "41164", "train_lr": "0.000487549", "train_gnorm": "0.391", "train_loss_scale": "2", "train_train_wall": "84", "train_gb_free": "40.5", "train_wall": "20728"}
[2024-10-08 06:25:41,061][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 06:25:41,065][fairseq.trainer][INFO] - begin training epoch 859
[2024-10-08 06:25:41,066][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 06:28:55,364][train_inner][INFO] - {"epoch": 859, "update": 858.75, "loss": "0.566", "ntokens": "260732", "nsentences": "1749.52", "wps": "68328.7", "ups": "0.26", "wpb": "260732", "bsz": "1749.5", "num_updates": "41200", "lr": "0.0004875", "gnorm": "0.394", "loss_scale": "2", "train_wall": "365", "gb_free": "39.6", "wall": "20922"}
[2024-10-08 06:29:06,766][fairseq_cli.train][INFO] - end of epoch 859 (average epoch stats below)
[2024-10-08 06:29:06,768][train][INFO] - {"epoch": 859, "train_loss": "0.562", "train_ntokens": "260798", "train_nsentences": "1750.04", "train_wps": "60794.1", "train_ups": "0.23", "train_wpb": "260798", "train_bsz": "1750", "train_num_updates": "41212", "train_lr": "0.000487484", "train_gnorm": "0.401", "train_loss_scale": "2", "train_train_wall": "86", "train_gb_free": "40.1", "train_wall": "20934"}
[2024-10-08 06:29:06,995][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 06:29:07,022][fairseq.trainer][INFO] - begin training epoch 860
[2024-10-08 06:29:07,022][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 06:32:44,654][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 860 @ 41260 updates
[2024-10-08 06:32:44,657][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-08 06:32:48,700][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-08 06:32:48,716][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 860 @ 41260 updates, score None) (writing took 4.062218612059951 seconds)
[2024-10-08 06:32:48,717][fairseq_cli.train][INFO] - end of epoch 860 (average epoch stats below)
[2024-10-08 06:32:48,720][train][INFO] - {"epoch": 860, "train_loss": "0.564", "train_ntokens": "260728", "train_nsentences": "1750.04", "train_wps": "56387", "train_ups": "0.22", "train_wpb": "260728", "train_bsz": "1750", "train_num_updates": "41260", "train_lr": "0.000487418", "train_gnorm": "0.369", "train_loss_scale": "2", "train_train_wall": "113", "train_gb_free": "39.3", "train_wall": "21156"}
[2024-10-08 06:32:48,884][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 06:32:48,925][fairseq.trainer][INFO] - begin training epoch 861
[2024-10-08 06:32:48,925][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 06:35:16,776][fairseq_cli.train][INFO] - end of epoch 861 (average epoch stats below)
[2024-10-08 06:35:16,781][train][INFO] - {"epoch": 861, "train_loss": "0.565", "train_ntokens": "260822", "train_nsentences": "1750.04", "train_wps": "84558.6", "train_ups": "0.32", "train_wpb": "260822", "train_bsz": "1750", "train_num_updates": "41308", "train_lr": "0.000487353", "train_gnorm": "0.368", "train_loss_scale": "2", "train_train_wall": "41", "train_gb_free": "39.6", "train_wall": "21304"}
[2024-10-08 06:35:16,867][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 06:35:16,870][fairseq.trainer][INFO] - begin training epoch 862
[2024-10-08 06:35:16,871][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 06:38:41,781][fairseq_cli.train][INFO] - end of epoch 862 (average epoch stats below)
[2024-10-08 06:38:41,794][train][INFO] - {"epoch": 862, "train_loss": "0.566", "train_ntokens": "260647", "train_nsentences": "1750.04", "train_wps": "61027.2", "train_ups": "0.23", "train_wpb": "260647", "train_bsz": "1750", "train_num_updates": "41356", "train_lr": "0.000487288", "train_gnorm": "0.388", "train_loss_scale": "2", "train_train_wall": "102", "train_gb_free": "39.3", "train_wall": "21509"}
[2024-10-08 06:38:42,075][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 06:38:42,099][fairseq.trainer][INFO] - begin training epoch 863
[2024-10-08 06:38:42,099][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 06:42:06,073][train_inner][INFO] - {"epoch": 863, "update": 862.917, "loss": "0.566", "ntokens": "260693", "nsentences": "1753.52", "wps": "65940", "ups": "0.25", "wpb": "260693", "bsz": "1753.5", "num_updates": "41400", "lr": "0.000487228", "gnorm": "0.386", "loss_scale": "2", "train_wall": "295", "gb_free": "39.4", "wall": "21713"}
[2024-10-08 06:42:07,030][fairseq_cli.train][INFO] - end of epoch 863 (average epoch stats below)
[2024-10-08 06:42:07,033][train][INFO] - {"epoch": 863, "train_loss": "0.57", "train_ntokens": "260729", "train_nsentences": "1750.04", "train_wps": "60978.7", "train_ups": "0.23", "train_wpb": "260729", "train_bsz": "1750", "train_num_updates": "41404", "train_lr": "0.000487223", "train_gnorm": "0.424", "train_loss_scale": "2", "train_train_wall": "29", "train_gb_free": "39.2", "train_wall": "21714"}
[2024-10-08 06:42:07,297][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 06:42:07,313][fairseq.trainer][INFO] - begin training epoch 864
[2024-10-08 06:42:07,313][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 06:45:18,339][fairseq_cli.train][INFO] - end of epoch 864 (average epoch stats below)
[2024-10-08 06:45:18,348][train][INFO] - {"epoch": 864, "train_loss": "0.566", "train_ntokens": "260872", "train_nsentences": "1750.04", "train_wps": "65452.5", "train_ups": "0.25", "train_wpb": "260872", "train_bsz": "1750", "train_num_updates": "41452", "train_lr": "0.000487158", "train_gnorm": "0.37", "train_loss_scale": "2", "train_train_wall": "77", "train_gb_free": "39.7", "train_wall": "21905"}
[2024-10-08 06:45:18,477][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 06:45:18,488][fairseq.trainer][INFO] - begin training epoch 865
[2024-10-08 06:45:18,488][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 06:48:17,658][fairseq_cli.train][INFO] - end of epoch 865 (average epoch stats below)
[2024-10-08 06:48:17,672][train][INFO] - {"epoch": 865, "train_loss": "0.568", "train_ntokens": "260657", "train_nsentences": "1750.04", "train_wps": "69772.1", "train_ups": "0.27", "train_wpb": "260657", "train_bsz": "1750", "train_num_updates": "41500", "train_lr": "0.000487092", "train_gnorm": "0.411", "train_loss_scale": "2", "train_train_wall": "76", "train_gb_free": "39.3", "train_wall": "22085"}
[2024-10-08 06:48:17,976][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 06:48:18,019][fairseq.trainer][INFO] - begin training epoch 866
[2024-10-08 06:48:18,020][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 06:51:46,447][fairseq_cli.train][INFO] - end of epoch 866 (average epoch stats below)
[2024-10-08 06:51:46,452][train][INFO] - {"epoch": 866, "train_loss": "0.568", "train_ntokens": "260721", "train_nsentences": "1750.04", "train_wps": "59942.7", "train_ups": "0.23", "train_wpb": "260721", "train_bsz": "1750", "train_num_updates": "41548", "train_lr": "0.000487027", "train_gnorm": "0.405", "train_loss_scale": "2", "train_train_wall": "50", "train_gb_free": "40.2", "train_wall": "22293"}
[2024-10-08 06:51:46,652][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 06:51:46,690][fairseq.trainer][INFO] - begin training epoch 867
[2024-10-08 06:51:46,691][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 06:55:08,728][fairseq_cli.train][INFO] - end of epoch 867 (average epoch stats below)
[2024-10-08 06:55:08,735][train][INFO] - {"epoch": 867, "train_loss": "0.564", "train_ntokens": "261030", "train_nsentences": "1750.04", "train_wps": "61941.3", "train_ups": "0.24", "train_wpb": "261030", "train_bsz": "1750", "train_num_updates": "41596", "train_lr": "0.000486962", "train_gnorm": "0.405", "train_loss_scale": "2", "train_train_wall": "97", "train_gb_free": "40.1", "train_wall": "22496"}
[2024-10-08 06:55:08,991][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 06:55:08,996][fairseq.trainer][INFO] - begin training epoch 868
[2024-10-08 06:55:08,997][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 06:58:09,557][train_inner][INFO] - {"epoch": 868, "update": 867.083, "loss": "0.566", "ntokens": "260866", "nsentences": "1745", "wps": "54151.4", "ups": "0.21", "wpb": "260866", "bsz": "1745", "num_updates": "41600", "lr": "0.000486957", "gnorm": "0.398", "loss_scale": "2", "train_wall": "371", "gb_free": "40.2", "wall": "22677"}
[2024-10-08 06:58:44,153][fairseq_cli.train][INFO] - end of epoch 868 (average epoch stats below)
[2024-10-08 06:58:44,157][train][INFO] - {"epoch": 868, "train_loss": "0.565", "train_ntokens": "260796", "train_nsentences": "1750.04", "train_wps": "58111.3", "train_ups": "0.22", "train_wpb": "260796", "train_bsz": "1750", "train_num_updates": "41644", "train_lr": "0.000486897", "train_gnorm": "0.413", "train_loss_scale": "2", "train_train_wall": "103", "train_gb_free": "39.6", "train_wall": "22711"}
[2024-10-08 06:58:44,433][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 06:58:44,439][fairseq.trainer][INFO] - begin training epoch 869
[2024-10-08 06:58:44,440][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 07:02:29,962][fairseq_cli.train][INFO] - end of epoch 869 (average epoch stats below)
[2024-10-08 07:02:29,972][train][INFO] - {"epoch": 869, "train_loss": "0.562", "train_ntokens": "260735", "train_nsentences": "1750.04", "train_wps": "55424.8", "train_ups": "0.21", "train_wpb": "260735", "train_bsz": "1750", "train_num_updates": "41692", "train_lr": "0.000486832", "train_gnorm": "0.374", "train_loss_scale": "2", "train_train_wall": "97", "train_gb_free": "39.6", "train_wall": "22937"}
[2024-10-08 07:02:30,253][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 07:02:30,256][fairseq.trainer][INFO] - begin training epoch 870
[2024-10-08 07:02:30,257][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 07:06:20,070][fairseq_cli.train][INFO] - end of epoch 870 (average epoch stats below)
[2024-10-08 07:06:20,075][train][INFO] - {"epoch": 870, "train_loss": "0.564", "train_ntokens": "260869", "train_nsentences": "1750.04", "train_wps": "54418.8", "train_ups": "0.21", "train_wpb": "260869", "train_bsz": "1750", "train_num_updates": "41740", "train_lr": "0.000486766", "train_gnorm": "0.363", "train_loss_scale": "2", "train_train_wall": "23", "train_gb_free": "40", "train_wall": "23167"}
[2024-10-08 07:06:20,318][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 07:06:20,340][fairseq.trainer][INFO] - begin training epoch 871
[2024-10-08 07:06:20,341][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 07:09:54,082][fairseq_cli.train][INFO] - end of epoch 871 (average epoch stats below)
[2024-10-08 07:09:54,098][train][INFO] - {"epoch": 871, "train_loss": "0.566", "train_ntokens": "260753", "train_nsentences": "1750.04", "train_wps": "58481.1", "train_ups": "0.22", "train_wpb": "260753", "train_bsz": "1750", "train_num_updates": "41788", "train_lr": "0.000486701", "train_gnorm": "0.382", "train_loss_scale": "2", "train_train_wall": "116", "train_gb_free": "39.8", "train_wall": "23381"}
[2024-10-08 07:09:54,414][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 07:09:54,428][fairseq.trainer][INFO] - begin training epoch 872
[2024-10-08 07:09:54,429][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 07:11:54,043][train_inner][INFO] - {"epoch": 872, "update": 871.25, "loss": "0.564", "ntokens": "260698", "nsentences": "1750.17", "wps": "63243.9", "ups": "0.24", "wpb": "260698", "bsz": "1750.2", "num_updates": "41800", "lr": "0.000486685", "gnorm": "0.383", "loss_scale": "4", "train_wall": "304", "gb_free": "40.2", "wall": "23501"}
[2024-10-08 07:12:18,748][fairseq_cli.train][INFO] - end of epoch 872 (average epoch stats below)
[2024-10-08 07:12:18,759][train][INFO] - {"epoch": 872, "train_loss": "0.56", "train_ntokens": "261089", "train_nsentences": "1750.04", "train_wps": "86639.4", "train_ups": "0.33", "train_wpb": "261089", "train_bsz": "1750", "train_num_updates": "41836", "train_lr": "0.000486636", "train_gnorm": "0.384", "train_loss_scale": "4", "train_train_wall": "58", "train_gb_free": "39.2", "train_wall": "23526"}
[2024-10-08 07:12:19,069][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 07:12:19,077][fairseq.trainer][INFO] - begin training epoch 873
[2024-10-08 07:12:19,078][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 07:14:28,184][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2024-10-08 07:14:44,912][fairseq_cli.train][INFO] - end of epoch 873 (average epoch stats below)
[2024-10-08 07:14:44,917][train][INFO] - {"epoch": 873, "train_loss": "0.565", "train_ntokens": "260994", "train_nsentences": "1731.7", "train_wps": "83942.3", "train_ups": "0.32", "train_wpb": "260994", "train_bsz": "1731.7", "train_num_updates": "41883", "train_lr": "0.000486572", "train_gnorm": "0.413", "train_loss_scale": "2", "train_train_wall": "33", "train_gb_free": "39.3", "train_wall": "23672"}
[2024-10-08 07:14:45,153][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 07:14:45,161][fairseq.trainer][INFO] - begin training epoch 874
[2024-10-08 07:14:45,161][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 07:17:10,028][fairseq_cli.train][INFO] - end of epoch 874 (average epoch stats below)
[2024-10-08 07:17:10,034][train][INFO] - {"epoch": 874, "train_loss": "0.56", "train_ntokens": "260989", "train_nsentences": "1750.04", "train_wps": "86329.3", "train_ups": "0.33", "train_wpb": "260989", "train_bsz": "1750", "train_num_updates": "41931", "train_lr": "0.000486507", "train_gnorm": "0.387", "train_loss_scale": "2", "train_train_wall": "55", "train_gb_free": "39.3", "train_wall": "23817"}
[2024-10-08 07:17:10,266][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 07:17:10,295][fairseq.trainer][INFO] - begin training epoch 875
[2024-10-08 07:17:10,296][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 07:20:54,602][fairseq_cli.train][INFO] - end of epoch 875 (average epoch stats below)
[2024-10-08 07:20:54,611][train][INFO] - {"epoch": 875, "train_loss": "0.561", "train_ntokens": "260821", "train_nsentences": "1750.04", "train_wps": "55748.4", "train_ups": "0.21", "train_wpb": "260821", "train_bsz": "1750", "train_num_updates": "41979", "train_lr": "0.000486442", "train_gnorm": "0.378", "train_loss_scale": "2", "train_train_wall": "48", "train_gb_free": "39.8", "train_wall": "24042"}
[2024-10-08 07:20:54,867][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 07:20:54,884][fairseq.trainer][INFO] - begin training epoch 876
[2024-10-08 07:20:54,884][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 07:24:08,950][train_inner][INFO] - {"epoch": 876, "update": 875.438, "loss": "0.563", "ntokens": "261076", "nsentences": "1752.42", "wps": "71056.3", "ups": "0.27", "wpb": "261076", "bsz": "1752.4", "num_updates": "42000", "lr": "0.000486413", "gnorm": "0.39", "loss_scale": "2", "train_wall": "234", "gb_free": "39.3", "wall": "24236"}
[2024-10-08 07:24:25,366][fairseq_cli.train][INFO] - end of epoch 876 (average epoch stats below)
[2024-10-08 07:24:25,368][train][INFO] - {"epoch": 876, "train_loss": "0.571", "train_ntokens": "260765", "train_nsentences": "1750.04", "train_wps": "59390.2", "train_ups": "0.23", "train_wpb": "260765", "train_bsz": "1750", "train_num_updates": "42027", "train_lr": "0.000486376", "train_gnorm": "0.375", "train_loss_scale": "2", "train_train_wall": "89", "train_gb_free": "39.2", "train_wall": "24252"}
[2024-10-08 07:24:25,582][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 07:24:25,606][fairseq.trainer][INFO] - begin training epoch 877
[2024-10-08 07:24:25,607][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 07:28:08,385][fairseq_cli.train][INFO] - end of epoch 877 (average epoch stats below)
[2024-10-08 07:28:08,399][train][INFO] - {"epoch": 877, "train_loss": "0.568", "train_ntokens": "260591", "train_nsentences": "1750.04", "train_wps": "56084.4", "train_ups": "0.22", "train_wpb": "260591", "train_bsz": "1750", "train_num_updates": "42075", "train_lr": "0.000486311", "train_gnorm": "0.403", "train_loss_scale": "2", "train_train_wall": "82", "train_gb_free": "39.4", "train_wall": "24475"}
[2024-10-08 07:28:08,606][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 07:28:08,622][fairseq.trainer][INFO] - begin training epoch 878
[2024-10-08 07:28:08,623][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 07:33:25,019][fairseq_cli.train][INFO] - end of epoch 878 (average epoch stats below)
[2024-10-08 07:33:25,071][train][INFO] - {"epoch": 878, "train_loss": "0.56", "train_ntokens": "261067", "train_nsentences": "1750.04", "train_wps": "39572.2", "train_ups": "0.15", "train_wpb": "261067", "train_bsz": "1750", "train_num_updates": "42123", "train_lr": "0.000486246", "train_gnorm": "0.376", "train_loss_scale": "2", "train_train_wall": "210", "train_gb_free": "39.1", "train_wall": "24792"}
[2024-10-08 07:33:25,707][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 07:33:25,717][fairseq.trainer][INFO] - begin training epoch 879
[2024-10-08 07:33:25,718][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 07:39:30,240][fairseq_cli.train][INFO] - end of epoch 879 (average epoch stats below)
[2024-10-08 07:39:30,276][train][INFO] - {"epoch": 879, "train_loss": "0.566", "train_ntokens": "260627", "train_nsentences": "1750.04", "train_wps": "34255.5", "train_ups": "0.13", "train_wpb": "260627", "train_bsz": "1750", "train_num_updates": "42171", "train_lr": "0.000486181", "train_gnorm": "0.428", "train_loss_scale": "2", "train_train_wall": "243", "train_gb_free": "39.3", "train_wall": "25157"}
[2024-10-08 07:39:30,735][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 07:39:30,809][fairseq.trainer][INFO] - begin training epoch 880
[2024-10-08 07:39:30,810][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 07:45:04,962][train_inner][INFO] - {"epoch": 880, "update": 879.604, "loss": "0.564", "ntokens": "260630", "nsentences": "1743.26", "wps": "41501.5", "ups": "0.16", "wpb": "260630", "bsz": "1743.3", "num_updates": "42200", "lr": "0.000486141", "gnorm": "0.401", "loss_scale": "2", "train_wall": "655", "gb_free": "40.1", "wall": "25492"}
[2024-10-08 07:45:19,040][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 880 @ 42219 updates
[2024-10-08 07:45:19,041][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-08 07:45:23,908][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-08 07:45:23,911][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 880 @ 42219 updates, score None) (writing took 4.870655519887805 seconds)
[2024-10-08 07:45:23,911][fairseq_cli.train][INFO] - end of epoch 880 (average epoch stats below)
[2024-10-08 07:45:23,915][train][INFO] - {"epoch": 880, "train_loss": "0.56", "train_ntokens": "260873", "train_nsentences": "1750.04", "train_wps": "35409.2", "train_ups": "0.14", "train_wpb": "260873", "train_bsz": "1750", "train_num_updates": "42219", "train_lr": "0.000486115", "train_gnorm": "0.408", "train_loss_scale": "2", "train_train_wall": "117", "train_gb_free": "40", "train_wall": "25511"}
[2024-10-08 07:45:24,285][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 07:45:24,293][fairseq.trainer][INFO] - begin training epoch 881
[2024-10-08 07:45:24,293][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 07:50:35,915][fairseq_cli.train][INFO] - end of epoch 881 (average epoch stats below)
[2024-10-08 07:50:35,931][train][INFO] - {"epoch": 881, "train_loss": "0.564", "train_ntokens": "260618", "train_nsentences": "1750.04", "train_wps": "40094.5", "train_ups": "0.15", "train_wpb": "260618", "train_bsz": "1750", "train_num_updates": "42267", "train_lr": "0.00048605", "train_gnorm": "0.398", "train_loss_scale": "2", "train_train_wall": "140", "train_gb_free": "39.6", "train_wall": "25823"}
[2024-10-08 07:50:36,307][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 07:50:36,326][fairseq.trainer][INFO] - begin training epoch 882
[2024-10-08 07:50:36,327][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 07:55:29,094][fairseq_cli.train][INFO] - end of epoch 882 (average epoch stats below)
[2024-10-08 07:55:29,104][train][INFO] - {"epoch": 882, "train_loss": "0.563", "train_ntokens": "260949", "train_nsentences": "1750.04", "train_wps": "42724.6", "train_ups": "0.16", "train_wpb": "260949", "train_bsz": "1750", "train_num_updates": "42315", "train_lr": "0.000485985", "train_gnorm": "0.411", "train_loss_scale": "2", "train_train_wall": "195", "train_gb_free": "39.2", "train_wall": "26116"}
[2024-10-08 07:55:29,426][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 07:55:29,432][fairseq.trainer][INFO] - begin training epoch 883
[2024-10-08 07:55:29,433][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 08:00:23,406][fairseq_cli.train][INFO] - end of epoch 883 (average epoch stats below)
[2024-10-08 08:00:23,418][train][INFO] - {"epoch": 883, "train_loss": "0.552", "train_ntokens": "260793", "train_nsentences": "1750.04", "train_wps": "42533.6", "train_ups": "0.16", "train_wpb": "260793", "train_bsz": "1750", "train_num_updates": "42363", "train_lr": "0.00048592", "train_gnorm": "0.425", "train_loss_scale": "2", "train_train_wall": "93", "train_gb_free": "39.3", "train_wall": "26410"}
[2024-10-08 08:00:23,690][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 08:00:23,721][fairseq.trainer][INFO] - begin training epoch 884
[2024-10-08 08:00:23,724][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 08:04:52,088][train_inner][INFO] - {"epoch": 884, "update": 883.771, "loss": "0.559", "ntokens": "260859", "nsentences": "1752.23", "wps": "43950.9", "ups": "0.17", "wpb": "260860", "bsz": "1752.2", "num_updates": "42400", "lr": "0.00048587", "gnorm": "0.402", "loss_scale": "2", "train_wall": "570", "gb_free": "39.8", "wall": "26679"}
[2024-10-08 08:05:04,561][fairseq_cli.train][INFO] - end of epoch 884 (average epoch stats below)
[2024-10-08 08:05:04,564][train][INFO] - {"epoch": 884, "train_loss": "0.553", "train_ntokens": "260423", "train_nsentences": "1750.04", "train_wps": "44462.5", "train_ups": "0.17", "train_wpb": "260423", "train_bsz": "1750", "train_num_updates": "42411", "train_lr": "0.000485855", "train_gnorm": "0.376", "train_loss_scale": "2", "train_train_wall": "140", "train_gb_free": "40.2", "train_wall": "26692"}
[2024-10-08 08:05:04,848][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 08:05:04,869][fairseq.trainer][INFO] - begin training epoch 885
[2024-10-08 08:05:04,870][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 08:09:09,108][fairseq_cli.train][INFO] - end of epoch 885 (average epoch stats below)
[2024-10-08 08:09:09,130][train][INFO] - {"epoch": 885, "train_loss": "0.553", "train_ntokens": "260441", "train_nsentences": "1750.04", "train_wps": "51116.9", "train_ups": "0.2", "train_wpb": "260441", "train_bsz": "1750", "train_num_updates": "42459", "train_lr": "0.000485789", "train_gnorm": "0.426", "train_loss_scale": "2", "train_train_wall": "45", "train_gb_free": "39.6", "train_wall": "26936"}
[2024-10-08 08:09:09,474][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 08:09:09,497][fairseq.trainer][INFO] - begin training epoch 886
[2024-10-08 08:09:09,498][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 08:13:40,079][fairseq_cli.train][INFO] - end of epoch 886 (average epoch stats below)
[2024-10-08 08:13:40,085][train][INFO] - {"epoch": 886, "train_loss": "0.558", "train_ntokens": "260633", "train_nsentences": "1750.04", "train_wps": "46172.7", "train_ups": "0.18", "train_wpb": "260633", "train_bsz": "1750", "train_num_updates": "42507", "train_lr": "0.000485724", "train_gnorm": "0.426", "train_loss_scale": "2", "train_train_wall": "76", "train_gb_free": "40.1", "train_wall": "27207"}
[2024-10-08 08:13:40,401][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 08:13:40,422][fairseq.trainer][INFO] - begin training epoch 887
[2024-10-08 08:13:40,422][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 08:17:30,811][fairseq_cli.train][INFO] - end of epoch 887 (average epoch stats below)
[2024-10-08 08:17:30,817][train][INFO] - {"epoch": 887, "train_loss": "0.569", "train_ntokens": "261029", "train_nsentences": "1750.04", "train_wps": "54303.5", "train_ups": "0.21", "train_wpb": "261029", "train_bsz": "1750", "train_num_updates": "42555", "train_lr": "0.000485659", "train_gnorm": "0.386", "train_loss_scale": "2", "train_train_wall": "104", "train_gb_free": "39.2", "train_wall": "27438"}
[2024-10-08 08:17:31,068][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 08:17:31,079][fairseq.trainer][INFO] - begin training epoch 888
[2024-10-08 08:17:31,079][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 08:21:21,732][train_inner][INFO] - {"epoch": 888, "update": 887.938, "loss": "0.558", "ntokens": "260675", "nsentences": "1745.67", "wps": "52687.4", "ups": "0.2", "wpb": "260675", "bsz": "1745.7", "num_updates": "42600", "lr": "0.000485598", "gnorm": "0.402", "loss_scale": "2", "train_wall": "310", "gb_free": "40.1", "wall": "27669"}
[2024-10-08 08:21:22,366][fairseq_cli.train][INFO] - end of epoch 888 (average epoch stats below)
[2024-10-08 08:21:22,369][train][INFO] - {"epoch": 888, "train_loss": "0.557", "train_ntokens": "260801", "train_nsentences": "1750.04", "train_wps": "54064.1", "train_ups": "0.21", "train_wpb": "260801", "train_bsz": "1750", "train_num_updates": "42603", "train_lr": "0.000485594", "train_gnorm": "0.38", "train_loss_scale": "2", "train_train_wall": "75", "train_gb_free": "40.2", "train_wall": "27669"}
[2024-10-08 08:21:22,672][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 08:21:22,677][fairseq.trainer][INFO] - begin training epoch 889
[2024-10-08 08:21:22,677][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 08:25:03,557][fairseq_cli.train][INFO] - end of epoch 889 (average epoch stats below)
[2024-10-08 08:25:03,570][train][INFO] - {"epoch": 889, "train_loss": "0.566", "train_ntokens": "261240", "train_nsentences": "1750.04", "train_wps": "56689.5", "train_ups": "0.22", "train_wpb": "261240", "train_bsz": "1750", "train_num_updates": "42651", "train_lr": "0.000485529", "train_gnorm": "0.409", "train_loss_scale": "2", "train_train_wall": "78", "train_gb_free": "39.2", "train_wall": "27891"}
[2024-10-08 08:25:03,698][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 08:25:03,712][fairseq.trainer][INFO] - begin training epoch 890
[2024-10-08 08:25:03,713][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 08:29:14,694][fairseq_cli.train][INFO] - end of epoch 890 (average epoch stats below)
[2024-10-08 08:29:14,703][train][INFO] - {"epoch": 890, "train_loss": "0.555", "train_ntokens": "260697", "train_nsentences": "1750.04", "train_wps": "49828.9", "train_ups": "0.19", "train_wpb": "260697", "train_bsz": "1750", "train_num_updates": "42699", "train_lr": "0.000485463", "train_gnorm": "0.357", "train_loss_scale": "2", "train_train_wall": "139", "train_gb_free": "39.2", "train_wall": "28142"}
[2024-10-08 08:29:14,935][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 08:29:14,965][fairseq.trainer][INFO] - begin training epoch 891
[2024-10-08 08:29:14,965][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 08:33:12,901][fairseq_cli.train][INFO] - end of epoch 891 (average epoch stats below)
[2024-10-08 08:33:12,921][train][INFO] - {"epoch": 891, "train_loss": "0.557", "train_ntokens": "260418", "train_nsentences": "1750.04", "train_wps": "52474.3", "train_ups": "0.2", "train_wpb": "260418", "train_bsz": "1750", "train_num_updates": "42747", "train_lr": "0.000485398", "train_gnorm": "0.383", "train_loss_scale": "2", "train_train_wall": "98", "train_gb_free": "40.2", "train_wall": "28380"}
[2024-10-08 08:33:13,306][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 08:33:13,310][fairseq.trainer][INFO] - begin training epoch 892
[2024-10-08 08:33:13,310][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 08:37:07,230][fairseq_cli.train][INFO] - end of epoch 892 (average epoch stats below)
[2024-10-08 08:37:07,235][train][INFO] - {"epoch": 892, "train_loss": "0.557", "train_ntokens": "261108", "train_nsentences": "1750.04", "train_wps": "53489.9", "train_ups": "0.2", "train_wpb": "261108", "train_bsz": "1750", "train_num_updates": "42795", "train_lr": "0.000485333", "train_gnorm": "0.392", "train_loss_scale": "2", "train_train_wall": "52", "train_gb_free": "39.2", "train_wall": "28614"}
[2024-10-08 08:37:07,549][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-08 08:37:07,583][fairseq.trainer][INFO] - begin training epoch 893
[2024-10-08 08:37:07,584][fairseq_cli.train][INFO] - Start iterating over samples
