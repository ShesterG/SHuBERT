[2024-10-08 17:23:18,308][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:12531', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'gloss'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_800k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-08 17:23:19,410][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:14074', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'gloss'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_800k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-08 17:23:19,903][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:17686', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'gloss'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_800k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-08 17:23:20,329][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:12908', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'gloss'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_800k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-08 17:23:20,585][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:15933', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'gloss'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_800k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-08 17:23:21,174][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:16104', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'gloss'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_800k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-08 17:23:21,275][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:10456', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'gloss'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_800k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-08 17:23:23,134][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:18744', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'gloss'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_800k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-08 17:23:25,216][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-08 17:23:25,222][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-08 17:23:25,222][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-08 17:23:25,222][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-08 17:23:25,223][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-08 17:23:25,230][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-08 17:23:25,351][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-08 17:23:25,370][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-08 17:23:25,370][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-08 17:23:25,370][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-08 17:23:25,371][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-08 17:23:25,372][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-08 17:23:25,988][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-08 17:23:26,028][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-08 17:23:26,028][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-08 17:23:26,028][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-08 17:23:26,045][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-08 17:23:26,046][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-08 17:23:27,102][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-08 17:23:27,104][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-08 17:23:27,104][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-08 17:23:27,104][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-08 17:23:27,121][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-08 17:23:27,122][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-08 17:23:28,574][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-08 17:23:28,582][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-08 17:23:28,582][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-08 17:23:28,582][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-08 17:23:28,583][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-08 17:23:28,590][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-08 17:23:30,658][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-08 17:23:31,587][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-08 17:23:31,920][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-08 17:23:34,326][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-08 17:23:35,078][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-08 17:23:35,080][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-08 17:23:35,080][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-08 17:23:35,080][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-08 17:23:35,081][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-08 17:23:35,106][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-08 17:23:39,751][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-08 17:23:44,088][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-08 17:23:44,329][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-08 17:23:44,395][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-08 17:23:44,395][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-08 17:23:44,395][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-08 17:23:44,396][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-08 17:23:44,397][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-08 17:23:53,618][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-08 17:23:53,620][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-08 17:23:53,620][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-08 17:23:53,620][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-08 17:23:53,725][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-08 17:23:53,726][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-08 17:24:33,849][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-08 17:24:41,086][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-08 17:27:08,350][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-08 17:27:08,358][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-08 17:27:08,358][fairseq.utils][INFO] - rank   1: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-08 17:27:08,358][fairseq.utils][INFO] - rank   2: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-08 17:27:08,358][fairseq.utils][INFO] - rank   3: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-08 17:27:08,358][fairseq.utils][INFO] - rank   4: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-08 17:27:08,358][fairseq.utils][INFO] - rank   5: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-08 17:27:08,358][fairseq.utils][INFO] - rank   6: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-08 17:27:08,358][fairseq.utils][INFO] - rank   7: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-08 17:27:08,358][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-08 17:27:08,358][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-08 17:27:08,361][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-08 17:27:08,361][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-08 17:27:51,469][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 161 @ 76607 updates)
[2024-10-08 17:27:51,480][fairseq.trainer][INFO] - loading train data for epoch 161
[2024-10-08 17:27:55,584][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-08 17:28:50,221][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-08 17:28:50,247][fairseq.trainer][INFO] - begin training epoch 161
[2024-10-08 17:28:50,247][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 17:29:17,910][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-08 17:29:17,910][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-08 17:29:17,911][fairseq.utils][INFO] - rank   1: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-08 17:29:17,911][fairseq.utils][INFO] - rank   2: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-08 17:29:17,911][fairseq.utils][INFO] - rank   3: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-08 17:29:17,911][fairseq.utils][INFO] - rank   4: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-08 17:29:17,911][fairseq.utils][INFO] - rank   5: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-08 17:29:17,911][fairseq.utils][INFO] - rank   6: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-08 17:29:17,911][fairseq.utils][INFO] - rank   7: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-08 17:29:17,911][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-08 17:29:17,911][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-08 17:29:17,911][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-08 17:29:17,913][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-08 17:29:55,198][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 161 @ 76607 updates)
[2024-10-08 17:29:55,206][fairseq.trainer][INFO] - loading train data for epoch 161
[2024-10-08 17:29:59,129][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-08 17:33:20,357][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-08 17:33:20,387][fairseq.trainer][INFO] - begin training epoch 161
[2024-10-08 17:33:20,388][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 17:33:36,327][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-08 17:33:36,327][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-08 17:33:36,327][fairseq.utils][INFO] - rank   1: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-08 17:33:36,327][fairseq.utils][INFO] - rank   2: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-08 17:33:36,327][fairseq.utils][INFO] - rank   3: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-08 17:33:36,327][fairseq.utils][INFO] - rank   4: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-08 17:33:36,327][fairseq.utils][INFO] - rank   5: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-08 17:33:36,327][fairseq.utils][INFO] - rank   6: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-08 17:33:36,327][fairseq.utils][INFO] - rank   7: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-08 17:33:36,327][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-08 17:33:36,328][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-08 17:33:36,328][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-08 17:33:36,329][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-08 17:34:28,207][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 161 @ 76607 updates)
[2024-10-08 17:34:28,240][fairseq.trainer][INFO] - loading train data for epoch 161
[2024-10-08 17:34:43,835][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-08 17:35:08,840][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-08 17:35:08,840][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-08 17:35:08,840][fairseq.utils][INFO] - rank   1: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-08 17:35:08,840][fairseq.utils][INFO] - rank   2: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-08 17:35:08,840][fairseq.utils][INFO] - rank   3: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-08 17:35:08,840][fairseq.utils][INFO] - rank   4: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-08 17:35:08,840][fairseq.utils][INFO] - rank   5: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-08 17:35:08,840][fairseq.utils][INFO] - rank   6: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-08 17:35:08,840][fairseq.utils][INFO] - rank   7: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-08 17:35:08,840][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-08 17:35:08,841][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-08 17:35:08,849][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-08 17:35:08,867][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-08 17:35:10,796][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-08 17:35:10,796][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-08 17:35:10,796][fairseq.utils][INFO] - rank   1: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-08 17:35:10,796][fairseq.utils][INFO] - rank   2: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-08 17:35:10,796][fairseq.utils][INFO] - rank   3: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-08 17:35:10,796][fairseq.utils][INFO] - rank   4: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-08 17:35:10,796][fairseq.utils][INFO] - rank   5: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-08 17:35:10,796][fairseq.utils][INFO] - rank   6: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-08 17:35:10,796][fairseq.utils][INFO] - rank   7: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-08 17:35:10,796][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-08 17:35:10,797][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-08 17:35:10,797][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-08 17:35:10,818][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-08 17:35:38,675][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 161 @ 76607 updates)
[2024-10-08 17:35:38,834][fairseq.trainer][INFO] - loading train data for epoch 161
[2024-10-08 17:35:40,346][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-08 17:35:40,347][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-08 17:35:40,347][fairseq.utils][INFO] - rank   1: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-08 17:35:40,347][fairseq.utils][INFO] - rank   2: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-08 17:35:40,347][fairseq.utils][INFO] - rank   3: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-08 17:35:40,347][fairseq.utils][INFO] - rank   4: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-08 17:35:40,347][fairseq.utils][INFO] - rank   5: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-08 17:35:40,347][fairseq.utils][INFO] - rank   6: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-08 17:35:40,347][fairseq.utils][INFO] - rank   7: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-08 17:35:40,347][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-08 17:35:40,347][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-08 17:35:40,347][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-08 17:35:40,348][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-08 17:35:42,310][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 161 @ 76607 updates)
[2024-10-08 17:35:42,532][fairseq.trainer][INFO] - loading train data for epoch 161
[2024-10-08 17:35:47,133][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-08 17:35:59,884][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-08 17:36:17,375][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 161 @ 76607 updates)
[2024-10-08 17:36:17,595][fairseq.trainer][INFO] - loading train data for epoch 161
[2024-10-08 17:36:24,383][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-08 17:36:46,184][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-08 17:36:46,185][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-08 17:36:46,185][fairseq.utils][INFO] - rank   1: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-08 17:36:46,185][fairseq.utils][INFO] - rank   2: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-08 17:36:46,185][fairseq.utils][INFO] - rank   3: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-08 17:36:46,185][fairseq.utils][INFO] - rank   4: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-08 17:36:46,185][fairseq.utils][INFO] - rank   5: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-08 17:36:46,185][fairseq.utils][INFO] - rank   6: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-08 17:36:46,185][fairseq.utils][INFO] - rank   7: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-08 17:36:46,185][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-08 17:36:46,185][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-08 17:36:46,188][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-08 17:36:46,195][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-08 17:37:02,317][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-08 17:37:02,333][fairseq.trainer][INFO] - begin training epoch 161
[2024-10-08 17:37:02,336][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 17:37:09,308][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-08 17:37:09,314][fairseq.trainer][INFO] - begin training epoch 161
[2024-10-08 17:37:09,315][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 17:37:28,047][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 161 @ 76607 updates)
[2024-10-08 17:37:28,160][fairseq.trainer][INFO] - loading train data for epoch 161
[2024-10-08 17:37:32,080][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-08 17:37:51,013][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-08 17:37:51,023][fairseq.trainer][INFO] - begin training epoch 161
[2024-10-08 17:37:51,023][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 17:38:09,015][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-08 17:38:09,032][fairseq.trainer][INFO] - begin training epoch 161
[2024-10-08 17:38:09,032][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 17:38:10,242][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-08 17:38:10,242][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-08 17:38:10,242][fairseq.utils][INFO] - rank   1: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-08 17:38:10,242][fairseq.utils][INFO] - rank   2: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-08 17:38:10,242][fairseq.utils][INFO] - rank   3: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-08 17:38:10,243][fairseq.utils][INFO] - rank   4: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-08 17:38:10,243][fairseq.utils][INFO] - rank   5: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-08 17:38:10,243][fairseq.utils][INFO] - rank   6: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-08 17:38:10,243][fairseq.utils][INFO] - rank   7: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-08 17:38:10,243][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-08 17:38:10,243][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-08 17:38:10,243][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-08 17:38:10,245][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-08 17:38:40,373][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-08 17:38:40,383][fairseq.trainer][INFO] - begin training epoch 161
[2024-10-08 17:38:40,383][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 17:38:48,724][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 161 @ 76607 updates)
[2024-10-08 17:38:48,899][fairseq.trainer][INFO] - loading train data for epoch 161
[2024-10-08 17:38:53,193][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-08 17:39:49,570][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-08 17:39:49,576][fairseq.trainer][INFO] - begin training epoch 161
[2024-10-08 17:39:49,576][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 18:24:50,862][train_inner][INFO] - {"epoch": 161, "update": 160.403, "loss": "1.017", "ntokens": "239610", "nsentences": "1753.11", "wps": "31066.1", "ups": "0.13", "wpb": "239610", "bsz": "1753.1", "num_updates": "76800", "lr": "0.00043913", "gnorm": "0.275", "loss_scale": "1", "train_wall": "1621", "gb_free": "40.1", "wall": "3074"}
[2024-10-08 18:24:51,043][train_inner][INFO] - {"epoch": 161, "update": 160.403, "loss": "1.018", "ntokens": "239610", "nsentences": "1753.11", "wps": "31048.3", "ups": "0.13", "wpb": "239610", "bsz": "1753.1", "num_updates": "76800", "lr": "0.00043913", "gnorm": "0.281", "loss_scale": "1", "train_wall": "1606", "gb_free": "40.1", "wall": "2980"}
[2024-10-08 18:51:52,804][train_inner][INFO] - {"epoch": 161, "update": 160.82, "loss": "1.022", "ntokens": "239718", "nsentences": "1757.81", "wps": "29578.3", "ups": "0.12", "wpb": "239718", "bsz": "1757.8", "num_updates": "77000", "lr": "0.000438859", "gnorm": "0.276", "loss_scale": "1", "train_wall": "1518", "gb_free": "39.9", "wall": "4696"}
[2024-10-08 18:51:52,800][train_inner][INFO] - {"epoch": 161, "update": 160.82, "loss": "1.022", "ntokens": "239718", "nsentences": "1757.81", "wps": "29580.4", "ups": "0.12", "wpb": "239718", "bsz": "1757.8", "num_updates": "77000", "lr": "0.000438859", "gnorm": "0.273", "loss_scale": "1", "train_wall": "1535", "gb_free": "39.9", "wall": "4601"}
[2024-10-08 19:00:50,310][fairseq_cli.train][INFO] - end of epoch 161 (average epoch stats below)
[2024-10-08 19:00:50,556][train][INFO] - {"epoch": 161, "train_loss": "1.02", "train_ntokens": "239248", "train_nsentences": "1753.71", "train_wps": "31407.9", "train_ups": "0.13", "train_wpb": "239248", "train_bsz": "1753.7", "train_num_updates": "77086", "train_lr": "0.000438742", "train_gnorm": "0.276", "train_loss_scale": "1", "train_train_wall": "3636", "train_gb_free": "40.6", "train_wall": "5234"}
[2024-10-08 19:00:52,030][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-08 19:00:52,076][fairseq.trainer][INFO] - begin training epoch 162
[2024-10-08 19:00:52,077][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 19:01:08,230][fairseq_cli.train][INFO] - end of epoch 161 (average epoch stats below)
[2024-10-08 19:01:08,235][train][INFO] - {"epoch": 161, "train_loss": "1.02", "train_ntokens": "239248", "train_nsentences": "1753.71", "train_wps": "31254", "train_ups": "0.13", "train_wpb": "239248", "train_bsz": "1753.7", "train_num_updates": "77086", "train_lr": "0.000438742", "train_gnorm": "0.277", "train_loss_scale": "1", "train_train_wall": "3654", "train_gb_free": "40.6", "train_wall": "5157"}
[2024-10-08 19:01:08,359][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-08 19:01:08,363][fairseq.trainer][INFO] - begin training epoch 162
[2024-10-08 19:01:08,363][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 19:33:55,521][train_inner][INFO] - {"epoch": 162, "update": 161.238, "loss": "1.019", "ntokens": "238938", "nsentences": "1726.36", "wps": "18949.2", "ups": "0.08", "wpb": "238938", "bsz": "1726.4", "num_updates": "77200", "lr": "0.000438587", "gnorm": "0.274", "loss_scale": "1", "train_wall": "1649", "gb_free": "39.6", "wall": "7124"}
[2024-10-08 19:33:55,521][train_inner][INFO] - {"epoch": 162, "update": 161.238, "loss": "1.019", "ntokens": "238938", "nsentences": "1726.36", "wps": "18945.1", "ups": "0.08", "wpb": "238938", "bsz": "1726.4", "num_updates": "77200", "lr": "0.000438587", "gnorm": "0.275", "loss_scale": "1", "train_wall": "1634", "gb_free": "39.6", "wall": "7219"}
[2024-10-08 20:02:30,683][train_inner][INFO] - {"epoch": 162, "update": 161.656, "loss": "1.019", "ntokens": "239785", "nsentences": "1741.21", "wps": "27971.6", "ups": "0.12", "wpb": "239785", "bsz": "1741.2", "num_updates": "77400", "lr": "0.000438315", "gnorm": "0.259", "loss_scale": "1", "train_wall": "1530", "gb_free": "39.3", "wall": "8839"}
[2024-10-08 20:02:33,673][train_inner][INFO] - {"epoch": 162, "update": 161.656, "loss": "1.019", "ntokens": "239785", "nsentences": "1741.21", "wps": "27914.6", "ups": "0.12", "wpb": "239785", "bsz": "1741.2", "num_updates": "77400", "lr": "0.000438315", "gnorm": "0.261", "loss_scale": "1", "train_wall": "1536", "gb_free": "39.3", "wall": "8937"}
[2024-10-08 20:19:14,007][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 162 @ 77565 updates
[2024-10-08 20:19:14,009][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-08 20:19:14,473][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 162 @ 77565 updates
[2024-10-08 20:19:14,474][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-08 20:19:18,684][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-08 20:19:18,687][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 162 @ 77565 updates, score None) (writing took 4.214236137006083 seconds)
[2024-10-08 20:19:18,687][fairseq_cli.train][INFO] - end of epoch 162 (average epoch stats below)
[2024-10-08 20:19:18,807][train][INFO] - {"epoch": 162, "train_loss": "1.019", "train_ntokens": "239162", "train_nsentences": "1753.71", "train_wps": "24332", "train_ups": "0.1", "train_wpb": "239162", "train_bsz": "1753.7", "train_num_updates": "77565", "train_lr": "0.000438091", "train_gnorm": "0.265", "train_loss_scale": "1", "train_train_wall": "3669", "train_gb_free": "39.4", "train_wall": "9942"}
[2024-10-08 20:19:18,871][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-08 20:19:18,907][fairseq.trainer][INFO] - begin training epoch 163
[2024-10-08 20:19:18,908][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 20:31:55,684][train_inner][INFO] - {"epoch": 163, "update": 162.073, "loss": "1.02", "ntokens": "238068", "nsentences": "1793.15", "wps": "27022.5", "ups": "0.11", "wpb": "238068", "bsz": "1793.2", "num_updates": "77600", "lr": "0.000438043", "gnorm": "0.267", "loss_scale": "1", "train_wall": "1253", "gb_free": "39.6", "wall": "10699"}
[2024-10-08 20:48:49,946][train_inner][INFO] - {"epoch": 163, "update": 162.491, "loss": "1.018", "ntokens": "239727", "nsentences": "1755.13", "wps": "47274.3", "ups": "0.2", "wpb": "239727", "bsz": "1755.1", "num_updates": "77800", "lr": "0.000437772", "gnorm": "0.283", "loss_scale": "1", "train_wall": "937", "gb_free": "39.7", "wall": "11714"}
[2024-10-08 21:09:29,989][train_inner][INFO] - {"epoch": 163, "update": 162.908, "loss": "1.02", "ntokens": "239704", "nsentences": "1751.2", "wps": "38665", "ups": "0.16", "wpb": "239704", "bsz": "1751.2", "num_updates": "78000", "lr": "0.0004375", "gnorm": "0.275", "loss_scale": "1", "train_wall": "974", "gb_free": "39.8", "wall": "12954"}
[2024-10-08 21:11:51,694][fairseq_cli.train][INFO] - end of epoch 163 (average epoch stats below)
[2024-10-08 21:11:51,703][train][INFO] - {"epoch": 163, "train_loss": "1.019", "train_ntokens": "239203", "train_nsentences": "1753.71", "train_wps": "36340.7", "train_ups": "0.15", "train_wpb": "239203", "train_bsz": "1753.7", "train_num_updates": "78044", "train_lr": "0.00043744", "train_gnorm": "0.278", "train_loss_scale": "1", "train_train_wall": "2295", "train_gb_free": "40.1", "train_wall": "13095"}
[2024-10-08 21:11:53,595][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-08 21:11:53,627][fairseq.trainer][INFO] - begin training epoch 164
[2024-10-08 21:11:53,627][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 21:23:16,681][train_inner][INFO] - {"epoch": 164, "update": 163.326, "loss": "1.019", "ntokens": "237852", "nsentences": "1793.58", "wps": "57546.9", "ups": "0.24", "wpb": "237852", "bsz": "1793.6", "num_updates": "78200", "lr": "0.000437228", "gnorm": "0.28", "loss_scale": "1", "train_wall": "305", "gb_free": "40.1", "wall": "13780"}
[2024-10-08 21:28:38,079][train_inner][INFO] - {"epoch": 164, "update": 163.743, "loss": "1.019", "ntokens": "239531", "nsentences": "1750.61", "wps": "149093", "ups": "0.62", "wpb": "239532", "bsz": "1750.6", "num_updates": "78400", "lr": "0.000436957", "gnorm": "0.273", "loss_scale": "1", "train_wall": "221", "gb_free": "39.7", "wall": "14102"}
[2024-10-08 21:33:34,947][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 164 @ 78523 updates
[2024-10-08 21:33:35,030][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-08 21:33:56,379][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-08 21:33:56,384][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 164 @ 78523 updates, score None) (writing took 21.436607900992385 seconds)
[2024-10-08 21:33:56,384][fairseq_cli.train][INFO] - end of epoch 164 (average epoch stats below)
[2024-10-08 21:33:56,404][train][INFO] - {"epoch": 164, "train_loss": "1.019", "train_ntokens": "239191", "train_nsentences": "1753.71", "train_wps": "86490.6", "train_ups": "0.36", "train_wpb": "239191", "train_bsz": "1753.7", "train_num_updates": "78523", "train_lr": "0.000436789", "train_gnorm": "0.28", "train_loss_scale": "1", "train_train_wall": "690", "train_gb_free": "39.6", "train_wall": "14420"}
[2024-10-08 21:33:56,511][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-08 21:33:56,531][fairseq.trainer][INFO] - begin training epoch 165
[2024-10-08 21:33:56,532][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 21:41:19,267][train_inner][INFO] - {"epoch": 165, "update": 164.161, "loss": "1.018", "ntokens": "239285", "nsentences": "1715.83", "wps": "62874.8", "ups": "0.26", "wpb": "239285", "bsz": "1715.8", "num_updates": "78600", "lr": "0.000436685", "gnorm": "0.292", "loss_scale": "1", "train_wall": "334", "gb_free": "39.4", "wall": "14863"}
[2024-10-08 21:43:04,277][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
[2024-10-08 21:46:06,329][train_inner][INFO] - {"epoch": 165, "update": 164.58, "loss": "1.019", "ntokens": "239257", "nsentences": "1766.92", "wps": "166718", "ups": "0.7", "wpb": "239257", "bsz": "1766.9", "num_updates": "78800", "lr": "0.000436413", "gnorm": "0.27", "loss_scale": "1", "train_wall": "132", "gb_free": "39.6", "wall": "15150"}
[2024-10-08 21:51:41,760][train_inner][INFO] - {"epoch": 165, "update": 164.998, "loss": "1.022", "ntokens": "240365", "nsentences": "1748.21", "wps": "143330", "ups": "0.6", "wpb": "240365", "bsz": "1748.2", "num_updates": "79000", "lr": "0.000436141", "gnorm": "0.286", "loss_scale": "1", "train_wall": "175", "gb_free": "40.1", "wall": "15485"}
[2024-10-08 21:51:42,049][fairseq_cli.train][INFO] - end of epoch 165 (average epoch stats below)
[2024-10-08 21:51:42,052][train][INFO] - {"epoch": 165, "train_loss": "1.02", "train_ntokens": "239359", "train_nsentences": "1753.68", "train_wps": "107366", "train_ups": "0.45", "train_wpb": "239359", "train_bsz": "1753.7", "train_num_updates": "79001", "train_lr": "0.00043614", "train_gnorm": "0.282", "train_loss_scale": "1", "train_train_wall": "352", "train_gb_free": "39.9", "train_wall": "15486"}
[2024-10-08 21:51:42,115][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-08 21:51:42,136][fairseq.trainer][INFO] - begin training epoch 166
[2024-10-08 21:51:42,137][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 22:01:41,575][train_inner][INFO] - {"epoch": 166, "update": 165.415, "loss": "1.016", "ntokens": "238575", "nsentences": "1767.28", "wps": "79554", "ups": "0.33", "wpb": "238575", "bsz": "1767.3", "num_updates": "79200", "lr": "0.00043587", "gnorm": "0.276", "loss_scale": "1", "train_wall": "165", "gb_free": "39.6", "wall": "16085"}
[2024-10-08 22:06:48,428][train_inner][INFO] - {"epoch": 166, "update": 165.833, "loss": "1.022", "ntokens": "239540", "nsentences": "1777.01", "wps": "156135", "ups": "0.65", "wpb": "239540", "bsz": "1777", "num_updates": "79400", "lr": "0.000435598", "gnorm": "0.274", "loss_scale": "1", "train_wall": "166", "gb_free": "39.8", "wall": "16392"}
[2024-10-08 22:08:45,583][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 166 @ 79480 updates
[2024-10-08 22:08:45,682][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-08 22:08:56,084][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-08 22:08:56,115][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 166 @ 79480 updates, score None) (writing took 10.532236557017313 seconds)
[2024-10-08 22:08:56,116][fairseq_cli.train][INFO] - end of epoch 166 (average epoch stats below)
[2024-10-08 22:08:56,118][train][INFO] - {"epoch": 166, "train_loss": "1.019", "train_ntokens": "239355", "train_nsentences": "1753.71", "train_wps": "110874", "train_ups": "0.46", "train_wpb": "239355", "train_bsz": "1753.7", "train_num_updates": "79480", "train_lr": "0.000435489", "train_gnorm": "0.277", "train_loss_scale": "1", "train_train_wall": "393", "train_gb_free": "39.2", "train_wall": "16520"}
[2024-10-08 22:08:56,325][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-08 22:08:56,350][fairseq.trainer][INFO] - begin training epoch 167
[2024-10-08 22:08:56,351][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 22:18:09,507][train_inner][INFO] - {"epoch": 167, "update": 166.251, "loss": "1.017", "ntokens": "238763", "nsentences": "1741.27", "wps": "70114.7", "ups": "0.29", "wpb": "238763", "bsz": "1741.3", "num_updates": "79600", "lr": "0.000435326", "gnorm": "0.281", "loss_scale": "1", "train_wall": "152", "gb_free": "39.6", "wall": "17073"}
[2024-10-08 22:22:53,576][train_inner][INFO] - {"epoch": 167, "update": 166.668, "loss": "1.017", "ntokens": "239571", "nsentences": "1720.04", "wps": "168675", "ups": "0.7", "wpb": "239571", "bsz": "1720", "num_updates": "79800", "lr": "0.000435054", "gnorm": "0.279", "loss_scale": "1", "train_wall": "139", "gb_free": "39.6", "wall": "17357"}
[2024-10-08 22:26:51,261][fairseq_cli.train][INFO] - end of epoch 167 (average epoch stats below)
[2024-10-08 22:26:51,281][train][INFO] - {"epoch": 167, "train_loss": "1.018", "train_ntokens": "239058", "train_nsentences": "1753.71", "train_wps": "106504", "train_ups": "0.45", "train_wpb": "239058", "train_bsz": "1753.7", "train_num_updates": "79959", "train_lr": "0.000434838", "train_gnorm": "0.278", "train_loss_scale": "1", "train_train_wall": "347", "train_gb_free": "39.3", "train_wall": "17595"}
[2024-10-08 22:26:51,701][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-08 22:26:51,739][fairseq.trainer][INFO] - begin training epoch 168
[2024-10-08 22:26:51,739][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 22:33:38,788][train_inner][INFO] - {"epoch": 168, "update": 167.086, "loss": "1.02", "ntokens": "238534", "nsentences": "1764.57", "wps": "73941.2", "ups": "0.31", "wpb": "238534", "bsz": "1764.6", "num_updates": "80000", "lr": "0.000434783", "gnorm": "0.278", "loss_scale": "1", "train_wall": "144", "gb_free": "40.2", "wall": "18002"}
[2024-10-08 22:37:57,929][train_inner][INFO] - {"epoch": 168, "update": 167.503, "loss": "1.017", "ntokens": "239710", "nsentences": "1751.62", "wps": "185020", "ups": "0.77", "wpb": "239710", "bsz": "1751.6", "num_updates": "80200", "lr": "0.000434511", "gnorm": "0.285", "loss_scale": "1", "train_wall": "149", "gb_free": "41.3", "wall": "18262"}
[2024-10-08 22:43:15,387][train_inner][INFO] - {"epoch": 168, "update": 167.921, "loss": "1.022", "ntokens": "239914", "nsentences": "1772.85", "wps": "151157", "ups": "0.63", "wpb": "239914", "bsz": "1772.9", "num_updates": "80400", "lr": "0.000434239", "gnorm": "0.286", "loss_scale": "1", "train_wall": "106", "gb_free": "39.2", "wall": "18579"}
[2024-10-08 22:44:00,663][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 168 @ 80438 updates
[2024-10-08 22:44:00,664][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-08 22:44:22,258][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-08 22:44:22,263][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 168 @ 80438 updates, score None) (writing took 21.600128406018484 seconds)
[2024-10-08 22:44:22,264][fairseq_cli.train][INFO] - end of epoch 168 (average epoch stats below)
[2024-10-08 22:44:22,268][train][INFO] - {"epoch": 168, "train_loss": "1.019", "train_ntokens": "239476", "train_nsentences": "1753.71", "train_wps": "109145", "train_ups": "0.46", "train_wpb": "239476", "train_bsz": "1753.7", "train_num_updates": "80438", "train_lr": "0.000434188", "train_gnorm": "0.283", "train_loss_scale": "1", "train_train_wall": "321", "train_gb_free": "39.2", "train_wall": "18646"}
[2024-10-08 22:44:22,388][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-08 22:44:22,403][fairseq.trainer][INFO] - begin training epoch 169
[2024-10-08 22:44:22,403][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 22:53:56,059][train_inner][INFO] - {"epoch": 169, "update": 168.338, "loss": "1.018", "ntokens": "238826", "nsentences": "1759.61", "wps": "74557.6", "ups": "0.31", "wpb": "238826", "bsz": "1759.6", "num_updates": "80600", "lr": "0.000433967", "gnorm": "0.275", "loss_scale": "1", "train_wall": "157", "gb_free": "39.8", "wall": "19220"}
[2024-10-08 22:58:45,785][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
[2024-10-08 22:59:11,256][train_inner][INFO] - {"epoch": 169, "update": 168.758, "loss": "1.017", "ntokens": "239040", "nsentences": "1787.24", "wps": "151682", "ups": "0.63", "wpb": "239040", "bsz": "1787.2", "num_updates": "80800", "lr": "0.000433696", "gnorm": "0.276", "loss_scale": "1", "train_wall": "137", "gb_free": "39.3", "wall": "19535"}
[2024-10-08 23:01:59,329][fairseq_cli.train][INFO] - end of epoch 169 (average epoch stats below)
[2024-10-08 23:01:59,640][train][INFO] - {"epoch": 169, "train_loss": "1.017", "train_ntokens": "239189", "train_nsentences": "1752.52", "train_wps": "108129", "train_ups": "0.45", "train_wpb": "239189", "train_bsz": "1752.5", "train_num_updates": "80916", "train_lr": "0.000433538", "train_gnorm": "0.277", "train_loss_scale": "1", "train_train_wall": "348", "train_gb_free": "39.6", "train_wall": "19703"}
[2024-10-08 23:02:05,705][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-08 23:02:05,907][fairseq.trainer][INFO] - begin training epoch 170
[2024-10-08 23:02:05,908][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 23:09:51,601][train_inner][INFO] - {"epoch": 170, "update": 169.175, "loss": "1.016", "ntokens": "239453", "nsentences": "1672.52", "wps": "74791.1", "ups": "0.31", "wpb": "239453", "bsz": "1672.5", "num_updates": "81000", "lr": "0.000433424", "gnorm": "0.282", "loss_scale": "1", "train_wall": "148", "gb_free": "40.1", "wall": "20175"}
[2024-10-08 23:14:53,787][train_inner][INFO] - {"epoch": 170, "update": 169.593, "loss": "1.016", "ntokens": "239239", "nsentences": "1759.81", "wps": "158354", "ups": "0.66", "wpb": "239239", "bsz": "1759.8", "num_updates": "81200", "lr": "0.000433152", "gnorm": "0.279", "loss_scale": "1", "train_wall": "137", "gb_free": "39.3", "wall": "20477"}
[2024-10-08 23:19:57,333][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 170 @ 81395 updates
[2024-10-08 23:19:57,341][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-08 23:20:00,913][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-08 23:20:00,918][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 170 @ 81395 updates, score None) (writing took 3.584878431982361 seconds)
[2024-10-08 23:20:00,919][fairseq_cli.train][INFO] - end of epoch 170 (average epoch stats below)
[2024-10-08 23:20:00,922][train][INFO] - {"epoch": 170, "train_loss": "1.017", "train_ntokens": "239050", "train_nsentences": "1753.71", "train_wps": "105898", "train_ups": "0.44", "train_wpb": "239050", "train_bsz": "1753.7", "train_num_updates": "81395", "train_lr": "0.000432887", "train_gnorm": "0.276", "train_loss_scale": "1", "train_train_wall": "348", "train_gb_free": "39.7", "train_wall": "20785"}
[2024-10-08 23:20:01,011][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-08 23:20:01,045][fairseq.trainer][INFO] - begin training epoch 171
[2024-10-08 23:20:01,046][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 23:26:01,743][train_inner][INFO] - {"epoch": 171, "update": 170.01, "loss": "1.018", "ntokens": "238523", "nsentences": "1758.57", "wps": "71421.1", "ups": "0.3", "wpb": "238523", "bsz": "1758.6", "num_updates": "81400", "lr": "0.00043288", "gnorm": "0.27", "loss_scale": "1", "train_wall": "163", "gb_free": "39.6", "wall": "21145"}
[2024-10-08 23:29:45,502][train_inner][INFO] - {"epoch": 171, "update": 170.428, "loss": "1.015", "ntokens": "240691", "nsentences": "1715.12", "wps": "215140", "ups": "0.89", "wpb": "240691", "bsz": "1715.1", "num_updates": "81600", "lr": "0.000432609", "gnorm": "0.276", "loss_scale": "1", "train_wall": "156", "gb_free": "39.6", "wall": "21369"}
[2024-10-08 23:34:49,346][train_inner][INFO] - {"epoch": 171, "update": 170.846, "loss": "1.019", "ntokens": "238833", "nsentences": "1810.8", "wps": "157210", "ups": "0.66", "wpb": "238833", "bsz": "1810.8", "num_updates": "81800", "lr": "0.000432337", "gnorm": "0.285", "loss_scale": "1", "train_wall": "124", "gb_free": "39.7", "wall": "21673"}
[2024-10-08 23:36:22,888][fairseq_cli.train][INFO] - end of epoch 171 (average epoch stats below)
[2024-10-08 23:36:22,934][train][INFO] - {"epoch": 171, "train_loss": "1.017", "train_ntokens": "239329", "train_nsentences": "1753.71", "train_wps": "116739", "train_ups": "0.49", "train_wpb": "239329", "train_bsz": "1753.7", "train_num_updates": "81874", "train_lr": "0.000432236", "train_gnorm": "0.281", "train_loss_scale": "1", "train_train_wall": "356", "train_gb_free": "40.6", "train_wall": "21767"}
[2024-10-08 23:36:23,838][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-08 23:36:23,885][fairseq.trainer][INFO] - begin training epoch 172
[2024-10-08 23:36:23,909][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 23:44:56,620][train_inner][INFO] - {"epoch": 172, "update": 171.263, "loss": "1.016", "ntokens": "238813", "nsentences": "1721.82", "wps": "78652.7", "ups": "0.33", "wpb": "238813", "bsz": "1721.8", "num_updates": "82000", "lr": "0.000432065", "gnorm": "0.27", "loss_scale": "1", "train_wall": "263", "gb_free": "40.1", "wall": "22280"}
[2024-10-08 23:49:38,160][train_inner][INFO] - {"epoch": 172, "update": 171.681, "loss": "1.015", "ntokens": "239637", "nsentences": "1753.56", "wps": "170242", "ups": "0.71", "wpb": "239637", "bsz": "1753.6", "num_updates": "82200", "lr": "0.000431793", "gnorm": "0.275", "loss_scale": "1", "train_wall": "278", "gb_free": "39.3", "wall": "22562"}
[2024-10-08 23:52:45,816][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 172 @ 82353 updates
[2024-10-08 23:52:45,818][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-08 23:53:05,182][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-08 23:53:05,657][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 172 @ 82353 updates, score None) (writing took 19.84098380402429 seconds)
[2024-10-08 23:53:05,690][fairseq_cli.train][INFO] - end of epoch 172 (average epoch stats below)
[2024-10-08 23:53:05,844][train][INFO] - {"epoch": 172, "train_loss": "1.017", "train_ntokens": "239317", "train_nsentences": "1753.71", "train_wps": "114318", "train_ups": "0.48", "train_wpb": "239317", "train_bsz": "1753.7", "train_num_updates": "82353", "train_lr": "0.000431586", "train_gnorm": "0.277", "train_loss_scale": "1", "train_train_wall": "656", "train_gb_free": "39.3", "train_wall": "22770"}
[2024-10-08 23:53:05,940][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-08 23:53:05,954][fairseq.trainer][INFO] - begin training epoch 173
[2024-10-08 23:53:05,955][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-08 23:59:58,798][train_inner][INFO] - {"epoch": 173, "update": 172.098, "loss": "1.02", "ntokens": "238611", "nsentences": "1775.99", "wps": "76892.7", "ups": "0.32", "wpb": "238611", "bsz": "1776", "num_updates": "82400", "lr": "0.000431522", "gnorm": "0.285", "loss_scale": "1", "train_wall": "291", "gb_free": "39.3", "wall": "23182"}
[2024-10-09 00:04:18,981][train_inner][INFO] - {"epoch": 173, "update": 172.516, "loss": "1.015", "ntokens": "239376", "nsentences": "1771.45", "wps": "184009", "ups": "0.77", "wpb": "239376", "bsz": "1771.5", "num_updates": "82600", "lr": "0.00043125", "gnorm": "0.285", "loss_scale": "1", "train_wall": "256", "gb_free": "39.2", "wall": "23443"}
[2024-10-09 00:09:06,230][train_inner][INFO] - {"epoch": 173, "update": 172.933, "loss": "1.018", "ntokens": "240036", "nsentences": "1716.86", "wps": "167133", "ups": "0.7", "wpb": "240036", "bsz": "1716.9", "num_updates": "82800", "lr": "0.000430978", "gnorm": "0.29", "loss_scale": "1", "train_wall": "284", "gb_free": "39.3", "wall": "23730"}
[2024-10-09 00:09:40,478][fairseq_cli.train][INFO] - end of epoch 173 (average epoch stats below)
[2024-10-09 00:09:40,520][train][INFO] - {"epoch": 173, "train_loss": "1.017", "train_ntokens": "239150", "train_nsentences": "1753.71", "train_wps": "115168", "train_ups": "0.48", "train_wpb": "239150", "train_bsz": "1753.7", "train_num_updates": "82832", "train_lr": "0.000430935", "train_gnorm": "0.288", "train_loss_scale": "2", "train_train_wall": "680", "train_gb_free": "39.2", "train_wall": "23764"}
[2024-10-09 00:09:40,968][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-09 00:09:41,007][fairseq.trainer][INFO] - begin training epoch 174
[2024-10-09 00:09:41,008][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 00:19:29,245][train_inner][INFO] - {"epoch": 174, "update": 173.351, "loss": "1.014", "ntokens": "238578", "nsentences": "1740.22", "wps": "76588.8", "ups": "0.32", "wpb": "238578", "bsz": "1740.2", "num_updates": "83000", "lr": "0.000430707", "gnorm": "0.278", "loss_scale": "2", "train_wall": "279", "gb_free": "40.1", "wall": "24353"}
[2024-10-09 00:24:10,801][train_inner][INFO] - {"epoch": 174, "update": 173.768, "loss": "1.017", "ntokens": "239479", "nsentences": "1757.92", "wps": "170116", "ups": "0.71", "wpb": "239480", "bsz": "1757.9", "num_updates": "83200", "lr": "0.000430435", "gnorm": "0.276", "loss_scale": "2", "train_wall": "278", "gb_free": "39.2", "wall": "24634"}
[2024-10-09 00:26:27,988][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 174 @ 83311 updates
[2024-10-09 00:26:27,993][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-09 00:26:44,667][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-09 00:26:45,999][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 174 @ 83311 updates, score None) (writing took 18.011236972000916 seconds)
[2024-10-09 00:26:46,013][fairseq_cli.train][INFO] - end of epoch 174 (average epoch stats below)
[2024-10-09 00:26:46,016][train][INFO] - {"epoch": 174, "train_loss": "1.016", "train_ntokens": "239187", "train_nsentences": "1753.71", "train_wps": "111723", "train_ups": "0.47", "train_wpb": "239188", "train_bsz": "1753.7", "train_num_updates": "83311", "train_lr": "0.000430284", "train_gnorm": "0.276", "train_loss_scale": "2", "train_train_wall": "658", "train_gb_free": "40.1", "train_wall": "24790"}
[2024-10-09 00:26:46,481][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-09 00:26:46,523][fairseq.trainer][INFO] - begin training epoch 175
[2024-10-09 00:26:46,523][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 00:34:50,358][train_inner][INFO] - {"epoch": 175, "update": 174.186, "loss": "1.018", "ntokens": "238060", "nsentences": "1797.9", "wps": "74447", "ups": "0.31", "wpb": "238060", "bsz": "1797.9", "num_updates": "83400", "lr": "0.000430163", "gnorm": "0.284", "loss_scale": "2", "train_wall": "296", "gb_free": "39.2", "wall": "25274"}
[2024-10-09 00:39:01,075][train_inner][INFO] - {"epoch": 175, "update": 174.603, "loss": "1.017", "ntokens": "239788", "nsentences": "1756.4", "wps": "191291", "ups": "0.8", "wpb": "239788", "bsz": "1756.4", "num_updates": "83600", "lr": "0.000429891", "gnorm": "0.267", "loss_scale": "2", "train_wall": "247", "gb_free": "39.3", "wall": "25525"}
[2024-10-09 00:40:13,214][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
[2024-10-09 00:43:06,493][fairseq_cli.train][INFO] - end of epoch 175 (average epoch stats below)
[2024-10-09 00:43:06,523][train][INFO] - {"epoch": 175, "train_loss": "1.016", "train_ntokens": "239371", "train_nsentences": "1754.14", "train_wps": "116699", "train_ups": "0.49", "train_wpb": "239371", "train_bsz": "1754.1", "train_num_updates": "83789", "train_lr": "0.000429635", "train_gnorm": "0.274", "train_loss_scale": "1", "train_train_wall": "650", "train_gb_free": "40.2", "train_wall": "25770"}
[2024-10-09 00:43:07,004][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-09 00:43:07,032][fairseq.trainer][INFO] - begin training epoch 176
[2024-10-09 00:43:07,033][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 00:49:36,192][train_inner][INFO] - {"epoch": 176, "update": 175.023, "loss": "1.015", "ntokens": "239394", "nsentences": "1724.7", "wps": "75386.9", "ups": "0.31", "wpb": "239394", "bsz": "1724.7", "num_updates": "83800", "lr": "0.00042962", "gnorm": "0.281", "loss_scale": "1", "train_wall": "273", "gb_free": "39.7", "wall": "26160"}
[2024-10-09 00:53:51,080][train_inner][INFO] - {"epoch": 176, "update": 175.441, "loss": "1.014", "ntokens": "239484", "nsentences": "1783.54", "wps": "187921", "ups": "0.78", "wpb": "239484", "bsz": "1783.5", "num_updates": "84000", "lr": "0.000429348", "gnorm": "0.286", "loss_scale": "1", "train_wall": "251", "gb_free": "39.2", "wall": "26415"}
[2024-10-09 00:58:26,802][train_inner][INFO] - {"epoch": 176, "update": 175.858, "loss": "1.014", "ntokens": "239518", "nsentences": "1753.51", "wps": "173779", "ups": "0.73", "wpb": "239518", "bsz": "1753.5", "num_updates": "84200", "lr": "0.000429076", "gnorm": "0.282", "loss_scale": "1", "train_wall": "270", "gb_free": "39.6", "wall": "26690"}
[2024-10-09 00:59:43,959][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 176 @ 84268 updates
[2024-10-09 00:59:43,966][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-09 00:59:52,093][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-09 00:59:52,293][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 176 @ 84268 updates, score None) (writing took 8.33347258597496 seconds)
[2024-10-09 00:59:52,294][fairseq_cli.train][INFO] - end of epoch 176 (average epoch stats below)
[2024-10-09 00:59:52,308][train][INFO] - {"epoch": 176, "train_loss": "1.015", "train_ntokens": "239211", "train_nsentences": "1753.71", "train_wps": "113924", "train_ups": "0.48", "train_wpb": "239211", "train_bsz": "1753.7", "train_num_updates": "84268", "train_lr": "0.000428984", "train_gnorm": "0.285", "train_loss_scale": "1", "train_train_wall": "629", "train_gb_free": "40.1", "train_wall": "26776"}
[2024-10-09 00:59:52,459][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-09 00:59:52,474][fairseq.trainer][INFO] - begin training epoch 177
[2024-10-09 00:59:52,475][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-09 01:08:32,917][train_inner][INFO] - {"epoch": 177, "update": 176.276, "loss": "1.015", "ntokens": "238658", "nsentences": "1725.89", "wps": "78751.2", "ups": "0.33", "wpb": "238658", "bsz": "1725.9", "num_updates": "84400", "lr": "0.000428804", "gnorm": "0.276", "loss_scale": "1", "train_wall": "258", "gb_free": "39.6", "wall": "27297"}
[2024-10-09 01:13:01,063][train_inner][INFO] - {"epoch": 177, "update": 176.693, "loss": "1.011", "ntokens": "239845", "nsentences": "1763.7", "wps": "178897", "ups": "0.75", "wpb": "239845", "bsz": "1763.7", "num_updates": "84600", "lr": "0.000428533", "gnorm": "0.275", "loss_scale": "1", "train_wall": "265", "gb_free": "39.6", "wall": "27565"}
[2024-10-09 01:16:21,379][fairseq_cli.train][INFO] - end of epoch 177 (average epoch stats below)
[2024-10-09 01:16:21,412][train][INFO] - {"epoch": 177, "train_loss": "1.014", "train_ntokens": "239233", "train_nsentences": "1753.71", "train_wps": "115856", "train_ups": "0.48", "train_wpb": "239233", "train_bsz": "1753.7", "train_num_updates": "84747", "train_lr": "0.000428333", "train_gnorm": "0.278", "train_loss_scale": "1", "train_train_wall": "644", "train_gb_free": "40.1", "train_wall": "27765"}
[2024-10-09 01:16:21,982][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-09 01:16:22,015][fairseq.trainer][INFO] - begin training epoch 178
[2024-10-09 01:16:22,016][fairseq_cli.train][INFO] - Start iterating over samples
