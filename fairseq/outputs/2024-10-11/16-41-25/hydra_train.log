[2024-10-11 16:42:02,779][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:11723', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-11 16:42:05,099][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:18356', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-11 16:42:05,984][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:14213', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-11 16:42:06,211][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:11656', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-11 16:42:06,482][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:11746', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-11 16:42:06,666][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:10227', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-11 16:42:07,185][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:14912', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-11 16:42:07,312][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-11 16:42:07,314][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-11 16:42:07,314][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-11 16:42:07,314][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-11 16:42:07,315][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-11 16:42:07,315][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-11 16:42:07,778][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-11 16:42:08,375][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:19571', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-11 16:42:08,803][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-11 16:42:08,835][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-11 16:42:08,841][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-11 16:42:08,841][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-11 16:42:08,842][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-11 16:42:08,843][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-11 16:42:09,489][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-11 16:42:09,491][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-11 16:42:09,511][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-11 16:42:09,511][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-11 16:42:09,512][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-11 16:42:09,512][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-11 16:42:09,853][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-11 16:42:09,955][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-11 16:42:10,841][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-11 16:42:10,848][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-11 16:42:10,855][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-11 16:42:10,855][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-11 16:42:10,856][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-11 16:42:10,856][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-11 16:42:11,634][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-11 16:42:12,353][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-11 16:42:12,354][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-11 16:42:12,364][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-11 16:42:12,364][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-11 16:42:12,365][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-11 16:42:12,366][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-11 16:42:12,924][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-11 16:42:15,760][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-11 16:42:15,762][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-11 16:42:15,762][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-11 16:42:15,762][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-11 16:42:15,763][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-11 16:42:15,764][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-11 16:42:16,209][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-11 16:42:18,514][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-11 16:42:18,669][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-11 16:42:18,669][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-11 16:42:18,669][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-11 16:42:18,670][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-11 16:42:18,671][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-11 16:42:36,055][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-11 16:42:50,437][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-11 16:42:50,523][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-11 16:42:50,523][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-11 16:42:50,523][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-11 16:42:50,524][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-11 16:42:50,524][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-11 16:42:54,142][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-11 16:43:05,950][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-11 16:43:05,950][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 16:43:05,950][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 16:43:05,950][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 16:43:05,950][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 16:43:05,950][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 16:43:05,951][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 16:43:05,951][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 16:43:05,951][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 16:43:05,951][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-11 16:43:05,951][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-11 16:43:05,951][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-11 16:43:05,952][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-11 16:43:23,118][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-11 16:43:23,202][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 16:43:23,202][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 16:43:23,202][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 16:43:23,202][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 16:43:23,202][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 16:43:23,202][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 16:43:23,202][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 16:43:23,202][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 16:43:23,202][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-11 16:43:23,203][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-11 16:43:23,203][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-11 16:43:23,206][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-11 16:43:43,990][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 1941 @ 93079 updates)
[2024-10-11 16:43:44,016][fairseq.trainer][INFO] - loading train data for epoch 1941
[2024-10-11 16:43:45,045][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-11 16:43:51,202][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-11 16:43:51,215][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 16:43:51,215][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 16:43:51,215][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 16:43:51,215][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 16:43:51,215][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 16:43:51,215][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 16:43:51,215][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 16:43:51,215][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 16:43:51,215][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-11 16:43:51,216][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-11 16:43:51,216][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-11 16:43:51,217][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-11 16:43:56,757][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 16:43:56,764][fairseq.trainer][INFO] - begin training epoch 1941
[2024-10-11 16:43:56,771][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 16:44:07,702][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-11 16:44:07,703][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 16:44:07,703][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 16:44:07,703][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 16:44:07,703][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 16:44:07,703][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 16:44:07,703][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 16:44:07,703][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 16:44:07,703][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 16:44:07,703][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-11 16:44:07,704][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-11 16:44:07,704][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-11 16:44:07,706][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-11 16:44:09,848][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 1941 @ 93079 updates)
[2024-10-11 16:44:09,852][fairseq.trainer][INFO] - loading train data for epoch 1941
[2024-10-11 16:44:10,414][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-11 16:44:25,582][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 16:44:25,590][fairseq.trainer][INFO] - begin training epoch 1941
[2024-10-11 16:44:25,591][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 16:44:25,664][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 1941 @ 93079 updates)
[2024-10-11 16:44:25,666][fairseq.trainer][INFO] - loading train data for epoch 1941
[2024-10-11 16:44:26,248][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-11 16:44:32,414][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 16:44:32,447][fairseq.trainer][INFO] - begin training epoch 1941
[2024-10-11 16:44:32,448][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 16:44:46,292][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 1941 @ 93079 updates)
[2024-10-11 16:44:46,294][fairseq.trainer][INFO] - loading train data for epoch 1941
[2024-10-11 16:44:47,009][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-11 16:44:51,820][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-11 16:44:51,820][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 16:44:51,820][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 16:44:51,820][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 16:44:51,820][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 16:44:51,820][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 16:44:51,820][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 16:44:51,820][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 16:44:51,820][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 16:44:51,820][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-11 16:44:51,821][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-11 16:44:51,821][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-11 16:44:51,822][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-11 16:44:59,761][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 16:44:59,772][fairseq.trainer][INFO] - begin training epoch 1941
[2024-10-11 16:44:59,773][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 16:45:25,792][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-11 16:45:25,803][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 16:45:25,803][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 16:45:25,803][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 16:45:25,803][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 16:45:25,803][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 16:45:25,803][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 16:45:25,803][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 16:45:25,803][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 16:45:25,803][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-11 16:45:25,804][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-11 16:45:25,804][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-11 16:45:25,805][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-11 16:45:43,951][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-11 16:45:43,951][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 16:45:43,951][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 16:45:43,951][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 16:45:43,951][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 16:45:43,951][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 16:45:43,951][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 16:45:43,951][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 16:45:43,951][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 16:45:43,951][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-11 16:45:43,952][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-11 16:45:43,952][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-11 16:45:43,953][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-11 16:46:03,626][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 1941 @ 93079 updates)
[2024-10-11 16:46:03,628][fairseq.trainer][INFO] - loading train data for epoch 1941
[2024-10-11 16:46:04,262][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-11 16:46:15,917][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 16:46:15,931][fairseq.trainer][INFO] - begin training epoch 1941
[2024-10-11 16:46:15,947][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 16:46:24,121][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 1941 @ 93079 updates)
[2024-10-11 16:46:24,141][fairseq.trainer][INFO] - loading train data for epoch 1941
[2024-10-11 16:46:24,498][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-11 16:46:24,687][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-11 16:46:24,687][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 16:46:24,688][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 16:46:24,688][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 16:46:24,688][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 16:46:24,688][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 16:46:24,688][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 16:46:24,688][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 16:46:24,688][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 16:46:24,688][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-11 16:46:24,688][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-11 16:46:24,689][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-11 16:46:24,690][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-11 16:46:32,049][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 16:46:32,053][fairseq.trainer][INFO] - begin training epoch 1941
[2024-10-11 16:46:32,054][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 16:46:38,793][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 1941 @ 93079 updates)
[2024-10-11 16:46:38,795][fairseq.trainer][INFO] - loading train data for epoch 1941
[2024-10-11 16:46:39,257][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 1941 @ 93079 updates)
[2024-10-11 16:46:39,260][fairseq.trainer][INFO] - loading train data for epoch 1941
[2024-10-11 16:46:39,614][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-11 16:46:39,907][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-11 16:46:46,989][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 16:46:46,997][fairseq.trainer][INFO] - begin training epoch 1941
[2024-10-11 16:46:46,997][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 16:47:02,628][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 16:47:02,633][fairseq.trainer][INFO] - begin training epoch 1941
[2024-10-11 16:47:02,635][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 16:52:59,707][fairseq.trainer][WARNING] - OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 176.00 MiB. GPU 
[2024-10-11 16:52:59,731][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 1            |        cudaMalloc retries: 1         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   5237 MiB |   5413 MiB |   7736 MiB |   2499 MiB |
|       from large pool |   5195 MiB |   5371 MiB |   7583 MiB |   2388 MiB |
|       from small pool |     41 MiB |    100 MiB |    152 MiB |    111 MiB |
|---------------------------------------------------------------------------|
| Active memory         |   5237 MiB |   5413 MiB |   7736 MiB |   2499 MiB |
|       from large pool |   5195 MiB |   5371 MiB |   7583 MiB |   2388 MiB |
|       from small pool |     41 MiB |    100 MiB |    152 MiB |    111 MiB |
|---------------------------------------------------------------------------|
| Requested memory      |   5218 MiB |   5394 MiB |   7711 MiB |   2492 MiB |
|       from large pool |   5177 MiB |   5353 MiB |   7559 MiB |   2381 MiB |
|       from small pool |     41 MiB |    100 MiB |    152 MiB |    111 MiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   5514 MiB |   5514 MiB |   5514 MiB |      0 B   |
|       from large pool |   5412 MiB |   5412 MiB |   5412 MiB |      0 B   |
|       from small pool |    102 MiB |    102 MiB |    102 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory | 102951 KiB | 284232 KiB |   2219 MiB |   2118 MiB |
|       from large pool |  41422 KiB | 219110 KiB |   2006 MiB |   1965 MiB |
|       from small pool |  61529 KiB |  67619 KiB |    212 MiB |    152 MiB |
|---------------------------------------------------------------------------|
| Allocations           |    3737    |    5225    |    5510    |    1773    |
|       from large pool |     202    |     203    |     243    |      41    |
|       from small pool |    3535    |    5147    |    5267    |    1732    |
|---------------------------------------------------------------------------|
| Active allocs         |    3737    |    5225    |    5510    |    1773    |
|       from large pool |     202    |     203    |     243    |      41    |
|       from small pool |    3535    |    5147    |    5267    |    1732    |
|---------------------------------------------------------------------------|
| GPU reserved segments |     117    |     117    |     117    |       0    |
|       from large pool |      66    |      66    |      66    |       0    |
|       from small pool |      51    |      51    |      51    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     452    |     460    |     756    |     304    |
|       from large pool |      12    |      20    |      55    |      43    |
|       from small pool |     440    |     442    |     701    |     261    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-11 16:52:59,732][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-11 16:52:59,732][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-11 16:52:59,733][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-11 16:52:59,733][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 4                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-11 16:52:59,733][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 5                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-11 16:52:59,734][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 6                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-11 16:52:59,734][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 7                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-11 16:52:59,734][fairseq.trainer][WARNING] - attempting to recover from OOM in forward/backward pass
[2024-10-11 16:53:46,733][fairseq_cli.train][INFO] - end of epoch 1941 (average epoch stats below)
[2024-10-11 16:53:46,892][train][INFO] - {"epoch": 1941, "train_loss": "0.415", "train_ntokens": "260945", "train_nsentences": "1750.04", "train_wps": "53049.9", "train_ups": "0.2", "train_wpb": "260945", "train_bsz": "1750", "train_num_updates": "93127", "train_lr": "0.000416947", "train_gnorm": "0.35", "train_loss_scale": "2", "train_train_wall": "279", "train_gb_free": "39.6", "train_wall": "641"}
[2024-10-11 16:53:47,309][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 16:53:47,325][fairseq.trainer][INFO] - begin training epoch 1942
[2024-10-11 16:53:47,325][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 17:02:32,099][fairseq_cli.train][INFO] - end of epoch 1942 (average epoch stats below)
[2024-10-11 17:02:32,112][train][INFO] - {"epoch": 1942, "train_loss": "0.409", "train_ntokens": "260626", "train_nsentences": "1750.04", "train_wps": "23819", "train_ups": "0.09", "train_wpb": "260626", "train_bsz": "1750", "train_num_updates": "93175", "train_lr": "0.000416882", "train_gnorm": "0.373", "train_loss_scale": "2", "train_train_wall": "178", "train_gb_free": "39.2", "train_wall": "1166"}
[2024-10-11 17:02:32,199][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 17:02:32,220][fairseq.trainer][INFO] - begin training epoch 1943
[2024-10-11 17:02:32,220][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 17:04:18,812][train_inner][INFO] - {"epoch": 1943, "update": 1942.521, "loss": "0.409", "ntokens": "261017", "nsentences": "1753.93", "wps": "36287.3", "ups": "0.14", "wpb": "261017", "bsz": "1753.9", "num_updates": "93200", "lr": "0.000416848", "gnorm": "0.359", "loss_scale": "2", "train_wall": "488", "gb_free": "40.2", "wall": "1273"}
[2024-10-11 17:04:37,883][fairseq_cli.train][INFO] - end of epoch 1943 (average epoch stats below)
[2024-10-11 17:04:37,891][train][INFO] - {"epoch": 1943, "train_loss": "0.406", "train_ntokens": "260937", "train_nsentences": "1750.04", "train_wps": "99583.9", "train_ups": "0.38", "train_wpb": "260937", "train_bsz": "1750", "train_num_updates": "93223", "train_lr": "0.000416817", "train_gnorm": "0.338", "train_loss_scale": "2", "train_train_wall": "50", "train_gb_free": "39.8", "train_wall": "1292"}
[2024-10-11 17:04:38,012][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 17:04:38,024][fairseq.trainer][INFO] - begin training epoch 1944
[2024-10-11 17:04:38,024][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 17:06:45,781][fairseq_cli.train][INFO] - end of epoch 1944 (average epoch stats below)
[2024-10-11 17:06:45,786][train][INFO] - {"epoch": 1944, "train_loss": "0.405", "train_ntokens": "260589", "train_nsentences": "1750.04", "train_wps": "97803.1", "train_ups": "0.38", "train_wpb": "260589", "train_bsz": "1750", "train_num_updates": "93271", "train_lr": "0.000416751", "train_gnorm": "0.344", "train_loss_scale": "2", "train_train_wall": "53", "train_gb_free": "40.1", "train_wall": "1420"}
[2024-10-11 17:06:45,882][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 17:06:45,894][fairseq.trainer][INFO] - begin training epoch 1945
[2024-10-11 17:06:45,895][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 17:08:55,772][fairseq_cli.train][INFO] - end of epoch 1945 (average epoch stats below)
[2024-10-11 17:08:55,782][train][INFO] - {"epoch": 1945, "train_loss": "0.413", "train_ntokens": "260630", "train_nsentences": "1750.04", "train_wps": "96236.7", "train_ups": "0.37", "train_wpb": "260630", "train_bsz": "1750", "train_num_updates": "93319", "train_lr": "0.000416686", "train_gnorm": "0.361", "train_loss_scale": "2", "train_train_wall": "56", "train_gb_free": "39.3", "train_wall": "1550"}
[2024-10-11 17:08:55,870][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 17:08:55,882][fairseq.trainer][INFO] - begin training epoch 1946
[2024-10-11 17:08:55,882][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 17:11:04,275][fairseq_cli.train][INFO] - end of epoch 1946 (average epoch stats below)
[2024-10-11 17:11:04,289][train][INFO] - {"epoch": 1946, "train_loss": "0.417", "train_ntokens": "260926", "train_nsentences": "1750.04", "train_wps": "97463.8", "train_ups": "0.37", "train_wpb": "260926", "train_bsz": "1750", "train_num_updates": "93367", "train_lr": "0.000416621", "train_gnorm": "0.367", "train_loss_scale": "2", "train_train_wall": "61", "train_gb_free": "40.2", "train_wall": "1678"}
[2024-10-11 17:11:04,404][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 17:11:04,443][fairseq.trainer][INFO] - begin training epoch 1947
[2024-10-11 17:11:04,443][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 17:12:58,400][train_inner][INFO] - {"epoch": 1947, "update": 1946.688, "loss": "0.411", "ntokens": "260558", "nsentences": "1744.92", "wps": "100295", "ups": "0.38", "wpb": "260558", "bsz": "1744.9", "num_updates": "93400", "lr": "0.000416576", "gnorm": "0.352", "loss_scale": "2", "train_wall": "229", "gb_free": "40.1", "wall": "1792"}
[2024-10-11 17:13:14,923][fairseq_cli.train][INFO] - end of epoch 1947 (average epoch stats below)
[2024-10-11 17:13:14,925][train][INFO] - {"epoch": 1947, "train_loss": "0.414", "train_ntokens": "260775", "train_nsentences": "1750.04", "train_wps": "95818.8", "train_ups": "0.37", "train_wpb": "260775", "train_bsz": "1750", "train_num_updates": "93415", "train_lr": "0.000416556", "train_gnorm": "0.348", "train_loss_scale": "2", "train_train_wall": "47", "train_gb_free": "39.3", "train_wall": "1809"}
[2024-10-11 17:13:14,985][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 17:13:14,990][fairseq.trainer][INFO] - begin training epoch 1948
[2024-10-11 17:13:14,991][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 17:15:21,561][fairseq_cli.train][INFO] - end of epoch 1948 (average epoch stats below)
[2024-10-11 17:15:21,566][train][INFO] - {"epoch": 1948, "train_loss": "0.413", "train_ntokens": "261116", "train_nsentences": "1750.04", "train_wps": "98970.7", "train_ups": "0.38", "train_wpb": "261116", "train_bsz": "1750", "train_num_updates": "93463", "train_lr": "0.00041649", "train_gnorm": "0.36", "train_loss_scale": "2", "train_train_wall": "57", "train_gb_free": "39.4", "train_wall": "1936"}
[2024-10-11 17:15:21,663][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 17:15:21,696][fairseq.trainer][INFO] - begin training epoch 1949
[2024-10-11 17:15:21,697][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 17:17:29,126][fairseq_cli.train][INFO] - end of epoch 1949 (average epoch stats below)
[2024-10-11 17:17:29,132][train][INFO] - {"epoch": 1949, "train_loss": "0.402", "train_ntokens": "260330", "train_nsentences": "1750.04", "train_wps": "97959.2", "train_ups": "0.38", "train_wpb": "260330", "train_bsz": "1750", "train_num_updates": "93511", "train_lr": "0.000416425", "train_gnorm": "0.336", "train_loss_scale": "2", "train_train_wall": "44", "train_gb_free": "39.8", "train_wall": "2063"}
[2024-10-11 17:17:29,190][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 17:17:29,194][fairseq.trainer][INFO] - begin training epoch 1950
[2024-10-11 17:17:29,194][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 17:19:39,433][fairseq_cli.train][INFO] - end of epoch 1950 (average epoch stats below)
[2024-10-11 17:19:39,436][train][INFO] - {"epoch": 1950, "train_loss": "0.409", "train_ntokens": "261002", "train_nsentences": "1750.04", "train_wps": "96147.7", "train_ups": "0.37", "train_wpb": "261002", "train_bsz": "1750", "train_num_updates": "93559", "train_lr": "0.00041636", "train_gnorm": "0.359", "train_loss_scale": "2", "train_train_wall": "56", "train_gb_free": "39.9", "train_wall": "2193"}
[2024-10-11 17:19:39,493][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 17:19:39,497][fairseq.trainer][INFO] - begin training epoch 1951
[2024-10-11 17:19:39,497][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 17:21:56,564][train_inner][INFO] - {"epoch": 1951, "update": 1950.854, "loss": "0.409", "ntokens": "260899", "nsentences": "1750.6", "wps": "96959.5", "ups": "0.37", "wpb": "260899", "bsz": "1750.6", "num_updates": "93600", "lr": "0.000416304", "gnorm": "0.353", "loss_scale": "2", "train_wall": "189", "gb_free": "39.6", "wall": "2331"}
[2024-10-11 17:21:59,272][fairseq_cli.train][INFO] - end of epoch 1951 (average epoch stats below)
[2024-10-11 17:21:59,274][train][INFO] - {"epoch": 1951, "train_loss": "0.409", "train_ntokens": "260661", "train_nsentences": "1750.04", "train_wps": "89474.6", "train_ups": "0.34", "train_wpb": "260661", "train_bsz": "1750", "train_num_updates": "93607", "train_lr": "0.000416295", "train_gnorm": "0.356", "train_loss_scale": "2", "train_train_wall": "28", "train_gb_free": "40.2", "train_wall": "2333"}
[2024-10-11 17:21:59,349][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 17:21:59,367][fairseq.trainer][INFO] - begin training epoch 1952
[2024-10-11 17:21:59,367][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 17:24:13,894][fairseq_cli.train][INFO] - end of epoch 1952 (average epoch stats below)
[2024-10-11 17:24:13,898][train][INFO] - {"epoch": 1952, "train_loss": "0.412", "train_ntokens": "260623", "train_nsentences": "1750.04", "train_wps": "92926.5", "train_ups": "0.36", "train_wpb": "260623", "train_bsz": "1750", "train_num_updates": "93655", "train_lr": "0.00041623", "train_gnorm": "0.338", "train_loss_scale": "2", "train_train_wall": "52", "train_gb_free": "39.7", "train_wall": "2468"}
[2024-10-11 17:24:13,963][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 17:24:13,966][fairseq.trainer][INFO] - begin training epoch 1953
[2024-10-11 17:24:13,967][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 17:26:18,800][fairseq_cli.train][INFO] - end of epoch 1953 (average epoch stats below)
[2024-10-11 17:26:18,827][train][INFO] - {"epoch": 1953, "train_loss": "0.415", "train_ntokens": "260735", "train_nsentences": "1750.04", "train_wps": "100199", "train_ups": "0.38", "train_wpb": "260735", "train_bsz": "1750", "train_num_updates": "93703", "train_lr": "0.000416164", "train_gnorm": "0.374", "train_loss_scale": "2", "train_train_wall": "35", "train_gb_free": "39.3", "train_wall": "2593"}
[2024-10-11 17:26:18,947][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 17:26:18,972][fairseq.trainer][INFO] - begin training epoch 1954
[2024-10-11 17:26:18,973][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 17:28:29,144][fairseq_cli.train][INFO] - end of epoch 1954 (average epoch stats below)
[2024-10-11 17:28:29,148][train][INFO] - {"epoch": 1954, "train_loss": "0.416", "train_ntokens": "261021", "train_nsentences": "1750.04", "train_wps": "96141.4", "train_ups": "0.37", "train_wpb": "261021", "train_bsz": "1750", "train_num_updates": "93751", "train_lr": "0.000416099", "train_gnorm": "0.39", "train_loss_scale": "2", "train_train_wall": "58", "train_gb_free": "39.3", "train_wall": "2723"}
[2024-10-11 17:28:29,293][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 17:28:29,297][fairseq.trainer][INFO] - begin training epoch 1955
[2024-10-11 17:28:29,297][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 17:30:51,800][fairseq_cli.train][INFO] - end of epoch 1955 (average epoch stats below)
[2024-10-11 17:30:51,811][train][INFO] - {"epoch": 1955, "train_loss": "0.405", "train_ntokens": "260388", "train_nsentences": "1750.04", "train_wps": "87611.6", "train_ups": "0.34", "train_wpb": "260388", "train_bsz": "1750", "train_num_updates": "93799", "train_lr": "0.000416034", "train_gnorm": "0.376", "train_loss_scale": "2", "train_train_wall": "38", "train_gb_free": "39.9", "train_wall": "2866"}
[2024-10-11 17:30:51,909][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 17:30:51,921][fairseq.trainer][INFO] - begin training epoch 1956
[2024-10-11 17:30:51,922][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 17:32:39,648][train_inner][INFO] - {"epoch": 1956, "update": 1955.021, "loss": "0.412", "ntokens": "260663", "nsentences": "1750.56", "wps": "81068.8", "ups": "0.31", "wpb": "260663", "bsz": "1750.6", "num_updates": "93800", "lr": "0.000416033", "gnorm": "0.369", "loss_scale": "2", "train_wall": "190", "gb_free": "39.6", "wall": "2974"}
[2024-10-11 17:33:25,948][fairseq_cli.train][INFO] - end of epoch 1956 (average epoch stats below)
[2024-10-11 17:33:25,951][train][INFO] - {"epoch": 1956, "train_loss": "0.403", "train_ntokens": "260644", "train_nsentences": "1750.04", "train_wps": "81166.8", "train_ups": "0.31", "train_wpb": "260644", "train_bsz": "1750", "train_num_updates": "93847", "train_lr": "0.000415969", "train_gnorm": "0.368", "train_loss_scale": "2", "train_train_wall": "46", "train_gb_free": "39.8", "train_wall": "3020"}
[2024-10-11 17:33:26,019][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 17:33:26,023][fairseq.trainer][INFO] - begin training epoch 1957
[2024-10-11 17:33:26,023][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 17:35:30,768][fairseq_cli.train][INFO] - end of epoch 1957 (average epoch stats below)
[2024-10-11 17:35:30,775][train][INFO] - {"epoch": 1957, "train_loss": "0.417", "train_ntokens": "260717", "train_nsentences": "1750.04", "train_wps": "100259", "train_ups": "0.38", "train_wpb": "260716", "train_bsz": "1750", "train_num_updates": "93895", "train_lr": "0.000415904", "train_gnorm": "0.332", "train_loss_scale": "2", "train_train_wall": "55", "train_gb_free": "39.3", "train_wall": "3145"}
[2024-10-11 17:35:30,897][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 17:35:30,911][fairseq.trainer][INFO] - begin training epoch 1958
[2024-10-11 17:35:30,912][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 17:37:40,312][fairseq_cli.train][INFO] - end of epoch 1958 (average epoch stats below)
[2024-10-11 17:37:40,316][train][INFO] - {"epoch": 1958, "train_loss": "0.412", "train_ntokens": "260406", "train_nsentences": "1750.04", "train_wps": "96492.6", "train_ups": "0.37", "train_wpb": "260406", "train_bsz": "1750", "train_num_updates": "93943", "train_lr": "0.000415838", "train_gnorm": "0.37", "train_loss_scale": "2", "train_train_wall": "57", "train_gb_free": "39.8", "train_wall": "3274"}
[2024-10-11 17:37:40,367][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 17:37:40,370][fairseq.trainer][INFO] - begin training epoch 1959
[2024-10-11 17:37:40,371][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 17:39:47,519][fairseq_cli.train][INFO] - end of epoch 1959 (average epoch stats below)
[2024-10-11 17:39:47,527][train][INFO] - {"epoch": 1959, "train_loss": "0.413", "train_ntokens": "260542", "train_nsentences": "1750.04", "train_wps": "98313.6", "train_ups": "0.38", "train_wpb": "260542", "train_bsz": "1750", "train_num_updates": "93991", "train_lr": "0.000415773", "train_gnorm": "0.363", "train_loss_scale": "2", "train_train_wall": "57", "train_gb_free": "40", "train_wall": "3402"}
[2024-10-11 17:39:47,631][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 17:39:47,635][fairseq.trainer][INFO] - begin training epoch 1960
[2024-10-11 17:39:47,636][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 17:41:22,745][train_inner][INFO] - {"epoch": 1960, "update": 1959.188, "loss": "0.411", "ntokens": "260522", "nsentences": "1749.49", "wps": "99608.3", "ups": "0.38", "wpb": "260522", "bsz": "1749.5", "num_updates": "94000", "lr": "0.000415761", "gnorm": "0.357", "loss_scale": "2", "train_wall": "237", "gb_free": "39.3", "wall": "3497"}
[2024-10-11 17:41:58,218][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 1960 @ 94039 updates
[2024-10-11 17:41:58,219][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-11 17:42:01,386][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-11 17:42:01,401][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 1960 @ 94039 updates, score None) (writing took 3.182612461037934 seconds)
[2024-10-11 17:42:01,401][fairseq_cli.train][INFO] - end of epoch 1960 (average epoch stats below)
[2024-10-11 17:42:01,404][train][INFO] - {"epoch": 1960, "train_loss": "0.407", "train_ntokens": "261173", "train_nsentences": "1750.04", "train_wps": "93643.1", "train_ups": "0.36", "train_wpb": "261173", "train_bsz": "1750", "train_num_updates": "94039", "train_lr": "0.000415708", "train_gnorm": "0.334", "train_loss_scale": "2", "train_train_wall": "61", "train_gb_free": "39.7", "train_wall": "3535"}
[2024-10-11 17:42:01,474][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 17:42:01,485][fairseq.trainer][INFO] - begin training epoch 1961
[2024-10-11 17:42:01,485][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 17:44:15,864][fairseq_cli.train][INFO] - end of epoch 1961 (average epoch stats below)
[2024-10-11 17:44:15,867][train][INFO] - {"epoch": 1961, "train_loss": "0.412", "train_ntokens": "260520", "train_nsentences": "1750.04", "train_wps": "93000.6", "train_ups": "0.36", "train_wpb": "260520", "train_bsz": "1750", "train_num_updates": "94087", "train_lr": "0.000415643", "train_gnorm": "0.346", "train_loss_scale": "2", "train_train_wall": "64", "train_gb_free": "39.8", "train_wall": "3670"}
[2024-10-11 17:44:15,939][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 17:44:15,943][fairseq.trainer][INFO] - begin training epoch 1962
[2024-10-11 17:44:15,943][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 17:46:25,538][fairseq_cli.train][INFO] - end of epoch 1962 (average epoch stats below)
[2024-10-11 17:46:25,552][train][INFO] - {"epoch": 1962, "train_loss": "0.407", "train_ntokens": "260826", "train_nsentences": "1750.04", "train_wps": "96549", "train_ups": "0.37", "train_wpb": "260826", "train_bsz": "1750", "train_num_updates": "94135", "train_lr": "0.000415577", "train_gnorm": "0.361", "train_loss_scale": "2", "train_train_wall": "36", "train_gb_free": "39.7", "train_wall": "3800"}
[2024-10-11 17:46:25,644][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 17:46:25,648][fairseq.trainer][INFO] - begin training epoch 1963
[2024-10-11 17:46:25,649][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 17:48:32,352][fairseq_cli.train][INFO] - end of epoch 1963 (average epoch stats below)
[2024-10-11 17:48:32,374][train][INFO] - {"epoch": 1963, "train_loss": "0.409", "train_ntokens": "260538", "train_nsentences": "1750.04", "train_wps": "98612.8", "train_ups": "0.38", "train_wpb": "260538", "train_bsz": "1750", "train_num_updates": "94183", "train_lr": "0.000415512", "train_gnorm": "0.353", "train_loss_scale": "2", "train_train_wall": "53", "train_gb_free": "39.6", "train_wall": "3926"}
[2024-10-11 17:48:32,503][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 17:48:32,508][fairseq.trainer][INFO] - begin training epoch 1964
[2024-10-11 17:48:32,508][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 17:50:11,997][train_inner][INFO] - {"epoch": 1964, "update": 1963.354, "loss": "0.409", "ntokens": "260875", "nsentences": "1745.71", "wps": "98583.4", "ups": "0.38", "wpb": "260875", "bsz": "1745.7", "num_updates": "94200", "lr": "0.000415489", "gnorm": "0.349", "loss_scale": "2", "train_wall": "213", "gb_free": "39.3", "wall": "4026"}
[2024-10-11 17:50:39,073][fairseq_cli.train][INFO] - end of epoch 1964 (average epoch stats below)
[2024-10-11 17:50:39,076][train][INFO] - {"epoch": 1964, "train_loss": "0.412", "train_ntokens": "261021", "train_nsentences": "1750.04", "train_wps": "98888.5", "train_ups": "0.38", "train_wpb": "261021", "train_bsz": "1750", "train_num_updates": "94231", "train_lr": "0.000415447", "train_gnorm": "0.347", "train_loss_scale": "2", "train_train_wall": "50", "train_gb_free": "39.3", "train_wall": "4053"}
[2024-10-11 17:50:39,182][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 17:50:39,189][fairseq.trainer][INFO] - begin training epoch 1965
[2024-10-11 17:50:39,189][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 17:52:49,385][fairseq_cli.train][INFO] - end of epoch 1965 (average epoch stats below)
[2024-10-11 17:52:49,402][train][INFO] - {"epoch": 1965, "train_loss": "0.407", "train_ntokens": "260834", "train_nsentences": "1750.04", "train_wps": "96073.8", "train_ups": "0.37", "train_wpb": "260834", "train_bsz": "1750", "train_num_updates": "94279", "train_lr": "0.000415382", "train_gnorm": "0.355", "train_loss_scale": "2", "train_train_wall": "58", "train_gb_free": "39.7", "train_wall": "4183"}
[2024-10-11 17:52:49,508][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 17:52:49,525][fairseq.trainer][INFO] - begin training epoch 1966
[2024-10-11 17:52:49,525][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 17:54:59,307][fairseq_cli.train][INFO] - end of epoch 1966 (average epoch stats below)
[2024-10-11 17:54:59,313][train][INFO] - {"epoch": 1966, "train_loss": "0.412", "train_ntokens": "260773", "train_nsentences": "1750.04", "train_wps": "96353.5", "train_ups": "0.37", "train_wpb": "260773", "train_bsz": "1750", "train_num_updates": "94327", "train_lr": "0.000415317", "train_gnorm": "0.354", "train_loss_scale": "2", "train_train_wall": "54", "train_gb_free": "39.6", "train_wall": "4313"}
[2024-10-11 17:54:59,402][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 17:54:59,416][fairseq.trainer][INFO] - begin training epoch 1967
[2024-10-11 17:54:59,417][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 17:57:05,023][fairseq_cli.train][INFO] - end of epoch 1967 (average epoch stats below)
[2024-10-11 17:57:05,067][train][INFO] - {"epoch": 1967, "train_loss": "0.419", "train_ntokens": "260760", "train_nsentences": "1750.04", "train_wps": "99553", "train_ups": "0.38", "train_wpb": "260760", "train_bsz": "1750", "train_num_updates": "94375", "train_lr": "0.000415251", "train_gnorm": "0.37", "train_loss_scale": "2", "train_train_wall": "48", "train_gb_free": "39.8", "train_wall": "4439"}
[2024-10-11 17:57:05,170][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 17:57:05,173][fairseq.trainer][INFO] - begin training epoch 1968
[2024-10-11 17:57:05,174][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 17:58:54,504][train_inner][INFO] - {"epoch": 1968, "update": 1967.521, "loss": "0.411", "ntokens": "260696", "nsentences": "1749.21", "wps": "99787.7", "ups": "0.38", "wpb": "260696", "bsz": "1749.2", "num_updates": "94400", "lr": "0.000415217", "gnorm": "0.359", "loss_scale": "2", "train_wall": "218", "gb_free": "39.3", "wall": "4549"}
[2024-10-11 17:59:12,634][fairseq_cli.train][INFO] - end of epoch 1968 (average epoch stats below)
[2024-10-11 17:59:12,651][train][INFO] - {"epoch": 1968, "train_loss": "0.401", "train_ntokens": "260606", "train_nsentences": "1750.04", "train_wps": "98059", "train_ups": "0.38", "train_wpb": "260606", "train_bsz": "1750", "train_num_updates": "94423", "train_lr": "0.000415186", "train_gnorm": "0.349", "train_loss_scale": "2", "train_train_wall": "49", "train_gb_free": "39.7", "train_wall": "4567"}
[2024-10-11 17:59:12,767][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 17:59:12,787][fairseq.trainer][INFO] - begin training epoch 1969
[2024-10-11 17:59:12,787][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 18:01:21,434][fairseq_cli.train][INFO] - end of epoch 1969 (average epoch stats below)
[2024-10-11 18:01:21,440][train][INFO] - {"epoch": 1969, "train_loss": "0.412", "train_ntokens": "260654", "train_nsentences": "1750.04", "train_wps": "97148.3", "train_ups": "0.37", "train_wpb": "260654", "train_bsz": "1750", "train_num_updates": "94471", "train_lr": "0.000415121", "train_gnorm": "0.355", "train_loss_scale": "2", "train_train_wall": "47", "train_gb_free": "39.8", "train_wall": "4695"}
[2024-10-11 18:01:21,589][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 18:01:21,594][fairseq.trainer][INFO] - begin training epoch 1970
[2024-10-11 18:01:21,594][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 18:03:30,253][fairseq_cli.train][INFO] - end of epoch 1970 (average epoch stats below)
[2024-10-11 18:03:30,257][train][INFO] - {"epoch": 1970, "train_loss": "0.417", "train_ntokens": "261054", "train_nsentences": "1750.04", "train_wps": "97276.5", "train_ups": "0.37", "train_wpb": "261054", "train_bsz": "1750", "train_num_updates": "94519", "train_lr": "0.000415056", "train_gnorm": "0.373", "train_loss_scale": "2", "train_train_wall": "54", "train_gb_free": "39.6", "train_wall": "4824"}
[2024-10-11 18:03:30,318][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 18:03:30,325][fairseq.trainer][INFO] - begin training epoch 1971
[2024-10-11 18:03:30,326][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 18:05:42,102][fairseq_cli.train][INFO] - end of epoch 1971 (average epoch stats below)
[2024-10-11 18:05:42,105][train][INFO] - {"epoch": 1971, "train_loss": "0.407", "train_ntokens": "260370", "train_nsentences": "1750.04", "train_wps": "94790.8", "train_ups": "0.36", "train_wpb": "260370", "train_bsz": "1750", "train_num_updates": "94567", "train_lr": "0.00041499", "train_gnorm": "0.361", "train_loss_scale": "2", "train_train_wall": "56", "train_gb_free": "39.2", "train_wall": "4956"}
[2024-10-11 18:05:42,163][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 18:05:42,167][fairseq.trainer][INFO] - begin training epoch 1972
[2024-10-11 18:05:42,167][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 18:07:41,609][train_inner][INFO] - {"epoch": 1972, "update": 1971.688, "loss": "0.408", "ntokens": "260775", "nsentences": "1753.34", "wps": "98946.9", "ups": "0.38", "wpb": "260775", "bsz": "1753.3", "num_updates": "94600", "lr": "0.000414946", "gnorm": "0.356", "loss_scale": "2", "train_wall": "222", "gb_free": "39.8", "wall": "5076"}
[2024-10-11 18:07:55,014][fairseq_cli.train][INFO] - end of epoch 1972 (average epoch stats below)
[2024-10-11 18:07:55,020][train][INFO] - {"epoch": 1972, "train_loss": "0.402", "train_ntokens": "260806", "train_nsentences": "1750.04", "train_wps": "94190.6", "train_ups": "0.36", "train_wpb": "260806", "train_bsz": "1750", "train_num_updates": "94615", "train_lr": "0.000414925", "train_gnorm": "0.35", "train_loss_scale": "2", "train_train_wall": "61", "train_gb_free": "39.1", "train_wall": "5089"}
[2024-10-11 18:07:55,078][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 18:07:55,082][fairseq.trainer][INFO] - begin training epoch 1973
[2024-10-11 18:07:55,082][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 18:10:04,951][fairseq_cli.train][INFO] - end of epoch 1973 (average epoch stats below)
[2024-10-11 18:10:04,954][train][INFO] - {"epoch": 1973, "train_loss": "0.413", "train_ntokens": "260832", "train_nsentences": "1750.04", "train_wps": "96358.2", "train_ups": "0.37", "train_wpb": "260832", "train_bsz": "1750", "train_num_updates": "94663", "train_lr": "0.00041486", "train_gnorm": "0.359", "train_loss_scale": "2", "train_train_wall": "52", "train_gb_free": "39.7", "train_wall": "5219"}
[2024-10-11 18:10:05,019][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 18:10:05,022][fairseq.trainer][INFO] - begin training epoch 1974
[2024-10-11 18:10:05,023][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 18:12:14,158][fairseq_cli.train][INFO] - end of epoch 1974 (average epoch stats below)
[2024-10-11 18:12:14,172][train][INFO] - {"epoch": 1974, "train_loss": "0.41", "train_ntokens": "260716", "train_nsentences": "1750.04", "train_wps": "96854.4", "train_ups": "0.37", "train_wpb": "260716", "train_bsz": "1750", "train_num_updates": "94711", "train_lr": "0.000414795", "train_gnorm": "0.357", "train_loss_scale": "2", "train_train_wall": "30", "train_gb_free": "39.1", "train_wall": "5348"}
[2024-10-11 18:12:14,315][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 18:12:14,325][fairseq.trainer][INFO] - begin training epoch 1975
[2024-10-11 18:12:14,326][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 18:14:22,461][fairseq_cli.train][INFO] - end of epoch 1975 (average epoch stats below)
[2024-10-11 18:14:22,464][train][INFO] - {"epoch": 1975, "train_loss": "0.407", "train_ntokens": "260843", "train_nsentences": "1750.04", "train_wps": "97599.9", "train_ups": "0.37", "train_wpb": "260843", "train_bsz": "1750", "train_num_updates": "94759", "train_lr": "0.00041473", "train_gnorm": "0.387", "train_loss_scale": "2", "train_train_wall": "53", "train_gb_free": "39.7", "train_wall": "5477"}
[2024-10-11 18:14:22,547][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 18:14:22,551][fairseq.trainer][INFO] - begin training epoch 1976
[2024-10-11 18:14:22,551][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 18:16:30,424][train_inner][INFO] - {"epoch": 1976, "update": 1975.854, "loss": "0.41", "ntokens": "260665", "nsentences": "1759.49", "wps": "98586.5", "ups": "0.38", "wpb": "260665", "bsz": "1759.5", "num_updates": "94800", "lr": "0.000414674", "gnorm": "0.365", "loss_scale": "2", "train_wall": "188", "gb_free": "39.8", "wall": "5604"}
[2024-10-11 18:16:33,315][fairseq_cli.train][INFO] - end of epoch 1976 (average epoch stats below)
[2024-10-11 18:16:33,325][train][INFO] - {"epoch": 1976, "train_loss": "0.404", "train_ntokens": "260704", "train_nsentences": "1750.04", "train_wps": "95634.3", "train_ups": "0.37", "train_wpb": "260704", "train_bsz": "1750", "train_num_updates": "94807", "train_lr": "0.000414664", "train_gnorm": "0.352", "train_loss_scale": "2", "train_train_wall": "43", "train_gb_free": "39.1", "train_wall": "5607"}
[2024-10-11 18:16:33,393][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 18:16:33,397][fairseq.trainer][INFO] - begin training epoch 1977
[2024-10-11 18:16:33,397][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 18:18:44,254][fairseq_cli.train][INFO] - end of epoch 1977 (average epoch stats below)
[2024-10-11 18:18:44,257][train][INFO] - {"epoch": 1977, "train_loss": "0.41", "train_ntokens": "260494", "train_nsentences": "1750.04", "train_wps": "95499.9", "train_ups": "0.37", "train_wpb": "260494", "train_bsz": "1750", "train_num_updates": "94855", "train_lr": "0.000414599", "train_gnorm": "0.353", "train_loss_scale": "2", "train_train_wall": "58", "train_gb_free": "39.7", "train_wall": "5738"}
[2024-10-11 18:18:44,378][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 18:18:44,392][fairseq.trainer][INFO] - begin training epoch 1978
[2024-10-11 18:18:44,393][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 18:20:53,134][fairseq_cli.train][INFO] - end of epoch 1978 (average epoch stats below)
[2024-10-11 18:20:53,138][train][INFO] - {"epoch": 1978, "train_loss": "0.407", "train_ntokens": "260686", "train_nsentences": "1750.04", "train_wps": "97091.6", "train_ups": "0.37", "train_wpb": "260686", "train_bsz": "1750", "train_num_updates": "94903", "train_lr": "0.000414534", "train_gnorm": "0.359", "train_loss_scale": "2", "train_train_wall": "27", "train_gb_free": "39.6", "train_wall": "5867"}
[2024-10-11 18:20:53,238][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 18:20:53,244][fairseq.trainer][INFO] - begin training epoch 1979
[2024-10-11 18:20:53,244][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 18:23:08,963][fairseq_cli.train][INFO] - end of epoch 1979 (average epoch stats below)
[2024-10-11 18:23:08,966][train][INFO] - {"epoch": 1979, "train_loss": "0.398", "train_ntokens": "260852", "train_nsentences": "1750.04", "train_wps": "92183.5", "train_ups": "0.35", "train_wpb": "260852", "train_bsz": "1750", "train_num_updates": "94951", "train_lr": "0.000414469", "train_gnorm": "0.354", "train_loss_scale": "2", "train_train_wall": "34", "train_gb_free": "39.6", "train_wall": "6003"}
[2024-10-11 18:23:09,022][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 18:23:09,026][fairseq.trainer][INFO] - begin training epoch 1980
[2024-10-11 18:23:09,026][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 18:25:20,363][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 1980 @ 94999 updates
[2024-10-11 18:25:20,364][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-11 18:25:23,696][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-11 18:25:23,698][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 1980 @ 94999 updates, score None) (writing took 3.335180588066578 seconds)
[2024-10-11 18:25:23,698][fairseq_cli.train][INFO] - end of epoch 1980 (average epoch stats below)
[2024-10-11 18:25:23,701][train][INFO] - {"epoch": 1980, "train_loss": "0.411", "train_ntokens": "260619", "train_nsentences": "1750.04", "train_wps": "92849.4", "train_ups": "0.36", "train_wpb": "260619", "train_bsz": "1750", "train_num_updates": "94999", "train_lr": "0.000414404", "train_gnorm": "0.346", "train_loss_scale": "2", "train_train_wall": "55", "train_gb_free": "40", "train_wall": "6138"}
[2024-10-11 18:25:23,758][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 18:25:23,775][fairseq.trainer][INFO] - begin training epoch 1981
[2024-10-11 18:25:23,775][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 18:26:42,289][train_inner][INFO] - {"epoch": 1981, "update": 1980.021, "loss": "0.406", "ntokens": "260787", "nsentences": "1742.37", "wps": "85247.4", "ups": "0.33", "wpb": "260787", "bsz": "1742.4", "num_updates": "95000", "lr": "0.000414402", "gnorm": "0.353", "loss_scale": "2", "train_wall": "182", "gb_free": "39.3", "wall": "6216"}
[2024-10-11 18:27:31,448][fairseq_cli.train][INFO] - end of epoch 1981 (average epoch stats below)
[2024-10-11 18:27:31,453][train][INFO] - {"epoch": 1981, "train_loss": "0.409", "train_ntokens": "261080", "train_nsentences": "1750.04", "train_wps": "98098.6", "train_ups": "0.38", "train_wpb": "261080", "train_bsz": "1750", "train_num_updates": "95047", "train_lr": "0.000414338", "train_gnorm": "0.363", "train_loss_scale": "2", "train_train_wall": "44", "train_gb_free": "39.1", "train_wall": "6265"}
[2024-10-11 18:27:31,536][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 18:27:31,547][fairseq.trainer][INFO] - begin training epoch 1982
[2024-10-11 18:27:31,548][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 18:29:46,583][fairseq_cli.train][INFO] - end of epoch 1982 (average epoch stats below)
[2024-10-11 18:29:46,586][train][INFO] - {"epoch": 1982, "train_loss": "0.405", "train_ntokens": "260695", "train_nsentences": "1750.04", "train_wps": "92601.5", "train_ups": "0.36", "train_wpb": "260695", "train_bsz": "1750", "train_num_updates": "95095", "train_lr": "0.000414273", "train_gnorm": "0.342", "train_loss_scale": "2", "train_train_wall": "58", "train_gb_free": "39.3", "train_wall": "6401"}
[2024-10-11 18:29:46,667][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 18:29:46,671][fairseq.trainer][INFO] - begin training epoch 1983
[2024-10-11 18:29:46,672][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 18:31:53,282][fairseq_cli.train][INFO] - end of epoch 1983 (average epoch stats below)
[2024-10-11 18:31:53,299][train][INFO] - {"epoch": 1983, "train_loss": "0.402", "train_ntokens": "260853", "train_nsentences": "1750.04", "train_wps": "98822.9", "train_ups": "0.38", "train_wpb": "260853", "train_bsz": "1750", "train_num_updates": "95143", "train_lr": "0.000414208", "train_gnorm": "0.353", "train_loss_scale": "4", "train_train_wall": "44", "train_gb_free": "39.2", "train_wall": "6527"}
[2024-10-11 18:31:53,427][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 18:31:53,430][fairseq.trainer][INFO] - begin training epoch 1984
[2024-10-11 18:31:53,431][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 18:34:01,303][fairseq_cli.train][INFO] - end of epoch 1984 (average epoch stats below)
[2024-10-11 18:34:01,307][train][INFO] - {"epoch": 1984, "train_loss": "0.399", "train_ntokens": "260777", "train_nsentences": "1750.04", "train_wps": "97787", "train_ups": "0.37", "train_wpb": "260777", "train_bsz": "1750", "train_num_updates": "95191", "train_lr": "0.000414143", "train_gnorm": "0.37", "train_loss_scale": "4", "train_train_wall": "51", "train_gb_free": "39.9", "train_wall": "6655"}
[2024-10-11 18:34:01,383][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 18:34:01,391][fairseq.trainer][INFO] - begin training epoch 1985
[2024-10-11 18:34:01,391][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 18:35:39,243][train_inner][INFO] - {"epoch": 1985, "update": 1984.188, "loss": "0.403", "ntokens": "260738", "nsentences": "1751.74", "wps": "97118.1", "ups": "0.37", "wpb": "260738", "bsz": "1751.7", "num_updates": "95200", "lr": "0.00041413", "gnorm": "0.357", "loss_scale": "4", "train_wall": "219", "gb_free": "39.3", "wall": "6753"}
[2024-10-11 18:36:08,495][fairseq_cli.train][INFO] - end of epoch 1985 (average epoch stats below)
[2024-10-11 18:36:08,497][train][INFO] - {"epoch": 1985, "train_loss": "0.411", "train_ntokens": "260403", "train_nsentences": "1750.04", "train_wps": "98275.2", "train_ups": "0.38", "train_wpb": "260403", "train_bsz": "1750", "train_num_updates": "95239", "train_lr": "0.000414077", "train_gnorm": "0.349", "train_loss_scale": "4", "train_train_wall": "55", "train_gb_free": "39.9", "train_wall": "6783"}
[2024-10-11 18:36:08,559][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 18:36:08,563][fairseq.trainer][INFO] - begin training epoch 1986
[2024-10-11 18:36:08,563][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 18:38:14,149][fairseq_cli.train][INFO] - end of epoch 1986 (average epoch stats below)
[2024-10-11 18:38:14,153][train][INFO] - {"epoch": 1986, "train_loss": "0.403", "train_ntokens": "260741", "train_nsentences": "1750.04", "train_wps": "99603.8", "train_ups": "0.38", "train_wpb": "260741", "train_bsz": "1750", "train_num_updates": "95287", "train_lr": "0.000414012", "train_gnorm": "0.378", "train_loss_scale": "4", "train_train_wall": "47", "train_gb_free": "40.3", "train_wall": "6908"}
[2024-10-11 18:38:14,256][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 18:38:14,265][fairseq.trainer][INFO] - begin training epoch 1987
[2024-10-11 18:38:14,265][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 18:40:24,870][fairseq_cli.train][INFO] - end of epoch 1987 (average epoch stats below)
[2024-10-11 18:40:24,875][train][INFO] - {"epoch": 1987, "train_loss": "0.408", "train_ntokens": "260714", "train_nsentences": "1750.04", "train_wps": "95735.3", "train_ups": "0.37", "train_wpb": "260714", "train_bsz": "1750", "train_num_updates": "95335", "train_lr": "0.000413947", "train_gnorm": "0.357", "train_loss_scale": "4", "train_train_wall": "28", "train_gb_free": "39.3", "train_wall": "7039"}
[2024-10-11 18:40:24,966][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 18:40:24,978][fairseq.trainer][INFO] - begin training epoch 1988
[2024-10-11 18:40:24,979][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 18:42:35,322][fairseq_cli.train][INFO] - end of epoch 1988 (average epoch stats below)
[2024-10-11 18:42:35,335][train][INFO] - {"epoch": 1988, "train_loss": "0.404", "train_ntokens": "260475", "train_nsentences": "1750.04", "train_wps": "95838.5", "train_ups": "0.37", "train_wpb": "260475", "train_bsz": "1750", "train_num_updates": "95383", "train_lr": "0.000413882", "train_gnorm": "0.36", "train_loss_scale": "4", "train_train_wall": "46", "train_gb_free": "39.6", "train_wall": "7169"}
[2024-10-11 18:42:35,962][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 18:42:35,981][fairseq.trainer][INFO] - begin training epoch 1989
[2024-10-11 18:42:35,981][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 18:44:22,635][train_inner][INFO] - {"epoch": 1989, "update": 1988.354, "loss": "0.408", "ntokens": "260590", "nsentences": "1756.42", "wps": "99578.7", "ups": "0.38", "wpb": "260590", "bsz": "1756.4", "num_updates": "95400", "lr": "0.000413859", "gnorm": "0.361", "loss_scale": "4", "train_wall": "185", "gb_free": "42", "wall": "7277"}
[2024-10-11 18:44:47,770][fairseq_cli.train][INFO] - end of epoch 1989 (average epoch stats below)
[2024-10-11 18:44:47,773][train][INFO] - {"epoch": 1989, "train_loss": "0.406", "train_ntokens": "260519", "train_nsentences": "1750.04", "train_wps": "94422.9", "train_ups": "0.36", "train_wpb": "260518", "train_bsz": "1750", "train_num_updates": "95431", "train_lr": "0.000413817", "train_gnorm": "0.364", "train_loss_scale": "4", "train_train_wall": "54", "train_gb_free": "40.5", "train_wall": "7302"}
[2024-10-11 18:44:47,842][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 18:44:47,846][fairseq.trainer][INFO] - begin training epoch 1990
[2024-10-11 18:44:47,846][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 18:47:04,158][fairseq_cli.train][INFO] - end of epoch 1990 (average epoch stats below)
[2024-10-11 18:47:04,160][train][INFO] - {"epoch": 1990, "train_loss": "0.402", "train_ntokens": "260907", "train_nsentences": "1750.04", "train_wps": "91824.9", "train_ups": "0.35", "train_wpb": "260907", "train_bsz": "1750", "train_num_updates": "95479", "train_lr": "0.000413751", "train_gnorm": "0.334", "train_loss_scale": "4", "train_train_wall": "59", "train_gb_free": "39.8", "train_wall": "7438"}
[2024-10-11 18:47:04,215][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 18:47:04,234][fairseq.trainer][INFO] - begin training epoch 1991
[2024-10-11 18:47:04,235][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 18:49:14,746][fairseq_cli.train][INFO] - end of epoch 1991 (average epoch stats below)
[2024-10-11 18:49:14,763][train][INFO] - {"epoch": 1991, "train_loss": "0.408", "train_ntokens": "260943", "train_nsentences": "1750.04", "train_wps": "95914", "train_ups": "0.37", "train_wpb": "260943", "train_bsz": "1750", "train_num_updates": "95527", "train_lr": "0.000413686", "train_gnorm": "0.366", "train_loss_scale": "4", "train_train_wall": "44", "train_gb_free": "40.1", "train_wall": "7569"}
[2024-10-11 18:49:14,877][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 18:49:14,885][fairseq.trainer][INFO] - begin training epoch 1992
[2024-10-11 18:49:14,885][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 18:51:23,405][fairseq_cli.train][INFO] - end of epoch 1992 (average epoch stats below)
[2024-10-11 18:51:23,421][train][INFO] - {"epoch": 1992, "train_loss": "0.403", "train_ntokens": "260492", "train_nsentences": "1750.04", "train_wps": "97188", "train_ups": "0.37", "train_wpb": "260492", "train_bsz": "1750", "train_num_updates": "95575", "train_lr": "0.000413621", "train_gnorm": "0.374", "train_loss_scale": "4", "train_train_wall": "63", "train_gb_free": "40", "train_wall": "7697"}
[2024-10-11 18:51:23,538][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 18:51:23,549][fairseq.trainer][INFO] - begin training epoch 1993
[2024-10-11 18:51:23,550][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 18:53:10,141][train_inner][INFO] - {"epoch": 1993, "update": 1992.521, "loss": "0.404", "ntokens": "260735", "nsentences": "1756.37", "wps": "98856.4", "ups": "0.38", "wpb": "260735", "bsz": "1756.4", "num_updates": "95600", "lr": "0.000413587", "gnorm": "0.356", "loss_scale": "4", "train_wall": "216", "gb_free": "39.3", "wall": "7804"}
[2024-10-11 18:53:33,511][fairseq_cli.train][INFO] - end of epoch 1993 (average epoch stats below)
[2024-10-11 18:53:33,513][train][INFO] - {"epoch": 1993, "train_loss": "0.412", "train_ntokens": "260686", "train_nsentences": "1750.04", "train_wps": "96187.4", "train_ups": "0.37", "train_wpb": "260686", "train_bsz": "1750", "train_num_updates": "95623", "train_lr": "0.000413556", "train_gnorm": "0.326", "train_loss_scale": "4", "train_train_wall": "54", "train_gb_free": "39.2", "train_wall": "7828"}
[2024-10-11 18:53:33,580][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 18:53:33,584][fairseq.trainer][INFO] - begin training epoch 1994
[2024-10-11 18:53:33,585][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 18:55:45,382][fairseq_cli.train][INFO] - end of epoch 1994 (average epoch stats below)
[2024-10-11 18:55:45,385][train][INFO] - {"epoch": 1994, "train_loss": "0.405", "train_ntokens": "260835", "train_nsentences": "1750.04", "train_wps": "94944.5", "train_ups": "0.36", "train_wpb": "260835", "train_bsz": "1750", "train_num_updates": "95671", "train_lr": "0.00041349", "train_gnorm": "0.353", "train_loss_scale": "4", "train_train_wall": "53", "train_gb_free": "40.2", "train_wall": "7959"}
[2024-10-11 18:55:45,442][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 18:55:45,445][fairseq.trainer][INFO] - begin training epoch 1995
[2024-10-11 18:55:45,445][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 18:57:55,873][fairseq_cli.train][INFO] - end of epoch 1995 (average epoch stats below)
[2024-10-11 18:57:55,886][train][INFO] - {"epoch": 1995, "train_loss": "0.414", "train_ntokens": "260776", "train_nsentences": "1750.04", "train_wps": "95919.3", "train_ups": "0.37", "train_wpb": "260776", "train_bsz": "1750", "train_num_updates": "95719", "train_lr": "0.000413425", "train_gnorm": "0.362", "train_loss_scale": "4", "train_train_wall": "55", "train_gb_free": "39.3", "train_wall": "8090"}
[2024-10-11 18:57:55,987][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 18:57:55,996][fairseq.trainer][INFO] - begin training epoch 1996
[2024-10-11 18:57:55,998][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 19:00:06,840][fairseq_cli.train][INFO] - end of epoch 1996 (average epoch stats below)
[2024-10-11 19:00:06,843][train][INFO] - {"epoch": 1996, "train_loss": "0.412", "train_ntokens": "261226", "train_nsentences": "1750.04", "train_wps": "95750", "train_ups": "0.37", "train_wpb": "261226", "train_bsz": "1750", "train_num_updates": "95767", "train_lr": "0.00041336", "train_gnorm": "0.372", "train_loss_scale": "4", "train_train_wall": "39", "train_gb_free": "39.6", "train_wall": "8221"}
[2024-10-11 19:00:06,905][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 19:00:06,909][fairseq.trainer][INFO] - begin training epoch 1997
[2024-10-11 19:00:06,909][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 19:02:00,552][train_inner][INFO] - {"epoch": 1997, "update": 1996.688, "loss": "0.41", "ntokens": "261051", "nsentences": "1741.84", "wps": "98434.4", "ups": "0.38", "wpb": "261051", "bsz": "1741.8", "num_updates": "95800", "lr": "0.000413315", "gnorm": "0.356", "loss_scale": "4", "train_wall": "206", "gb_free": "39.3", "wall": "8335"}
[2024-10-11 19:02:13,290][fairseq_cli.train][INFO] - end of epoch 1997 (average epoch stats below)
[2024-10-11 19:02:13,293][train][INFO] - {"epoch": 1997, "train_loss": "0.404", "train_ntokens": "260549", "train_nsentences": "1750.04", "train_wps": "98905.5", "train_ups": "0.38", "train_wpb": "260549", "train_bsz": "1750", "train_num_updates": "95815", "train_lr": "0.000413295", "train_gnorm": "0.361", "train_loss_scale": "4", "train_train_wall": "47", "train_gb_free": "40", "train_wall": "8347"}
[2024-10-11 19:02:13,414][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 19:02:13,421][fairseq.trainer][INFO] - begin training epoch 1998
[2024-10-11 19:02:13,422][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 19:04:23,741][fairseq_cli.train][INFO] - end of epoch 1998 (average epoch stats below)
[2024-10-11 19:04:23,759][train][INFO] - {"epoch": 1998, "train_loss": "0.407", "train_ntokens": "260921", "train_nsentences": "1750.04", "train_wps": "96007.5", "train_ups": "0.37", "train_wpb": "260921", "train_bsz": "1750", "train_num_updates": "95863", "train_lr": "0.00041323", "train_gnorm": "0.377", "train_loss_scale": "4", "train_train_wall": "28", "train_gb_free": "39.6", "train_wall": "8478"}
[2024-10-11 19:04:23,849][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 19:04:23,859][fairseq.trainer][INFO] - begin training epoch 1999
[2024-10-11 19:04:23,860][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 19:06:34,309][fairseq_cli.train][INFO] - end of epoch 1999 (average epoch stats below)
[2024-10-11 19:06:34,313][train][INFO] - {"epoch": 1999, "train_loss": "0.404", "train_ntokens": "260898", "train_nsentences": "1750.04", "train_wps": "95925.1", "train_ups": "0.37", "train_wpb": "260898", "train_bsz": "1750", "train_num_updates": "95911", "train_lr": "0.000413164", "train_gnorm": "0.379", "train_loss_scale": "4", "train_train_wall": "48", "train_gb_free": "39.5", "train_wall": "8608"}
[2024-10-11 19:06:34,407][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 19:06:34,424][fairseq.trainer][INFO] - begin training epoch 2000
[2024-10-11 19:06:34,425][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 19:08:45,881][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 2000 @ 95959 updates
[2024-10-11 19:08:45,883][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-11 19:08:49,766][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-11 19:08:49,769][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 2000 @ 95959 updates, score None) (writing took 3.888391399756074 seconds)
[2024-10-11 19:08:49,770][fairseq_cli.train][INFO] - end of epoch 2000 (average epoch stats below)
[2024-10-11 19:08:49,772][train][INFO] - {"epoch": 2000, "train_loss": "0.403", "train_ntokens": "260388", "train_nsentences": "1750.04", "train_wps": "92271.4", "train_ups": "0.35", "train_wpb": "260388", "train_bsz": "1750", "train_num_updates": "95959", "train_lr": "0.000413099", "train_gnorm": "0.333", "train_loss_scale": "4", "train_train_wall": "53", "train_gb_free": "39.6", "train_wall": "8744"}
[2024-10-11 19:08:49,849][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 19:08:49,854][fairseq.trainer][INFO] - begin training epoch 2001
[2024-10-11 19:08:49,854][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 19:10:48,047][train_inner][INFO] - {"epoch": 2001, "update": 2000.854, "loss": "0.405", "ntokens": "260555", "nsentences": "1744.52", "wps": "98792.8", "ups": "0.38", "wpb": "260555", "bsz": "1744.5", "num_updates": "96000", "lr": "0.000413043", "gnorm": "0.358", "loss_scale": "4", "train_wall": "189", "gb_free": "39.8", "wall": "8862"}
[2024-10-11 19:10:53,746][fairseq_cli.train][INFO] - end of epoch 2001 (average epoch stats below)
[2024-10-11 19:10:53,749][train][INFO] - {"epoch": 2001, "train_loss": "0.41", "train_ntokens": "260847", "train_nsentences": "1750.04", "train_wps": "100994", "train_ups": "0.39", "train_wpb": "260847", "train_bsz": "1750", "train_num_updates": "96007", "train_lr": "0.000413034", "train_gnorm": "0.336", "train_loss_scale": "4", "train_train_wall": "55", "train_gb_free": "40.1", "train_wall": "8868"}
[2024-10-11 19:10:53,812][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 19:10:53,816][fairseq.trainer][INFO] - begin training epoch 2002
[2024-10-11 19:10:53,816][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 19:13:01,471][fairseq_cli.train][INFO] - end of epoch 2002 (average epoch stats below)
[2024-10-11 19:13:01,475][train][INFO] - {"epoch": 2002, "train_loss": "0.41", "train_ntokens": "260819", "train_nsentences": "1750.04", "train_wps": "98018.3", "train_ups": "0.38", "train_wpb": "260819", "train_bsz": "1750", "train_num_updates": "96055", "train_lr": "0.000412969", "train_gnorm": "0.352", "train_loss_scale": "4", "train_train_wall": "42", "train_gb_free": "39.3", "train_wall": "8996"}
[2024-10-11 19:13:01,531][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 19:13:01,535][fairseq.trainer][INFO] - begin training epoch 2003
[2024-10-11 19:13:01,535][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 19:15:09,603][fairseq_cli.train][INFO] - end of epoch 2003 (average epoch stats below)
[2024-10-11 19:15:09,617][train][INFO] - {"epoch": 2003, "train_loss": "0.405", "train_ntokens": "261036", "train_nsentences": "1750.04", "train_wps": "97787", "train_ups": "0.37", "train_wpb": "261036", "train_bsz": "1750", "train_num_updates": "96103", "train_lr": "0.000412904", "train_gnorm": "0.37", "train_loss_scale": "4", "train_train_wall": "48", "train_gb_free": "39.6", "train_wall": "9124"}
[2024-10-11 19:15:09,735][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 19:15:09,739][fairseq.trainer][INFO] - begin training epoch 2004
[2024-10-11 19:15:09,740][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 19:17:24,046][fairseq_cli.train][INFO] - end of epoch 2004 (average epoch stats below)
[2024-10-11 19:17:24,059][train][INFO] - {"epoch": 2004, "train_loss": "0.412", "train_ntokens": "260895", "train_nsentences": "1750.04", "train_wps": "93155.9", "train_ups": "0.36", "train_wpb": "260895", "train_bsz": "1750", "train_num_updates": "96151", "train_lr": "0.000412838", "train_gnorm": "0.368", "train_loss_scale": "4", "train_train_wall": "63", "train_gb_free": "39.3", "train_wall": "9258"}
[2024-10-11 19:17:24,183][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 19:17:24,190][fairseq.trainer][INFO] - begin training epoch 2005
[2024-10-11 19:17:24,191][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 19:19:31,805][fairseq_cli.train][INFO] - end of epoch 2005 (average epoch stats below)
[2024-10-11 19:19:31,816][train][INFO] - {"epoch": 2005, "train_loss": "0.407", "train_ntokens": "260964", "train_nsentences": "1750.04", "train_wps": "98052.4", "train_ups": "0.38", "train_wpb": "260964", "train_bsz": "1750", "train_num_updates": "96199", "train_lr": "0.000412773", "train_gnorm": "0.352", "train_loss_scale": "4", "train_train_wall": "40", "train_gb_free": "40", "train_wall": "9386"}
[2024-10-11 19:19:31,924][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 19:19:31,938][fairseq.trainer][INFO] - begin training epoch 2006
[2024-10-11 19:19:31,938][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 19:20:55,922][train_inner][INFO] - {"epoch": 2006, "update": 2005.021, "loss": "0.409", "ntokens": "260996", "nsentences": "1751.33", "wps": "85873.2", "ups": "0.33", "wpb": "260996", "bsz": "1751.3", "num_updates": "96200", "lr": "0.000412772", "gnorm": "0.359", "loss_scale": "4", "train_wall": "210", "gb_free": "39.8", "wall": "9470"}
[2024-10-11 19:21:40,439][fairseq_cli.train][INFO] - end of epoch 2006 (average epoch stats below)
[2024-10-11 19:21:40,443][train][INFO] - {"epoch": 2006, "train_loss": "0.408", "train_ntokens": "260398", "train_nsentences": "1750.04", "train_wps": "97176.9", "train_ups": "0.37", "train_wpb": "260398", "train_bsz": "1750", "train_num_updates": "96247", "train_lr": "0.000412708", "train_gnorm": "0.366", "train_loss_scale": "4", "train_train_wall": "52", "train_gb_free": "39.8", "train_wall": "9514"}
[2024-10-11 19:21:40,507][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 19:21:40,511][fairseq.trainer][INFO] - begin training epoch 2007
[2024-10-11 19:21:40,511][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 19:23:49,421][fairseq_cli.train][INFO] - end of epoch 2007 (average epoch stats below)
[2024-10-11 19:23:49,424][train][INFO] - {"epoch": 2007, "train_loss": "0.403", "train_ntokens": "260579", "train_nsentences": "1750.04", "train_wps": "96975.8", "train_ups": "0.37", "train_wpb": "260579", "train_bsz": "1750", "train_num_updates": "96295", "train_lr": "0.000412643", "train_gnorm": "0.366", "train_loss_scale": "4", "train_train_wall": "59", "train_gb_free": "39.8", "train_wall": "9643"}
[2024-10-11 19:23:49,516][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 19:23:49,536][fairseq.trainer][INFO] - begin training epoch 2008
[2024-10-11 19:23:49,537][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 19:25:58,749][fairseq_cli.train][INFO] - end of epoch 2008 (average epoch stats below)
[2024-10-11 19:25:58,765][train][INFO] - {"epoch": 2008, "train_loss": "0.409", "train_ntokens": "260643", "train_nsentences": "1750.04", "train_wps": "96729.8", "train_ups": "0.37", "train_wpb": "260643", "train_bsz": "1750", "train_num_updates": "96343", "train_lr": "0.000412577", "train_gnorm": "0.354", "train_loss_scale": "4", "train_train_wall": "58", "train_gb_free": "39.8", "train_wall": "9773"}
[2024-10-11 19:25:58,881][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 19:25:58,884][fairseq.trainer][INFO] - begin training epoch 2009
[2024-10-11 19:25:58,885][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 19:28:13,242][fairseq_cli.train][INFO] - end of epoch 2009 (average epoch stats below)
[2024-10-11 19:28:13,245][train][INFO] - {"epoch": 2009, "train_loss": "0.403", "train_ntokens": "261205", "train_nsentences": "1750.04", "train_wps": "93234.3", "train_ups": "0.36", "train_wpb": "261205", "train_bsz": "1750", "train_num_updates": "96391", "train_lr": "0.000412512", "train_gnorm": "0.345", "train_loss_scale": "4", "train_train_wall": "60", "train_gb_free": "39.7", "train_wall": "9907"}
[2024-10-11 19:28:13,300][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 19:28:13,306][fairseq.trainer][INFO] - begin training epoch 2010
[2024-10-11 19:28:13,306][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 19:29:50,374][train_inner][INFO] - {"epoch": 2010, "update": 2009.188, "loss": "0.406", "ntokens": "260583", "nsentences": "1747.38", "wps": "97515.2", "ups": "0.37", "wpb": "260583", "bsz": "1747.4", "num_updates": "96400", "lr": "0.0004125", "gnorm": "0.358", "loss_scale": "4", "train_wall": "245", "gb_free": "39.3", "wall": "10004"}
[2024-10-11 19:30:22,972][fairseq_cli.train][INFO] - end of epoch 2010 (average epoch stats below)
[2024-10-11 19:30:22,983][train][INFO] - {"epoch": 2010, "train_loss": "0.408", "train_ntokens": "260521", "train_nsentences": "1750.04", "train_wps": "96395.7", "train_ups": "0.37", "train_wpb": "260521", "train_bsz": "1750", "train_num_updates": "96439", "train_lr": "0.000412447", "train_gnorm": "0.365", "train_loss_scale": "4", "train_train_wall": "60", "train_gb_free": "40.1", "train_wall": "10037"}
[2024-10-11 19:30:23,119][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 19:30:23,123][fairseq.trainer][INFO] - begin training epoch 2011
[2024-10-11 19:30:23,123][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 19:32:40,160][fairseq_cli.train][INFO] - end of epoch 2011 (average epoch stats below)
[2024-10-11 19:32:40,179][train][INFO] - {"epoch": 2011, "train_loss": "0.405", "train_ntokens": "260888", "train_nsentences": "1750.04", "train_wps": "91280", "train_ups": "0.35", "train_wpb": "260888", "train_bsz": "1750", "train_num_updates": "96487", "train_lr": "0.000412382", "train_gnorm": "0.341", "train_loss_scale": "4", "train_train_wall": "64", "train_gb_free": "39.8", "train_wall": "10174"}
[2024-10-11 19:32:40,282][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 19:32:40,290][fairseq.trainer][INFO] - begin training epoch 2012
[2024-10-11 19:32:40,290][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 19:34:47,672][fairseq_cli.train][INFO] - end of epoch 2012 (average epoch stats below)
[2024-10-11 19:34:47,677][train][INFO] - {"epoch": 2012, "train_loss": "0.409", "train_ntokens": "260678", "train_nsentences": "1750.04", "train_wps": "98142", "train_ups": "0.38", "train_wpb": "260678", "train_bsz": "1750", "train_num_updates": "96535", "train_lr": "0.000412317", "train_gnorm": "0.382", "train_loss_scale": "4", "train_train_wall": "42", "train_gb_free": "39.7", "train_wall": "10302"}
[2024-10-11 19:34:47,733][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 19:34:47,737][fairseq.trainer][INFO] - begin training epoch 2013
[2024-10-11 19:34:47,738][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 19:36:54,941][fairseq_cli.train][INFO] - end of epoch 2013 (average epoch stats below)
[2024-10-11 19:36:54,950][train][INFO] - {"epoch": 2013, "train_loss": "0.408", "train_ntokens": "261117", "train_nsentences": "1750.04", "train_wps": "98480.5", "train_ups": "0.38", "train_wpb": "261117", "train_bsz": "1750", "train_num_updates": "96583", "train_lr": "0.000412251", "train_gnorm": "0.385", "train_loss_scale": "4", "train_train_wall": "51", "train_gb_free": "40", "train_wall": "10429"}
[2024-10-11 19:36:55,034][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 19:36:55,044][fairseq.trainer][INFO] - begin training epoch 2014
[2024-10-11 19:36:55,045][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 19:38:39,344][train_inner][INFO] - {"epoch": 2014, "update": 2013.354, "loss": "0.407", "ntokens": "260838", "nsentences": "1753.36", "wps": "98622.3", "ups": "0.38", "wpb": "260838", "bsz": "1753.4", "num_updates": "96600", "lr": "0.000412228", "gnorm": "0.371", "loss_scale": "4", "train_wall": "219", "gb_free": "39.2", "wall": "10533"}
[2024-10-11 19:39:01,764][fairseq_cli.train][INFO] - end of epoch 2014 (average epoch stats below)
[2024-10-11 19:39:01,775][train][INFO] - {"epoch": 2014, "train_loss": "0.409", "train_ntokens": "261051", "train_nsentences": "1750.04", "train_wps": "98811.1", "train_ups": "0.38", "train_wpb": "261051", "train_bsz": "1750", "train_num_updates": "96631", "train_lr": "0.000412186", "train_gnorm": "0.374", "train_loss_scale": "4", "train_train_wall": "51", "train_gb_free": "39.8", "train_wall": "10556"}
[2024-10-11 19:39:01,893][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 19:39:01,901][fairseq.trainer][INFO] - begin training epoch 2015
[2024-10-11 19:39:01,901][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 19:41:11,742][fairseq_cli.train][INFO] - end of epoch 2015 (average epoch stats below)
[2024-10-11 19:41:11,748][train][INFO] - {"epoch": 2015, "train_loss": "0.403", "train_ntokens": "260871", "train_nsentences": "1750.04", "train_wps": "96352.5", "train_ups": "0.37", "train_wpb": "260871", "train_bsz": "1750", "train_num_updates": "96679", "train_lr": "0.000412121", "train_gnorm": "0.364", "train_loss_scale": "4", "train_train_wall": "56", "train_gb_free": "39.8", "train_wall": "10686"}
[2024-10-11 19:41:11,874][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 19:41:11,886][fairseq.trainer][INFO] - begin training epoch 2016
[2024-10-11 19:41:11,886][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 19:43:25,557][fairseq_cli.train][INFO] - end of epoch 2016 (average epoch stats below)
[2024-10-11 19:43:25,573][train][INFO] - {"epoch": 2016, "train_loss": "0.398", "train_ntokens": "260856", "train_nsentences": "1750.04", "train_wps": "93568.4", "train_ups": "0.36", "train_wpb": "260856", "train_bsz": "1750", "train_num_updates": "96727", "train_lr": "0.000412056", "train_gnorm": "0.335", "train_loss_scale": "4", "train_train_wall": "57", "train_gb_free": "40.6", "train_wall": "10820"}
[2024-10-11 19:43:25,717][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 19:43:25,724][fairseq.trainer][INFO] - begin training epoch 2017
[2024-10-11 19:43:25,725][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 19:45:38,384][fairseq_cli.train][INFO] - end of epoch 2017 (average epoch stats below)
[2024-10-11 19:45:38,388][train][INFO] - {"epoch": 2017, "train_loss": "0.405", "train_ntokens": "260759", "train_nsentences": "1750.04", "train_wps": "94242.6", "train_ups": "0.36", "train_wpb": "260759", "train_bsz": "1750", "train_num_updates": "96775", "train_lr": "0.00041199", "train_gnorm": "0.346", "train_loss_scale": "4", "train_train_wall": "51", "train_gb_free": "39.3", "train_wall": "10952"}
[2024-10-11 19:45:38,582][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 19:45:38,585][fairseq.trainer][INFO] - begin training epoch 2018
[2024-10-11 19:45:38,586][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 19:47:26,180][train_inner][INFO] - {"epoch": 2018, "update": 2017.521, "loss": "0.404", "ntokens": "260947", "nsentences": "1741.39", "wps": "99063.1", "ups": "0.38", "wpb": "260947", "bsz": "1741.4", "num_updates": "96800", "lr": "0.000411957", "gnorm": "0.348", "loss_scale": "4", "train_wall": "216", "gb_free": "39.6", "wall": "11060"}
[2024-10-11 19:47:46,582][fairseq_cli.train][INFO] - end of epoch 2018 (average epoch stats below)
[2024-10-11 19:47:46,591][train][INFO] - {"epoch": 2018, "train_loss": "0.41", "train_ntokens": "260776", "train_nsentences": "1750.04", "train_wps": "97643.6", "train_ups": "0.37", "train_wpb": "260776", "train_bsz": "1750", "train_num_updates": "96823", "train_lr": "0.000411925", "train_gnorm": "0.346", "train_loss_scale": "4", "train_train_wall": "51", "train_gb_free": "39.7", "train_wall": "11081"}
[2024-10-11 19:47:46,682][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 19:47:46,690][fairseq.trainer][INFO] - begin training epoch 2019
[2024-10-11 19:47:46,691][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 19:49:54,072][fairseq_cli.train][INFO] - end of epoch 2019 (average epoch stats below)
[2024-10-11 19:49:54,076][train][INFO] - {"epoch": 2019, "train_loss": "0.409", "train_ntokens": "260417", "train_nsentences": "1750.04", "train_wps": "98053.5", "train_ups": "0.38", "train_wpb": "260417", "train_bsz": "1750", "train_num_updates": "96871", "train_lr": "0.00041186", "train_gnorm": "0.375", "train_loss_scale": "4", "train_train_wall": "53", "train_gb_free": "40.6", "train_wall": "11208"}
[2024-10-11 19:49:54,163][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 19:49:54,169][fairseq.trainer][INFO] - begin training epoch 2020
[2024-10-11 19:49:54,169][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 19:52:07,343][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 2020 @ 96919 updates
[2024-10-11 19:52:07,345][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-11 19:52:11,312][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-11 19:52:11,315][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 2020 @ 96919 updates, score None) (writing took 3.9716919250786304 seconds)
[2024-10-11 19:52:11,315][fairseq_cli.train][INFO] - end of epoch 2020 (average epoch stats below)
[2024-10-11 19:52:11,317][train][INFO] - {"epoch": 2020, "train_loss": "0.415", "train_ntokens": "260861", "train_nsentences": "1750.04", "train_wps": "91237.9", "train_ups": "0.35", "train_wpb": "260861", "train_bsz": "1750", "train_num_updates": "96919", "train_lr": "0.000411795", "train_gnorm": "0.36", "train_loss_scale": "4", "train_train_wall": "57", "train_gb_free": "39.6", "train_wall": "11345"}
[2024-10-11 19:52:11,391][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 19:52:11,395][fairseq.trainer][INFO] - begin training epoch 2021
[2024-10-11 19:52:11,395][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 19:54:19,194][fairseq_cli.train][INFO] - end of epoch 2021 (average epoch stats below)
[2024-10-11 19:54:19,201][train][INFO] - {"epoch": 2021, "train_loss": "0.406", "train_ntokens": "260829", "train_nsentences": "1750.04", "train_wps": "97902.9", "train_ups": "0.38", "train_wpb": "260830", "train_bsz": "1750", "train_num_updates": "96967", "train_lr": "0.00041173", "train_gnorm": "0.361", "train_loss_scale": "4", "train_train_wall": "54", "train_gb_free": "39.7", "train_wall": "11473"}
[2024-10-11 19:54:19,271][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 19:54:19,275][fairseq.trainer][INFO] - begin training epoch 2022
[2024-10-11 19:54:19,276][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 19:56:18,129][train_inner][INFO] - {"epoch": 2022, "update": 2021.688, "loss": "0.411", "ntokens": "260739", "nsentences": "1758.27", "wps": "98032.3", "ups": "0.38", "wpb": "260739", "bsz": "1758.3", "num_updates": "97000", "lr": "0.000411685", "gnorm": "0.365", "loss_scale": "4", "train_wall": "201", "gb_free": "40.1", "wall": "11592"}
[2024-10-11 19:56:30,284][fairseq_cli.train][INFO] - end of epoch 2022 (average epoch stats below)
[2024-10-11 19:56:30,299][train][INFO] - {"epoch": 2022, "train_loss": "0.416", "train_ntokens": "260560", "train_nsentences": "1750.04", "train_wps": "95412", "train_ups": "0.37", "train_wpb": "260560", "train_bsz": "1750", "train_num_updates": "97015", "train_lr": "0.000411664", "train_gnorm": "0.384", "train_loss_scale": "4", "train_train_wall": "28", "train_gb_free": "39.6", "train_wall": "11604"}
[2024-10-11 19:56:30,441][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 19:56:30,454][fairseq.trainer][INFO] - begin training epoch 2023
[2024-10-11 19:56:30,454][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 19:58:38,599][fairseq_cli.train][INFO] - end of epoch 2023 (average epoch stats below)
[2024-10-11 19:58:38,604][train][INFO] - {"epoch": 2023, "train_loss": "0.407", "train_ntokens": "260808", "train_nsentences": "1750.04", "train_wps": "97572.8", "train_ups": "0.37", "train_wpb": "260808", "train_bsz": "1750", "train_num_updates": "97063", "train_lr": "0.000411599", "train_gnorm": "0.346", "train_loss_scale": "4", "train_train_wall": "48", "train_gb_free": "40.1", "train_wall": "11733"}
[2024-10-11 19:58:38,759][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 19:58:38,762][fairseq.trainer][INFO] - begin training epoch 2024
[2024-10-11 19:58:38,763][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 20:00:49,802][fairseq_cli.train][INFO] - end of epoch 2024 (average epoch stats below)
[2024-10-11 20:00:49,806][train][INFO] - {"epoch": 2024, "train_loss": "0.404", "train_ntokens": "260713", "train_nsentences": "1750.04", "train_wps": "95383.9", "train_ups": "0.37", "train_wpb": "260713", "train_bsz": "1750", "train_num_updates": "97111", "train_lr": "0.000411534", "train_gnorm": "0.369", "train_loss_scale": "4", "train_train_wall": "65", "train_gb_free": "40", "train_wall": "11864"}
[2024-10-11 20:00:49,866][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 20:00:49,869][fairseq.trainer][INFO] - begin training epoch 2025
[2024-10-11 20:00:49,870][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 20:02:55,446][fairseq_cli.train][INFO] - end of epoch 2025 (average epoch stats below)
[2024-10-11 20:02:55,456][train][INFO] - {"epoch": 2025, "train_loss": "0.407", "train_ntokens": "260408", "train_nsentences": "1750.04", "train_wps": "99485.6", "train_ups": "0.38", "train_wpb": "260408", "train_bsz": "1750", "train_num_updates": "97159", "train_lr": "0.000411469", "train_gnorm": "0.369", "train_loss_scale": "4", "train_train_wall": "51", "train_gb_free": "39.8", "train_wall": "11989"}
[2024-10-11 20:02:55,601][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 20:02:55,609][fairseq.trainer][INFO] - begin training epoch 2026
[2024-10-11 20:02:55,610][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 20:04:39,976][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
[2024-10-11 20:05:02,402][train_inner][INFO] - {"epoch": 2026, "update": 2025.875, "loss": "0.406", "ntokens": "260586", "nsentences": "1747.38", "wps": "99409.3", "ups": "0.38", "wpb": "260586", "bsz": "1747.4", "num_updates": "97200", "lr": "0.000411413", "gnorm": "0.365", "loss_scale": "4", "train_wall": "227", "gb_free": "40.3", "wall": "12116"}
[2024-10-11 20:05:03,856][fairseq_cli.train][INFO] - end of epoch 2026 (average epoch stats below)
[2024-10-11 20:05:03,858][train][INFO] - {"epoch": 2026, "train_loss": "0.4", "train_ntokens": "260868", "train_nsentences": "1743.91", "train_wps": "95489.5", "train_ups": "0.37", "train_wpb": "260868", "train_bsz": "1743.9", "train_num_updates": "97206", "train_lr": "0.000411405", "train_gnorm": "0.353", "train_loss_scale": "4", "train_train_wall": "53", "train_gb_free": "39.6", "train_wall": "12118"}
[2024-10-11 20:05:03,943][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 20:05:03,963][fairseq.trainer][INFO] - begin training epoch 2027
[2024-10-11 20:05:03,964][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 20:07:11,156][fairseq_cli.train][INFO] - end of epoch 2027 (average epoch stats below)
[2024-10-11 20:07:11,167][train][INFO] - {"epoch": 2027, "train_loss": "0.402", "train_ntokens": "260785", "train_nsentences": "1750.04", "train_wps": "98333.5", "train_ups": "0.38", "train_wpb": "260785", "train_bsz": "1750", "train_num_updates": "97254", "train_lr": "0.00041134", "train_gnorm": "0.367", "train_loss_scale": "4", "train_train_wall": "54", "train_gb_free": "39.3", "train_wall": "12245"}
[2024-10-11 20:07:11,316][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 20:07:11,330][fairseq.trainer][INFO] - begin training epoch 2028
[2024-10-11 20:07:11,330][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 20:09:20,518][fairseq_cli.train][INFO] - end of epoch 2028 (average epoch stats below)
[2024-10-11 20:09:20,529][train][INFO] - {"epoch": 2028, "train_loss": "0.411", "train_ntokens": "260777", "train_nsentences": "1750.04", "train_wps": "96765.6", "train_ups": "0.37", "train_wpb": "260777", "train_bsz": "1750", "train_num_updates": "97302", "train_lr": "0.000411274", "train_gnorm": "0.389", "train_loss_scale": "4", "train_train_wall": "54", "train_gb_free": "39.3", "train_wall": "12375"}
[2024-10-11 20:09:20,650][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 20:09:20,672][fairseq.trainer][INFO] - begin training epoch 2029
[2024-10-11 20:09:20,673][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 20:11:30,387][fairseq_cli.train][INFO] - end of epoch 2029 (average epoch stats below)
[2024-10-11 20:11:30,390][train][INFO] - {"epoch": 2029, "train_loss": "0.406", "train_ntokens": "260459", "train_nsentences": "1750.04", "train_wps": "96275.1", "train_ups": "0.37", "train_wpb": "260459", "train_bsz": "1750", "train_num_updates": "97350", "train_lr": "0.000411209", "train_gnorm": "0.36", "train_loss_scale": "4", "train_train_wall": "63", "train_gb_free": "39.8", "train_wall": "12504"}
[2024-10-11 20:11:30,451][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 20:11:30,454][fairseq.trainer][INFO] - begin training epoch 2030
[2024-10-11 20:11:30,455][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 20:13:37,044][fairseq_cli.train][INFO] - end of epoch 2030 (average epoch stats below)
[2024-10-11 20:13:37,047][train][INFO] - {"epoch": 2030, "train_loss": "0.396", "train_ntokens": "260627", "train_nsentences": "1750.04", "train_wps": "98774", "train_ups": "0.38", "train_wpb": "260627", "train_bsz": "1750", "train_num_updates": "97398", "train_lr": "0.000411144", "train_gnorm": "0.358", "train_loss_scale": "4", "train_train_wall": "47", "train_gb_free": "39.3", "train_wall": "12631"}
[2024-10-11 20:13:37,146][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 20:13:37,153][fairseq.trainer][INFO] - begin training epoch 2031
[2024-10-11 20:13:37,153][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 20:15:03,635][train_inner][INFO] - {"epoch": 2031, "update": 2030.042, "loss": "0.403", "ntokens": "260728", "nsentences": "1753.06", "wps": "86731.9", "ups": "0.33", "wpb": "260728", "bsz": "1753.1", "num_updates": "97400", "lr": "0.000411141", "gnorm": "0.367", "loss_scale": "4", "train_wall": "228", "gb_free": "39.8", "wall": "12718"}
[2024-10-11 20:15:44,537][fairseq_cli.train][INFO] - end of epoch 2031 (average epoch stats below)
[2024-10-11 20:15:44,542][train][INFO] - {"epoch": 2031, "train_loss": "0.403", "train_ntokens": "261111", "train_nsentences": "1750.04", "train_wps": "98306.8", "train_ups": "0.38", "train_wpb": "261111", "train_bsz": "1750", "train_num_updates": "97446", "train_lr": "0.000411079", "train_gnorm": "0.374", "train_loss_scale": "4", "train_train_wall": "44", "train_gb_free": "40.3", "train_wall": "12759"}
[2024-10-11 20:15:44,598][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 20:15:44,604][fairseq.trainer][INFO] - begin training epoch 2032
[2024-10-11 20:15:44,604][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 20:17:53,081][fairseq_cli.train][INFO] - end of epoch 2032 (average epoch stats below)
[2024-10-11 20:17:53,090][train][INFO] - {"epoch": 2032, "train_loss": "0.402", "train_ntokens": "260782", "train_nsentences": "1750.04", "train_wps": "97379.3", "train_ups": "0.37", "train_wpb": "260782", "train_bsz": "1750", "train_num_updates": "97494", "train_lr": "0.000411014", "train_gnorm": "0.36", "train_loss_scale": "4", "train_train_wall": "47", "train_gb_free": "39.8", "train_wall": "12887"}
[2024-10-11 20:17:53,207][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 20:17:53,220][fairseq.trainer][INFO] - begin training epoch 2033
[2024-10-11 20:17:53,221][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 20:20:02,181][fairseq_cli.train][INFO] - end of epoch 2033 (average epoch stats below)
[2024-10-11 20:20:02,197][train][INFO] - {"epoch": 2033, "train_loss": "0.394", "train_ntokens": "260198", "train_nsentences": "1750.04", "train_wps": "96744.5", "train_ups": "0.37", "train_wpb": "260198", "train_bsz": "1750", "train_num_updates": "97542", "train_lr": "0.000410948", "train_gnorm": "0.349", "train_loss_scale": "4", "train_train_wall": "48", "train_gb_free": "39.6", "train_wall": "13016"}
[2024-10-11 20:20:02,285][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 20:20:02,289][fairseq.trainer][INFO] - begin training epoch 2034
[2024-10-11 20:20:02,289][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 20:22:08,635][fairseq_cli.train][INFO] - end of epoch 2034 (average epoch stats below)
[2024-10-11 20:22:08,641][train][INFO] - {"epoch": 2034, "train_loss": "0.408", "train_ntokens": "260664", "train_nsentences": "1750.04", "train_wps": "98955.2", "train_ups": "0.38", "train_wpb": "260664", "train_bsz": "1750", "train_num_updates": "97590", "train_lr": "0.000410883", "train_gnorm": "0.385", "train_loss_scale": "4", "train_train_wall": "48", "train_gb_free": "39.3", "train_wall": "13143"}
[2024-10-11 20:22:08,790][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 20:22:08,796][fairseq.trainer][INFO] - begin training epoch 2035
[2024-10-11 20:22:08,796][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 20:23:44,178][train_inner][INFO] - {"epoch": 2035, "update": 2034.208, "loss": "0.402", "ntokens": "260452", "nsentences": "1754.73", "wps": "100070", "ups": "0.38", "wpb": "260452", "bsz": "1754.7", "num_updates": "97600", "lr": "0.00041087", "gnorm": "0.37", "loss_scale": "4", "train_wall": "203", "gb_free": "39.8", "wall": "13238"}
[2024-10-11 20:24:16,680][fairseq_cli.train][INFO] - end of epoch 2035 (average epoch stats below)
[2024-10-11 20:24:16,682][train][INFO] - {"epoch": 2035, "train_loss": "0.402", "train_ntokens": "260654", "train_nsentences": "1750.04", "train_wps": "97716", "train_ups": "0.37", "train_wpb": "260654", "train_bsz": "1750", "train_num_updates": "97638", "train_lr": "0.000410818", "train_gnorm": "0.377", "train_loss_scale": "4", "train_train_wall": "55", "train_gb_free": "40.1", "train_wall": "13271"}
[2024-10-11 20:24:16,837][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 20:24:16,843][fairseq.trainer][INFO] - begin training epoch 2036
[2024-10-11 20:24:16,844][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 20:26:26,447][fairseq_cli.train][INFO] - end of epoch 2036 (average epoch stats below)
[2024-10-11 20:26:26,451][train][INFO] - {"epoch": 2036, "train_loss": "0.397", "train_ntokens": "260800", "train_nsentences": "1750.04", "train_wps": "96470", "train_ups": "0.37", "train_wpb": "260800", "train_bsz": "1750", "train_num_updates": "97686", "train_lr": "0.000410753", "train_gnorm": "0.333", "train_loss_scale": "4", "train_train_wall": "56", "train_gb_free": "39.6", "train_wall": "13400"}
[2024-10-11 20:26:26,543][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 20:26:26,550][fairseq.trainer][INFO] - begin training epoch 2037
[2024-10-11 20:26:26,551][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 20:28:37,324][fairseq_cli.train][INFO] - end of epoch 2037 (average epoch stats below)
[2024-10-11 20:28:37,336][train][INFO] - {"epoch": 2037, "train_loss": "0.405", "train_ntokens": "260950", "train_nsentences": "1750.04", "train_wps": "95706.1", "train_ups": "0.37", "train_wpb": "260950", "train_bsz": "1750", "train_num_updates": "97734", "train_lr": "0.000410687", "train_gnorm": "0.361", "train_loss_scale": "4", "train_train_wall": "49", "train_gb_free": "39.7", "train_wall": "13531"}
[2024-10-11 20:28:37,445][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 20:28:37,449][fairseq.trainer][INFO] - begin training epoch 2038
[2024-10-11 20:28:37,449][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 20:30:45,568][fairseq_cli.train][INFO] - end of epoch 2038 (average epoch stats below)
[2024-10-11 20:30:45,572][train][INFO] - {"epoch": 2038, "train_loss": "0.408", "train_ntokens": "260581", "train_nsentences": "1750.04", "train_wps": "97541.1", "train_ups": "0.37", "train_wpb": "260581", "train_bsz": "1750", "train_num_updates": "97782", "train_lr": "0.000410622", "train_gnorm": "0.349", "train_loss_scale": "4", "train_train_wall": "51", "train_gb_free": "39.2", "train_wall": "13660"}
[2024-10-11 20:30:45,717][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 20:30:45,721][fairseq.trainer][INFO] - begin training epoch 2039
[2024-10-11 20:30:45,721][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 20:32:25,529][train_inner][INFO] - {"epoch": 2039, "update": 2038.375, "loss": "0.402", "ntokens": "261075", "nsentences": "1733.48", "wps": "100155", "ups": "0.38", "wpb": "261075", "bsz": "1733.5", "num_updates": "97800", "lr": "0.000410598", "gnorm": "0.35", "loss_scale": "4", "train_wall": "215", "gb_free": "39.3", "wall": "13760"}
[2024-10-11 20:32:53,515][fairseq_cli.train][INFO] - end of epoch 2039 (average epoch stats below)
[2024-10-11 20:32:53,517][train][INFO] - {"epoch": 2039, "train_loss": "0.403", "train_ntokens": "260971", "train_nsentences": "1750.04", "train_wps": "97912.4", "train_ups": "0.38", "train_wpb": "260971", "train_bsz": "1750", "train_num_updates": "97830", "train_lr": "0.000410557", "train_gnorm": "0.347", "train_loss_scale": "4", "train_train_wall": "55", "train_gb_free": "39.2", "train_wall": "13788"}
[2024-10-11 20:32:53,593][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 20:32:53,600][fairseq.trainer][INFO] - begin training epoch 2040
[2024-10-11 20:32:53,600][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 20:35:01,013][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 2040 @ 97878 updates
[2024-10-11 20:35:01,014][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-11 20:35:04,283][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-11 20:35:04,285][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 2040 @ 97878 updates, score None) (writing took 3.2717104740440845 seconds)
[2024-10-11 20:35:04,285][fairseq_cli.train][INFO] - end of epoch 2040 (average epoch stats below)
[2024-10-11 20:35:04,287][train][INFO] - {"epoch": 2040, "train_loss": "0.406", "train_ntokens": "260926", "train_nsentences": "1750.04", "train_wps": "95777.2", "train_ups": "0.37", "train_wpb": "260926", "train_bsz": "1750", "train_num_updates": "97878", "train_lr": "0.000410492", "train_gnorm": "0.355", "train_loss_scale": "4", "train_train_wall": "46", "train_gb_free": "39.7", "train_wall": "13918"}
[2024-10-11 20:35:04,356][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 20:35:04,382][fairseq.trainer][INFO] - begin training epoch 2041
[2024-10-11 20:35:04,382][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 20:37:07,417][fairseq_cli.train][INFO] - end of epoch 2041 (average epoch stats below)
[2024-10-11 20:37:07,439][train][INFO] - {"epoch": 2041, "train_loss": "0.398", "train_ntokens": "260758", "train_nsentences": "1750.04", "train_wps": "101650", "train_ups": "0.39", "train_wpb": "260758", "train_bsz": "1750", "train_num_updates": "97926", "train_lr": "0.000410427", "train_gnorm": "0.355", "train_loss_scale": "4", "train_train_wall": "47", "train_gb_free": "42.3", "train_wall": "14041"}
[2024-10-11 20:37:07,542][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 20:37:07,561][fairseq.trainer][INFO] - begin training epoch 2042
[2024-10-11 20:37:07,561][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 20:39:14,949][fairseq_cli.train][INFO] - end of epoch 2042 (average epoch stats below)
[2024-10-11 20:39:14,952][train][INFO] - {"epoch": 2042, "train_loss": "0.407", "train_ntokens": "260525", "train_nsentences": "1750.04", "train_wps": "98072.5", "train_ups": "0.38", "train_wpb": "260525", "train_bsz": "1750", "train_num_updates": "97974", "train_lr": "0.000410361", "train_gnorm": "0.377", "train_loss_scale": "4", "train_train_wall": "65", "train_gb_free": "39.2", "train_wall": "14169"}
[2024-10-11 20:39:15,009][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 20:39:15,015][fairseq.trainer][INFO] - begin training epoch 2043
[2024-10-11 20:39:15,015][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 20:41:04,385][train_inner][INFO] - {"epoch": 2043, "update": 2042.542, "loss": "0.405", "ntokens": "260616", "nsentences": "1759.8", "wps": "100459", "ups": "0.39", "wpb": "260616", "bsz": "1759.8", "num_updates": "98000", "lr": "0.000410326", "gnorm": "0.361", "loss_scale": "4", "train_wall": "212", "gb_free": "40.5", "wall": "14278"}
[2024-10-11 20:41:23,001][fairseq_cli.train][INFO] - end of epoch 2043 (average epoch stats below)
[2024-10-11 20:41:23,007][train][INFO] - {"epoch": 2043, "train_loss": "0.406", "train_ntokens": "260720", "train_nsentences": "1750.04", "train_wps": "97733.3", "train_ups": "0.37", "train_wpb": "260720", "train_bsz": "1750", "train_num_updates": "98022", "train_lr": "0.000410296", "train_gnorm": "0.362", "train_loss_scale": "4", "train_train_wall": "40", "train_gb_free": "39.6", "train_wall": "14297"}
[2024-10-11 20:41:23,105][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 20:41:23,124][fairseq.trainer][INFO] - begin training epoch 2044
[2024-10-11 20:41:23,125][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 20:43:30,829][fairseq_cli.train][INFO] - end of epoch 2044 (average epoch stats below)
[2024-10-11 20:43:30,840][train][INFO] - {"epoch": 2044, "train_loss": "0.402", "train_ntokens": "260807", "train_nsentences": "1750.04", "train_wps": "97937.7", "train_ups": "0.38", "train_wpb": "260807", "train_bsz": "1750", "train_num_updates": "98070", "train_lr": "0.000410231", "train_gnorm": "0.343", "train_loss_scale": "4", "train_train_wall": "44", "train_gb_free": "39.3", "train_wall": "14425"}
[2024-10-11 20:43:30,973][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 20:43:30,982][fairseq.trainer][INFO] - begin training epoch 2045
[2024-10-11 20:43:30,983][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 20:45:37,654][fairseq_cli.train][INFO] - end of epoch 2045 (average epoch stats below)
[2024-10-11 20:45:37,662][train][INFO] - {"epoch": 2045, "train_loss": "0.401", "train_ntokens": "260719", "train_nsentences": "1750.04", "train_wps": "98680.8", "train_ups": "0.38", "train_wpb": "260719", "train_bsz": "1750", "train_num_updates": "98118", "train_lr": "0.000410166", "train_gnorm": "0.36", "train_loss_scale": "4", "train_train_wall": "27", "train_gb_free": "39.7", "train_wall": "14552"}
[2024-10-11 20:45:37,729][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 20:45:37,733][fairseq.trainer][INFO] - begin training epoch 2046
[2024-10-11 20:45:37,733][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 20:47:49,710][fairseq_cli.train][INFO] - end of epoch 2046 (average epoch stats below)
[2024-10-11 20:47:49,714][train][INFO] - {"epoch": 2046, "train_loss": "0.395", "train_ntokens": "260771", "train_nsentences": "1750.04", "train_wps": "94791.5", "train_ups": "0.36", "train_wpb": "260771", "train_bsz": "1750", "train_num_updates": "98166", "train_lr": "0.000410101", "train_gnorm": "0.358", "train_loss_scale": "4", "train_train_wall": "62", "train_gb_free": "40.1", "train_wall": "14684"}
[2024-10-11 20:47:49,823][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 20:47:49,841][fairseq.trainer][INFO] - begin training epoch 2047
[2024-10-11 20:47:49,842][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 20:49:47,968][train_inner][INFO] - {"epoch": 2047, "update": 2046.708, "loss": "0.4", "ntokens": "260812", "nsentences": "1749.53", "wps": "99626.9", "ups": "0.38", "wpb": "260812", "bsz": "1749.5", "num_updates": "98200", "lr": "0.000410054", "gnorm": "0.354", "loss_scale": "4", "train_wall": "194", "gb_free": "39.7", "wall": "14802"}
[2024-10-11 20:50:00,777][fairseq_cli.train][INFO] - end of epoch 2047 (average epoch stats below)
[2024-10-11 20:50:00,779][train][INFO] - {"epoch": 2047, "train_loss": "0.401", "train_ntokens": "260843", "train_nsentences": "1750.04", "train_wps": "95531.2", "train_ups": "0.37", "train_wpb": "260843", "train_bsz": "1750", "train_num_updates": "98214", "train_lr": "0.000410035", "train_gnorm": "0.362", "train_loss_scale": "4", "train_train_wall": "58", "train_gb_free": "39.7", "train_wall": "14815"}
[2024-10-11 20:50:00,847][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 20:50:00,851][fairseq.trainer][INFO] - begin training epoch 2048
[2024-10-11 20:50:00,851][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 20:52:08,702][fairseq_cli.train][INFO] - end of epoch 2048 (average epoch stats below)
[2024-10-11 20:52:08,712][train][INFO] - {"epoch": 2048, "train_loss": "0.402", "train_ntokens": "260475", "train_nsentences": "1750.04", "train_wps": "97732.1", "train_ups": "0.38", "train_wpb": "260475", "train_bsz": "1750", "train_num_updates": "98262", "train_lr": "0.00040997", "train_gnorm": "0.351", "train_loss_scale": "4", "train_train_wall": "54", "train_gb_free": "40.5", "train_wall": "14943"}
[2024-10-11 20:52:08,823][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 20:52:08,827][fairseq.trainer][INFO] - begin training epoch 2049
[2024-10-11 20:52:08,827][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 20:54:17,616][fairseq_cli.train][INFO] - end of epoch 2049 (average epoch stats below)
[2024-10-11 20:54:17,629][train][INFO] - {"epoch": 2049, "train_loss": "0.403", "train_ntokens": "260988", "train_nsentences": "1750.04", "train_wps": "97180", "train_ups": "0.37", "train_wpb": "260988", "train_bsz": "1750", "train_num_updates": "98310", "train_lr": "0.000409905", "train_gnorm": "0.367", "train_loss_scale": "4", "train_train_wall": "53", "train_gb_free": "39.1", "train_wall": "15072"}
[2024-10-11 20:54:17,767][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 20:54:17,787][fairseq.trainer][INFO] - begin training epoch 2050
[2024-10-11 20:54:17,788][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 20:56:29,001][fairseq_cli.train][INFO] - end of epoch 2050 (average epoch stats below)
[2024-10-11 20:56:29,024][train][INFO] - {"epoch": 2050, "train_loss": "0.407", "train_ntokens": "260913", "train_nsentences": "1750.04", "train_wps": "95323", "train_ups": "0.37", "train_wpb": "260913", "train_bsz": "1750", "train_num_updates": "98358", "train_lr": "0.00040984", "train_gnorm": "0.384", "train_loss_scale": "4", "train_train_wall": "58", "train_gb_free": "40.2", "train_wall": "15203"}
[2024-10-11 20:56:29,157][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 20:56:29,173][fairseq.trainer][INFO] - begin training epoch 2051
[2024-10-11 20:56:29,173][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 20:58:37,176][train_inner][INFO] - {"epoch": 2051, "update": 2050.875, "loss": "0.402", "ntokens": "260905", "nsentences": "1751.29", "wps": "98603.9", "ups": "0.38", "wpb": "260905", "bsz": "1751.3", "num_updates": "98400", "lr": "0.000409783", "gnorm": "0.365", "loss_scale": "4", "train_wall": "230", "gb_free": "40", "wall": "15331"}
[2024-10-11 20:58:40,630][fairseq_cli.train][INFO] - end of epoch 2051 (average epoch stats below)
[2024-10-11 20:58:40,632][train][INFO] - {"epoch": 2051, "train_loss": "0.396", "train_ntokens": "260807", "train_nsentences": "1750.04", "train_wps": "95124.9", "train_ups": "0.36", "train_wpb": "260807", "train_bsz": "1750", "train_num_updates": "98406", "train_lr": "0.000409774", "train_gnorm": "0.348", "train_loss_scale": "4", "train_train_wall": "57", "train_gb_free": "39.3", "train_wall": "15335"}
[2024-10-11 20:58:40,760][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 20:58:40,780][fairseq.trainer][INFO] - begin training epoch 2052
[2024-10-11 20:58:40,781][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 21:00:47,639][fairseq_cli.train][INFO] - end of epoch 2052 (average epoch stats below)
[2024-10-11 21:00:47,657][train][INFO] - {"epoch": 2052, "train_loss": "0.398", "train_ntokens": "260653", "train_nsentences": "1750.04", "train_wps": "98498.5", "train_ups": "0.38", "train_wpb": "260653", "train_bsz": "1750", "train_num_updates": "98454", "train_lr": "0.000409709", "train_gnorm": "0.373", "train_loss_scale": "4", "train_train_wall": "56", "train_gb_free": "40", "train_wall": "15462"}
[2024-10-11 21:00:47,742][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 21:00:47,754][fairseq.trainer][INFO] - begin training epoch 2053
[2024-10-11 21:00:47,755][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 21:03:01,047][fairseq_cli.train][INFO] - end of epoch 2053 (average epoch stats below)
[2024-10-11 21:03:01,054][train][INFO] - {"epoch": 2053, "train_loss": "0.407", "train_ntokens": "260786", "train_nsentences": "1750.04", "train_wps": "93841.7", "train_ups": "0.36", "train_wpb": "260786", "train_bsz": "1750", "train_num_updates": "98502", "train_lr": "0.000409644", "train_gnorm": "0.363", "train_loss_scale": "4", "train_train_wall": "30", "train_gb_free": "40.2", "train_wall": "15595"}
[2024-10-11 21:03:01,161][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 21:03:01,179][fairseq.trainer][INFO] - begin training epoch 2054
[2024-10-11 21:03:01,179][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 21:05:07,782][fairseq_cli.train][INFO] - end of epoch 2054 (average epoch stats below)
[2024-10-11 21:05:07,789][train][INFO] - {"epoch": 2054, "train_loss": "0.397", "train_ntokens": "260313", "train_nsentences": "1750.04", "train_wps": "98595.6", "train_ups": "0.38", "train_wpb": "260313", "train_bsz": "1750", "train_num_updates": "98550", "train_lr": "0.000409579", "train_gnorm": "0.352", "train_loss_scale": "4", "train_train_wall": "47", "train_gb_free": "39.8", "train_wall": "15722"}
[2024-10-11 21:05:07,931][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 21:05:07,938][fairseq.trainer][INFO] - begin training epoch 2055
[2024-10-11 21:05:07,939][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 21:07:19,075][fairseq_cli.train][INFO] - end of epoch 2055 (average epoch stats below)
[2024-10-11 21:07:19,078][train][INFO] - {"epoch": 2055, "train_loss": "0.402", "train_ntokens": "260485", "train_nsentences": "1750.04", "train_wps": "95236.9", "train_ups": "0.37", "train_wpb": "260485", "train_bsz": "1750", "train_num_updates": "98598", "train_lr": "0.000409514", "train_gnorm": "0.35", "train_loss_scale": "4", "train_train_wall": "53", "train_gb_free": "40", "train_wall": "15853"}
[2024-10-11 21:07:19,156][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 21:07:19,164][fairseq.trainer][INFO] - begin training epoch 2056
[2024-10-11 21:07:19,165][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 21:08:48,535][train_inner][INFO] - {"epoch": 2056, "update": 2055.042, "loss": "0.401", "ntokens": "260510", "nsentences": "1750.26", "wps": "85226", "ups": "0.33", "wpb": "260510", "bsz": "1750.3", "num_updates": "98600", "lr": "0.000409511", "gnorm": "0.359", "loss_scale": "4", "train_wall": "203", "gb_free": "39.7", "wall": "15943"}
[2024-10-11 21:09:30,299][fairseq_cli.train][INFO] - end of epoch 2056 (average epoch stats below)
[2024-10-11 21:09:30,302][train][INFO] - {"epoch": 2056, "train_loss": "0.395", "train_ntokens": "260768", "train_nsentences": "1750.04", "train_wps": "95388", "train_ups": "0.37", "train_wpb": "260768", "train_bsz": "1750", "train_num_updates": "98646", "train_lr": "0.000409448", "train_gnorm": "0.355", "train_loss_scale": "4", "train_train_wall": "55", "train_gb_free": "39.2", "train_wall": "15984"}
[2024-10-11 21:09:30,403][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 21:09:30,428][fairseq.trainer][INFO] - begin training epoch 2057
[2024-10-11 21:09:30,429][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 21:11:41,214][fairseq_cli.train][INFO] - end of epoch 2057 (average epoch stats below)
[2024-10-11 21:11:41,237][train][INFO] - {"epoch": 2057, "train_loss": "0.399", "train_ntokens": "260844", "train_nsentences": "1750.04", "train_wps": "95637.5", "train_ups": "0.37", "train_wpb": "260844", "train_bsz": "1750", "train_num_updates": "98694", "train_lr": "0.000409383", "train_gnorm": "0.366", "train_loss_scale": "4", "train_train_wall": "60", "train_gb_free": "39.3", "train_wall": "16115"}
[2024-10-11 21:11:41,347][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 21:11:41,360][fairseq.trainer][INFO] - begin training epoch 2058
[2024-10-11 21:11:41,360][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 21:13:52,927][fairseq_cli.train][INFO] - end of epoch 2058 (average epoch stats below)
[2024-10-11 21:13:52,967][train][INFO] - {"epoch": 2058, "train_loss": "0.401", "train_ntokens": "260850", "train_nsentences": "1750.04", "train_wps": "95057.3", "train_ups": "0.36", "train_wpb": "260850", "train_bsz": "1750", "train_num_updates": "98742", "train_lr": "0.000409318", "train_gnorm": "0.358", "train_loss_scale": "4", "train_train_wall": "42", "train_gb_free": "39.3", "train_wall": "16247"}
[2024-10-11 21:13:53,114][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 21:13:53,131][fairseq.trainer][INFO] - begin training epoch 2059
[2024-10-11 21:13:53,132][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 21:16:01,800][fairseq_cli.train][INFO] - end of epoch 2059 (average epoch stats below)
[2024-10-11 21:16:01,803][train][INFO] - {"epoch": 2059, "train_loss": "0.403", "train_ntokens": "260926", "train_nsentences": "1750.04", "train_wps": "97214.9", "train_ups": "0.37", "train_wpb": "260926", "train_bsz": "1750", "train_num_updates": "98790", "train_lr": "0.000409253", "train_gnorm": "0.36", "train_loss_scale": "4", "train_train_wall": "43", "train_gb_free": "39.9", "train_wall": "16376"}
[2024-10-11 21:16:01,939][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 21:16:01,943][fairseq.trainer][INFO] - begin training epoch 2060
[2024-10-11 21:16:01,944][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 21:17:45,541][train_inner][INFO] - {"epoch": 2060, "update": 2059.208, "loss": "0.399", "ntokens": "260653", "nsentences": "1751.62", "wps": "97078.3", "ups": "0.37", "wpb": "260654", "bsz": "1751.6", "num_updates": "98800", "lr": "0.000409239", "gnorm": "0.359", "loss_scale": "4", "train_wall": "222", "gb_free": "39.8", "wall": "16480"}
[2024-10-11 21:18:10,539][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 2060 @ 98838 updates
[2024-10-11 21:18:10,539][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-11 21:18:14,253][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-11 21:18:14,256][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 2060 @ 98838 updates, score None) (writing took 3.7173374546691775 seconds)
[2024-10-11 21:18:14,257][fairseq_cli.train][INFO] - end of epoch 2060 (average epoch stats below)
[2024-10-11 21:18:14,260][train][INFO] - {"epoch": 2060, "train_loss": "0.401", "train_ntokens": "260512", "train_nsentences": "1750.04", "train_wps": "94408.2", "train_ups": "0.36", "train_wpb": "260512", "train_bsz": "1750", "train_num_updates": "98838", "train_lr": "0.000409187", "train_gnorm": "0.347", "train_loss_scale": "4", "train_train_wall": "59", "train_gb_free": "39.6", "train_wall": "16508"}
[2024-10-11 21:18:14,336][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 21:18:14,355][fairseq.trainer][INFO] - begin training epoch 2061
[2024-10-11 21:18:14,355][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 21:20:21,950][fairseq_cli.train][INFO] - end of epoch 2061 (average epoch stats below)
[2024-10-11 21:20:21,967][train][INFO] - {"epoch": 2061, "train_loss": "0.397", "train_ntokens": "260513", "train_nsentences": "1750.04", "train_wps": "97921.2", "train_ups": "0.38", "train_wpb": "260513", "train_bsz": "1750", "train_num_updates": "98886", "train_lr": "0.000409122", "train_gnorm": "0.376", "train_loss_scale": "4", "train_train_wall": "55", "train_gb_free": "40", "train_wall": "16636"}
[2024-10-11 21:20:22,073][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 21:20:22,081][fairseq.trainer][INFO] - begin training epoch 2062
[2024-10-11 21:20:22,082][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 21:22:32,787][fairseq_cli.train][INFO] - end of epoch 2062 (average epoch stats below)
[2024-10-11 21:22:32,791][train][INFO] - {"epoch": 2062, "train_loss": "0.409", "train_ntokens": "260307", "train_nsentences": "1750.04", "train_wps": "95511.7", "train_ups": "0.37", "train_wpb": "260307", "train_bsz": "1750", "train_num_updates": "98934", "train_lr": "0.000409057", "train_gnorm": "0.355", "train_loss_scale": "4", "train_train_wall": "50", "train_gb_free": "39.8", "train_wall": "16767"}
[2024-10-11 21:22:32,868][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 21:22:32,874][fairseq.trainer][INFO] - begin training epoch 2063
[2024-10-11 21:22:32,874][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 21:24:42,922][fairseq_cli.train][INFO] - end of epoch 2063 (average epoch stats below)
[2024-10-11 21:24:42,929][train][INFO] - {"epoch": 2063, "train_loss": "0.404", "train_ntokens": "260845", "train_nsentences": "1750.04", "train_wps": "96212", "train_ups": "0.37", "train_wpb": "260845", "train_bsz": "1750", "train_num_updates": "98982", "train_lr": "0.000408992", "train_gnorm": "0.369", "train_loss_scale": "4", "train_train_wall": "64", "train_gb_free": "39.6", "train_wall": "16897"}
[2024-10-11 21:24:43,041][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 21:24:43,048][fairseq.trainer][INFO] - begin training epoch 2064
[2024-10-11 21:24:43,049][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 21:26:28,985][train_inner][INFO] - {"epoch": 2064, "update": 2063.375, "loss": "0.403", "ntokens": "260798", "nsentences": "1745.1", "wps": "99648.6", "ups": "0.38", "wpb": "260798", "bsz": "1745.1", "num_updates": "99000", "lr": "0.000408967", "gnorm": "0.365", "loss_scale": "4", "train_wall": "224", "gb_free": "39.7", "wall": "17003"}
[2024-10-11 21:26:53,101][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2024-10-11 21:26:53,365][fairseq_cli.train][INFO] - end of epoch 2064 (average epoch stats below)
[2024-10-11 21:26:53,368][train][INFO] - {"epoch": 2064, "train_loss": "0.402", "train_ntokens": "260893", "train_nsentences": "1738.06", "train_wps": "94008.9", "train_ups": "0.36", "train_wpb": "260893", "train_bsz": "1738.1", "train_num_updates": "99029", "train_lr": "0.000408928", "train_gnorm": "0.38", "train_loss_scale": "2", "train_train_wall": "55", "train_gb_free": "39.3", "train_wall": "17027"}
[2024-10-11 21:26:53,438][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 21:26:53,444][fairseq.trainer][INFO] - begin training epoch 2065
[2024-10-11 21:26:53,444][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 21:29:05,957][fairseq_cli.train][INFO] - end of epoch 2065 (average epoch stats below)
[2024-10-11 21:29:05,978][train][INFO] - {"epoch": 2065, "train_loss": "0.415", "train_ntokens": "260857", "train_nsentences": "1750.04", "train_wps": "94423.6", "train_ups": "0.36", "train_wpb": "260857", "train_bsz": "1750", "train_num_updates": "99077", "train_lr": "0.000408863", "train_gnorm": "0.377", "train_loss_scale": "2", "train_train_wall": "50", "train_gb_free": "41", "train_wall": "17160"}
[2024-10-11 21:29:06,070][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 21:29:06,084][fairseq.trainer][INFO] - begin training epoch 2066
[2024-10-11 21:29:06,085][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 21:31:12,657][fairseq_cli.train][INFO] - end of epoch 2066 (average epoch stats below)
[2024-10-11 21:31:12,660][train][INFO] - {"epoch": 2066, "train_loss": "0.398", "train_ntokens": "260849", "train_nsentences": "1750.04", "train_wps": "98838.8", "train_ups": "0.38", "train_wpb": "260849", "train_bsz": "1750", "train_num_updates": "99125", "train_lr": "0.000408798", "train_gnorm": "0.344", "train_loss_scale": "2", "train_train_wall": "44", "train_gb_free": "39.6", "train_wall": "17287"}
[2024-10-11 21:31:12,711][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 21:31:12,715][fairseq.trainer][INFO] - begin training epoch 2067
[2024-10-11 21:31:12,715][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 21:33:22,446][fairseq_cli.train][INFO] - end of epoch 2067 (average epoch stats below)
[2024-10-11 21:33:22,449][train][INFO] - {"epoch": 2067, "train_loss": "0.4", "train_ntokens": "260609", "train_nsentences": "1750.04", "train_wps": "96383.5", "train_ups": "0.37", "train_wpb": "260610", "train_bsz": "1750", "train_num_updates": "99173", "train_lr": "0.000408732", "train_gnorm": "0.345", "train_loss_scale": "2", "train_train_wall": "65", "train_gb_free": "39.6", "train_wall": "17416"}
[2024-10-11 21:33:22,506][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 21:33:22,510][fairseq.trainer][INFO] - begin training epoch 2068
[2024-10-11 21:33:22,511][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 21:35:13,262][train_inner][INFO] - {"epoch": 2068, "update": 2067.562, "loss": "0.403", "ntokens": "260875", "nsentences": "1745.48", "wps": "99518.9", "ups": "0.38", "wpb": "260875", "bsz": "1745.5", "num_updates": "99200", "lr": "0.000408696", "gnorm": "0.36", "loss_scale": "2", "train_wall": "227", "gb_free": "39.7", "wall": "17527"}
[2024-10-11 21:35:29,899][fairseq_cli.train][INFO] - end of epoch 2068 (average epoch stats below)
[2024-10-11 21:35:29,902][train][INFO] - {"epoch": 2068, "train_loss": "0.402", "train_ntokens": "261008", "train_nsentences": "1750.04", "train_wps": "98301.2", "train_ups": "0.38", "train_wpb": "261008", "train_bsz": "1750", "train_num_updates": "99221", "train_lr": "0.000408667", "train_gnorm": "0.373", "train_loss_scale": "2", "train_train_wall": "60", "train_gb_free": "40", "train_wall": "17544"}
[2024-10-11 21:35:29,997][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 21:35:30,002][fairseq.trainer][INFO] - begin training epoch 2069
[2024-10-11 21:35:30,003][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 21:37:36,230][fairseq_cli.train][INFO] - end of epoch 2069 (average epoch stats below)
[2024-10-11 21:37:36,234][train][INFO] - {"epoch": 2069, "train_loss": "0.413", "train_ntokens": "260394", "train_nsentences": "1750.04", "train_wps": "98940.1", "train_ups": "0.38", "train_wpb": "260394", "train_bsz": "1750", "train_num_updates": "99269", "train_lr": "0.000408602", "train_gnorm": "0.341", "train_loss_scale": "2", "train_train_wall": "54", "train_gb_free": "39.3", "train_wall": "17670"}
[2024-10-11 21:37:36,316][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 21:37:36,326][fairseq.trainer][INFO] - begin training epoch 2070
[2024-10-11 21:37:36,327][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 21:39:43,742][fairseq_cli.train][INFO] - end of epoch 2070 (average epoch stats below)
[2024-10-11 21:39:43,748][train][INFO] - {"epoch": 2070, "train_loss": "0.404", "train_ntokens": "260184", "train_nsentences": "1750.04", "train_wps": "97944.5", "train_ups": "0.38", "train_wpb": "260184", "train_bsz": "1750", "train_num_updates": "99317", "train_lr": "0.000408537", "train_gnorm": "0.376", "train_loss_scale": "2", "train_train_wall": "51", "train_gb_free": "39.6", "train_wall": "17798"}
[2024-10-11 21:39:43,807][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 21:39:43,811][fairseq.trainer][INFO] - begin training epoch 2071
[2024-10-11 21:39:43,811][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 21:41:51,412][fairseq_cli.train][INFO] - end of epoch 2071 (average epoch stats below)
[2024-10-11 21:41:51,420][train][INFO] - {"epoch": 2071, "train_loss": "0.407", "train_ntokens": "260615", "train_nsentences": "1750.04", "train_wps": "97984.4", "train_ups": "0.38", "train_wpb": "260615", "train_bsz": "1750", "train_num_updates": "99365", "train_lr": "0.000408471", "train_gnorm": "0.344", "train_loss_scale": "2", "train_train_wall": "48", "train_gb_free": "39.2", "train_wall": "17925"}
[2024-10-11 21:41:51,530][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 21:41:51,541][fairseq.trainer][INFO] - begin training epoch 2072
[2024-10-11 21:41:51,541][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 21:43:52,516][train_inner][INFO] - {"epoch": 2072, "update": 2071.729, "loss": "0.407", "ntokens": "260573", "nsentences": "1755.74", "wps": "100366", "ups": "0.39", "wpb": "260573", "bsz": "1755.7", "num_updates": "99400", "lr": "0.000408424", "gnorm": "0.355", "loss_scale": "2", "train_wall": "213", "gb_free": "39.7", "wall": "18047"}
[2024-10-11 21:44:04,889][fairseq_cli.train][INFO] - end of epoch 2072 (average epoch stats below)
[2024-10-11 21:44:04,902][train][INFO] - {"epoch": 2072, "train_loss": "0.402", "train_ntokens": "260757", "train_nsentences": "1750.04", "train_wps": "93778.2", "train_ups": "0.36", "train_wpb": "260757", "train_bsz": "1750", "train_num_updates": "99413", "train_lr": "0.000408406", "train_gnorm": "0.363", "train_loss_scale": "2", "train_train_wall": "56", "train_gb_free": "39.7", "train_wall": "18059"}
[2024-10-11 21:44:04,975][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 21:44:04,979][fairseq.trainer][INFO] - begin training epoch 2073
[2024-10-11 21:44:04,980][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 21:46:20,456][fairseq_cli.train][INFO] - end of epoch 2073 (average epoch stats below)
[2024-10-11 21:46:20,465][train][INFO] - {"epoch": 2073, "train_loss": "0.395", "train_ntokens": "260587", "train_nsentences": "1750.04", "train_wps": "92271.8", "train_ups": "0.35", "train_wpb": "260587", "train_bsz": "1750", "train_num_updates": "99461", "train_lr": "0.000408341", "train_gnorm": "0.35", "train_loss_scale": "2", "train_train_wall": "63", "train_gb_free": "39.6", "train_wall": "18195"}
[2024-10-11 21:46:20,600][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 21:46:20,603][fairseq.trainer][INFO] - begin training epoch 2074
[2024-10-11 21:46:20,604][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 21:48:28,503][fairseq_cli.train][INFO] - end of epoch 2074 (average epoch stats below)
[2024-10-11 21:48:28,517][train][INFO] - {"epoch": 2074, "train_loss": "0.399", "train_ntokens": "260929", "train_nsentences": "1750.04", "train_wps": "97816.4", "train_ups": "0.37", "train_wpb": "260929", "train_bsz": "1750", "train_num_updates": "99509", "train_lr": "0.000408276", "train_gnorm": "0.343", "train_loss_scale": "2", "train_train_wall": "51", "train_gb_free": "39.6", "train_wall": "18323"}
[2024-10-11 21:48:28,640][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 21:48:28,653][fairseq.trainer][INFO] - begin training epoch 2075
[2024-10-11 21:48:28,653][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 21:50:34,977][fairseq_cli.train][INFO] - end of epoch 2075 (average epoch stats below)
[2024-10-11 21:50:34,981][train][INFO] - {"epoch": 2075, "train_loss": "0.399", "train_ntokens": "261108", "train_nsentences": "1750.04", "train_wps": "99112.6", "train_ups": "0.38", "train_wpb": "261108", "train_bsz": "1750", "train_num_updates": "99557", "train_lr": "0.000408211", "train_gnorm": "0.362", "train_loss_scale": "2", "train_train_wall": "41", "train_gb_free": "39.3", "train_wall": "18449"}
[2024-10-11 21:50:35,100][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 21:50:35,114][fairseq.trainer][INFO] - begin training epoch 2076
[2024-10-11 21:50:35,115][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 21:52:49,219][train_inner][INFO] - {"epoch": 2076, "update": 2075.896, "loss": "0.396", "ntokens": "260676", "nsentences": "1750.86", "wps": "97143.3", "ups": "0.37", "wpb": "260676", "bsz": "1750.9", "num_updates": "99600", "lr": "0.000408152", "gnorm": "0.354", "loss_scale": "2", "train_wall": "203", "gb_free": "40", "wall": "18583"}
[2024-10-11 21:52:50,508][fairseq_cli.train][INFO] - end of epoch 2076 (average epoch stats below)
[2024-10-11 21:52:50,511][train][INFO] - {"epoch": 2076, "train_loss": "0.392", "train_ntokens": "260748", "train_nsentences": "1750.04", "train_wps": "92350.7", "train_ups": "0.35", "train_wpb": "260748", "train_bsz": "1750", "train_num_updates": "99605", "train_lr": "0.000408145", "train_gnorm": "0.351", "train_loss_scale": "2", "train_train_wall": "38", "train_gb_free": "40.8", "train_wall": "18585"}
[2024-10-11 21:52:50,623][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 21:52:50,627][fairseq.trainer][INFO] - begin training epoch 2077
[2024-10-11 21:52:50,628][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 21:55:00,920][fairseq_cli.train][INFO] - end of epoch 2077 (average epoch stats below)
[2024-10-11 21:55:00,924][train][INFO] - {"epoch": 2077, "train_loss": "0.4", "train_ntokens": "260706", "train_nsentences": "1750.04", "train_wps": "95958.5", "train_ups": "0.37", "train_wpb": "260706", "train_bsz": "1750", "train_num_updates": "99653", "train_lr": "0.00040808", "train_gnorm": "0.353", "train_loss_scale": "2", "train_train_wall": "50", "train_gb_free": "40.1", "train_wall": "18715"}
[2024-10-11 21:55:01,034][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 21:55:01,044][fairseq.trainer][INFO] - begin training epoch 2078
[2024-10-11 21:55:01,045][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 21:57:14,481][fairseq_cli.train][INFO] - end of epoch 2078 (average epoch stats below)
[2024-10-11 21:57:14,484][train][INFO] - {"epoch": 2078, "train_loss": "0.397", "train_ntokens": "260915", "train_nsentences": "1750.04", "train_wps": "93772.7", "train_ups": "0.36", "train_wpb": "260915", "train_bsz": "1750", "train_num_updates": "99701", "train_lr": "0.000408015", "train_gnorm": "0.371", "train_loss_scale": "2", "train_train_wall": "59", "train_gb_free": "39.8", "train_wall": "18849"}
[2024-10-11 21:57:14,570][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 21:57:14,608][fairseq.trainer][INFO] - begin training epoch 2079
[2024-10-11 21:57:14,609][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 21:59:21,790][fairseq_cli.train][INFO] - end of epoch 2079 (average epoch stats below)
[2024-10-11 21:59:21,805][train][INFO] - {"epoch": 2079, "train_loss": "0.401", "train_ntokens": "261032", "train_nsentences": "1750.04", "train_wps": "98415.3", "train_ups": "0.38", "train_wpb": "261032", "train_bsz": "1750", "train_num_updates": "99749", "train_lr": "0.00040795", "train_gnorm": "0.352", "train_loss_scale": "2", "train_train_wall": "66", "train_gb_free": "40.1", "train_wall": "18976"}
[2024-10-11 21:59:21,912][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 21:59:21,925][fairseq.trainer][INFO] - begin training epoch 2080
[2024-10-11 21:59:21,925][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 22:01:33,401][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 2080 @ 99797 updates
[2024-10-11 22:01:33,403][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-11 22:01:37,364][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-11 22:01:37,367][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 2080 @ 99797 updates, score None) (writing took 3.9660823801532388 seconds)
[2024-10-11 22:01:37,368][fairseq_cli.train][INFO] - end of epoch 2080 (average epoch stats below)
[2024-10-11 22:01:37,371][train][INFO] - {"epoch": 2080, "train_loss": "0.406", "train_ntokens": "260435", "train_nsentences": "1750.04", "train_wps": "92216.1", "train_ups": "0.35", "train_wpb": "260435", "train_bsz": "1750", "train_num_updates": "99797", "train_lr": "0.000407885", "train_gnorm": "0.352", "train_loss_scale": "2", "train_train_wall": "29", "train_gb_free": "39.8", "train_wall": "19111"}
[2024-10-11 22:01:37,431][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 22:01:37,470][fairseq.trainer][INFO] - begin training epoch 2081
[2024-10-11 22:01:37,471][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 22:03:09,856][train_inner][INFO] - {"epoch": 2081, "update": 2080.062, "loss": "0.4", "ntokens": "260738", "nsentences": "1750.52", "wps": "84023.6", "ups": "0.32", "wpb": "260738", "bsz": "1750.5", "num_updates": "99800", "lr": "0.00040788", "gnorm": "0.357", "loss_scale": "2", "train_wall": "219", "gb_free": "39.6", "wall": "19204"}
[2024-10-11 22:03:44,132][fairseq_cli.train][INFO] - end of epoch 2081 (average epoch stats below)
[2024-10-11 22:03:44,136][train][INFO] - {"epoch": 2081, "train_loss": "0.397", "train_ntokens": "260669", "train_nsentences": "1750.04", "train_wps": "98706.6", "train_ups": "0.38", "train_wpb": "260669", "train_bsz": "1750", "train_num_updates": "99845", "train_lr": "0.000407819", "train_gnorm": "0.368", "train_loss_scale": "2", "train_train_wall": "47", "train_gb_free": "40.1", "train_wall": "19238"}
[2024-10-11 22:03:44,232][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 22:03:44,385][fairseq.trainer][INFO] - begin training epoch 2082
[2024-10-11 22:03:44,386][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 22:05:54,210][fairseq_cli.train][INFO] - end of epoch 2082 (average epoch stats below)
[2024-10-11 22:05:54,217][train][INFO] - {"epoch": 2082, "train_loss": "0.4", "train_ntokens": "260722", "train_nsentences": "1750.04", "train_wps": "96211.7", "train_ups": "0.37", "train_wpb": "260722", "train_bsz": "1750", "train_num_updates": "99893", "train_lr": "0.000407754", "train_gnorm": "0.359", "train_loss_scale": "2", "train_train_wall": "43", "train_gb_free": "40.1", "train_wall": "19368"}
[2024-10-11 22:05:54,288][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 22:05:54,297][fairseq.trainer][INFO] - begin training epoch 2083
[2024-10-11 22:05:54,298][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 22:08:01,444][fairseq_cli.train][INFO] - end of epoch 2083 (average epoch stats below)
[2024-10-11 22:08:01,459][train][INFO] - {"epoch": 2083, "train_loss": "0.401", "train_ntokens": "260276", "train_nsentences": "1750.04", "train_wps": "98194.4", "train_ups": "0.38", "train_wpb": "260276", "train_bsz": "1750", "train_num_updates": "99941", "train_lr": "0.000407689", "train_gnorm": "0.367", "train_loss_scale": "2", "train_train_wall": "45", "train_gb_free": "39.8", "train_wall": "19495"}
[2024-10-11 22:08:01,578][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 22:08:01,593][fairseq.trainer][INFO] - begin training epoch 2084
[2024-10-11 22:08:01,593][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 22:10:13,599][fairseq_cli.train][INFO] - end of epoch 2084 (average epoch stats below)
[2024-10-11 22:10:13,602][train][INFO] - {"epoch": 2084, "train_loss": "0.399", "train_ntokens": "260985", "train_nsentences": "1750.04", "train_wps": "94804.9", "train_ups": "0.36", "train_wpb": "260985", "train_bsz": "1750", "train_num_updates": "99989", "train_lr": "0.000407624", "train_gnorm": "0.392", "train_loss_scale": "2", "train_train_wall": "60", "train_gb_free": "40.3", "train_wall": "19628"}
[2024-10-11 22:10:13,657][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 22:10:13,663][fairseq.trainer][INFO] - begin training epoch 2085
[2024-10-11 22:10:13,663][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 22:11:49,452][train_inner][INFO] - {"epoch": 2085, "update": 2084.229, "loss": "0.399", "ntokens": "260638", "nsentences": "1750.46", "wps": "100324", "ups": "0.38", "wpb": "260638", "bsz": "1750.5", "num_updates": "100000", "lr": "0.000407609", "gnorm": "0.374", "loss_scale": "2", "train_wall": "202", "gb_free": "39.6", "wall": "19723"}
[2024-10-11 22:11:49,463][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 2085 @ 100000 updates
[2024-10-11 22:11:49,464][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_2085_100000.pt
[2024-10-11 22:11:52,498][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_2085_100000.pt
[2024-10-11 22:11:58,660][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_2085_100000.pt (epoch 2085 @ 100000 updates, score None) (writing took 9.196482417173684 seconds)
[2024-10-11 22:12:20,856][fairseq_cli.train][INFO] - end of epoch 2085 (average epoch stats below)
[2024-10-11 22:12:20,860][train][INFO] - {"epoch": 2085, "train_loss": "0.398", "train_ntokens": "260755", "train_nsentences": "1750.04", "train_wps": "98355.8", "train_ups": "0.38", "train_wpb": "260755", "train_bsz": "1750", "train_num_updates": "100037", "train_lr": "0.000407558", "train_gnorm": "0.365", "train_loss_scale": "2", "train_train_wall": "43", "train_gb_free": "40.7", "train_wall": "19755"}
[2024-10-11 22:12:20,950][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 22:12:20,961][fairseq.trainer][INFO] - begin training epoch 2086
[2024-10-11 22:12:20,962][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 22:14:28,932][fairseq_cli.train][INFO] - end of epoch 2086 (average epoch stats below)
[2024-10-11 22:14:28,947][train][INFO] - {"epoch": 2086, "train_loss": "0.403", "train_ntokens": "260648", "train_nsentences": "1750.04", "train_wps": "97682.2", "train_ups": "0.37", "train_wpb": "260648", "train_bsz": "1750", "train_num_updates": "100085", "train_lr": "0.000407493", "train_gnorm": "0.379", "train_loss_scale": "2", "train_train_wall": "47", "train_gb_free": "39.7", "train_wall": "19883"}
[2024-10-11 22:14:29,040][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 22:14:29,044][fairseq.trainer][INFO] - begin training epoch 2087
[2024-10-11 22:14:29,059][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 22:16:38,919][fairseq_cli.train][INFO] - end of epoch 2087 (average epoch stats below)
[2024-10-11 22:16:38,923][train][INFO] - {"epoch": 2087, "train_loss": "0.397", "train_ntokens": "260868", "train_nsentences": "1750.04", "train_wps": "96341.8", "train_ups": "0.37", "train_wpb": "260868", "train_bsz": "1750", "train_num_updates": "100133", "train_lr": "0.000407428", "train_gnorm": "0.358", "train_loss_scale": "2", "train_train_wall": "55", "train_gb_free": "40", "train_wall": "20013"}
[2024-10-11 22:16:38,978][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 22:16:38,997][fairseq.trainer][INFO] - begin training epoch 2088
[2024-10-11 22:16:38,997][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 22:18:46,396][fairseq_cli.train][INFO] - end of epoch 2088 (average epoch stats below)
[2024-10-11 22:18:46,403][train][INFO] - {"epoch": 2088, "train_loss": "0.396", "train_ntokens": "260917", "train_nsentences": "1750.04", "train_wps": "98246.6", "train_ups": "0.38", "train_wpb": "260917", "train_bsz": "1750", "train_num_updates": "100181", "train_lr": "0.000407363", "train_gnorm": "0.352", "train_loss_scale": "2", "train_train_wall": "57", "train_gb_free": "40.3", "train_wall": "20140"}
[2024-10-11 22:18:46,508][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 22:18:46,524][fairseq.trainer][INFO] - begin training epoch 2089
[2024-10-11 22:18:46,524][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 22:20:28,257][train_inner][INFO] - {"epoch": 2089, "update": 2088.396, "loss": "0.399", "ntokens": "261009", "nsentences": "1738.93", "wps": "100621", "ups": "0.39", "wpb": "261009", "bsz": "1738.9", "num_updates": "100200", "lr": "0.000407337", "gnorm": "0.363", "loss_scale": "2", "train_wall": "198", "gb_free": "39.8", "wall": "20242"}
[2024-10-11 22:20:54,815][fairseq_cli.train][INFO] - end of epoch 2089 (average epoch stats below)
[2024-10-11 22:20:54,820][train][INFO] - {"epoch": 2089, "train_loss": "0.405", "train_ntokens": "260617", "train_nsentences": "1750.04", "train_wps": "97419.8", "train_ups": "0.37", "train_wpb": "260617", "train_bsz": "1750", "train_num_updates": "100229", "train_lr": "0.000407298", "train_gnorm": "0.366", "train_loss_scale": "2", "train_train_wall": "43", "train_gb_free": "39.6", "train_wall": "20269"}
[2024-10-11 22:20:54,888][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 22:20:54,891][fairseq.trainer][INFO] - begin training epoch 2090
[2024-10-11 22:20:54,892][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 22:23:02,181][fairseq_cli.train][INFO] - end of epoch 2090 (average epoch stats below)
[2024-10-11 22:23:02,184][train][INFO] - {"epoch": 2090, "train_loss": "0.399", "train_ntokens": "260449", "train_nsentences": "1750.04", "train_wps": "98158.5", "train_ups": "0.38", "train_wpb": "260450", "train_bsz": "1750", "train_num_updates": "100277", "train_lr": "0.000407232", "train_gnorm": "0.352", "train_loss_scale": "2", "train_train_wall": "58", "train_gb_free": "39.3", "train_wall": "20396"}
[2024-10-11 22:23:02,261][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 22:23:02,266][fairseq.trainer][INFO] - begin training epoch 2091
[2024-10-11 22:23:02,267][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 22:25:07,986][fairseq_cli.train][INFO] - end of epoch 2091 (average epoch stats below)
[2024-10-11 22:25:07,991][train][INFO] - {"epoch": 2091, "train_loss": "0.394", "train_ntokens": "260695", "train_nsentences": "1750.04", "train_wps": "99470.2", "train_ups": "0.38", "train_wpb": "260695", "train_bsz": "1750", "train_num_updates": "100325", "train_lr": "0.000407167", "train_gnorm": "0.358", "train_loss_scale": "2", "train_train_wall": "51", "train_gb_free": "39.5", "train_wall": "20522"}
[2024-10-11 22:25:08,113][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 22:25:08,116][fairseq.trainer][INFO] - begin training epoch 2092
[2024-10-11 22:25:08,117][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 22:27:16,751][fairseq_cli.train][INFO] - end of epoch 2092 (average epoch stats below)
[2024-10-11 22:27:16,754][train][INFO] - {"epoch": 2092, "train_loss": "0.397", "train_ntokens": "261060", "train_nsentences": "1750.04", "train_wps": "97320.3", "train_ups": "0.37", "train_wpb": "261060", "train_bsz": "1750", "train_num_updates": "100373", "train_lr": "0.000407102", "train_gnorm": "0.367", "train_loss_scale": "2", "train_train_wall": "52", "train_gb_free": "39.6", "train_wall": "20651"}
[2024-10-11 22:27:16,803][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 22:27:16,809][fairseq.trainer][INFO] - begin training epoch 2093
[2024-10-11 22:27:16,809][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 22:29:11,536][train_inner][INFO] - {"epoch": 2093, "update": 2092.562, "loss": "0.398", "ntokens": "260267", "nsentences": "1782.45", "wps": "99477", "ups": "0.38", "wpb": "260267", "bsz": "1782.5", "num_updates": "100400", "lr": "0.000407065", "gnorm": "0.356", "loss_scale": "2", "train_wall": "226", "gb_free": "40.3", "wall": "20766"}
[2024-10-11 22:29:28,457][fairseq_cli.train][INFO] - end of epoch 2093 (average epoch stats below)
[2024-10-11 22:29:28,472][train][INFO] - {"epoch": 2093, "train_loss": "0.403", "train_ntokens": "260347", "train_nsentences": "1750.04", "train_wps": "94885.5", "train_ups": "0.36", "train_wpb": "260346", "train_bsz": "1750", "train_num_updates": "100421", "train_lr": "0.000407037", "train_gnorm": "0.366", "train_loss_scale": "2", "train_train_wall": "56", "train_gb_free": "39.6", "train_wall": "20783"}
[2024-10-11 22:29:28,633][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 22:29:28,637][fairseq.trainer][INFO] - begin training epoch 2094
[2024-10-11 22:29:28,638][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 22:31:37,305][fairseq_cli.train][INFO] - end of epoch 2094 (average epoch stats below)
[2024-10-11 22:31:37,311][train][INFO] - {"epoch": 2094, "train_loss": "0.39", "train_ntokens": "261008", "train_nsentences": "1750.04", "train_wps": "97245.8", "train_ups": "0.37", "train_wpb": "261008", "train_bsz": "1750", "train_num_updates": "100469", "train_lr": "0.000406971", "train_gnorm": "0.351", "train_loss_scale": "2", "train_train_wall": "66", "train_gb_free": "39.6", "train_wall": "20911"}
[2024-10-11 22:31:37,395][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 22:31:37,412][fairseq.trainer][INFO] - begin training epoch 2095
[2024-10-11 22:31:37,413][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 22:33:48,272][fairseq_cli.train][INFO] - end of epoch 2095 (average epoch stats below)
[2024-10-11 22:33:48,276][train][INFO] - {"epoch": 2095, "train_loss": "0.395", "train_ntokens": "260907", "train_nsentences": "1750.04", "train_wps": "95629.3", "train_ups": "0.37", "train_wpb": "260907", "train_bsz": "1750", "train_num_updates": "100517", "train_lr": "0.000406906", "train_gnorm": "0.387", "train_loss_scale": "2", "train_train_wall": "52", "train_gb_free": "45.1", "train_wall": "21042"}
[2024-10-11 22:33:48,361][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 22:33:48,370][fairseq.trainer][INFO] - begin training epoch 2096
[2024-10-11 22:33:48,371][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 22:35:57,402][fairseq_cli.train][INFO] - end of epoch 2096 (average epoch stats below)
[2024-10-11 22:35:57,405][train][INFO] - {"epoch": 2096, "train_loss": "0.403", "train_ntokens": "260845", "train_nsentences": "1750.04", "train_wps": "96964.4", "train_ups": "0.37", "train_wpb": "260845", "train_bsz": "1750", "train_num_updates": "100565", "train_lr": "0.000406841", "train_gnorm": "0.352", "train_loss_scale": "2", "train_train_wall": "46", "train_gb_free": "39.6", "train_wall": "21171"}
[2024-10-11 22:35:57,490][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 22:35:57,506][fairseq.trainer][INFO] - begin training epoch 2097
[2024-10-11 22:35:57,506][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 22:37:55,666][train_inner][INFO] - {"epoch": 2097, "update": 2096.729, "loss": "0.398", "ntokens": "260973", "nsentences": "1732.26", "wps": "99584.1", "ups": "0.38", "wpb": "260973", "bsz": "1732.3", "num_updates": "100600", "lr": "0.000406793", "gnorm": "0.362", "loss_scale": "2", "train_wall": "230", "gb_free": "40.3", "wall": "21290"}
[2024-10-11 22:38:08,180][fairseq_cli.train][INFO] - end of epoch 2097 (average epoch stats below)
[2024-10-11 22:38:08,182][train][INFO] - {"epoch": 2097, "train_loss": "0.402", "train_ntokens": "260613", "train_nsentences": "1750.04", "train_wps": "95656.9", "train_ups": "0.37", "train_wpb": "260613", "train_bsz": "1750", "train_num_updates": "100613", "train_lr": "0.000406776", "train_gnorm": "0.35", "train_loss_scale": "2", "train_train_wall": "61", "train_gb_free": "39.3", "train_wall": "21302"}
[2024-10-11 22:38:08,238][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 22:38:08,243][fairseq.trainer][INFO] - begin training epoch 2098
[2024-10-11 22:38:08,244][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 22:40:16,230][fairseq_cli.train][INFO] - end of epoch 2098 (average epoch stats below)
[2024-10-11 22:40:16,236][train][INFO] - {"epoch": 2098, "train_loss": "0.4", "train_ntokens": "260400", "train_nsentences": "1750.04", "train_wps": "97613.5", "train_ups": "0.37", "train_wpb": "260400", "train_bsz": "1750", "train_num_updates": "100661", "train_lr": "0.000406711", "train_gnorm": "0.364", "train_loss_scale": "2", "train_train_wall": "51", "train_gb_free": "39.3", "train_wall": "21430"}
[2024-10-11 22:40:16,354][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 22:40:16,357][fairseq.trainer][INFO] - begin training epoch 2099
[2024-10-11 22:40:16,358][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 22:42:28,374][fairseq_cli.train][INFO] - end of epoch 2099 (average epoch stats below)
[2024-10-11 22:42:28,379][train][INFO] - {"epoch": 2099, "train_loss": "0.395", "train_ntokens": "260582", "train_nsentences": "1750.04", "train_wps": "94657.1", "train_ups": "0.36", "train_wpb": "260582", "train_bsz": "1750", "train_num_updates": "100709", "train_lr": "0.000406645", "train_gnorm": "0.349", "train_loss_scale": "2", "train_train_wall": "61", "train_gb_free": "40", "train_wall": "21562"}
[2024-10-11 22:42:28,479][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 22:42:28,491][fairseq.trainer][INFO] - begin training epoch 2100
[2024-10-11 22:42:28,492][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 22:44:35,896][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 2100 @ 100757 updates
[2024-10-11 22:44:35,897][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-11 22:44:39,698][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-11 22:44:39,701][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 2100 @ 100757 updates, score None) (writing took 3.8041560612618923 seconds)
[2024-10-11 22:44:39,701][fairseq_cli.train][INFO] - end of epoch 2100 (average epoch stats below)
[2024-10-11 22:44:39,704][train][INFO] - {"epoch": 2100, "train_loss": "0.405", "train_ntokens": "260249", "train_nsentences": "1750.04", "train_wps": "95126.8", "train_ups": "0.37", "train_wpb": "260249", "train_bsz": "1750", "train_num_updates": "100757", "train_lr": "0.00040658", "train_gnorm": "0.37", "train_loss_scale": "2", "train_train_wall": "42", "train_gb_free": "39.7", "train_wall": "21694"}
[2024-10-11 22:44:39,779][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 22:44:39,783][fairseq.trainer][INFO] - begin training epoch 2101
[2024-10-11 22:44:39,784][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 22:46:43,491][train_inner][INFO] - {"epoch": 2101, "update": 2100.896, "loss": "0.402", "ntokens": "260469", "nsentences": "1747.4", "wps": "98700.4", "ups": "0.38", "wpb": "260469", "bsz": "1747.4", "num_updates": "100800", "lr": "0.000406522", "gnorm": "0.359", "loss_scale": "2", "train_wall": "208", "gb_free": "39.2", "wall": "21818"}
[2024-10-11 22:46:44,742][fairseq_cli.train][INFO] - end of epoch 2101 (average epoch stats below)
[2024-10-11 22:46:44,744][train][INFO] - {"epoch": 2101, "train_loss": "0.407", "train_ntokens": "260621", "train_nsentences": "1750.04", "train_wps": "100050", "train_ups": "0.38", "train_wpb": "260621", "train_bsz": "1750", "train_num_updates": "100805", "train_lr": "0.000406515", "train_gnorm": "0.347", "train_loss_scale": "2", "train_train_wall": "43", "train_gb_free": "39.8", "train_wall": "21819"}
[2024-10-11 22:46:44,804][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 22:46:44,807][fairseq.trainer][INFO] - begin training epoch 2102
[2024-10-11 22:46:44,808][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 22:48:54,110][fairseq_cli.train][INFO] - end of epoch 2102 (average epoch stats below)
[2024-10-11 22:48:54,126][train][INFO] - {"epoch": 2102, "train_loss": "0.399", "train_ntokens": "260355", "train_nsentences": "1750.04", "train_wps": "96592.5", "train_ups": "0.37", "train_wpb": "260355", "train_bsz": "1750", "train_num_updates": "100853", "train_lr": "0.00040645", "train_gnorm": "0.351", "train_loss_scale": "2", "train_train_wall": "51", "train_gb_free": "40.2", "train_wall": "21948"}
[2024-10-11 22:48:54,211][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 22:48:54,221][fairseq.trainer][INFO] - begin training epoch 2103
[2024-10-11 22:48:54,222][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 22:51:02,539][fairseq_cli.train][INFO] - end of epoch 2103 (average epoch stats below)
[2024-10-11 22:51:02,548][train][INFO] - {"epoch": 2103, "train_loss": "0.405", "train_ntokens": "261212", "train_nsentences": "1750.04", "train_wps": "97636.8", "train_ups": "0.37", "train_wpb": "261212", "train_bsz": "1750", "train_num_updates": "100901", "train_lr": "0.000406385", "train_gnorm": "0.361", "train_loss_scale": "2", "train_train_wall": "53", "train_gb_free": "39.3", "train_wall": "22077"}
[2024-10-11 22:51:02,620][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 22:51:02,644][fairseq.trainer][INFO] - begin training epoch 2104
[2024-10-11 22:51:02,645][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 22:53:13,679][fairseq_cli.train][INFO] - end of epoch 2104 (average epoch stats below)
[2024-10-11 22:53:13,689][train][INFO] - {"epoch": 2104, "train_loss": "0.387", "train_ntokens": "260383", "train_nsentences": "1750.04", "train_wps": "95310.3", "train_ups": "0.37", "train_wpb": "260383", "train_bsz": "1750", "train_num_updates": "100949", "train_lr": "0.000406319", "train_gnorm": "0.356", "train_loss_scale": "2", "train_train_wall": "47", "train_gb_free": "39.6", "train_wall": "22208"}
[2024-10-11 22:53:13,765][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 22:53:13,778][fairseq.trainer][INFO] - begin training epoch 2105
[2024-10-11 22:53:13,778][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 22:55:22,445][fairseq_cli.train][INFO] - end of epoch 2105 (average epoch stats below)
[2024-10-11 22:55:22,452][train][INFO] - {"epoch": 2105, "train_loss": "0.398", "train_ntokens": "261096", "train_nsentences": "1750.04", "train_wps": "97335", "train_ups": "0.37", "train_wpb": "261096", "train_bsz": "1750", "train_num_updates": "100997", "train_lr": "0.000406254", "train_gnorm": "0.369", "train_loss_scale": "2", "train_train_wall": "57", "train_gb_free": "39.3", "train_wall": "22336"}
[2024-10-11 22:55:22,578][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 22:55:22,585][fairseq.trainer][INFO] - begin training epoch 2106
[2024-10-11 22:55:22,585][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 22:56:49,417][train_inner][INFO] - {"epoch": 2106, "update": 2105.062, "loss": "0.397", "ntokens": "260884", "nsentences": "1740.7", "wps": "86111.9", "ups": "0.33", "wpb": "260884", "bsz": "1740.7", "num_updates": "101000", "lr": "0.00040625", "gnorm": "0.359", "loss_scale": "2", "train_wall": "225", "gb_free": "39.6", "wall": "22423"}
[2024-10-11 22:57:34,226][fairseq_cli.train][INFO] - end of epoch 2106 (average epoch stats below)
[2024-10-11 22:57:34,229][train][INFO] - {"epoch": 2106, "train_loss": "0.4", "train_ntokens": "260886", "train_nsentences": "1750.04", "train_wps": "95031.4", "train_ups": "0.36", "train_wpb": "260886", "train_bsz": "1750", "train_num_updates": "101045", "train_lr": "0.000406189", "train_gnorm": "0.373", "train_loss_scale": "2", "train_train_wall": "50", "train_gb_free": "39.6", "train_wall": "22468"}
[2024-10-11 22:57:34,280][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 22:57:34,284][fairseq.trainer][INFO] - begin training epoch 2107
[2024-10-11 22:57:34,285][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 22:59:40,729][fairseq_cli.train][INFO] - end of epoch 2107 (average epoch stats below)
[2024-10-11 22:59:40,746][train][INFO] - {"epoch": 2107, "train_loss": "0.399", "train_ntokens": "260699", "train_nsentences": "1750.04", "train_wps": "98911.6", "train_ups": "0.38", "train_wpb": "260699", "train_bsz": "1750", "train_num_updates": "101093", "train_lr": "0.000406124", "train_gnorm": "0.355", "train_loss_scale": "4", "train_train_wall": "47", "train_gb_free": "39.7", "train_wall": "22595"}
[2024-10-11 22:59:40,870][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 22:59:40,889][fairseq.trainer][INFO] - begin training epoch 2108
[2024-10-11 22:59:40,889][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 23:01:53,135][fairseq_cli.train][INFO] - end of epoch 2108 (average epoch stats below)
[2024-10-11 23:01:53,138][train][INFO] - {"epoch": 2108, "train_loss": "0.393", "train_ntokens": "260531", "train_nsentences": "1750.04", "train_wps": "94461.9", "train_ups": "0.36", "train_wpb": "260531", "train_bsz": "1750", "train_num_updates": "101141", "train_lr": "0.000406058", "train_gnorm": "0.355", "train_loss_scale": "4", "train_train_wall": "49", "train_gb_free": "39.4", "train_wall": "22727"}
[2024-10-11 23:01:53,190][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 23:01:53,195][fairseq.trainer][INFO] - begin training epoch 2109
[2024-10-11 23:01:53,196][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 23:04:03,844][fairseq_cli.train][INFO] - end of epoch 2109 (average epoch stats below)
[2024-10-11 23:04:03,847][train][INFO] - {"epoch": 2109, "train_loss": "0.395", "train_ntokens": "261056", "train_nsentences": "1750.04", "train_wps": "95870.6", "train_ups": "0.37", "train_wpb": "261056", "train_bsz": "1750", "train_num_updates": "101189", "train_lr": "0.000405993", "train_gnorm": "0.361", "train_loss_scale": "4", "train_train_wall": "52", "train_gb_free": "39.6", "train_wall": "22858"}
[2024-10-11 23:04:03,903][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 23:04:03,907][fairseq.trainer][INFO] - begin training epoch 2110
[2024-10-11 23:04:03,907][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 23:05:37,589][train_inner][INFO] - {"epoch": 2110, "update": 2109.229, "loss": "0.397", "ntokens": "260801", "nsentences": "1757.81", "wps": "98757.3", "ups": "0.38", "wpb": "260801", "bsz": "1757.8", "num_updates": "101200", "lr": "0.000405978", "gnorm": "0.36", "loss_scale": "4", "train_wall": "192", "gb_free": "40", "wall": "22952"}
[2024-10-11 23:06:13,369][fairseq_cli.train][INFO] - end of epoch 2110 (average epoch stats below)
[2024-10-11 23:06:13,371][train][INFO] - {"epoch": 2110, "train_loss": "0.409", "train_ntokens": "260861", "train_nsentences": "1750.04", "train_wps": "96675.4", "train_ups": "0.37", "train_wpb": "260861", "train_bsz": "1750", "train_num_updates": "101237", "train_lr": "0.000405928", "train_gnorm": "0.344", "train_loss_scale": "4", "train_train_wall": "48", "train_gb_free": "39.3", "train_wall": "22987"}
[2024-10-11 23:06:13,423][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 23:06:13,428][fairseq.trainer][INFO] - begin training epoch 2111
[2024-10-11 23:06:13,429][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 23:08:21,536][fairseq_cli.train][INFO] - end of epoch 2111 (average epoch stats below)
[2024-10-11 23:08:21,552][train][INFO] - {"epoch": 2111, "train_loss": "0.396", "train_ntokens": "260908", "train_nsentences": "1750.04", "train_wps": "97706.5", "train_ups": "0.37", "train_wpb": "260908", "train_bsz": "1750", "train_num_updates": "101285", "train_lr": "0.000405863", "train_gnorm": "0.396", "train_loss_scale": "4", "train_train_wall": "55", "train_gb_free": "39.2", "train_wall": "23116"}
[2024-10-11 23:08:21,644][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 23:08:21,667][fairseq.trainer][INFO] - begin training epoch 2112
[2024-10-11 23:08:21,668][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 23:10:33,383][fairseq_cli.train][INFO] - end of epoch 2112 (average epoch stats below)
[2024-10-11 23:10:33,391][train][INFO] - {"epoch": 2112, "train_loss": "0.406", "train_ntokens": "260978", "train_nsentences": "1750.04", "train_wps": "95019.3", "train_ups": "0.36", "train_wpb": "260978", "train_bsz": "1750", "train_num_updates": "101333", "train_lr": "0.000405798", "train_gnorm": "0.391", "train_loss_scale": "4", "train_train_wall": "44", "train_gb_free": "39.3", "train_wall": "23247"}
[2024-10-11 23:10:33,481][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 23:10:33,497][fairseq.trainer][INFO] - begin training epoch 2113
[2024-10-11 23:10:33,497][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 23:12:40,520][fairseq_cli.train][INFO] - end of epoch 2113 (average epoch stats below)
[2024-10-11 23:12:40,524][train][INFO] - {"epoch": 2113, "train_loss": "0.398", "train_ntokens": "260690", "train_nsentences": "1750.04", "train_wps": "98429.9", "train_ups": "0.38", "train_wpb": "260690", "train_bsz": "1750", "train_num_updates": "101381", "train_lr": "0.000405732", "train_gnorm": "0.357", "train_loss_scale": "4", "train_train_wall": "46", "train_gb_free": "40.1", "train_wall": "23375"}
[2024-10-11 23:12:40,585][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 23:12:40,589][fairseq.trainer][INFO] - begin training epoch 2114
[2024-10-11 23:12:40,589][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 23:14:28,429][train_inner][INFO] - {"epoch": 2114, "update": 2113.396, "loss": "0.401", "ntokens": "261002", "nsentences": "1733.08", "wps": "98336.8", "ups": "0.38", "wpb": "261002", "bsz": "1733.1", "num_updates": "101400", "lr": "0.000405707", "gnorm": "0.371", "loss_scale": "4", "train_wall": "215", "gb_free": "39.4", "wall": "23482"}
[2024-10-11 23:14:53,566][fairseq_cli.train][INFO] - end of epoch 2114 (average epoch stats below)
[2024-10-11 23:14:53,569][train][INFO] - {"epoch": 2114, "train_loss": "0.397", "train_ntokens": "260721", "train_nsentences": "1750.04", "train_wps": "94066.1", "train_ups": "0.36", "train_wpb": "260721", "train_bsz": "1750", "train_num_updates": "101429", "train_lr": "0.000405667", "train_gnorm": "0.354", "train_loss_scale": "4", "train_train_wall": "60", "train_gb_free": "39.7", "train_wall": "23508"}
[2024-10-11 23:14:53,671][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 23:14:53,676][fairseq.trainer][INFO] - begin training epoch 2115
[2024-10-11 23:14:53,677][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 23:16:59,800][fairseq_cli.train][INFO] - end of epoch 2115 (average epoch stats below)
[2024-10-11 23:16:59,815][train][INFO] - {"epoch": 2115, "train_loss": "0.395", "train_ntokens": "260947", "train_nsentences": "1750.04", "train_wps": "99229", "train_ups": "0.38", "train_wpb": "260947", "train_bsz": "1750", "train_num_updates": "101477", "train_lr": "0.000405602", "train_gnorm": "0.361", "train_loss_scale": "4", "train_train_wall": "44", "train_gb_free": "39.7", "train_wall": "23634"}
[2024-10-11 23:16:59,936][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 23:16:59,948][fairseq.trainer][INFO] - begin training epoch 2116
[2024-10-11 23:16:59,948][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 23:19:08,651][fairseq_cli.train][INFO] - end of epoch 2116 (average epoch stats below)
[2024-10-11 23:19:08,662][train][INFO] - {"epoch": 2116, "train_loss": "0.407", "train_ntokens": "260569", "train_nsentences": "1750.04", "train_wps": "97076.1", "train_ups": "0.37", "train_wpb": "260569", "train_bsz": "1750", "train_num_updates": "101525", "train_lr": "0.000405537", "train_gnorm": "0.35", "train_loss_scale": "4", "train_train_wall": "49", "train_gb_free": "39.3", "train_wall": "23763"}
[2024-10-11 23:19:08,753][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 23:19:08,768][fairseq.trainer][INFO] - begin training epoch 2117
[2024-10-11 23:19:08,768][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 23:21:24,129][fairseq_cli.train][INFO] - end of epoch 2117 (average epoch stats below)
[2024-10-11 23:21:24,137][train][INFO] - {"epoch": 2117, "train_loss": "0.401", "train_ntokens": "260554", "train_nsentences": "1750.04", "train_wps": "92320.1", "train_ups": "0.35", "train_wpb": "260554", "train_bsz": "1750", "train_num_updates": "101573", "train_lr": "0.000405471", "train_gnorm": "0.357", "train_loss_scale": "4", "train_train_wall": "47", "train_gb_free": "39.6", "train_wall": "23898"}
[2024-10-11 23:21:24,220][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 23:21:24,224][fairseq.trainer][INFO] - begin training epoch 2118
[2024-10-11 23:21:24,225][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 23:23:13,242][train_inner][INFO] - {"epoch": 2118, "update": 2117.562, "loss": "0.4", "ntokens": "260488", "nsentences": "1758.99", "wps": "99270", "ups": "0.38", "wpb": "260488", "bsz": "1759", "num_updates": "101600", "lr": "0.000405435", "gnorm": "0.358", "loss_scale": "4", "train_wall": "192", "gb_free": "39.5", "wall": "24007"}
[2024-10-11 23:23:31,454][fairseq_cli.train][INFO] - end of epoch 2118 (average epoch stats below)
[2024-10-11 23:23:31,456][train][INFO] - {"epoch": 2118, "train_loss": "0.399", "train_ntokens": "260776", "train_nsentences": "1750.04", "train_wps": "98317.9", "train_ups": "0.38", "train_wpb": "260776", "train_bsz": "1750", "train_num_updates": "101621", "train_lr": "0.000405406", "train_gnorm": "0.37", "train_loss_scale": "4", "train_train_wall": "45", "train_gb_free": "39.6", "train_wall": "24026"}
[2024-10-11 23:23:31,511][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 23:23:31,515][fairseq.trainer][INFO] - begin training epoch 2119
[2024-10-11 23:23:31,515][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 23:25:50,376][fairseq_cli.train][INFO] - end of epoch 2119 (average epoch stats below)
[2024-10-11 23:25:50,380][train][INFO] - {"epoch": 2119, "train_loss": "0.402", "train_ntokens": "260495", "train_nsentences": "1750.04", "train_wps": "90007.1", "train_ups": "0.35", "train_wpb": "260496", "train_bsz": "1750", "train_num_updates": "101669", "train_lr": "0.000405341", "train_gnorm": "0.338", "train_loss_scale": "4", "train_train_wall": "76", "train_gb_free": "40", "train_wall": "24164"}
[2024-10-11 23:25:50,468][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 23:25:50,477][fairseq.trainer][INFO] - begin training epoch 2120
[2024-10-11 23:25:50,478][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 23:28:00,774][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 2120 @ 101717 updates
[2024-10-11 23:28:00,775][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-11 23:28:04,193][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-11 23:28:04,195][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 2120 @ 101717 updates, score None) (writing took 3.421610850840807 seconds)
[2024-10-11 23:28:04,196][fairseq_cli.train][INFO] - end of epoch 2120 (average epoch stats below)
[2024-10-11 23:28:04,199][train][INFO] - {"epoch": 2120, "train_loss": "0.396", "train_ntokens": "260637", "train_nsentences": "1750.04", "train_wps": "93493", "train_ups": "0.36", "train_wpb": "260636", "train_bsz": "1750", "train_num_updates": "101717", "train_lr": "0.000405276", "train_gnorm": "0.366", "train_loss_scale": "4", "train_train_wall": "47", "train_gb_free": "40.5", "train_wall": "24298"}
[2024-10-11 23:28:04,259][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 23:28:04,263][fairseq.trainer][INFO] - begin training epoch 2121
[2024-10-11 23:28:04,263][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 23:30:08,081][fairseq_cli.train][INFO] - end of epoch 2121 (average epoch stats below)
[2024-10-11 23:30:08,108][train][INFO] - {"epoch": 2121, "train_loss": "0.393", "train_ntokens": "260705", "train_nsentences": "1750.04", "train_wps": "100997", "train_ups": "0.39", "train_wpb": "260705", "train_bsz": "1750", "train_num_updates": "101765", "train_lr": "0.000405211", "train_gnorm": "0.375", "train_loss_scale": "4", "train_train_wall": "39", "train_gb_free": "39.8", "train_wall": "24422"}
[2024-10-11 23:30:08,242][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 23:30:08,260][fairseq.trainer][INFO] - begin training epoch 2122
[2024-10-11 23:30:08,261][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 23:32:06,752][train_inner][INFO] - {"epoch": 2122, "update": 2121.729, "loss": "0.397", "ntokens": "260741", "nsentences": "1748.64", "wps": "97746.5", "ups": "0.37", "wpb": "260741", "bsz": "1748.6", "num_updates": "101800", "lr": "0.000405163", "gnorm": "0.36", "loss_scale": "4", "train_wall": "225", "gb_free": "39.3", "wall": "24541"}
[2024-10-11 23:32:19,614][fairseq_cli.train][INFO] - end of epoch 2122 (average epoch stats below)
[2024-10-11 23:32:19,616][train][INFO] - {"epoch": 2122, "train_loss": "0.392", "train_ntokens": "260810", "train_nsentences": "1750.04", "train_wps": "95197", "train_ups": "0.37", "train_wpb": "260810", "train_bsz": "1750", "train_num_updates": "101813", "train_lr": "0.000405145", "train_gnorm": "0.357", "train_loss_scale": "4", "train_train_wall": "58", "train_gb_free": "39.6", "train_wall": "24554"}
[2024-10-11 23:32:19,674][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 23:32:19,680][fairseq.trainer][INFO] - begin training epoch 2123
[2024-10-11 23:32:19,680][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 23:34:28,520][fairseq_cli.train][INFO] - end of epoch 2123 (average epoch stats below)
[2024-10-11 23:34:28,552][train][INFO] - {"epoch": 2123, "train_loss": "0.409", "train_ntokens": "260445", "train_nsentences": "1750.04", "train_wps": "96971.8", "train_ups": "0.37", "train_wpb": "260445", "train_bsz": "1750", "train_num_updates": "101861", "train_lr": "0.00040508", "train_gnorm": "0.365", "train_loss_scale": "4", "train_train_wall": "49", "train_gb_free": "39.3", "train_wall": "24683"}
[2024-10-11 23:34:28,652][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 23:34:28,665][fairseq.trainer][INFO] - begin training epoch 2124
[2024-10-11 23:34:28,665][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 23:36:39,018][fairseq_cli.train][INFO] - end of epoch 2124 (average epoch stats below)
[2024-10-11 23:36:39,036][train][INFO] - {"epoch": 2124, "train_loss": "0.397", "train_ntokens": "260962", "train_nsentences": "1750.04", "train_wps": "96002.4", "train_ups": "0.37", "train_wpb": "260962", "train_bsz": "1750", "train_num_updates": "101909", "train_lr": "0.000405015", "train_gnorm": "0.357", "train_loss_scale": "4", "train_train_wall": "56", "train_gb_free": "39.6", "train_wall": "24813"}
[2024-10-11 23:36:39,143][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 23:36:39,161][fairseq.trainer][INFO] - begin training epoch 2125
[2024-10-11 23:36:39,161][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 23:38:47,482][fairseq_cli.train][INFO] - end of epoch 2125 (average epoch stats below)
[2024-10-11 23:38:47,499][train][INFO] - {"epoch": 2125, "train_loss": "0.409", "train_ntokens": "261075", "train_nsentences": "1750.04", "train_wps": "97555.3", "train_ups": "0.37", "train_wpb": "261075", "train_bsz": "1750", "train_num_updates": "101957", "train_lr": "0.00040495", "train_gnorm": "0.375", "train_loss_scale": "4", "train_train_wall": "57", "train_gb_free": "40.6", "train_wall": "24942"}
[2024-10-11 23:38:47,603][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 23:38:47,609][fairseq.trainer][INFO] - begin training epoch 2126
[2024-10-11 23:38:47,609][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 23:40:57,295][train_inner][INFO] - {"epoch": 2126, "update": 2125.896, "loss": "0.403", "ntokens": "260735", "nsentences": "1760.55", "wps": "98290.8", "ups": "0.38", "wpb": "260735", "bsz": "1760.5", "num_updates": "102000", "lr": "0.000404891", "gnorm": "0.361", "loss_scale": "4", "train_wall": "239", "gb_free": "39.7", "wall": "25071"}
[2024-10-11 23:40:59,700][fairseq_cli.train][INFO] - end of epoch 2126 (average epoch stats below)
[2024-10-11 23:40:59,703][train][INFO] - {"epoch": 2126, "train_loss": "0.401", "train_ntokens": "261040", "train_nsentences": "1750.04", "train_wps": "94780.8", "train_ups": "0.36", "train_wpb": "261040", "train_bsz": "1750", "train_num_updates": "102005", "train_lr": "0.000404885", "train_gnorm": "0.345", "train_loss_scale": "4", "train_train_wall": "67", "train_gb_free": "39.2", "train_wall": "25074"}
[2024-10-11 23:40:59,755][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 23:40:59,761][fairseq.trainer][INFO] - begin training epoch 2127
[2024-10-11 23:40:59,761][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 23:43:10,491][fairseq_cli.train][INFO] - end of epoch 2127 (average epoch stats below)
[2024-10-11 23:43:10,495][train][INFO] - {"epoch": 2127, "train_loss": "0.399", "train_ntokens": "260497", "train_nsentences": "1750.04", "train_wps": "95604.1", "train_ups": "0.37", "train_wpb": "260497", "train_bsz": "1750", "train_num_updates": "102053", "train_lr": "0.000404819", "train_gnorm": "0.371", "train_loss_scale": "4", "train_train_wall": "58", "train_gb_free": "39.6", "train_wall": "25205"}
[2024-10-11 23:43:10,545][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 23:43:10,550][fairseq.trainer][INFO] - begin training epoch 2128
[2024-10-11 23:43:10,550][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 23:45:17,654][fairseq_cli.train][INFO] - end of epoch 2128 (average epoch stats below)
[2024-10-11 23:45:17,661][train][INFO] - {"epoch": 2128, "train_loss": "0.399", "train_ntokens": "260695", "train_nsentences": "1750.04", "train_wps": "98406.6", "train_ups": "0.38", "train_wpb": "260695", "train_bsz": "1750", "train_num_updates": "102101", "train_lr": "0.000404754", "train_gnorm": "0.366", "train_loss_scale": "4", "train_train_wall": "62", "train_gb_free": "39.3", "train_wall": "25332"}
[2024-10-11 23:45:17,811][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 23:45:17,815][fairseq.trainer][INFO] - begin training epoch 2129
[2024-10-11 23:45:17,815][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 23:47:26,172][fairseq_cli.train][INFO] - end of epoch 2129 (average epoch stats below)
[2024-10-11 23:47:26,177][train][INFO] - {"epoch": 2129, "train_loss": "0.403", "train_ntokens": "260806", "train_nsentences": "1750.04", "train_wps": "97413.9", "train_ups": "0.37", "train_wpb": "260806", "train_bsz": "1750", "train_num_updates": "102149", "train_lr": "0.000404689", "train_gnorm": "0.346", "train_loss_scale": "4", "train_train_wall": "51", "train_gb_free": "39.7", "train_wall": "25460"}
[2024-10-11 23:47:26,306][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 23:47:26,313][fairseq.trainer][INFO] - begin training epoch 2130
[2024-10-11 23:47:26,313][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 23:49:35,146][fairseq_cli.train][INFO] - end of epoch 2130 (average epoch stats below)
[2024-10-11 23:49:35,161][train][INFO] - {"epoch": 2130, "train_loss": "0.397", "train_ntokens": "260669", "train_nsentences": "1750.04", "train_wps": "97007.8", "train_ups": "0.37", "train_wpb": "260669", "train_bsz": "1750", "train_num_updates": "102197", "train_lr": "0.000404624", "train_gnorm": "0.355", "train_loss_scale": "4", "train_train_wall": "52", "train_gb_free": "39.6", "train_wall": "25589"}
[2024-10-11 23:49:35,228][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 23:49:35,231][fairseq.trainer][INFO] - begin training epoch 2131
[2024-10-11 23:49:35,232][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 23:51:00,848][train_inner][INFO] - {"epoch": 2131, "update": 2130.062, "loss": "0.4", "ntokens": "260767", "nsentences": "1745.03", "wps": "86413.1", "ups": "0.33", "wpb": "260767", "bsz": "1745", "num_updates": "102200", "lr": "0.00040462", "gnorm": "0.359", "loss_scale": "4", "train_wall": "236", "gb_free": "39.2", "wall": "25675"}
[2024-10-11 23:51:46,015][fairseq_cli.train][INFO] - end of epoch 2131 (average epoch stats below)
[2024-10-11 23:51:46,018][train][INFO] - {"epoch": 2131, "train_loss": "0.404", "train_ntokens": "260247", "train_nsentences": "1750.04", "train_wps": "95465.7", "train_ups": "0.37", "train_wpb": "260247", "train_bsz": "1750", "train_num_updates": "102245", "train_lr": "0.000404558", "train_gnorm": "0.351", "train_loss_scale": "4", "train_train_wall": "53", "train_gb_free": "39.7", "train_wall": "25720"}
[2024-10-11 23:51:46,080][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 23:51:46,084][fairseq.trainer][INFO] - begin training epoch 2132
[2024-10-11 23:51:46,085][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 23:53:53,166][fairseq_cli.train][INFO] - end of epoch 2132 (average epoch stats below)
[2024-10-11 23:53:53,169][train][INFO] - {"epoch": 2132, "train_loss": "0.394", "train_ntokens": "260257", "train_nsentences": "1750.04", "train_wps": "98250.7", "train_ups": "0.38", "train_wpb": "260257", "train_bsz": "1750", "train_num_updates": "102293", "train_lr": "0.000404493", "train_gnorm": "0.374", "train_loss_scale": "4", "train_train_wall": "57", "train_gb_free": "40.3", "train_wall": "25847"}
[2024-10-11 23:53:53,269][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 23:53:53,283][fairseq.trainer][INFO] - begin training epoch 2133
[2024-10-11 23:53:53,284][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 23:56:02,094][fairseq_cli.train][INFO] - end of epoch 2133 (average epoch stats below)
[2024-10-11 23:56:02,096][train][INFO] - {"epoch": 2133, "train_loss": "0.395", "train_ntokens": "260612", "train_nsentences": "1750.04", "train_wps": "97029.9", "train_ups": "0.37", "train_wpb": "260612", "train_bsz": "1750", "train_num_updates": "102341", "train_lr": "0.000404428", "train_gnorm": "0.376", "train_loss_scale": "4", "train_train_wall": "56", "train_gb_free": "40.8", "train_wall": "25976"}
[2024-10-11 23:56:02,171][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 23:56:02,176][fairseq.trainer][INFO] - begin training epoch 2134
[2024-10-11 23:56:02,176][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 23:58:15,803][fairseq_cli.train][INFO] - end of epoch 2134 (average epoch stats below)
[2024-10-11 23:58:15,807][train][INFO] - {"epoch": 2134, "train_loss": "0.397", "train_ntokens": "260415", "train_nsentences": "1750.04", "train_wps": "93487.3", "train_ups": "0.36", "train_wpb": "260415", "train_bsz": "1750", "train_num_updates": "102389", "train_lr": "0.000404363", "train_gnorm": "0.354", "train_loss_scale": "4", "train_train_wall": "70", "train_gb_free": "39.6", "train_wall": "26110"}
[2024-10-11 23:58:15,860][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 23:58:15,866][fairseq.trainer][INFO] - begin training epoch 2135
[2024-10-11 23:58:15,866][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 23:59:47,578][train_inner][INFO] - {"epoch": 2135, "update": 2134.229, "loss": "0.397", "ntokens": "260465", "nsentences": "1749.07", "wps": "98900.6", "ups": "0.38", "wpb": "260465", "bsz": "1749.1", "num_updates": "102400", "lr": "0.000404348", "gnorm": "0.363", "loss_scale": "4", "train_wall": "254", "gb_free": "39.7", "wall": "26202"}
[2024-10-12 00:00:23,195][fairseq_cli.train][INFO] - end of epoch 2135 (average epoch stats below)
[2024-10-12 00:00:23,199][train][INFO] - {"epoch": 2135, "train_loss": "0.397", "train_ntokens": "260465", "train_nsentences": "1750.04", "train_wps": "98143.8", "train_ups": "0.38", "train_wpb": "260465", "train_bsz": "1750", "train_num_updates": "102437", "train_lr": "0.000404298", "train_gnorm": "0.35", "train_loss_scale": "4", "train_train_wall": "64", "train_gb_free": "42.5", "train_wall": "26237"}
[2024-10-12 00:00:23,322][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 00:00:23,326][fairseq.trainer][INFO] - begin training epoch 2136
[2024-10-12 00:00:23,326][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 00:02:35,555][fairseq_cli.train][INFO] - end of epoch 2136 (average epoch stats below)
[2024-10-12 00:02:35,561][train][INFO] - {"epoch": 2136, "train_loss": "0.392", "train_ntokens": "260492", "train_nsentences": "1750.04", "train_wps": "94468", "train_ups": "0.36", "train_wpb": "260492", "train_bsz": "1750", "train_num_updates": "102485", "train_lr": "0.000404232", "train_gnorm": "0.349", "train_loss_scale": "4", "train_train_wall": "50", "train_gb_free": "39.7", "train_wall": "26370"}
[2024-10-12 00:02:35,666][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 00:02:35,675][fairseq.trainer][INFO] - begin training epoch 2137
[2024-10-12 00:02:35,675][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 00:04:45,402][fairseq_cli.train][INFO] - end of epoch 2137 (average epoch stats below)
[2024-10-12 00:04:45,406][train][INFO] - {"epoch": 2137, "train_loss": "0.395", "train_ntokens": "260762", "train_nsentences": "1750.04", "train_wps": "96401.6", "train_ups": "0.37", "train_wpb": "260762", "train_bsz": "1750", "train_num_updates": "102533", "train_lr": "0.000404167", "train_gnorm": "0.351", "train_loss_scale": "4", "train_train_wall": "57", "train_gb_free": "39.7", "train_wall": "26499"}
[2024-10-12 00:04:45,470][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 00:04:45,474][fairseq.trainer][INFO] - begin training epoch 2138
[2024-10-12 00:04:45,474][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 00:06:56,576][fairseq_cli.train][INFO] - end of epoch 2138 (average epoch stats below)
[2024-10-12 00:06:56,579][train][INFO] - {"epoch": 2138, "train_loss": "0.396", "train_ntokens": "260830", "train_nsentences": "1750.04", "train_wps": "95449.3", "train_ups": "0.37", "train_wpb": "260830", "train_bsz": "1750", "train_num_updates": "102581", "train_lr": "0.000404102", "train_gnorm": "0.388", "train_loss_scale": "4", "train_train_wall": "59", "train_gb_free": "39.9", "train_wall": "26631"}
[2024-10-12 00:06:56,629][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 00:06:56,634][fairseq.trainer][INFO] - begin training epoch 2139
[2024-10-12 00:06:56,635][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 00:08:42,140][train_inner][INFO] - {"epoch": 2139, "update": 2138.396, "loss": "0.395", "ntokens": "260690", "nsentences": "1756.37", "wps": "97536", "ups": "0.37", "wpb": "260690", "bsz": "1756.4", "num_updates": "102600", "lr": "0.000404076", "gnorm": "0.362", "loss_scale": "4", "train_wall": "218", "gb_free": "40.1", "wall": "26736"}
[2024-10-12 00:09:07,355][fairseq_cli.train][INFO] - end of epoch 2139 (average epoch stats below)
[2024-10-12 00:09:07,379][train][INFO] - {"epoch": 2139, "train_loss": "0.394", "train_ntokens": "260806", "train_nsentences": "1750.04", "train_wps": "95727.6", "train_ups": "0.37", "train_wpb": "260806", "train_bsz": "1750", "train_num_updates": "102629", "train_lr": "0.000404037", "train_gnorm": "0.374", "train_loss_scale": "4", "train_train_wall": "42", "train_gb_free": "39.3", "train_wall": "26761"}
[2024-10-12 00:09:07,471][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 00:09:07,481][fairseq.trainer][INFO] - begin training epoch 2140
[2024-10-12 00:09:07,481][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 00:11:16,727][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 2140 @ 102677 updates
[2024-10-12 00:11:16,729][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-12 00:11:20,468][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-12 00:11:20,470][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 2140 @ 102677 updates, score None) (writing took 3.743427826091647 seconds)
[2024-10-12 00:11:20,471][fairseq_cli.train][INFO] - end of epoch 2140 (average epoch stats below)
[2024-10-12 00:11:20,473][train][INFO] - {"epoch": 2140, "train_loss": "0.402", "train_ntokens": "260860", "train_nsentences": "1750.04", "train_wps": "94082", "train_ups": "0.36", "train_wpb": "260860", "train_bsz": "1750", "train_num_updates": "102677", "train_lr": "0.000403971", "train_gnorm": "0.36", "train_loss_scale": "4", "train_train_wall": "51", "train_gb_free": "39.7", "train_wall": "26895"}
[2024-10-12 00:11:20,548][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 00:11:20,569][fairseq.trainer][INFO] - begin training epoch 2141
[2024-10-12 00:11:20,569][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 00:13:32,840][fairseq_cli.train][INFO] - end of epoch 2141 (average epoch stats below)
[2024-10-12 00:13:32,843][train][INFO] - {"epoch": 2141, "train_loss": "0.404", "train_ntokens": "260513", "train_nsentences": "1750.04", "train_wps": "94470.1", "train_ups": "0.36", "train_wpb": "260513", "train_bsz": "1750", "train_num_updates": "102725", "train_lr": "0.000403906", "train_gnorm": "0.365", "train_loss_scale": "4", "train_train_wall": "54", "train_gb_free": "40", "train_wall": "27027"}
[2024-10-12 00:13:32,921][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 00:13:32,932][fairseq.trainer][INFO] - begin training epoch 2142
[2024-10-12 00:13:32,933][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 00:15:40,975][fairseq_cli.train][INFO] - end of epoch 2142 (average epoch stats below)
[2024-10-12 00:15:40,978][train][INFO] - {"epoch": 2142, "train_loss": "0.396", "train_ntokens": "260532", "train_nsentences": "1750.04", "train_wps": "97601.2", "train_ups": "0.37", "train_wpb": "260532", "train_bsz": "1750", "train_num_updates": "102773", "train_lr": "0.000403841", "train_gnorm": "0.347", "train_loss_scale": "4", "train_train_wall": "59", "train_gb_free": "39.2", "train_wall": "27155"}
[2024-10-12 00:15:41,031][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 00:15:41,039][fairseq.trainer][INFO] - begin training epoch 2143
[2024-10-12 00:15:41,040][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 00:17:29,183][train_inner][INFO] - {"epoch": 2143, "update": 2142.562, "loss": "0.399", "ntokens": "260681", "nsentences": "1738.64", "wps": "98923.5", "ups": "0.38", "wpb": "260681", "bsz": "1738.6", "num_updates": "102800", "lr": "0.000403804", "gnorm": "0.359", "loss_scale": "4", "train_wall": "231", "gb_free": "39.6", "wall": "27263"}
[2024-10-12 00:17:49,045][fairseq_cli.train][INFO] - end of epoch 2143 (average epoch stats below)
[2024-10-12 00:17:49,047][train][INFO] - {"epoch": 2143, "train_loss": "0.394", "train_ntokens": "260503", "train_nsentences": "1750.04", "train_wps": "97639.8", "train_ups": "0.37", "train_wpb": "260503", "train_bsz": "1750", "train_num_updates": "102821", "train_lr": "0.000403776", "train_gnorm": "0.369", "train_loss_scale": "4", "train_train_wall": "61", "train_gb_free": "40.1", "train_wall": "27283"}
[2024-10-12 00:17:49,101][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 00:17:49,105][fairseq.trainer][INFO] - begin training epoch 2144
[2024-10-12 00:17:49,105][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 00:20:01,508][fairseq_cli.train][INFO] - end of epoch 2144 (average epoch stats below)
[2024-10-12 00:20:01,512][train][INFO] - {"epoch": 2144, "train_loss": "0.4", "train_ntokens": "260941", "train_nsentences": "1750.04", "train_wps": "94558.2", "train_ups": "0.36", "train_wpb": "260941", "train_bsz": "1750", "train_num_updates": "102869", "train_lr": "0.000403711", "train_gnorm": "0.346", "train_loss_scale": "4", "train_train_wall": "57", "train_gb_free": "40.1", "train_wall": "27416"}
[2024-10-12 00:20:01,563][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 00:20:01,568][fairseq.trainer][INFO] - begin training epoch 2145
[2024-10-12 00:20:01,569][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 00:22:09,134][fairseq_cli.train][INFO] - end of epoch 2145 (average epoch stats below)
[2024-10-12 00:22:09,147][train][INFO] - {"epoch": 2145, "train_loss": "0.399", "train_ntokens": "260855", "train_nsentences": "1750.04", "train_wps": "98107.2", "train_ups": "0.38", "train_wpb": "260855", "train_bsz": "1750", "train_num_updates": "102917", "train_lr": "0.000403645", "train_gnorm": "0.381", "train_loss_scale": "4", "train_train_wall": "55", "train_gb_free": "40", "train_wall": "27543"}
[2024-10-12 00:22:09,242][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 00:22:09,249][fairseq.trainer][INFO] - begin training epoch 2146
[2024-10-12 00:22:09,250][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 00:24:18,415][fairseq_cli.train][INFO] - end of epoch 2146 (average epoch stats below)
[2024-10-12 00:24:18,418][train][INFO] - {"epoch": 2146, "train_loss": "0.393", "train_ntokens": "260304", "train_nsentences": "1750.04", "train_wps": "96662.9", "train_ups": "0.37", "train_wpb": "260304", "train_bsz": "1750", "train_num_updates": "102965", "train_lr": "0.00040358", "train_gnorm": "0.34", "train_loss_scale": "4", "train_train_wall": "47", "train_gb_free": "39.5", "train_wall": "27672"}
[2024-10-12 00:24:18,535][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 00:24:18,556][fairseq.trainer][INFO] - begin training epoch 2147
[2024-10-12 00:24:18,556][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 00:26:20,285][train_inner][INFO] - {"epoch": 2147, "update": 2146.729, "loss": "0.397", "ntokens": "260532", "nsentences": "1765.94", "wps": "98110.8", "ups": "0.38", "wpb": "260532", "bsz": "1765.9", "num_updates": "103000", "lr": "0.000403533", "gnorm": "0.357", "loss_scale": "4", "train_wall": "232", "gb_free": "40.1", "wall": "27794"}
[2024-10-12 00:26:30,961][fairseq_cli.train][INFO] - end of epoch 2147 (average epoch stats below)
[2024-10-12 00:26:30,975][train][INFO] - {"epoch": 2147, "train_loss": "0.397", "train_ntokens": "260438", "train_nsentences": "1750.04", "train_wps": "94318.6", "train_ups": "0.36", "train_wpb": "260438", "train_bsz": "1750", "train_num_updates": "103013", "train_lr": "0.000403515", "train_gnorm": "0.348", "train_loss_scale": "4", "train_train_wall": "64", "train_gb_free": "39.3", "train_wall": "27805"}
[2024-10-12 00:26:31,068][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 00:26:31,081][fairseq.trainer][INFO] - begin training epoch 2148
[2024-10-12 00:26:31,081][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 00:28:41,987][fairseq_cli.train][INFO] - end of epoch 2148 (average epoch stats below)
[2024-10-12 00:28:41,994][train][INFO] - {"epoch": 2148, "train_loss": "0.392", "train_ntokens": "261102", "train_nsentences": "1750.04", "train_wps": "95661.6", "train_ups": "0.37", "train_wpb": "261102", "train_bsz": "1750", "train_num_updates": "103061", "train_lr": "0.00040345", "train_gnorm": "0.364", "train_loss_scale": "4", "train_train_wall": "53", "train_gb_free": "39.9", "train_wall": "27936"}
[2024-10-12 00:28:42,104][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 00:28:42,110][fairseq.trainer][INFO] - begin training epoch 2149
[2024-10-12 00:28:42,110][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 00:30:50,750][fairseq_cli.train][INFO] - end of epoch 2149 (average epoch stats below)
[2024-10-12 00:30:50,757][train][INFO] - {"epoch": 2149, "train_loss": "0.39", "train_ntokens": "260454", "train_nsentences": "1750.04", "train_wps": "97098.2", "train_ups": "0.37", "train_wpb": "260454", "train_bsz": "1750", "train_num_updates": "103109", "train_lr": "0.000403385", "train_gnorm": "0.376", "train_loss_scale": "4", "train_train_wall": "49", "train_gb_free": "39.7", "train_wall": "28065"}
[2024-10-12 00:30:50,867][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 00:30:50,873][fairseq.trainer][INFO] - begin training epoch 2150
[2024-10-12 00:30:50,874][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 00:32:35,333][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
[2024-10-12 00:32:58,161][fairseq_cli.train][INFO] - end of epoch 2150 (average epoch stats below)
[2024-10-12 00:32:58,177][train][INFO] - {"epoch": 2150, "train_loss": "0.391", "train_ntokens": "260703", "train_nsentences": "1747.45", "train_wps": "96165.9", "train_ups": "0.37", "train_wpb": "260703", "train_bsz": "1747.4", "train_num_updates": "103156", "train_lr": "0.000403321", "train_gnorm": "0.346", "train_loss_scale": "4", "train_train_wall": "61", "train_gb_free": "39.2", "train_wall": "28192"}
[2024-10-12 00:32:58,265][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 00:32:58,279][fairseq.trainer][INFO] - begin training epoch 2151
[2024-10-12 00:32:58,279][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 00:35:09,886][train_inner][INFO] - {"epoch": 2151, "update": 2150.917, "loss": "0.392", "ntokens": "260685", "nsentences": "1741.35", "wps": "98450.3", "ups": "0.38", "wpb": "260685", "bsz": "1741.3", "num_updates": "103200", "lr": "0.000403261", "gnorm": "0.356", "loss_scale": "4", "train_wall": "224", "gb_free": "39.6", "wall": "28324"}
[2024-10-12 00:35:10,838][fairseq_cli.train][INFO] - end of epoch 2151 (average epoch stats below)
[2024-10-12 00:35:10,847][train][INFO] - {"epoch": 2151, "train_loss": "0.391", "train_ntokens": "260623", "train_nsentences": "1750.04", "train_wps": "94301.2", "train_ups": "0.36", "train_wpb": "260623", "train_bsz": "1750", "train_num_updates": "103204", "train_lr": "0.000403255", "train_gnorm": "0.332", "train_loss_scale": "4", "train_train_wall": "51", "train_gb_free": "39.8", "train_wall": "28325"}
[2024-10-12 00:35:10,926][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 00:35:10,937][fairseq.trainer][INFO] - begin training epoch 2152
[2024-10-12 00:35:10,937][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 00:37:17,044][fairseq_cli.train][INFO] - end of epoch 2152 (average epoch stats below)
[2024-10-12 00:37:17,066][train][INFO] - {"epoch": 2152, "train_loss": "0.405", "train_ntokens": "260575", "train_nsentences": "1750.04", "train_wps": "99099.2", "train_ups": "0.38", "train_wpb": "260575", "train_bsz": "1750", "train_num_updates": "103252", "train_lr": "0.00040319", "train_gnorm": "0.362", "train_loss_scale": "4", "train_train_wall": "40", "train_gb_free": "39.8", "train_wall": "28451"}
[2024-10-12 00:37:17,168][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 00:37:17,175][fairseq.trainer][INFO] - begin training epoch 2153
[2024-10-12 00:37:17,177][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 00:39:27,453][fairseq_cli.train][INFO] - end of epoch 2153 (average epoch stats below)
[2024-10-12 00:39:27,457][train][INFO] - {"epoch": 2153, "train_loss": "0.393", "train_ntokens": "260757", "train_nsentences": "1750.04", "train_wps": "95994.9", "train_ups": "0.37", "train_wpb": "260757", "train_bsz": "1750", "train_num_updates": "103300", "train_lr": "0.000403125", "train_gnorm": "0.364", "train_loss_scale": "4", "train_train_wall": "32", "train_gb_free": "40.2", "train_wall": "28582"}
[2024-10-12 00:39:27,518][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 00:39:27,525][fairseq.trainer][INFO] - begin training epoch 2154
[2024-10-12 00:39:27,525][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-12 00:41:35,491][fairseq_cli.train][INFO] - end of epoch 2154 (average epoch stats below)
[2024-10-12 00:41:35,495][train][INFO] - {"epoch": 2154, "train_loss": "0.389", "train_ntokens": "260426", "train_nsentences": "1750.04", "train_wps": "97634.4", "train_ups": "0.37", "train_wpb": "260426", "train_bsz": "1750", "train_num_updates": "103348", "train_lr": "0.00040306", "train_gnorm": "0.341", "train_loss_scale": "4", "train_train_wall": "39", "train_gb_free": "39.8", "train_wall": "28710"}
[2024-10-12 00:41:35,556][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-12 00:41:35,559][fairseq.trainer][INFO] - begin training epoch 2155
[2024-10-12 00:41:35,560][fairseq_cli.train][INFO] - Start iterating over samples
