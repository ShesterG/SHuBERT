[2024-10-11 04:01:55,471][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:17727', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-11 04:01:58,408][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:19455', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-11 04:01:58,439][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-11 04:01:58,441][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-11 04:01:58,441][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-11 04:01:58,441][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-11 04:01:58,442][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-11 04:01:58,442][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-11 04:01:58,813][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-11 04:02:00,783][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:14798', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-11 04:02:01,908][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:14802', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-11 04:02:02,392][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:15765', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-11 04:02:02,415][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:12142', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-11 04:02:03,104][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:18793', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-11 04:02:03,303][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:14837', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-11 04:02:03,984][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-11 04:02:04,015][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-11 04:02:04,015][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-11 04:02:04,016][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-11 04:02:04,016][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-11 04:02:04,017][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-11 04:02:04,909][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-11 04:02:06,405][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-11 04:02:06,407][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-11 04:02:06,411][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-11 04:02:06,411][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-11 04:02:06,412][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-11 04:02:06,415][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-11 04:02:07,397][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-11 04:02:07,398][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-11 04:02:07,398][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-11 04:02:07,399][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-11 04:02:07,399][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-11 04:02:07,403][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-11 04:02:07,418][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-11 04:02:07,902][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-11 04:02:09,229][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-11 04:02:09,251][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-11 04:02:09,251][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-11 04:02:09,251][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-11 04:02:09,252][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-11 04:02:09,259][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-11 04:02:09,868][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-11 04:02:09,870][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-11 04:02:09,870][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-11 04:02:09,870][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-11 04:02:09,871][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-11 04:02:09,871][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-11 04:02:10,186][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-11 04:02:10,345][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-11 04:02:16,183][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-11 04:02:16,469][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-11 04:02:16,469][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-11 04:02:16,469][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-11 04:02:16,470][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-11 04:02:16,470][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-11 04:02:30,566][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-11 04:02:33,062][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-11 04:02:33,064][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-11 04:02:33,064][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-11 04:02:33,064][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-11 04:02:33,065][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-11 04:02:33,066][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-11 04:02:33,877][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-11 04:03:17,571][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-11 04:03:17,575][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 04:03:17,575][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 04:03:17,575][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 04:03:17,576][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 04:03:17,576][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 04:03:17,576][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 04:03:17,576][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 04:03:17,576][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 04:03:17,576][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-11 04:03:17,576][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-11 04:03:17,576][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-11 04:03:17,577][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-11 04:03:59,705][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 1701 @ 81564 updates)
[2024-10-11 04:03:59,710][fairseq.trainer][INFO] - loading train data for epoch 1701
[2024-10-11 04:04:00,044][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-11 04:04:03,514][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-11 04:04:03,551][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 04:04:03,551][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 04:04:03,551][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 04:04:03,551][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 04:04:03,552][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 04:04:03,552][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 04:04:03,552][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 04:04:03,552][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 04:04:03,552][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-11 04:04:03,552][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-11 04:04:03,557][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-11 04:04:03,558][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-11 04:04:06,017][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 04:04:06,020][fairseq.trainer][INFO] - begin training epoch 1701
[2024-10-11 04:04:06,020][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 04:04:15,479][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-11 04:04:15,479][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 04:04:15,480][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 04:04:15,480][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 04:04:15,480][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 04:04:15,480][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 04:04:15,480][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 04:04:15,480][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 04:04:15,480][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 04:04:15,480][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-11 04:04:15,480][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-11 04:04:15,480][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-11 04:04:15,481][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-11 04:04:36,133][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 1701 @ 81564 updates)
[2024-10-11 04:04:36,140][fairseq.trainer][INFO] - loading train data for epoch 1701
[2024-10-11 04:04:36,618][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-11 04:04:42,943][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 04:04:42,953][fairseq.trainer][INFO] - begin training epoch 1701
[2024-10-11 04:04:42,959][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 04:05:08,205][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-11 04:05:08,205][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 04:05:08,205][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 04:05:08,205][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 04:05:08,205][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 04:05:08,206][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 04:05:08,206][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 04:05:08,206][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 04:05:08,206][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 04:05:08,206][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-11 04:05:08,206][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-11 04:05:08,206][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-11 04:05:08,207][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-11 04:05:14,552][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-11 04:05:14,552][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 04:05:14,552][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 04:05:14,552][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 04:05:14,552][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 04:05:14,552][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 04:05:14,553][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 04:05:14,553][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 04:05:14,553][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 04:05:14,553][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-11 04:05:14,553][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-11 04:05:14,553][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-11 04:05:14,554][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-11 04:05:25,406][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 1701 @ 81564 updates)
[2024-10-11 04:05:25,408][fairseq.trainer][INFO] - loading train data for epoch 1701
[2024-10-11 04:05:25,792][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-11 04:05:25,792][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 04:05:25,792][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 04:05:25,792][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 04:05:25,792][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 04:05:25,792][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 04:05:25,792][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 04:05:25,792][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 04:05:25,792][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 04:05:25,792][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-11 04:05:25,792][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-11 04:05:25,793][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-11 04:05:25,793][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-11 04:05:25,917][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-11 04:05:28,998][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 1701 @ 81564 updates)
[2024-10-11 04:05:29,000][fairseq.trainer][INFO] - loading train data for epoch 1701
[2024-10-11 04:05:32,981][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-11 04:05:33,903][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 04:05:33,940][fairseq.trainer][INFO] - begin training epoch 1701
[2024-10-11 04:05:33,941][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 04:05:37,812][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-11 04:05:37,812][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 04:05:37,812][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 04:05:37,812][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 04:05:37,812][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 04:05:37,812][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 04:05:37,812][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 04:05:37,812][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 04:05:37,812][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 04:05:37,812][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-11 04:05:37,813][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-11 04:05:37,813][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-11 04:05:37,813][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-11 04:05:52,381][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 1701 @ 81564 updates)
[2024-10-11 04:05:52,382][fairseq.trainer][INFO] - loading train data for epoch 1701
[2024-10-11 04:05:52,979][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-11 04:06:00,542][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 1701 @ 81564 updates)
[2024-10-11 04:06:00,546][fairseq.trainer][INFO] - loading train data for epoch 1701
[2024-10-11 04:06:00,961][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-11 04:06:03,487][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 04:06:03,490][fairseq.trainer][INFO] - begin training epoch 1701
[2024-10-11 04:06:03,491][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 04:06:07,261][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-11 04:06:07,261][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 04:06:07,261][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 04:06:07,261][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 04:06:07,261][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 04:06:07,261][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 04:06:07,261][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 04:06:07,261][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 04:06:07,261][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-11 04:06:07,261][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-11 04:06:07,261][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-11 04:06:07,262][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-11 04:06:07,262][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-11 04:06:07,502][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 04:06:07,516][fairseq.trainer][INFO] - begin training epoch 1701
[2024-10-11 04:06:07,517][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 04:06:15,986][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 1701 @ 81564 updates)
[2024-10-11 04:06:16,007][fairseq.trainer][INFO] - loading train data for epoch 1701
[2024-10-11 04:06:16,430][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-11 04:06:21,409][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 04:06:21,412][fairseq.trainer][INFO] - begin training epoch 1701
[2024-10-11 04:06:21,413][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 04:06:21,675][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 04:06:21,678][fairseq.trainer][INFO] - begin training epoch 1701
[2024-10-11 04:06:21,680][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 04:06:22,259][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 1701 @ 81564 updates)
[2024-10-11 04:06:22,261][fairseq.trainer][INFO] - loading train data for epoch 1701
[2024-10-11 04:06:22,548][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-11 04:06:28,041][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 04:06:28,068][fairseq.trainer][INFO] - begin training epoch 1701
[2024-10-11 04:06:28,085][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 04:11:27,327][train_inner][INFO] - {"epoch": 1701, "update": 1700.75, "loss": "0.426", "ntokens": "260490", "nsentences": "1787.19", "wps": "142817", "ups": "0.55", "wpb": "260490", "bsz": "1787.2", "num_updates": "81600", "lr": "0.000432609", "gnorm": "0.38", "loss_scale": "4", "train_wall": "215", "gb_free": "39.4", "wall": "490"}
[2024-10-11 04:12:31,788][fairseq_cli.train][INFO] - end of epoch 1701 (average epoch stats below)
[2024-10-11 04:12:31,868][train][INFO] - {"epoch": 1701, "train_loss": "0.431", "train_ntokens": "260436", "train_nsentences": "1750.04", "train_wps": "95254.1", "train_ups": "0.37", "train_wpb": "260436", "train_bsz": "1750", "train_num_updates": "81612", "train_lr": "0.000432592", "train_gnorm": "0.388", "train_loss_scale": "4", "train_train_wall": "278", "train_gb_free": "40.7", "train_wall": "554"}
[2024-10-11 04:12:32,187][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 04:12:32,193][fairseq.trainer][INFO] - begin training epoch 1702
[2024-10-11 04:12:32,193][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 04:21:48,732][fairseq_cli.train][INFO] - end of epoch 1702 (average epoch stats below)
[2024-10-11 04:21:48,767][train][INFO] - {"epoch": 1702, "train_loss": "0.432", "train_ntokens": "260959", "train_nsentences": "1750.04", "train_wps": "22493.3", "train_ups": "0.09", "train_wpb": "260959", "train_bsz": "1750", "train_num_updates": "81660", "train_lr": "0.000432527", "train_gnorm": "0.364", "train_loss_scale": "4", "train_train_wall": "100", "train_gb_free": "39.2", "train_wall": "1111"}
[2024-10-11 04:21:48,945][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 04:21:48,948][fairseq.trainer][INFO] - begin training epoch 1703
[2024-10-11 04:21:48,949][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 04:24:57,340][fairseq_cli.train][INFO] - end of epoch 1703 (average epoch stats below)
[2024-10-11 04:24:57,344][train][INFO] - {"epoch": 1703, "train_loss": "0.428", "train_ntokens": "260724", "train_nsentences": "1750.04", "train_wps": "66365", "train_ups": "0.25", "train_wpb": "260724", "train_bsz": "1750", "train_num_updates": "81708", "train_lr": "0.000432462", "train_gnorm": "0.357", "train_loss_scale": "4", "train_train_wall": "38", "train_gb_free": "39.7", "train_wall": "1300"}
[2024-10-11 04:24:57,395][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 04:24:57,400][fairseq.trainer][INFO] - begin training epoch 1704
[2024-10-11 04:24:57,400][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 04:26:47,342][fairseq_cli.train][INFO] - end of epoch 1704 (average epoch stats below)
[2024-10-11 04:26:47,350][train][INFO] - {"epoch": 1704, "train_loss": "0.429", "train_ntokens": "260811", "train_nsentences": "1750.04", "train_wps": "113804", "train_ups": "0.44", "train_wpb": "260811", "train_bsz": "1750", "train_num_updates": "81756", "train_lr": "0.000432397", "train_gnorm": "0.356", "train_loss_scale": "4", "train_train_wall": "46", "train_gb_free": "39.2", "train_wall": "1410"}
[2024-10-11 04:26:47,433][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 04:26:47,436][fairseq.trainer][INFO] - begin training epoch 1705
[2024-10-11 04:26:47,437][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 04:28:36,104][train_inner][INFO] - {"epoch": 1705, "update": 1704.917, "loss": "0.433", "ntokens": "260776", "nsentences": "1742.69", "wps": "50697.7", "ups": "0.19", "wpb": "260776", "bsz": "1742.7", "num_updates": "81800", "lr": "0.000432337", "gnorm": "0.361", "loss_scale": "4", "train_wall": "289", "gb_free": "39.8", "wall": "1519"}
[2024-10-11 04:28:36,936][fairseq_cli.train][INFO] - end of epoch 1705 (average epoch stats below)
[2024-10-11 04:28:36,938][train][INFO] - {"epoch": 1705, "train_loss": "0.442", "train_ntokens": "260733", "train_nsentences": "1750.04", "train_wps": "114205", "train_ups": "0.44", "train_wpb": "260733", "train_bsz": "1750", "train_num_updates": "81804", "train_lr": "0.000432332", "train_gnorm": "0.359", "train_loss_scale": "4", "train_train_wall": "43", "train_gb_free": "39.6", "train_wall": "1519"}
[2024-10-11 04:28:36,994][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 04:28:36,997][fairseq.trainer][INFO] - begin training epoch 1706
[2024-10-11 04:28:36,997][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 04:30:27,079][fairseq_cli.train][INFO] - end of epoch 1706 (average epoch stats below)
[2024-10-11 04:30:27,082][train][INFO] - {"epoch": 1706, "train_loss": "0.44", "train_ntokens": "260617", "train_nsentences": "1750.04", "train_wps": "113577", "train_ups": "0.44", "train_wpb": "260617", "train_bsz": "1750", "train_num_updates": "81852", "train_lr": "0.000432266", "train_gnorm": "0.372", "train_loss_scale": "4", "train_train_wall": "45", "train_gb_free": "39.1", "train_wall": "1630"}
[2024-10-11 04:30:27,131][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 04:30:27,135][fairseq.trainer][INFO] - begin training epoch 1707
[2024-10-11 04:30:27,136][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 04:31:53,395][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2024-10-11 04:32:16,717][fairseq_cli.train][INFO] - end of epoch 1707 (average epoch stats below)
[2024-10-11 04:32:16,719][train][INFO] - {"epoch": 1707, "train_loss": "0.442", "train_ntokens": "260748", "train_nsentences": "1750.57", "train_wps": "111781", "train_ups": "0.43", "train_wpb": "260748", "train_bsz": "1750.6", "train_num_updates": "81899", "train_lr": "0.000432202", "train_gnorm": "0.392", "train_loss_scale": "2", "train_train_wall": "49", "train_gb_free": "40.1", "train_wall": "1739"}
[2024-10-11 04:32:16,813][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 04:32:16,816][fairseq.trainer][INFO] - begin training epoch 1708
[2024-10-11 04:32:16,817][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 04:34:04,952][fairseq_cli.train][INFO] - end of epoch 1708 (average epoch stats below)
[2024-10-11 04:34:04,960][train][INFO] - {"epoch": 1708, "train_loss": "0.43", "train_ntokens": "260791", "train_nsentences": "1750.04", "train_wps": "115653", "train_ups": "0.44", "train_wpb": "260791", "train_bsz": "1750", "train_num_updates": "81947", "train_lr": "0.000432137", "train_gnorm": "0.355", "train_loss_scale": "2", "train_train_wall": "36", "train_gb_free": "40", "train_wall": "1847"}
[2024-10-11 04:34:05,028][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 04:34:05,031][fairseq.trainer][INFO] - begin training epoch 1709
[2024-10-11 04:34:05,031][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 04:35:54,602][fairseq_cli.train][INFO] - end of epoch 1709 (average epoch stats below)
[2024-10-11 04:35:54,613][train][INFO] - {"epoch": 1709, "train_loss": "0.433", "train_ntokens": "260874", "train_nsentences": "1750.04", "train_wps": "114198", "train_ups": "0.44", "train_wpb": "260874", "train_bsz": "1750", "train_num_updates": "81995", "train_lr": "0.000432072", "train_gnorm": "0.372", "train_loss_scale": "2", "train_train_wall": "39", "train_gb_free": "40.5", "train_wall": "1957"}
[2024-10-11 04:35:54,697][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 04:35:54,708][fairseq.trainer][INFO] - begin training epoch 1710
[2024-10-11 04:35:54,709][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 04:37:13,921][train_inner][INFO] - {"epoch": 1710, "update": 1709.104, "loss": "0.436", "ntokens": "260725", "nsentences": "1756.25", "wps": "100702", "ups": "0.39", "wpb": "260725", "bsz": "1756.2", "num_updates": "82000", "lr": "0.000432065", "gnorm": "0.374", "loss_scale": "2", "train_wall": "188", "gb_free": "39.2", "wall": "2036"}
[2024-10-11 04:37:43,736][fairseq_cli.train][INFO] - end of epoch 1710 (average epoch stats below)
[2024-10-11 04:37:43,739][train][INFO] - {"epoch": 1710, "train_loss": "0.433", "train_ntokens": "260847", "train_nsentences": "1750.04", "train_wps": "114739", "train_ups": "0.44", "train_wpb": "260847", "train_bsz": "1750", "train_num_updates": "82043", "train_lr": "0.000432007", "train_gnorm": "0.362", "train_loss_scale": "2", "train_train_wall": "48", "train_gb_free": "40", "train_wall": "2066"}
[2024-10-11 04:37:43,807][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 04:37:43,824][fairseq.trainer][INFO] - begin training epoch 1711
[2024-10-11 04:37:43,825][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 04:39:32,914][fairseq_cli.train][INFO] - end of epoch 1711 (average epoch stats below)
[2024-10-11 04:39:32,919][train][INFO] - {"epoch": 1711, "train_loss": "0.431", "train_ntokens": "260782", "train_nsentences": "1750.04", "train_wps": "114653", "train_ups": "0.44", "train_wpb": "260782", "train_bsz": "1750", "train_num_updates": "82091", "train_lr": "0.000431942", "train_gnorm": "0.343", "train_loss_scale": "2", "train_train_wall": "29", "train_gb_free": "40.3", "train_wall": "2175"}
[2024-10-11 04:39:33,000][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 04:39:33,003][fairseq.trainer][INFO] - begin training epoch 1712
[2024-10-11 04:39:33,003][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 04:41:24,321][fairseq_cli.train][INFO] - end of epoch 1712 (average epoch stats below)
[2024-10-11 04:41:24,324][train][INFO] - {"epoch": 1712, "train_loss": "0.425", "train_ntokens": "260627", "train_nsentences": "1750.04", "train_wps": "112296", "train_ups": "0.43", "train_wpb": "260627", "train_bsz": "1750", "train_num_updates": "82139", "train_lr": "0.000431876", "train_gnorm": "0.367", "train_loss_scale": "2", "train_train_wall": "55", "train_gb_free": "39.6", "train_wall": "2287"}
[2024-10-11 04:41:24,425][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 04:41:24,464][fairseq.trainer][INFO] - begin training epoch 1713
[2024-10-11 04:41:24,465][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 04:43:18,102][fairseq_cli.train][INFO] - end of epoch 1713 (average epoch stats below)
[2024-10-11 04:43:18,105][train][INFO] - {"epoch": 1713, "train_loss": "0.433", "train_ntokens": "260572", "train_nsentences": "1750.04", "train_wps": "109928", "train_ups": "0.42", "train_wpb": "260572", "train_bsz": "1750", "train_num_updates": "82187", "train_lr": "0.000431811", "train_gnorm": "0.372", "train_loss_scale": "2", "train_train_wall": "34", "train_gb_free": "39.3", "train_wall": "2401"}
[2024-10-11 04:43:18,192][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 04:43:18,222][fairseq.trainer][INFO] - begin training epoch 1714
[2024-10-11 04:43:18,222][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 04:44:41,994][train_inner][INFO] - {"epoch": 1714, "update": 1713.271, "loss": "0.429", "ntokens": "260790", "nsentences": "1738.8", "wps": "116406", "ups": "0.45", "wpb": "260790", "bsz": "1738.8", "num_updates": "82200", "lr": "0.000431793", "gnorm": "0.36", "loss_scale": "2", "train_wall": "165", "gb_free": "40.1", "wall": "2484"}
[2024-10-11 04:45:07,553][fairseq_cli.train][INFO] - end of epoch 1714 (average epoch stats below)
[2024-10-11 04:45:07,556][train][INFO] - {"epoch": 1714, "train_loss": "0.425", "train_ntokens": "260587", "train_nsentences": "1750.04", "train_wps": "114284", "train_ups": "0.44", "train_wpb": "260587", "train_bsz": "1750", "train_num_updates": "82235", "train_lr": "0.000431746", "train_gnorm": "0.352", "train_loss_scale": "2", "train_train_wall": "43", "train_gb_free": "40.3", "train_wall": "2510"}
[2024-10-11 04:45:07,619][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 04:45:07,622][fairseq.trainer][INFO] - begin training epoch 1715
[2024-10-11 04:45:07,622][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 04:46:54,796][fairseq_cli.train][INFO] - end of epoch 1715 (average epoch stats below)
[2024-10-11 04:46:54,813][train][INFO] - {"epoch": 1715, "train_loss": "0.431", "train_ntokens": "260474", "train_nsentences": "1750.04", "train_wps": "116572", "train_ups": "0.45", "train_wpb": "260474", "train_bsz": "1750", "train_num_updates": "82283", "train_lr": "0.000431681", "train_gnorm": "0.348", "train_loss_scale": "2", "train_train_wall": "45", "train_gb_free": "39.2", "train_wall": "2617"}
[2024-10-11 04:46:54,898][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 04:46:54,900][fairseq.trainer][INFO] - begin training epoch 1716
[2024-10-11 04:46:54,901][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 04:48:44,742][fairseq_cli.train][INFO] - end of epoch 1716 (average epoch stats below)
[2024-10-11 04:48:44,763][train][INFO] - {"epoch": 1716, "train_loss": "0.428", "train_ntokens": "260823", "train_nsentences": "1750.04", "train_wps": "113889", "train_ups": "0.44", "train_wpb": "260823", "train_bsz": "1750", "train_num_updates": "82331", "train_lr": "0.000431615", "train_gnorm": "0.352", "train_loss_scale": "2", "train_train_wall": "47", "train_gb_free": "39.8", "train_wall": "2727"}
[2024-10-11 04:48:44,827][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 04:48:44,840][fairseq.trainer][INFO] - begin training epoch 1717
[2024-10-11 04:48:44,840][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 04:50:34,439][fairseq_cli.train][INFO] - end of epoch 1717 (average epoch stats below)
[2024-10-11 04:50:34,456][train][INFO] - {"epoch": 1717, "train_loss": "0.433", "train_ntokens": "260476", "train_nsentences": "1750.04", "train_wps": "113996", "train_ups": "0.44", "train_wpb": "260476", "train_bsz": "1750", "train_num_updates": "82379", "train_lr": "0.00043155", "train_gnorm": "0.376", "train_loss_scale": "2", "train_train_wall": "43", "train_gb_free": "39.8", "train_wall": "2837"}
[2024-10-11 04:50:34,539][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 04:50:34,541][fairseq.trainer][INFO] - begin training epoch 1718
[2024-10-11 04:50:34,542][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 04:52:02,551][train_inner][INFO] - {"epoch": 1718, "update": 1717.438, "loss": "0.431", "ntokens": "260576", "nsentences": "1758.02", "wps": "118295", "ups": "0.45", "wpb": "260576", "bsz": "1758", "num_updates": "82400", "lr": "0.000431522", "gnorm": "0.356", "loss_scale": "2", "train_wall": "182", "gb_free": "39.7", "wall": "2925"}
[2024-10-11 04:52:22,667][fairseq_cli.train][INFO] - end of epoch 1718 (average epoch stats below)
[2024-10-11 04:52:22,669][train][INFO] - {"epoch": 1718, "train_loss": "0.435", "train_ntokens": "260392", "train_nsentences": "1750.04", "train_wps": "115504", "train_ups": "0.44", "train_wpb": "260392", "train_bsz": "1750", "train_num_updates": "82427", "train_lr": "0.000431485", "train_gnorm": "0.352", "train_loss_scale": "2", "train_train_wall": "42", "train_gb_free": "40.6", "train_wall": "2945"}
[2024-10-11 04:52:22,721][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 04:52:22,723][fairseq.trainer][INFO] - begin training epoch 1719
[2024-10-11 04:52:22,723][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 04:54:11,942][fairseq_cli.train][INFO] - end of epoch 1719 (average epoch stats below)
[2024-10-11 04:54:11,945][train][INFO] - {"epoch": 1719, "train_loss": "0.427", "train_ntokens": "260886", "train_nsentences": "1750.04", "train_wps": "114597", "train_ups": "0.44", "train_wpb": "260886", "train_bsz": "1750", "train_num_updates": "82475", "train_lr": "0.00043142", "train_gnorm": "0.36", "train_loss_scale": "2", "train_train_wall": "42", "train_gb_free": "39.7", "train_wall": "3054"}
[2024-10-11 04:54:11,999][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 04:54:12,002][fairseq.trainer][INFO] - begin training epoch 1720
[2024-10-11 04:54:12,003][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 04:56:01,625][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 1720 @ 82523 updates
[2024-10-11 04:56:01,626][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-11 04:56:12,397][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-11 04:56:12,409][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 1720 @ 82523 updates, score None) (writing took 10.784223177470267 seconds)
[2024-10-11 04:56:12,409][fairseq_cli.train][INFO] - end of epoch 1720 (average epoch stats below)
[2024-10-11 04:56:12,412][train][INFO] - {"epoch": 1720, "train_loss": "0.437", "train_ntokens": "260722", "train_nsentences": "1750.04", "train_wps": "103887", "train_ups": "0.4", "train_wpb": "260722", "train_bsz": "1750", "train_num_updates": "82523", "train_lr": "0.000431355", "train_gnorm": "0.358", "train_loss_scale": "2", "train_train_wall": "46", "train_gb_free": "39.9", "train_wall": "3175"}
[2024-10-11 04:56:12,457][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 04:56:12,492][fairseq.trainer][INFO] - begin training epoch 1721
[2024-10-11 04:56:12,492][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 04:58:00,262][fairseq_cli.train][INFO] - end of epoch 1721 (average epoch stats below)
[2024-10-11 04:58:00,270][train][INFO] - {"epoch": 1721, "train_loss": "0.423", "train_ntokens": "260931", "train_nsentences": "1750.04", "train_wps": "116125", "train_ups": "0.45", "train_wpb": "260931", "train_bsz": "1750", "train_num_updates": "82571", "train_lr": "0.000431289", "train_gnorm": "0.36", "train_loss_scale": "2", "train_train_wall": "42", "train_gb_free": "40.1", "train_wall": "3283"}
[2024-10-11 04:58:00,350][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 04:58:00,353][fairseq.trainer][INFO] - begin training epoch 1722
[2024-10-11 04:58:00,353][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 04:59:34,018][train_inner][INFO] - {"epoch": 1722, "update": 1721.604, "loss": "0.43", "ntokens": "260726", "nsentences": "1737.28", "wps": "115502", "ups": "0.44", "wpb": "260726", "bsz": "1737.3", "num_updates": "82600", "lr": "0.00043125", "gnorm": "0.357", "loss_scale": "2", "train_wall": "178", "gb_free": "40.1", "wall": "3376"}
[2024-10-11 04:59:48,664][fairseq_cli.train][INFO] - end of epoch 1722 (average epoch stats below)
[2024-10-11 04:59:48,666][train][INFO] - {"epoch": 1722, "train_loss": "0.43", "train_ntokens": "260664", "train_nsentences": "1750.04", "train_wps": "115430", "train_ups": "0.44", "train_wpb": "260664", "train_bsz": "1750", "train_num_updates": "82619", "train_lr": "0.000431224", "train_gnorm": "0.344", "train_loss_scale": "2", "train_train_wall": "43", "train_gb_free": "40", "train_wall": "3391"}
[2024-10-11 04:59:48,716][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 04:59:48,719][fairseq.trainer][INFO] - begin training epoch 1723
[2024-10-11 04:59:48,719][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 05:01:38,184][fairseq_cli.train][INFO] - end of epoch 1723 (average epoch stats below)
[2024-10-11 05:01:38,187][train][INFO] - {"epoch": 1723, "train_loss": "0.428", "train_ntokens": "260777", "train_nsentences": "1750.04", "train_wps": "114293", "train_ups": "0.44", "train_wpb": "260777", "train_bsz": "1750", "train_num_updates": "82667", "train_lr": "0.000431159", "train_gnorm": "0.379", "train_loss_scale": "2", "train_train_wall": "46", "train_gb_free": "39.8", "train_wall": "3501"}
[2024-10-11 05:01:38,261][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 05:01:38,266][fairseq.trainer][INFO] - begin training epoch 1724
[2024-10-11 05:01:38,267][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 05:03:27,651][fairseq_cli.train][INFO] - end of epoch 1724 (average epoch stats below)
[2024-10-11 05:03:27,667][train][INFO] - {"epoch": 1724, "train_loss": "0.434", "train_ntokens": "260742", "train_nsentences": "1750.04", "train_wps": "114323", "train_ups": "0.44", "train_wpb": "260742", "train_bsz": "1750", "train_num_updates": "82715", "train_lr": "0.000431094", "train_gnorm": "0.342", "train_loss_scale": "2", "train_train_wall": "51", "train_gb_free": "40.7", "train_wall": "3610"}
[2024-10-11 05:03:27,783][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 05:03:27,801][fairseq.trainer][INFO] - begin training epoch 1725
[2024-10-11 05:03:27,801][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 05:05:19,337][fairseq_cli.train][INFO] - end of epoch 1725 (average epoch stats below)
[2024-10-11 05:05:19,345][train][INFO] - {"epoch": 1725, "train_loss": "0.43", "train_ntokens": "260656", "train_nsentences": "1750.04", "train_wps": "112035", "train_ups": "0.43", "train_wpb": "260656", "train_bsz": "1750", "train_num_updates": "82763", "train_lr": "0.000431029", "train_gnorm": "0.359", "train_loss_scale": "2", "train_train_wall": "52", "train_gb_free": "39.6", "train_wall": "3722"}
[2024-10-11 05:05:19,477][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 05:05:19,483][fairseq.trainer][INFO] - begin training epoch 1726
[2024-10-11 05:05:19,483][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 05:06:59,625][train_inner][INFO] - {"epoch": 1726, "update": 1725.771, "loss": "0.43", "ntokens": "260628", "nsentences": "1778.09", "wps": "116978", "ups": "0.45", "wpb": "260628", "bsz": "1778.1", "num_updates": "82800", "lr": "0.000430978", "gnorm": "0.358", "loss_scale": "2", "train_wall": "202", "gb_free": "40.1", "wall": "3822"}
[2024-10-11 05:07:09,788][fairseq_cli.train][INFO] - end of epoch 1726 (average epoch stats below)
[2024-10-11 05:07:09,791][train][INFO] - {"epoch": 1726, "train_loss": "0.427", "train_ntokens": "260419", "train_nsentences": "1750.04", "train_wps": "113181", "train_ups": "0.43", "train_wpb": "260419", "train_bsz": "1750", "train_num_updates": "82811", "train_lr": "0.000430963", "train_gnorm": "0.362", "train_loss_scale": "2", "train_train_wall": "49", "train_gb_free": "39.2", "train_wall": "3832"}
[2024-10-11 05:07:09,861][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 05:07:09,865][fairseq.trainer][INFO] - begin training epoch 1727
[2024-10-11 05:07:09,866][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 05:09:02,855][fairseq_cli.train][INFO] - end of epoch 1727 (average epoch stats below)
[2024-10-11 05:09:02,872][train][INFO] - {"epoch": 1727, "train_loss": "0.437", "train_ntokens": "260761", "train_nsentences": "1750.04", "train_wps": "110701", "train_ups": "0.42", "train_wpb": "260761", "train_bsz": "1750", "train_num_updates": "82859", "train_lr": "0.000430898", "train_gnorm": "0.395", "train_loss_scale": "2", "train_train_wall": "33", "train_gb_free": "40.8", "train_wall": "3945"}
[2024-10-11 05:09:02,978][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 05:09:02,982][fairseq.trainer][INFO] - begin training epoch 1728
[2024-10-11 05:09:02,982][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 05:10:51,857][fairseq_cli.train][INFO] - end of epoch 1728 (average epoch stats below)
[2024-10-11 05:10:51,859][train][INFO] - {"epoch": 1728, "train_loss": "0.429", "train_ntokens": "260402", "train_nsentences": "1750.04", "train_wps": "114689", "train_ups": "0.44", "train_wpb": "260402", "train_bsz": "1750", "train_num_updates": "82907", "train_lr": "0.000430833", "train_gnorm": "0.371", "train_loss_scale": "2", "train_train_wall": "49", "train_gb_free": "39.7", "train_wall": "4054"}
[2024-10-11 05:10:51,921][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 05:10:51,924][fairseq.trainer][INFO] - begin training epoch 1729
[2024-10-11 05:10:51,924][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 05:12:40,046][fairseq_cli.train][INFO] - end of epoch 1729 (average epoch stats below)
[2024-10-11 05:12:40,051][train][INFO] - {"epoch": 1729, "train_loss": "0.431", "train_ntokens": "260772", "train_nsentences": "1750.04", "train_wps": "115697", "train_ups": "0.44", "train_wpb": "260772", "train_bsz": "1750", "train_num_updates": "82955", "train_lr": "0.000430768", "train_gnorm": "0.362", "train_loss_scale": "2", "train_train_wall": "33", "train_gb_free": "40.5", "train_wall": "4162"}
[2024-10-11 05:12:40,117][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 05:12:40,122][fairseq.trainer][INFO] - begin training epoch 1730
[2024-10-11 05:12:40,123][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 05:14:29,184][train_inner][INFO] - {"epoch": 1730, "update": 1729.938, "loss": "0.429", "ntokens": "260677", "nsentences": "1730.76", "wps": "115989", "ups": "0.44", "wpb": "260677", "bsz": "1730.8", "num_updates": "83000", "lr": "0.000430707", "gnorm": "0.37", "loss_scale": "2", "train_wall": "160", "gb_free": "39.8", "wall": "4272"}
[2024-10-11 05:14:29,816][fairseq_cli.train][INFO] - end of epoch 1730 (average epoch stats below)
[2024-10-11 05:14:29,818][train][INFO] - {"epoch": 1730, "train_loss": "0.418", "train_ntokens": "260668", "train_nsentences": "1750.04", "train_wps": "113990", "train_ups": "0.44", "train_wpb": "260668", "train_bsz": "1750", "train_num_updates": "83003", "train_lr": "0.000430702", "train_gnorm": "0.345", "train_loss_scale": "2", "train_train_wall": "36", "train_gb_free": "39.6", "train_wall": "4272"}
[2024-10-11 05:14:29,867][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 05:14:29,870][fairseq.trainer][INFO] - begin training epoch 1731
[2024-10-11 05:14:29,870][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 05:16:19,172][fairseq_cli.train][INFO] - end of epoch 1731 (average epoch stats below)
[2024-10-11 05:16:19,175][train][INFO] - {"epoch": 1731, "train_loss": "0.429", "train_ntokens": "260515", "train_nsentences": "1750.04", "train_wps": "114350", "train_ups": "0.44", "train_wpb": "260515", "train_bsz": "1750", "train_num_updates": "83051", "train_lr": "0.000430637", "train_gnorm": "0.372", "train_loss_scale": "2", "train_train_wall": "38", "train_gb_free": "39.2", "train_wall": "4382"}
[2024-10-11 05:16:19,250][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 05:16:19,253][fairseq.trainer][INFO] - begin training epoch 1732
[2024-10-11 05:16:19,253][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 05:18:06,283][fairseq_cli.train][INFO] - end of epoch 1732 (average epoch stats below)
[2024-10-11 05:18:06,287][train][INFO] - {"epoch": 1732, "train_loss": "0.427", "train_ntokens": "260701", "train_nsentences": "1750.04", "train_wps": "116832", "train_ups": "0.45", "train_wpb": "260701", "train_bsz": "1750", "train_num_updates": "83099", "train_lr": "0.000430572", "train_gnorm": "0.386", "train_loss_scale": "2", "train_train_wall": "37", "train_gb_free": "41", "train_wall": "4489"}
[2024-10-11 05:18:06,432][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 05:18:06,446][fairseq.trainer][INFO] - begin training epoch 1733
[2024-10-11 05:18:06,447][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 05:19:57,517][fairseq_cli.train][INFO] - end of epoch 1733 (average epoch stats below)
[2024-10-11 05:19:57,520][train][INFO] - {"epoch": 1733, "train_loss": "0.433", "train_ntokens": "261254", "train_nsentences": "1750.04", "train_wps": "112740", "train_ups": "0.43", "train_wpb": "261254", "train_bsz": "1750", "train_num_updates": "83147", "train_lr": "0.000430507", "train_gnorm": "0.358", "train_loss_scale": "2", "train_train_wall": "43", "train_gb_free": "39.6", "train_wall": "4600"}
[2024-10-11 05:19:57,573][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 05:19:57,575][fairseq.trainer][INFO] - begin training epoch 1734
[2024-10-11 05:19:57,576][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 05:21:48,026][fairseq_cli.train][INFO] - end of epoch 1734 (average epoch stats below)
[2024-10-11 05:21:48,030][train][INFO] - {"epoch": 1734, "train_loss": "0.437", "train_ntokens": "260715", "train_nsentences": "1750.04", "train_wps": "113245", "train_ups": "0.43", "train_wpb": "260715", "train_bsz": "1750", "train_num_updates": "83195", "train_lr": "0.000430442", "train_gnorm": "0.347", "train_loss_scale": "2", "train_train_wall": "41", "train_gb_free": "40", "train_wall": "4710"}
[2024-10-11 05:21:48,114][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 05:21:48,118][fairseq.trainer][INFO] - begin training epoch 1735
[2024-10-11 05:21:48,118][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 05:23:06,249][train_inner][INFO] - {"epoch": 1735, "update": 1734.104, "loss": "0.43", "ntokens": "260848", "nsentences": "1751.08", "wps": "100897", "ups": "0.39", "wpb": "260848", "bsz": "1751.1", "num_updates": "83200", "lr": "0.000430435", "gnorm": "0.364", "loss_scale": "2", "train_wall": "172", "gb_free": "40.1", "wall": "4789"}
[2024-10-11 05:23:35,701][fairseq_cli.train][INFO] - end of epoch 1735 (average epoch stats below)
[2024-10-11 05:23:35,703][train][INFO] - {"epoch": 1735, "train_loss": "0.424", "train_ntokens": "261127", "train_nsentences": "1750.04", "train_wps": "116412", "train_ups": "0.45", "train_wpb": "261127", "train_bsz": "1750", "train_num_updates": "83243", "train_lr": "0.000430376", "train_gnorm": "0.369", "train_loss_scale": "2", "train_train_wall": "41", "train_gb_free": "39.7", "train_wall": "4818"}
[2024-10-11 05:23:35,834][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 05:23:35,846][fairseq.trainer][INFO] - begin training epoch 1736
[2024-10-11 05:23:35,847][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 05:25:27,661][fairseq_cli.train][INFO] - end of epoch 1736 (average epoch stats below)
[2024-10-11 05:25:27,666][train][INFO] - {"epoch": 1736, "train_loss": "0.428", "train_ntokens": "260685", "train_nsentences": "1750.04", "train_wps": "111762", "train_ups": "0.43", "train_wpb": "260685", "train_bsz": "1750", "train_num_updates": "83291", "train_lr": "0.000430311", "train_gnorm": "0.37", "train_loss_scale": "2", "train_train_wall": "45", "train_gb_free": "40.5", "train_wall": "4930"}
[2024-10-11 05:25:27,739][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 05:25:27,742][fairseq.trainer][INFO] - begin training epoch 1737
[2024-10-11 05:25:27,742][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 05:27:19,233][fairseq_cli.train][INFO] - end of epoch 1737 (average epoch stats below)
[2024-10-11 05:27:19,236][train][INFO] - {"epoch": 1737, "train_loss": "0.433", "train_ntokens": "260712", "train_nsentences": "1750.04", "train_wps": "112168", "train_ups": "0.43", "train_wpb": "260712", "train_bsz": "1750", "train_num_updates": "83339", "train_lr": "0.000430246", "train_gnorm": "0.358", "train_loss_scale": "2", "train_train_wall": "48", "train_gb_free": "39.3", "train_wall": "5042"}
[2024-10-11 05:27:19,284][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 05:27:19,288][fairseq.trainer][INFO] - begin training epoch 1738
[2024-10-11 05:27:19,288][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 05:29:05,182][fairseq_cli.train][INFO] - end of epoch 1738 (average epoch stats below)
[2024-10-11 05:29:05,186][train][INFO] - {"epoch": 1738, "train_loss": "0.427", "train_ntokens": "260969", "train_nsentences": "1750.04", "train_wps": "118233", "train_ups": "0.45", "train_wpb": "260969", "train_bsz": "1750", "train_num_updates": "83387", "train_lr": "0.000430181", "train_gnorm": "0.371", "train_loss_scale": "2", "train_train_wall": "37", "train_gb_free": "39.3", "train_wall": "5148"}
[2024-10-11 05:29:05,270][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 05:29:05,274][fairseq.trainer][INFO] - begin training epoch 1739
[2024-10-11 05:29:05,277][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 05:30:28,628][train_inner][INFO] - {"epoch": 1739, "update": 1738.271, "loss": "0.427", "ntokens": "260797", "nsentences": "1748.99", "wps": "117908", "ups": "0.45", "wpb": "260797", "bsz": "1749", "num_updates": "83400", "lr": "0.000430163", "gnorm": "0.365", "loss_scale": "2", "train_wall": "186", "gb_free": "39.6", "wall": "5231"}
[2024-10-11 05:30:56,349][fairseq_cli.train][INFO] - end of epoch 1739 (average epoch stats below)
[2024-10-11 05:30:56,352][train][INFO] - {"epoch": 1739, "train_loss": "0.428", "train_ntokens": "260662", "train_nsentences": "1750.04", "train_wps": "112555", "train_ups": "0.43", "train_wpb": "260662", "train_bsz": "1750", "train_num_updates": "83435", "train_lr": "0.000430115", "train_gnorm": "0.364", "train_loss_scale": "2", "train_train_wall": "55", "train_gb_free": "40.5", "train_wall": "5259"}
[2024-10-11 05:30:56,550][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 05:30:56,553][fairseq.trainer][INFO] - begin training epoch 1740
[2024-10-11 05:30:56,553][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 05:32:47,035][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 1740 @ 83483 updates
[2024-10-11 05:32:47,036][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-11 05:32:58,464][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-11 05:32:58,466][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 1740 @ 83483 updates, score None) (writing took 11.430768998339772 seconds)
[2024-10-11 05:32:58,466][fairseq_cli.train][INFO] - end of epoch 1740 (average epoch stats below)
[2024-10-11 05:32:58,468][train][INFO] - {"epoch": 1740, "train_loss": "0.427", "train_ntokens": "260858", "train_nsentences": "1750.04", "train_wps": "102542", "train_ups": "0.39", "train_wpb": "260858", "train_bsz": "1750", "train_num_updates": "83483", "train_lr": "0.00043005", "train_gnorm": "0.362", "train_loss_scale": "2", "train_train_wall": "43", "train_gb_free": "39.3", "train_wall": "5381"}
[2024-10-11 05:32:58,517][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 05:32:58,530][fairseq.trainer][INFO] - begin training epoch 1741
[2024-10-11 05:32:58,530][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 05:34:45,769][fairseq_cli.train][INFO] - end of epoch 1741 (average epoch stats below)
[2024-10-11 05:34:45,775][train][INFO] - {"epoch": 1741, "train_loss": "0.423", "train_ntokens": "260568", "train_nsentences": "1750.04", "train_wps": "116559", "train_ups": "0.45", "train_wpb": "260568", "train_bsz": "1750", "train_num_updates": "83531", "train_lr": "0.000429985", "train_gnorm": "0.361", "train_loss_scale": "2", "train_train_wall": "33", "train_gb_free": "39.6", "train_wall": "5488"}
[2024-10-11 05:34:45,890][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 05:34:45,896][fairseq.trainer][INFO] - begin training epoch 1742
[2024-10-11 05:34:45,896][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 05:36:35,275][fairseq_cli.train][INFO] - end of epoch 1742 (average epoch stats below)
[2024-10-11 05:36:35,287][train][INFO] - {"epoch": 1742, "train_loss": "0.43", "train_ntokens": "261058", "train_nsentences": "1750.04", "train_wps": "114430", "train_ups": "0.44", "train_wpb": "261058", "train_bsz": "1750", "train_num_updates": "83579", "train_lr": "0.00042992", "train_gnorm": "0.372", "train_loss_scale": "2", "train_train_wall": "39", "train_gb_free": "39.6", "train_wall": "5598"}
[2024-10-11 05:36:35,374][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 05:36:35,377][fairseq.trainer][INFO] - begin training epoch 1743
[2024-10-11 05:36:35,377][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 05:38:09,101][train_inner][INFO] - {"epoch": 1743, "update": 1742.438, "loss": "0.428", "ntokens": "260720", "nsentences": "1756.05", "wps": "113242", "ups": "0.43", "wpb": "260720", "bsz": "1756", "num_updates": "83600", "lr": "0.000429891", "gnorm": "0.373", "loss_scale": "2", "train_wall": "166", "gb_free": "40.3", "wall": "5692"}
[2024-10-11 05:38:29,342][fairseq_cli.train][INFO] - end of epoch 1743 (average epoch stats below)
[2024-10-11 05:38:29,344][train][INFO] - {"epoch": 1743, "train_loss": "0.43", "train_ntokens": "260475", "train_nsentences": "1750.04", "train_wps": "109622", "train_ups": "0.42", "train_wpb": "260475", "train_bsz": "1750", "train_num_updates": "83627", "train_lr": "0.000429855", "train_gnorm": "0.397", "train_loss_scale": "2", "train_train_wall": "45", "train_gb_free": "41", "train_wall": "5712"}
[2024-10-11 05:38:29,402][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 05:38:29,409][fairseq.trainer][INFO] - begin training epoch 1744
[2024-10-11 05:38:29,409][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 05:40:20,300][fairseq_cli.train][INFO] - end of epoch 1744 (average epoch stats below)
[2024-10-11 05:40:20,303][train][INFO] - {"epoch": 1744, "train_loss": "0.426", "train_ntokens": "260448", "train_nsentences": "1750.04", "train_wps": "112671", "train_ups": "0.43", "train_wpb": "260448", "train_bsz": "1750", "train_num_updates": "83675", "train_lr": "0.000429789", "train_gnorm": "0.364", "train_loss_scale": "2", "train_train_wall": "48", "train_gb_free": "39.4", "train_wall": "5823"}
[2024-10-11 05:40:20,352][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 05:40:20,355][fairseq.trainer][INFO] - begin training epoch 1745
[2024-10-11 05:40:20,355][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 05:42:08,526][fairseq_cli.train][INFO] - end of epoch 1745 (average epoch stats below)
[2024-10-11 05:42:08,530][train][INFO] - {"epoch": 1745, "train_loss": "0.431", "train_ntokens": "260806", "train_nsentences": "1750.04", "train_wps": "115673", "train_ups": "0.44", "train_wpb": "260806", "train_bsz": "1750", "train_num_updates": "83723", "train_lr": "0.000429724", "train_gnorm": "0.362", "train_loss_scale": "2", "train_train_wall": "44", "train_gb_free": "39.6", "train_wall": "5931"}
[2024-10-11 05:42:08,629][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 05:42:08,631][fairseq.trainer][INFO] - begin training epoch 1746
[2024-10-11 05:42:08,632][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 05:43:58,485][fairseq_cli.train][INFO] - end of epoch 1746 (average epoch stats below)
[2024-10-11 05:43:58,487][train][INFO] - {"epoch": 1746, "train_loss": "0.431", "train_ntokens": "260678", "train_nsentences": "1750.04", "train_wps": "113799", "train_ups": "0.44", "train_wpb": "260678", "train_bsz": "1750", "train_num_updates": "83771", "train_lr": "0.000429659", "train_gnorm": "0.366", "train_loss_scale": "2", "train_train_wall": "46", "train_gb_free": "39.3", "train_wall": "6041"}
[2024-10-11 05:43:58,536][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 05:43:58,539][fairseq.trainer][INFO] - begin training epoch 1747
[2024-10-11 05:43:58,539][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 05:45:32,661][train_inner][INFO] - {"epoch": 1747, "update": 1746.604, "loss": "0.429", "ntokens": "260670", "nsentences": "1748.89", "wps": "117536", "ups": "0.45", "wpb": "260670", "bsz": "1748.9", "num_updates": "83800", "lr": "0.00042962", "gnorm": "0.363", "loss_scale": "2", "train_wall": "188", "gb_free": "39.4", "wall": "6135"}
[2024-10-11 05:45:48,888][fairseq_cli.train][INFO] - end of epoch 1747 (average epoch stats below)
[2024-10-11 05:45:48,890][train][INFO] - {"epoch": 1747, "train_loss": "0.435", "train_ntokens": "260871", "train_nsentences": "1750.04", "train_wps": "113422", "train_ups": "0.43", "train_wpb": "260872", "train_bsz": "1750", "train_num_updates": "83819", "train_lr": "0.000429594", "train_gnorm": "0.355", "train_loss_scale": "2", "train_train_wall": "47", "train_gb_free": "39.6", "train_wall": "6151"}
[2024-10-11 05:45:48,973][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 05:45:48,989][fairseq.trainer][INFO] - begin training epoch 1748
[2024-10-11 05:45:48,989][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 05:47:38,820][fairseq_cli.train][INFO] - end of epoch 1748 (average epoch stats below)
[2024-10-11 05:47:38,825][train][INFO] - {"epoch": 1748, "train_loss": "0.437", "train_ntokens": "260688", "train_nsentences": "1750.04", "train_wps": "113825", "train_ups": "0.44", "train_wpb": "260688", "train_bsz": "1750", "train_num_updates": "83867", "train_lr": "0.000429529", "train_gnorm": "0.36", "train_loss_scale": "2", "train_train_wall": "45", "train_gb_free": "41.1", "train_wall": "6261"}
[2024-10-11 05:47:38,899][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 05:47:38,903][fairseq.trainer][INFO] - begin training epoch 1749
[2024-10-11 05:47:38,904][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 05:49:28,304][fairseq_cli.train][INFO] - end of epoch 1749 (average epoch stats below)
[2024-10-11 05:49:28,316][train][INFO] - {"epoch": 1749, "train_loss": "0.429", "train_ntokens": "260619", "train_nsentences": "1750.04", "train_wps": "114257", "train_ups": "0.44", "train_wpb": "260619", "train_bsz": "1750", "train_num_updates": "83915", "train_lr": "0.000429463", "train_gnorm": "0.349", "train_loss_scale": "4", "train_train_wall": "47", "train_gb_free": "39.8", "train_wall": "6371"}
[2024-10-11 05:49:28,399][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 05:49:28,402][fairseq.trainer][INFO] - begin training epoch 1750
[2024-10-11 05:49:28,403][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 05:51:20,718][fairseq_cli.train][INFO] - end of epoch 1750 (average epoch stats below)
[2024-10-11 05:51:20,724][train][INFO] - {"epoch": 1750, "train_loss": "0.43", "train_ntokens": "260354", "train_nsentences": "1750.04", "train_wps": "111181", "train_ups": "0.43", "train_wpb": "260354", "train_bsz": "1750", "train_num_updates": "83963", "train_lr": "0.000429398", "train_gnorm": "0.363", "train_loss_scale": "4", "train_train_wall": "47", "train_gb_free": "39.7", "train_wall": "6483"}
[2024-10-11 05:51:20,814][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 05:51:20,817][fairseq.trainer][INFO] - begin training epoch 1751
[2024-10-11 05:51:20,818][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 05:53:01,465][train_inner][INFO] - {"epoch": 1751, "update": 1750.771, "loss": "0.433", "ntokens": "260655", "nsentences": "1753.46", "wps": "116164", "ups": "0.45", "wpb": "260655", "bsz": "1753.5", "num_updates": "84000", "lr": "0.000429348", "gnorm": "0.36", "loss_scale": "4", "train_wall": "198", "gb_free": "40", "wall": "6584"}
[2024-10-11 05:53:12,645][fairseq_cli.train][INFO] - end of epoch 1751 (average epoch stats below)
[2024-10-11 05:53:12,647][train][INFO] - {"epoch": 1751, "train_loss": "0.43", "train_ntokens": "260914", "train_nsentences": "1750.04", "train_wps": "111900", "train_ups": "0.43", "train_wpb": "260914", "train_bsz": "1750", "train_num_updates": "84011", "train_lr": "0.000429333", "train_gnorm": "0.359", "train_loss_scale": "4", "train_train_wall": "53", "train_gb_free": "40.1", "train_wall": "6595"}
[2024-10-11 05:53:12,701][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 05:53:12,709][fairseq.trainer][INFO] - begin training epoch 1752
[2024-10-11 05:53:12,709][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 05:55:01,011][fairseq_cli.train][INFO] - end of epoch 1752 (average epoch stats below)
[2024-10-11 05:55:01,014][train][INFO] - {"epoch": 1752, "train_loss": "0.429", "train_ntokens": "260708", "train_nsentences": "1750.04", "train_wps": "115480", "train_ups": "0.44", "train_wpb": "260708", "train_bsz": "1750", "train_num_updates": "84059", "train_lr": "0.000429268", "train_gnorm": "0.377", "train_loss_scale": "4", "train_train_wall": "33", "train_gb_free": "39.4", "train_wall": "6703"}
[2024-10-11 05:55:01,096][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 05:55:01,108][fairseq.trainer][INFO] - begin training epoch 1753
[2024-10-11 05:55:01,109][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 05:56:49,823][fairseq_cli.train][INFO] - end of epoch 1753 (average epoch stats below)
[2024-10-11 05:56:49,828][train][INFO] - {"epoch": 1753, "train_loss": "0.429", "train_ntokens": "260941", "train_nsentences": "1750.04", "train_wps": "115110", "train_ups": "0.44", "train_wpb": "260941", "train_bsz": "1750", "train_num_updates": "84107", "train_lr": "0.000429202", "train_gnorm": "0.377", "train_loss_scale": "4", "train_train_wall": "45", "train_gb_free": "39.4", "train_wall": "6812"}
[2024-10-11 05:56:49,878][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 05:56:49,881][fairseq.trainer][INFO] - begin training epoch 1754
[2024-10-11 05:56:49,881][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 05:58:44,622][fairseq_cli.train][INFO] - end of epoch 1754 (average epoch stats below)
[2024-10-11 05:58:44,625][train][INFO] - {"epoch": 1754, "train_loss": "0.426", "train_ntokens": "260669", "train_nsentences": "1750.04", "train_wps": "108996", "train_ups": "0.42", "train_wpb": "260669", "train_bsz": "1750", "train_num_updates": "84155", "train_lr": "0.000429137", "train_gnorm": "0.342", "train_loss_scale": "4", "train_train_wall": "59", "train_gb_free": "40.5", "train_wall": "6927"}
[2024-10-11 05:58:44,675][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 05:58:44,678][fairseq.trainer][INFO] - begin training epoch 1755
[2024-10-11 05:58:44,678][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 06:00:34,718][train_inner][INFO] - {"epoch": 1755, "update": 1754.938, "loss": "0.428", "ntokens": "260820", "nsentences": "1742.85", "wps": "115098", "ups": "0.44", "wpb": "260820", "bsz": "1742.9", "num_updates": "84200", "lr": "0.000429076", "gnorm": "0.362", "loss_scale": "4", "train_wall": "193", "gb_free": "40.3", "wall": "7037"}
[2024-10-11 06:00:35,338][fairseq_cli.train][INFO] - end of epoch 1755 (average epoch stats below)
[2024-10-11 06:00:35,339][train][INFO] - {"epoch": 1755, "train_loss": "0.431", "train_ntokens": "260980", "train_nsentences": "1750.04", "train_wps": "113149", "train_ups": "0.43", "train_wpb": "260980", "train_bsz": "1750", "train_num_updates": "84203", "train_lr": "0.000429072", "train_gnorm": "0.365", "train_loss_scale": "4", "train_train_wall": "46", "train_gb_free": "39.6", "train_wall": "7038"}
[2024-10-11 06:00:35,402][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 06:00:35,409][fairseq.trainer][INFO] - begin training epoch 1756
[2024-10-11 06:00:35,410][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 06:02:24,862][fairseq_cli.train][INFO] - end of epoch 1756 (average epoch stats below)
[2024-10-11 06:02:24,883][train][INFO] - {"epoch": 1756, "train_loss": "0.428", "train_ntokens": "260679", "train_nsentences": "1750.04", "train_wps": "114242", "train_ups": "0.44", "train_wpb": "260679", "train_bsz": "1750", "train_num_updates": "84251", "train_lr": "0.000429007", "train_gnorm": "0.366", "train_loss_scale": "4", "train_train_wall": "46", "train_gb_free": "39.2", "train_wall": "7147"}
[2024-10-11 06:02:24,954][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 06:02:24,970][fairseq.trainer][INFO] - begin training epoch 1757
[2024-10-11 06:02:24,971][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 06:04:13,967][fairseq_cli.train][INFO] - end of epoch 1757 (average epoch stats below)
[2024-10-11 06:04:13,983][train][INFO] - {"epoch": 1757, "train_loss": "0.427", "train_ntokens": "260878", "train_nsentences": "1750.04", "train_wps": "114780", "train_ups": "0.44", "train_wpb": "260878", "train_bsz": "1750", "train_num_updates": "84299", "train_lr": "0.000428942", "train_gnorm": "0.358", "train_loss_scale": "4", "train_train_wall": "40", "train_gb_free": "39.6", "train_wall": "7256"}
[2024-10-11 06:04:14,083][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 06:04:14,086][fairseq.trainer][INFO] - begin training epoch 1758
[2024-10-11 06:04:14,087][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 06:06:04,112][fairseq_cli.train][INFO] - end of epoch 1758 (average epoch stats below)
[2024-10-11 06:06:04,117][train][INFO] - {"epoch": 1758, "train_loss": "0.426", "train_ntokens": "260634", "train_nsentences": "1750.04", "train_wps": "113595", "train_ups": "0.44", "train_wpb": "260634", "train_bsz": "1750", "train_num_updates": "84347", "train_lr": "0.000428876", "train_gnorm": "0.364", "train_loss_scale": "4", "train_train_wall": "43", "train_gb_free": "40.2", "train_wall": "7367"}
[2024-10-11 06:06:04,211][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 06:06:04,214][fairseq.trainer][INFO] - begin training epoch 1759
[2024-10-11 06:06:04,214][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 06:07:55,854][fairseq_cli.train][INFO] - end of epoch 1759 (average epoch stats below)
[2024-10-11 06:07:55,858][train][INFO] - {"epoch": 1759, "train_loss": "0.429", "train_ntokens": "260826", "train_nsentences": "1750.04", "train_wps": "112044", "train_ups": "0.43", "train_wpb": "260826", "train_bsz": "1750", "train_num_updates": "84395", "train_lr": "0.000428811", "train_gnorm": "0.357", "train_loss_scale": "4", "train_train_wall": "47", "train_gb_free": "40.3", "train_wall": "7478"}
[2024-10-11 06:07:55,957][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 06:07:55,969][fairseq.trainer][INFO] - begin training epoch 1760
[2024-10-11 06:07:55,969][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 06:09:14,291][train_inner][INFO] - {"epoch": 1760, "update": 1759.104, "loss": "0.427", "ntokens": "260726", "nsentences": "1755", "wps": "100364", "ups": "0.38", "wpb": "260726", "bsz": "1755", "num_updates": "84400", "lr": "0.000428804", "gnorm": "0.362", "loss_scale": "4", "train_wall": "194", "gb_free": "39.6", "wall": "7557"}
[2024-10-11 06:09:43,889][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 1760 @ 84443 updates
[2024-10-11 06:09:43,889][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-11 06:09:54,875][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-11 06:09:54,878][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 1760 @ 84443 updates, score None) (writing took 10.989092842675745 seconds)
[2024-10-11 06:09:54,878][fairseq_cli.train][INFO] - end of epoch 1760 (average epoch stats below)
[2024-10-11 06:09:54,880][train][INFO] - {"epoch": 1760, "train_loss": "0.428", "train_ntokens": "260558", "train_nsentences": "1750.04", "train_wps": "105082", "train_ups": "0.4", "train_wpb": "260558", "train_bsz": "1750", "train_num_updates": "84443", "train_lr": "0.000428746", "train_gnorm": "0.342", "train_loss_scale": "4", "train_train_wall": "45", "train_gb_free": "39.3", "train_wall": "7597"}
[2024-10-11 06:09:54,925][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 06:09:54,929][fairseq.trainer][INFO] - begin training epoch 1761
[2024-10-11 06:09:54,929][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 06:11:40,402][fairseq_cli.train][INFO] - end of epoch 1761 (average epoch stats below)
[2024-10-11 06:11:40,405][train][INFO] - {"epoch": 1761, "train_loss": "0.427", "train_ntokens": "261174", "train_nsentences": "1750.04", "train_wps": "118803", "train_ups": "0.45", "train_wpb": "261174", "train_bsz": "1750", "train_num_updates": "84491", "train_lr": "0.000428681", "train_gnorm": "0.356", "train_loss_scale": "4", "train_train_wall": "43", "train_gb_free": "41", "train_wall": "7703"}
[2024-10-11 06:11:40,506][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 06:11:40,528][fairseq.trainer][INFO] - begin training epoch 1762
[2024-10-11 06:11:40,529][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 06:13:29,026][fairseq_cli.train][INFO] - end of epoch 1762 (average epoch stats below)
[2024-10-11 06:13:29,031][train][INFO] - {"epoch": 1762, "train_loss": "0.425", "train_ntokens": "260447", "train_nsentences": "1750.04", "train_wps": "115089", "train_ups": "0.44", "train_wpb": "260447", "train_bsz": "1750", "train_num_updates": "84539", "train_lr": "0.000428615", "train_gnorm": "0.381", "train_loss_scale": "4", "train_train_wall": "35", "train_gb_free": "39.3", "train_wall": "7811"}
[2024-10-11 06:13:29,120][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 06:13:29,140][fairseq.trainer][INFO] - begin training epoch 1763
[2024-10-11 06:13:29,147][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 06:15:18,008][fairseq_cli.train][INFO] - end of epoch 1763 (average epoch stats below)
[2024-10-11 06:15:18,011][train][INFO] - {"epoch": 1763, "train_loss": "0.424", "train_ntokens": "260850", "train_nsentences": "1750.04", "train_wps": "114895", "train_ups": "0.44", "train_wpb": "260850", "train_bsz": "1750", "train_num_updates": "84587", "train_lr": "0.00042855", "train_gnorm": "0.361", "train_loss_scale": "4", "train_train_wall": "38", "train_gb_free": "40.1", "train_wall": "7920"}
[2024-10-11 06:15:18,092][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 06:15:18,098][fairseq.trainer][INFO] - begin training epoch 1764
[2024-10-11 06:15:18,098][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 06:16:43,841][train_inner][INFO] - {"epoch": 1764, "update": 1763.271, "loss": "0.426", "ntokens": "260754", "nsentences": "1754.05", "wps": "116007", "ups": "0.44", "wpb": "260754", "bsz": "1754", "num_updates": "84600", "lr": "0.000428533", "gnorm": "0.36", "loss_scale": "4", "train_wall": "162", "gb_free": "39.8", "wall": "8006"}
[2024-10-11 06:17:06,735][fairseq_cli.train][INFO] - end of epoch 1764 (average epoch stats below)
[2024-10-11 06:17:06,736][train][INFO] - {"epoch": 1764, "train_loss": "0.428", "train_ntokens": "260368", "train_nsentences": "1750.04", "train_wps": "114949", "train_ups": "0.44", "train_wpb": "260368", "train_bsz": "1750", "train_num_updates": "84635", "train_lr": "0.000428485", "train_gnorm": "0.371", "train_loss_scale": "4", "train_train_wall": "40", "train_gb_free": "39.9", "train_wall": "8029"}
[2024-10-11 06:17:06,823][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 06:17:06,825][fairseq.trainer][INFO] - begin training epoch 1765
[2024-10-11 06:17:06,826][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 06:18:54,725][fairseq_cli.train][INFO] - end of epoch 1765 (average epoch stats below)
[2024-10-11 06:18:54,728][train][INFO] - {"epoch": 1765, "train_loss": "0.422", "train_ntokens": "260581", "train_nsentences": "1750.04", "train_wps": "115825", "train_ups": "0.44", "train_wpb": "260581", "train_bsz": "1750", "train_num_updates": "84683", "train_lr": "0.00042842", "train_gnorm": "0.36", "train_loss_scale": "4", "train_train_wall": "38", "train_gb_free": "39.8", "train_wall": "8137"}
[2024-10-11 06:18:54,812][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 06:18:54,816][fairseq.trainer][INFO] - begin training epoch 1766
[2024-10-11 06:18:54,816][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 06:20:43,109][fairseq_cli.train][INFO] - end of epoch 1766 (average epoch stats below)
[2024-10-11 06:20:43,111][train][INFO] - {"epoch": 1766, "train_loss": "0.437", "train_ntokens": "260641", "train_nsentences": "1750.04", "train_wps": "115434", "train_ups": "0.44", "train_wpb": "260641", "train_bsz": "1750", "train_num_updates": "84731", "train_lr": "0.000428355", "train_gnorm": "0.375", "train_loss_scale": "4", "train_train_wall": "29", "train_gb_free": "40", "train_wall": "8246"}
[2024-10-11 06:20:43,229][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 06:20:43,231][fairseq.trainer][INFO] - begin training epoch 1767
[2024-10-11 06:20:43,232][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 06:22:35,194][fairseq_cli.train][INFO] - end of epoch 1767 (average epoch stats below)
[2024-10-11 06:22:35,204][train][INFO] - {"epoch": 1767, "train_loss": "0.431", "train_ntokens": "260632", "train_nsentences": "1750.04", "train_wps": "111614", "train_ups": "0.43", "train_wpb": "260632", "train_bsz": "1750", "train_num_updates": "84779", "train_lr": "0.000428289", "train_gnorm": "0.361", "train_loss_scale": "4", "train_train_wall": "26", "train_gb_free": "40.8", "train_wall": "8358"}
[2024-10-11 06:22:35,319][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 06:22:35,336][fairseq.trainer][INFO] - begin training epoch 1768
[2024-10-11 06:22:35,337][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 06:24:04,427][train_inner][INFO] - {"epoch": 1768, "update": 1767.438, "loss": "0.429", "ntokens": "260497", "nsentences": "1745.6", "wps": "118251", "ups": "0.45", "wpb": "260497", "bsz": "1745.6", "num_updates": "84800", "lr": "0.000428261", "gnorm": "0.368", "loss_scale": "4", "train_wall": "142", "gb_free": "39.7", "wall": "8447"}
[2024-10-11 06:24:23,009][fairseq_cli.train][INFO] - end of epoch 1768 (average epoch stats below)
[2024-10-11 06:24:23,019][train][INFO] - {"epoch": 1768, "train_loss": "0.427", "train_ntokens": "260617", "train_nsentences": "1750.04", "train_wps": "116040", "train_ups": "0.45", "train_wpb": "260617", "train_bsz": "1750", "train_num_updates": "84827", "train_lr": "0.000428224", "train_gnorm": "0.375", "train_loss_scale": "4", "train_train_wall": "43", "train_gb_free": "39.2", "train_wall": "8465"}
[2024-10-11 06:24:23,120][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 06:24:23,123][fairseq.trainer][INFO] - begin training epoch 1769
[2024-10-11 06:24:23,123][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 06:26:13,821][fairseq_cli.train][INFO] - end of epoch 1769 (average epoch stats below)
[2024-10-11 06:26:13,825][train][INFO] - {"epoch": 1769, "train_loss": "0.426", "train_ntokens": "260552", "train_nsentences": "1750.04", "train_wps": "112871", "train_ups": "0.43", "train_wpb": "260552", "train_bsz": "1750", "train_num_updates": "84875", "train_lr": "0.000428159", "train_gnorm": "0.355", "train_loss_scale": "4", "train_train_wall": "48", "train_gb_free": "40.2", "train_wall": "8576"}
[2024-10-11 06:26:13,879][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 06:26:13,882][fairseq.trainer][INFO] - begin training epoch 1770
[2024-10-11 06:26:13,882][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 06:28:00,279][fairseq_cli.train][INFO] - end of epoch 1770 (average epoch stats below)
[2024-10-11 06:28:00,281][train][INFO] - {"epoch": 1770, "train_loss": "0.431", "train_ntokens": "260644", "train_nsentences": "1750.04", "train_wps": "117524", "train_ups": "0.45", "train_wpb": "260644", "train_bsz": "1750", "train_num_updates": "84923", "train_lr": "0.000428094", "train_gnorm": "0.367", "train_loss_scale": "4", "train_train_wall": "43", "train_gb_free": "40.1", "train_wall": "8683"}
[2024-10-11 06:28:00,353][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 06:28:00,367][fairseq.trainer][INFO] - begin training epoch 1771
[2024-10-11 06:28:00,367][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 06:29:47,861][fairseq_cli.train][INFO] - end of epoch 1771 (average epoch stats below)
[2024-10-11 06:29:47,887][train][INFO] - {"epoch": 1771, "train_loss": "0.425", "train_ntokens": "260864", "train_nsentences": "1750.04", "train_wps": "116383", "train_ups": "0.45", "train_wpb": "260864", "train_bsz": "1750", "train_num_updates": "84971", "train_lr": "0.000428029", "train_gnorm": "0.359", "train_loss_scale": "4", "train_train_wall": "49", "train_gb_free": "40.1", "train_wall": "8790"}
[2024-10-11 06:29:47,985][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 06:29:47,989][fairseq.trainer][INFO] - begin training epoch 1772
[2024-10-11 06:29:47,989][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 06:31:18,734][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2024-10-11 06:31:24,497][train_inner][INFO] - {"epoch": 1772, "update": 1771.625, "loss": "0.428", "ntokens": "260714", "nsentences": "1740.67", "wps": "118489", "ups": "0.45", "wpb": "260714", "bsz": "1740.7", "num_updates": "85000", "lr": "0.000427989", "gnorm": "0.367", "loss_scale": "2", "train_wall": "188", "gb_free": "39.3", "wall": "8887"}
[2024-10-11 06:31:38,426][fairseq_cli.train][INFO] - end of epoch 1772 (average epoch stats below)
[2024-10-11 06:31:38,429][train][INFO] - {"epoch": 1772, "train_loss": "0.432", "train_ntokens": "260339", "train_nsentences": "1757.26", "train_wps": "110694", "train_ups": "0.43", "train_wpb": "260339", "train_bsz": "1757.3", "train_num_updates": "85018", "train_lr": "0.000427965", "train_gnorm": "0.372", "train_loss_scale": "2", "train_train_wall": "44", "train_gb_free": "39.3", "train_wall": "8901"}
[2024-10-11 06:31:38,496][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 06:31:38,505][fairseq.trainer][INFO] - begin training epoch 1773
[2024-10-11 06:31:38,505][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 06:33:28,476][fairseq_cli.train][INFO] - end of epoch 1773 (average epoch stats below)
[2024-10-11 06:33:28,495][train][INFO] - {"epoch": 1773, "train_loss": "0.423", "train_ntokens": "260364", "train_nsentences": "1750.04", "train_wps": "113563", "train_ups": "0.44", "train_wpb": "260364", "train_bsz": "1750", "train_num_updates": "85066", "train_lr": "0.000427899", "train_gnorm": "0.365", "train_loss_scale": "2", "train_train_wall": "47", "train_gb_free": "39.3", "train_wall": "9011"}
[2024-10-11 06:33:28,574][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 06:33:28,587][fairseq.trainer][INFO] - begin training epoch 1774
[2024-10-11 06:33:28,601][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 06:35:18,814][fairseq_cli.train][INFO] - end of epoch 1774 (average epoch stats below)
[2024-10-11 06:35:18,829][train][INFO] - {"epoch": 1774, "train_loss": "0.427", "train_ntokens": "260718", "train_nsentences": "1750.04", "train_wps": "113426", "train_ups": "0.44", "train_wpb": "260718", "train_bsz": "1750", "train_num_updates": "85114", "train_lr": "0.000427834", "train_gnorm": "0.366", "train_loss_scale": "2", "train_train_wall": "29", "train_gb_free": "39.3", "train_wall": "9121"}
[2024-10-11 06:35:18,941][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 06:35:18,969][fairseq.trainer][INFO] - begin training epoch 1775
[2024-10-11 06:35:18,970][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 06:37:09,386][fairseq_cli.train][INFO] - end of epoch 1775 (average epoch stats below)
[2024-10-11 06:37:09,390][train][INFO] - {"epoch": 1775, "train_loss": "0.417", "train_ntokens": "261032", "train_nsentences": "1750.04", "train_wps": "113330", "train_ups": "0.43", "train_wpb": "261032", "train_bsz": "1750", "train_num_updates": "85162", "train_lr": "0.000427769", "train_gnorm": "0.357", "train_loss_scale": "2", "train_train_wall": "47", "train_gb_free": "39.6", "train_wall": "9232"}
[2024-10-11 06:37:09,460][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 06:37:09,478][fairseq.trainer][INFO] - begin training epoch 1776
[2024-10-11 06:37:09,478][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 06:38:52,007][train_inner][INFO] - {"epoch": 1776, "update": 1775.792, "loss": "0.425", "ntokens": "260581", "nsentences": "1754.49", "wps": "116466", "ups": "0.45", "wpb": "260581", "bsz": "1754.5", "num_updates": "85200", "lr": "0.000427717", "gnorm": "0.361", "loss_scale": "2", "train_wall": "176", "gb_free": "39.1", "wall": "9334"}
[2024-10-11 06:39:01,343][fairseq_cli.train][INFO] - end of epoch 1776 (average epoch stats below)
[2024-10-11 06:39:01,345][train][INFO] - {"epoch": 1776, "train_loss": "0.43", "train_ntokens": "260377", "train_nsentences": "1750.04", "train_wps": "111637", "train_ups": "0.43", "train_wpb": "260377", "train_bsz": "1750", "train_num_updates": "85210", "train_lr": "0.000427704", "train_gnorm": "0.369", "train_loss_scale": "2", "train_train_wall": "49", "train_gb_free": "40.5", "train_wall": "9344"}
[2024-10-11 06:39:01,412][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 06:39:01,433][fairseq.trainer][INFO] - begin training epoch 1777
[2024-10-11 06:39:01,433][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 06:40:49,860][fairseq_cli.train][INFO] - end of epoch 1777 (average epoch stats below)
[2024-10-11 06:40:49,874][train][INFO] - {"epoch": 1777, "train_loss": "0.428", "train_ntokens": "260812", "train_nsentences": "1750.04", "train_wps": "115354", "train_ups": "0.44", "train_wpb": "260812", "train_bsz": "1750", "train_num_updates": "85258", "train_lr": "0.000427639", "train_gnorm": "0.346", "train_loss_scale": "2", "train_train_wall": "42", "train_gb_free": "39.1", "train_wall": "9452"}
[2024-10-11 06:40:49,982][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 06:40:49,992][fairseq.trainer][INFO] - begin training epoch 1778
[2024-10-11 06:40:49,993][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 06:42:42,006][fairseq_cli.train][INFO] - end of epoch 1778 (average epoch stats below)
[2024-10-11 06:42:42,010][train][INFO] - {"epoch": 1778, "train_loss": "0.432", "train_ntokens": "260573", "train_nsentences": "1750.04", "train_wps": "111542", "train_ups": "0.43", "train_wpb": "260573", "train_bsz": "1750", "train_num_updates": "85306", "train_lr": "0.000427573", "train_gnorm": "0.366", "train_loss_scale": "2", "train_train_wall": "45", "train_gb_free": "40", "train_wall": "9564"}
[2024-10-11 06:42:42,068][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 06:42:42,074][fairseq.trainer][INFO] - begin training epoch 1779
[2024-10-11 06:42:42,074][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 06:44:29,340][fairseq_cli.train][INFO] - end of epoch 1779 (average epoch stats below)
[2024-10-11 06:44:29,345][train][INFO] - {"epoch": 1779, "train_loss": "0.424", "train_ntokens": "261031", "train_nsentences": "1750.04", "train_wps": "116735", "train_ups": "0.45", "train_wpb": "261031", "train_bsz": "1750", "train_num_updates": "85354", "train_lr": "0.000427508", "train_gnorm": "0.375", "train_loss_scale": "2", "train_train_wall": "44", "train_gb_free": "39.8", "train_wall": "9672"}
[2024-10-11 06:44:29,451][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 06:44:29,454][fairseq.trainer][INFO] - begin training epoch 1780
[2024-10-11 06:44:29,454][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 06:46:26,065][train_inner][INFO] - {"epoch": 1780, "update": 1779.958, "loss": "0.43", "ntokens": "260843", "nsentences": "1752.81", "wps": "114899", "ups": "0.44", "wpb": "260843", "bsz": "1752.8", "num_updates": "85400", "lr": "0.000427446", "gnorm": "0.365", "loss_scale": "2", "train_wall": "185", "gb_free": "39.6", "wall": "9788"}
[2024-10-11 06:46:26,462][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 1780 @ 85402 updates
[2024-10-11 06:46:26,462][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-11 06:46:37,318][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-11 06:46:37,320][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 1780 @ 85402 updates, score None) (writing took 10.858213434927166 seconds)
[2024-10-11 06:46:37,320][fairseq_cli.train][INFO] - end of epoch 1780 (average epoch stats below)
[2024-10-11 06:46:37,322][train][INFO] - {"epoch": 1780, "train_loss": "0.429", "train_ntokens": "260957", "train_nsentences": "1750.04", "train_wps": "97878.6", "train_ups": "0.38", "train_wpb": "260957", "train_bsz": "1750", "train_num_updates": "85402", "train_lr": "0.000427443", "train_gnorm": "0.365", "train_loss_scale": "2", "train_train_wall": "45", "train_gb_free": "39.3", "train_wall": "9800"}
[2024-10-11 06:46:37,370][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 06:46:37,389][fairseq.trainer][INFO] - begin training epoch 1781
[2024-10-11 06:46:37,390][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 06:48:22,323][fairseq_cli.train][INFO] - end of epoch 1781 (average epoch stats below)
[2024-10-11 06:48:22,338][train][INFO] - {"epoch": 1781, "train_loss": "0.423", "train_ntokens": "260900", "train_nsentences": "1750.04", "train_wps": "119254", "train_ups": "0.46", "train_wpb": "260900", "train_bsz": "1750", "train_num_updates": "85450", "train_lr": "0.000427378", "train_gnorm": "0.363", "train_loss_scale": "2", "train_train_wall": "33", "train_gb_free": "39.7", "train_wall": "9905"}
[2024-10-11 06:48:22,434][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 06:48:22,453][fairseq.trainer][INFO] - begin training epoch 1782
[2024-10-11 06:48:22,453][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 06:50:10,731][fairseq_cli.train][INFO] - end of epoch 1782 (average epoch stats below)
[2024-10-11 06:50:10,746][train][INFO] - {"epoch": 1782, "train_loss": "0.421", "train_ntokens": "260754", "train_nsentences": "1750.04", "train_wps": "115459", "train_ups": "0.44", "train_wpb": "260754", "train_bsz": "1750", "train_num_updates": "85498", "train_lr": "0.000427313", "train_gnorm": "0.37", "train_loss_scale": "2", "train_train_wall": "45", "train_gb_free": "39.3", "train_wall": "10013"}
[2024-10-11 06:50:10,872][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 06:50:10,875][fairseq.trainer][INFO] - begin training epoch 1783
[2024-10-11 06:50:10,875][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 06:51:59,813][fairseq_cli.train][INFO] - end of epoch 1783 (average epoch stats below)
[2024-10-11 06:51:59,816][train][INFO] - {"epoch": 1783, "train_loss": "0.426", "train_ntokens": "260388", "train_nsentences": "1750.04", "train_wps": "114596", "train_ups": "0.44", "train_wpb": "260388", "train_bsz": "1750", "train_num_updates": "85546", "train_lr": "0.000427247", "train_gnorm": "0.349", "train_loss_scale": "2", "train_train_wall": "41", "train_gb_free": "40.5", "train_wall": "10122"}
[2024-10-11 06:51:59,910][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 06:51:59,913][fairseq.trainer][INFO] - begin training epoch 1784
[2024-10-11 06:51:59,913][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 06:53:48,320][fairseq_cli.train][INFO] - end of epoch 1784 (average epoch stats below)
[2024-10-11 06:53:48,325][train][INFO] - {"epoch": 1784, "train_loss": "0.427", "train_ntokens": "260606", "train_nsentences": "1750.04", "train_wps": "115284", "train_ups": "0.44", "train_wpb": "260606", "train_bsz": "1750", "train_num_updates": "85594", "train_lr": "0.000427182", "train_gnorm": "0.363", "train_loss_scale": "2", "train_train_wall": "38", "train_gb_free": "40.6", "train_wall": "10231"}
[2024-10-11 06:53:48,437][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 06:53:48,439][fairseq.trainer][INFO] - begin training epoch 1785
[2024-10-11 06:53:48,440][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 06:55:08,626][train_inner][INFO] - {"epoch": 1785, "update": 1784.125, "loss": "0.423", "ntokens": "260673", "nsentences": "1757.38", "wps": "99768.7", "ups": "0.38", "wpb": "260674", "bsz": "1757.4", "num_updates": "85600", "lr": "0.000427174", "gnorm": "0.361", "loss_scale": "2", "train_wall": "176", "gb_free": "39.7", "wall": "10311"}
[2024-10-11 06:55:38,108][fairseq_cli.train][INFO] - end of epoch 1785 (average epoch stats below)
[2024-10-11 06:55:38,112][train][INFO] - {"epoch": 1785, "train_loss": "0.421", "train_ntokens": "260890", "train_nsentences": "1750.04", "train_wps": "114068", "train_ups": "0.44", "train_wpb": "260890", "train_bsz": "1750", "train_num_updates": "85642", "train_lr": "0.000427117", "train_gnorm": "0.367", "train_loss_scale": "2", "train_train_wall": "47", "train_gb_free": "39.9", "train_wall": "10341"}
[2024-10-11 06:55:38,188][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 06:55:38,201][fairseq.trainer][INFO] - begin training epoch 1786
[2024-10-11 06:55:38,201][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 06:57:23,058][fairseq_cli.train][INFO] - end of epoch 1786 (average epoch stats below)
[2024-10-11 06:57:23,074][train][INFO] - {"epoch": 1786, "train_loss": "0.419", "train_ntokens": "260495", "train_nsentences": "1750.04", "train_wps": "119130", "train_ups": "0.46", "train_wpb": "260495", "train_bsz": "1750", "train_num_updates": "85690", "train_lr": "0.000427052", "train_gnorm": "0.384", "train_loss_scale": "2", "train_train_wall": "39", "train_gb_free": "39.7", "train_wall": "10445"}
[2024-10-11 06:57:23,170][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 06:57:23,177][fairseq.trainer][INFO] - begin training epoch 1787
[2024-10-11 06:57:23,178][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 06:59:13,581][fairseq_cli.train][INFO] - end of epoch 1787 (average epoch stats below)
[2024-10-11 06:59:13,584][train][INFO] - {"epoch": 1787, "train_loss": "0.424", "train_ntokens": "260751", "train_nsentences": "1750.04", "train_wps": "113260", "train_ups": "0.43", "train_wpb": "260751", "train_bsz": "1750", "train_num_updates": "85738", "train_lr": "0.000426986", "train_gnorm": "0.377", "train_loss_scale": "2", "train_train_wall": "42", "train_gb_free": "39.7", "train_wall": "10556"}
[2024-10-11 06:59:13,634][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 06:59:13,636][fairseq.trainer][INFO] - begin training epoch 1788
[2024-10-11 06:59:13,637][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 07:01:02,704][fairseq_cli.train][INFO] - end of epoch 1788 (average epoch stats below)
[2024-10-11 07:01:02,706][train][INFO] - {"epoch": 1788, "train_loss": "0.419", "train_ntokens": "260403", "train_nsentences": "1750.04", "train_wps": "114548", "train_ups": "0.44", "train_wpb": "260402", "train_bsz": "1750", "train_num_updates": "85786", "train_lr": "0.000426921", "train_gnorm": "0.38", "train_loss_scale": "2", "train_train_wall": "41", "train_gb_free": "40", "train_wall": "10665"}
[2024-10-11 07:01:02,760][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 07:01:02,763][fairseq.trainer][INFO] - begin training epoch 1789
[2024-10-11 07:01:02,763][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 07:03:18,689][train_inner][INFO] - {"epoch": 1789, "update": 1788.292, "loss": "0.421", "ntokens": "260431", "nsentences": "1752.2", "wps": "106286", "ups": "0.41", "wpb": "260431", "bsz": "1752.2", "num_updates": "85800", "lr": "0.000426902", "gnorm": "0.381", "loss_scale": "2", "train_wall": "172", "gb_free": "39.3", "wall": "10801"}
[2024-10-11 07:03:36,825][fairseq_cli.train][INFO] - end of epoch 1789 (average epoch stats below)
[2024-10-11 07:03:36,828][train][INFO] - {"epoch": 1789, "train_loss": "0.423", "train_ntokens": "260657", "train_nsentences": "1750.04", "train_wps": "81181.5", "train_ups": "0.31", "train_wpb": "260657", "train_bsz": "1750", "train_num_updates": "85834", "train_lr": "0.000426856", "train_gnorm": "0.372", "train_loss_scale": "2", "train_train_wall": "40", "train_gb_free": "39.7", "train_wall": "10819"}
[2024-10-11 07:03:36,910][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 07:03:36,921][fairseq.trainer][INFO] - begin training epoch 1790
[2024-10-11 07:03:36,921][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 07:05:26,363][fairseq_cli.train][INFO] - end of epoch 1790 (average epoch stats below)
[2024-10-11 07:05:26,367][train][INFO] - {"epoch": 1790, "train_loss": "0.42", "train_ntokens": "260267", "train_nsentences": "1750.04", "train_wps": "114052", "train_ups": "0.44", "train_wpb": "260267", "train_bsz": "1750", "train_num_updates": "85882", "train_lr": "0.000426791", "train_gnorm": "0.345", "train_loss_scale": "2", "train_train_wall": "43", "train_gb_free": "40.2", "train_wall": "10929"}
[2024-10-11 07:05:26,436][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 07:05:26,439][fairseq.trainer][INFO] - begin training epoch 1791
[2024-10-11 07:05:26,440][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 07:07:16,490][fairseq_cli.train][INFO] - end of epoch 1791 (average epoch stats below)
[2024-10-11 07:07:16,493][train][INFO] - {"epoch": 1791, "train_loss": "0.427", "train_ntokens": "260820", "train_nsentences": "1750.04", "train_wps": "113685", "train_ups": "0.44", "train_wpb": "260820", "train_bsz": "1750", "train_num_updates": "85930", "train_lr": "0.000426726", "train_gnorm": "0.36", "train_loss_scale": "2", "train_train_wall": "41", "train_gb_free": "39.6", "train_wall": "11039"}
[2024-10-11 07:07:16,543][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 07:07:16,546][fairseq.trainer][INFO] - begin training epoch 1792
[2024-10-11 07:07:16,546][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 07:09:06,825][fairseq_cli.train][INFO] - end of epoch 1792 (average epoch stats below)
[2024-10-11 07:09:06,828][train][INFO] - {"epoch": 1792, "train_loss": "0.428", "train_ntokens": "260693", "train_nsentences": "1750.04", "train_wps": "113415", "train_ups": "0.44", "train_wpb": "260693", "train_bsz": "1750", "train_num_updates": "85978", "train_lr": "0.00042666", "train_gnorm": "0.378", "train_loss_scale": "2", "train_train_wall": "54", "train_gb_free": "40", "train_wall": "11149"}
[2024-10-11 07:09:06,903][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 07:09:06,914][fairseq.trainer][INFO] - begin training epoch 1793
[2024-10-11 07:09:06,920][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 07:10:36,931][train_inner][INFO] - {"epoch": 1793, "update": 1792.458, "loss": "0.424", "ntokens": "261022", "nsentences": "1727.97", "wps": "119123", "ups": "0.46", "wpb": "261022", "bsz": "1728", "num_updates": "86000", "lr": "0.00042663", "gnorm": "0.359", "loss_scale": "2", "train_wall": "180", "gb_free": "40.5", "wall": "11239"}
[2024-10-11 07:10:54,791][fairseq_cli.train][INFO] - end of epoch 1793 (average epoch stats below)
[2024-10-11 07:10:54,795][train][INFO] - {"epoch": 1793, "train_loss": "0.424", "train_ntokens": "260680", "train_nsentences": "1750.04", "train_wps": "115898", "train_ups": "0.44", "train_wpb": "260680", "train_bsz": "1750", "train_num_updates": "86026", "train_lr": "0.000426595", "train_gnorm": "0.373", "train_loss_scale": "2", "train_train_wall": "43", "train_gb_free": "39.6", "train_wall": "11257"}
[2024-10-11 07:10:54,892][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 07:10:54,895][fairseq.trainer][INFO] - begin training epoch 1794
[2024-10-11 07:10:54,895][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 07:12:45,160][fairseq_cli.train][INFO] - end of epoch 1794 (average epoch stats below)
[2024-10-11 07:12:45,162][train][INFO] - {"epoch": 1794, "train_loss": "0.421", "train_ntokens": "261048", "train_nsentences": "1750.04", "train_wps": "113536", "train_ups": "0.43", "train_wpb": "261048", "train_bsz": "1750", "train_num_updates": "86074", "train_lr": "0.00042653", "train_gnorm": "0.361", "train_loss_scale": "2", "train_train_wall": "46", "train_gb_free": "39.7", "train_wall": "11368"}
[2024-10-11 07:12:45,278][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 07:12:45,292][fairseq.trainer][INFO] - begin training epoch 1795
[2024-10-11 07:12:45,293][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 07:14:34,114][fairseq_cli.train][INFO] - end of epoch 1795 (average epoch stats below)
[2024-10-11 07:14:34,117][train][INFO] - {"epoch": 1795, "train_loss": "0.417", "train_ntokens": "260525", "train_nsentences": "1750.04", "train_wps": "114777", "train_ups": "0.44", "train_wpb": "260525", "train_bsz": "1750", "train_num_updates": "86122", "train_lr": "0.000426465", "train_gnorm": "0.358", "train_loss_scale": "2", "train_train_wall": "37", "train_gb_free": "39.8", "train_wall": "11477"}
[2024-10-11 07:14:34,238][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 07:14:34,241][fairseq.trainer][INFO] - begin training epoch 1796
[2024-10-11 07:14:34,241][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 07:16:23,891][fairseq_cli.train][INFO] - end of epoch 1796 (average epoch stats below)
[2024-10-11 07:16:23,898][train][INFO] - {"epoch": 1796, "train_loss": "0.426", "train_ntokens": "260643", "train_nsentences": "1750.04", "train_wps": "113964", "train_ups": "0.44", "train_wpb": "260643", "train_bsz": "1750", "train_num_updates": "86170", "train_lr": "0.000426399", "train_gnorm": "0.374", "train_loss_scale": "2", "train_train_wall": "35", "train_gb_free": "40.1", "train_wall": "11586"}
[2024-10-11 07:16:23,990][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 07:16:24,005][fairseq.trainer][INFO] - begin training epoch 1797
[2024-10-11 07:16:24,005][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 07:17:56,402][train_inner][INFO] - {"epoch": 1797, "update": 1796.625, "loss": "0.422", "ntokens": "260623", "nsentences": "1753.5", "wps": "118609", "ups": "0.46", "wpb": "260623", "bsz": "1753.5", "num_updates": "86200", "lr": "0.000426359", "gnorm": "0.366", "loss_scale": "2", "train_wall": "149", "gb_free": "40.1", "wall": "11679"}
[2024-10-11 07:18:11,656][fairseq_cli.train][INFO] - end of epoch 1797 (average epoch stats below)
[2024-10-11 07:18:11,659][train][INFO] - {"epoch": 1797, "train_loss": "0.423", "train_ntokens": "260813", "train_nsentences": "1750.04", "train_wps": "116178", "train_ups": "0.45", "train_wpb": "260813", "train_bsz": "1750", "train_num_updates": "86218", "train_lr": "0.000426334", "train_gnorm": "0.364", "train_loss_scale": "2", "train_train_wall": "28", "train_gb_free": "39.2", "train_wall": "11694"}
[2024-10-11 07:18:11,790][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 07:18:11,804][fairseq.trainer][INFO] - begin training epoch 1798
[2024-10-11 07:18:11,804][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 07:20:01,534][fairseq_cli.train][INFO] - end of epoch 1798 (average epoch stats below)
[2024-10-11 07:20:01,551][train][INFO] - {"epoch": 1798, "train_loss": "0.425", "train_ntokens": "260327", "train_nsentences": "1750.04", "train_wps": "113721", "train_ups": "0.44", "train_wpb": "260327", "train_bsz": "1750", "train_num_updates": "86266", "train_lr": "0.000426269", "train_gnorm": "0.375", "train_loss_scale": "2", "train_train_wall": "39", "train_gb_free": "39.3", "train_wall": "11804"}
[2024-10-11 07:20:01,646][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 07:20:01,648][fairseq.trainer][INFO] - begin training epoch 1799
[2024-10-11 07:20:01,648][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 07:21:50,711][fairseq_cli.train][INFO] - end of epoch 1799 (average epoch stats below)
[2024-10-11 07:21:50,729][train][INFO] - {"epoch": 1799, "train_loss": "0.419", "train_ntokens": "260719", "train_nsentences": "1750.04", "train_wps": "114628", "train_ups": "0.44", "train_wpb": "260719", "train_bsz": "1750", "train_num_updates": "86314", "train_lr": "0.000426204", "train_gnorm": "0.35", "train_loss_scale": "2", "train_train_wall": "44", "train_gb_free": "39.8", "train_wall": "11913"}
[2024-10-11 07:21:50,800][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 07:21:50,805][fairseq.trainer][INFO] - begin training epoch 1800
[2024-10-11 07:21:50,806][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 07:23:40,817][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 1800 @ 86362 updates
[2024-10-11 07:23:40,818][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-11 07:23:51,784][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-11 07:23:51,797][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 1800 @ 86362 updates, score None) (writing took 10.980605765245855 seconds)
[2024-10-11 07:23:51,798][fairseq_cli.train][INFO] - end of epoch 1800 (average epoch stats below)
[2024-10-11 07:23:51,800][train][INFO] - {"epoch": 1800, "train_loss": "0.418", "train_ntokens": "260698", "train_nsentences": "1750.04", "train_wps": "103360", "train_ups": "0.4", "train_wpb": "260698", "train_bsz": "1750", "train_num_updates": "86362", "train_lr": "0.000426139", "train_gnorm": "0.368", "train_loss_scale": "2", "train_train_wall": "34", "train_gb_free": "39.4", "train_wall": "12034"}
[2024-10-11 07:23:51,857][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 07:23:51,861][fairseq.trainer][INFO] - begin training epoch 1801
[2024-10-11 07:23:51,862][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 07:25:28,719][train_inner][INFO] - {"epoch": 1801, "update": 1800.792, "loss": "0.419", "ntokens": "260563", "nsentences": "1753.73", "wps": "115220", "ups": "0.44", "wpb": "260563", "bsz": "1753.7", "num_updates": "86400", "lr": "0.000426087", "gnorm": "0.362", "loss_scale": "2", "train_wall": "160", "gb_free": "40.6", "wall": "12131"}
[2024-10-11 07:25:36,328][fairseq_cli.train][INFO] - end of epoch 1801 (average epoch stats below)
[2024-10-11 07:25:36,329][train][INFO] - {"epoch": 1801, "train_loss": "0.417", "train_ntokens": "260659", "train_nsentences": "1750.04", "train_wps": "119698", "train_ups": "0.46", "train_wpb": "260659", "train_bsz": "1750", "train_num_updates": "86410", "train_lr": "0.000426073", "train_gnorm": "0.355", "train_loss_scale": "2", "train_train_wall": "36", "train_gb_free": "39.3", "train_wall": "12139"}
[2024-10-11 07:25:36,421][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 07:25:36,426][fairseq.trainer][INFO] - begin training epoch 1802
[2024-10-11 07:25:36,427][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 07:27:25,174][fairseq_cli.train][INFO] - end of epoch 1802 (average epoch stats below)
[2024-10-11 07:27:25,180][train][INFO] - {"epoch": 1802, "train_loss": "0.427", "train_ntokens": "260642", "train_nsentences": "1750.04", "train_wps": "114939", "train_ups": "0.44", "train_wpb": "260642", "train_bsz": "1750", "train_num_updates": "86458", "train_lr": "0.000426008", "train_gnorm": "0.366", "train_loss_scale": "2", "train_train_wall": "43", "train_gb_free": "39.6", "train_wall": "12248"}
[2024-10-11 07:27:25,274][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 07:27:25,277][fairseq.trainer][INFO] - begin training epoch 1803
[2024-10-11 07:27:25,277][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 07:29:12,701][fairseq_cli.train][INFO] - end of epoch 1803 (average epoch stats below)
[2024-10-11 07:29:12,707][train][INFO] - {"epoch": 1803, "train_loss": "0.427", "train_ntokens": "260284", "train_nsentences": "1750.04", "train_wps": "116198", "train_ups": "0.45", "train_wpb": "260284", "train_bsz": "1750", "train_num_updates": "86506", "train_lr": "0.000425943", "train_gnorm": "0.368", "train_loss_scale": "2", "train_train_wall": "35", "train_gb_free": "39.8", "train_wall": "12355"}
[2024-10-11 07:29:12,773][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 07:29:12,776][fairseq.trainer][INFO] - begin training epoch 1804
[2024-10-11 07:29:12,776][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 07:31:01,223][fairseq_cli.train][INFO] - end of epoch 1804 (average epoch stats below)
[2024-10-11 07:31:01,238][train][INFO] - {"epoch": 1804, "train_loss": "0.426", "train_ntokens": "261043", "train_nsentences": "1750.04", "train_wps": "115454", "train_ups": "0.44", "train_wpb": "261043", "train_bsz": "1750", "train_num_updates": "86554", "train_lr": "0.000425878", "train_gnorm": "0.357", "train_loss_scale": "2", "train_train_wall": "39", "train_gb_free": "39.6", "train_wall": "12464"}
[2024-10-11 07:31:01,355][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 07:31:01,358][fairseq.trainer][INFO] - begin training epoch 1805
[2024-10-11 07:31:01,358][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 07:32:48,043][train_inner][INFO] - {"epoch": 1805, "update": 1804.958, "loss": "0.424", "ntokens": "260712", "nsentences": "1758.25", "wps": "118694", "ups": "0.46", "wpb": "260712", "bsz": "1758.2", "num_updates": "86600", "lr": "0.000425815", "gnorm": "0.365", "loss_scale": "2", "train_wall": "156", "gb_free": "39.3", "wall": "12570"}
[2024-10-11 07:32:48,432][fairseq_cli.train][INFO] - end of epoch 1805 (average epoch stats below)
[2024-10-11 07:32:48,447][train][INFO] - {"epoch": 1805, "train_loss": "0.416", "train_ntokens": "261106", "train_nsentences": "1750.04", "train_wps": "116922", "train_ups": "0.45", "train_wpb": "261106", "train_bsz": "1750", "train_num_updates": "86602", "train_lr": "0.000425812", "train_gnorm": "0.361", "train_loss_scale": "2", "train_train_wall": "32", "train_gb_free": "40", "train_wall": "12571"}
[2024-10-11 07:32:48,539][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 07:32:48,541][fairseq.trainer][INFO] - begin training epoch 1806
[2024-10-11 07:32:48,542][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 07:34:37,739][fairseq_cli.train][INFO] - end of epoch 1806 (average epoch stats below)
[2024-10-11 07:34:37,743][train][INFO] - {"epoch": 1806, "train_loss": "0.421", "train_ntokens": "260748", "train_nsentences": "1750.04", "train_wps": "114516", "train_ups": "0.44", "train_wpb": "260748", "train_bsz": "1750", "train_num_updates": "86650", "train_lr": "0.000425747", "train_gnorm": "0.367", "train_loss_scale": "2", "train_train_wall": "46", "train_gb_free": "40", "train_wall": "12680"}
[2024-10-11 07:34:37,857][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 07:34:37,861][fairseq.trainer][INFO] - begin training epoch 1807
[2024-10-11 07:34:37,861][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 07:36:27,402][fairseq_cli.train][INFO] - end of epoch 1807 (average epoch stats below)
[2024-10-11 07:36:27,408][train][INFO] - {"epoch": 1807, "train_loss": "0.423", "train_ntokens": "260644", "train_nsentences": "1750.04", "train_wps": "114087", "train_ups": "0.44", "train_wpb": "260644", "train_bsz": "1750", "train_num_updates": "86698", "train_lr": "0.000425682", "train_gnorm": "0.354", "train_loss_scale": "2", "train_train_wall": "52", "train_gb_free": "39.9", "train_wall": "12790"}
[2024-10-11 07:36:27,499][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 07:36:27,507][fairseq.trainer][INFO] - begin training epoch 1808
[2024-10-11 07:36:27,508][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 07:38:16,842][fairseq_cli.train][INFO] - end of epoch 1808 (average epoch stats below)
[2024-10-11 07:38:16,847][train][INFO] - {"epoch": 1808, "train_loss": "0.419", "train_ntokens": "260530", "train_nsentences": "1750.04", "train_wps": "114272", "train_ups": "0.44", "train_wpb": "260530", "train_bsz": "1750", "train_num_updates": "86746", "train_lr": "0.000425617", "train_gnorm": "0.373", "train_loss_scale": "2", "train_train_wall": "42", "train_gb_free": "39.6", "train_wall": "12899"}
[2024-10-11 07:38:16,968][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 07:38:16,977][fairseq.trainer][INFO] - begin training epoch 1809
[2024-10-11 07:38:16,977][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 07:40:07,584][fairseq_cli.train][INFO] - end of epoch 1809 (average epoch stats below)
[2024-10-11 07:40:07,598][train][INFO] - {"epoch": 1809, "train_loss": "0.423", "train_ntokens": "260635", "train_nsentences": "1750.04", "train_wps": "112965", "train_ups": "0.43", "train_wpb": "260635", "train_bsz": "1750", "train_num_updates": "86794", "train_lr": "0.000425552", "train_gnorm": "0.366", "train_loss_scale": "2", "train_train_wall": "44", "train_gb_free": "39.8", "train_wall": "13010"}
[2024-10-11 07:40:07,714][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 07:40:07,717][fairseq.trainer][INFO] - begin training epoch 1810
[2024-10-11 07:40:07,717][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 07:41:30,321][train_inner][INFO] - {"epoch": 1810, "update": 1809.125, "loss": "0.422", "ntokens": "260651", "nsentences": "1746.12", "wps": "99814.5", "ups": "0.38", "wpb": "260652", "bsz": "1746.1", "num_updates": "86800", "lr": "0.000425543", "gnorm": "0.365", "loss_scale": "2", "train_wall": "211", "gb_free": "39.3", "wall": "13093"}
[2024-10-11 07:41:58,178][fairseq_cli.train][INFO] - end of epoch 1810 (average epoch stats below)
[2024-10-11 07:41:58,180][train][INFO] - {"epoch": 1810, "train_loss": "0.42", "train_ntokens": "260719", "train_nsentences": "1750.04", "train_wps": "113172", "train_ups": "0.43", "train_wpb": "260719", "train_bsz": "1750", "train_num_updates": "86842", "train_lr": "0.000425486", "train_gnorm": "0.351", "train_loss_scale": "2", "train_train_wall": "54", "train_gb_free": "39.7", "train_wall": "13121"}
[2024-10-11 07:41:58,277][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 07:41:58,281][fairseq.trainer][INFO] - begin training epoch 1811
[2024-10-11 07:41:58,282][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 07:43:48,692][fairseq_cli.train][INFO] - end of epoch 1811 (average epoch stats below)
[2024-10-11 07:43:48,695][train][INFO] - {"epoch": 1811, "train_loss": "0.422", "train_ntokens": "260479", "train_nsentences": "1750.04", "train_wps": "113137", "train_ups": "0.43", "train_wpb": "260479", "train_bsz": "1750", "train_num_updates": "86890", "train_lr": "0.000425421", "train_gnorm": "0.377", "train_loss_scale": "2", "train_train_wall": "44", "train_gb_free": "39.7", "train_wall": "13231"}
[2024-10-11 07:43:48,748][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 07:43:48,751][fairseq.trainer][INFO] - begin training epoch 1812
[2024-10-11 07:43:48,751][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 07:45:38,122][fairseq_cli.train][INFO] - end of epoch 1812 (average epoch stats below)
[2024-10-11 07:45:38,127][train][INFO] - {"epoch": 1812, "train_loss": "0.418", "train_ntokens": "260762", "train_nsentences": "1750.04", "train_wps": "114382", "train_ups": "0.44", "train_wpb": "260762", "train_bsz": "1750", "train_num_updates": "86938", "train_lr": "0.000425356", "train_gnorm": "0.354", "train_loss_scale": "2", "train_train_wall": "45", "train_gb_free": "39.7", "train_wall": "13341"}
[2024-10-11 07:45:38,202][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 07:45:38,212][fairseq.trainer][INFO] - begin training epoch 1813
[2024-10-11 07:45:38,212][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 07:47:26,640][fairseq_cli.train][INFO] - end of epoch 1813 (average epoch stats below)
[2024-10-11 07:47:26,646][train][INFO] - {"epoch": 1813, "train_loss": "0.426", "train_ntokens": "260969", "train_nsentences": "1750.04", "train_wps": "115438", "train_ups": "0.44", "train_wpb": "260969", "train_bsz": "1750", "train_num_updates": "86986", "train_lr": "0.000425291", "train_gnorm": "0.349", "train_loss_scale": "2", "train_train_wall": "40", "train_gb_free": "39.7", "train_wall": "13449"}
[2024-10-11 07:47:26,733][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 07:47:26,735][fairseq.trainer][INFO] - begin training epoch 1814
[2024-10-11 07:47:26,736][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 07:48:50,256][train_inner][INFO] - {"epoch": 1814, "update": 1813.292, "loss": "0.421", "ntokens": "260843", "nsentences": "1749.84", "wps": "118584", "ups": "0.45", "wpb": "260843", "bsz": "1749.8", "num_updates": "87000", "lr": "0.000425272", "gnorm": "0.36", "loss_scale": "2", "train_wall": "175", "gb_free": "39.9", "wall": "13533"}
[2024-10-11 07:49:15,495][fairseq_cli.train][INFO] - end of epoch 1814 (average epoch stats below)
[2024-10-11 07:49:15,497][train][INFO] - {"epoch": 1814, "train_loss": "0.428", "train_ntokens": "261374", "train_nsentences": "1750.04", "train_wps": "115260", "train_ups": "0.44", "train_wpb": "261374", "train_bsz": "1750", "train_num_updates": "87034", "train_lr": "0.000425226", "train_gnorm": "0.369", "train_loss_scale": "2", "train_train_wall": "43", "train_gb_free": "39.3", "train_wall": "13558"}
[2024-10-11 07:49:15,550][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 07:49:15,553][fairseq.trainer][INFO] - begin training epoch 1815
[2024-10-11 07:49:15,553][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 07:51:06,024][fairseq_cli.train][INFO] - end of epoch 1815 (average epoch stats below)
[2024-10-11 07:51:06,027][train][INFO] - {"epoch": 1815, "train_loss": "0.42", "train_ntokens": "260884", "train_nsentences": "1750.04", "train_wps": "113298", "train_ups": "0.43", "train_wpb": "260884", "train_bsz": "1750", "train_num_updates": "87082", "train_lr": "0.00042516", "train_gnorm": "0.352", "train_loss_scale": "4", "train_train_wall": "46", "train_gb_free": "39.8", "train_wall": "13668"}
[2024-10-11 07:51:06,109][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 07:51:06,113][fairseq.trainer][INFO] - begin training epoch 1816
[2024-10-11 07:51:06,113][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 07:52:54,294][fairseq_cli.train][INFO] - end of epoch 1816 (average epoch stats below)
[2024-10-11 07:52:54,297][train][INFO] - {"epoch": 1816, "train_loss": "0.421", "train_ntokens": "260342", "train_nsentences": "1750.04", "train_wps": "115422", "train_ups": "0.44", "train_wpb": "260342", "train_bsz": "1750", "train_num_updates": "87130", "train_lr": "0.000425095", "train_gnorm": "0.354", "train_loss_scale": "4", "train_train_wall": "36", "train_gb_free": "39.2", "train_wall": "13777"}
[2024-10-11 07:52:54,350][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 07:52:54,352][fairseq.trainer][INFO] - begin training epoch 1817
[2024-10-11 07:52:54,352][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 07:54:42,272][fairseq_cli.train][INFO] - end of epoch 1817 (average epoch stats below)
[2024-10-11 07:54:42,274][train][INFO] - {"epoch": 1817, "train_loss": "0.425", "train_ntokens": "260820", "train_nsentences": "1750.04", "train_wps": "115947", "train_ups": "0.44", "train_wpb": "260820", "train_bsz": "1750", "train_num_updates": "87178", "train_lr": "0.00042503", "train_gnorm": "0.396", "train_loss_scale": "4", "train_train_wall": "41", "train_gb_free": "39.6", "train_wall": "13885"}
[2024-10-11 07:54:42,363][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 07:54:42,375][fairseq.trainer][INFO] - begin training epoch 1818
[2024-10-11 07:54:42,375][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 07:56:13,325][train_inner][INFO] - {"epoch": 1818, "update": 1817.458, "loss": "0.423", "ntokens": "260899", "nsentences": "1734.78", "wps": "117771", "ups": "0.45", "wpb": "260899", "bsz": "1734.8", "num_updates": "87200", "lr": "0.000425", "gnorm": "0.364", "loss_scale": "4", "train_wall": "175", "gb_free": "40.2", "wall": "13976"}
[2024-10-11 07:56:32,993][fairseq_cli.train][INFO] - end of epoch 1818 (average epoch stats below)
[2024-10-11 07:56:32,995][train][INFO] - {"epoch": 1818, "train_loss": "0.419", "train_ntokens": "260585", "train_nsentences": "1750.04", "train_wps": "112972", "train_ups": "0.43", "train_wpb": "260585", "train_bsz": "1750", "train_num_updates": "87226", "train_lr": "0.000424965", "train_gnorm": "0.35", "train_loss_scale": "4", "train_train_wall": "46", "train_gb_free": "40.2", "train_wall": "13995"}
[2024-10-11 07:56:33,055][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 07:56:33,058][fairseq.trainer][INFO] - begin training epoch 1819
[2024-10-11 07:56:33,058][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 07:57:54,937][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2024-10-11 07:58:23,638][fairseq_cli.train][INFO] - end of epoch 1819 (average epoch stats below)
[2024-10-11 07:58:23,640][train][INFO] - {"epoch": 1819, "train_loss": "0.421", "train_ntokens": "261184", "train_nsentences": "1737.68", "train_wps": "110950", "train_ups": "0.42", "train_wpb": "261184", "train_bsz": "1737.7", "train_num_updates": "87273", "train_lr": "0.000424901", "train_gnorm": "0.396", "train_loss_scale": "2", "train_train_wall": "42", "train_gb_free": "39.3", "train_wall": "14106"}
[2024-10-11 07:58:23,690][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 07:58:23,692][fairseq.trainer][INFO] - begin training epoch 1820
[2024-10-11 07:58:23,692][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 08:00:15,236][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 1820 @ 87321 updates
[2024-10-11 08:00:15,237][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-11 08:00:25,830][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-11 08:00:25,831][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 1820 @ 87321 updates, score None) (writing took 10.59530610498041 seconds)
[2024-10-11 08:00:25,832][fairseq_cli.train][INFO] - end of epoch 1820 (average epoch stats below)
[2024-10-11 08:00:25,833][train][INFO] - {"epoch": 1820, "train_loss": "0.428", "train_ntokens": "260798", "train_nsentences": "1750.04", "train_wps": "102449", "train_ups": "0.39", "train_wpb": "260798", "train_bsz": "1750", "train_num_updates": "87321", "train_lr": "0.000424836", "train_gnorm": "0.366", "train_loss_scale": "2", "train_train_wall": "50", "train_gb_free": "39.8", "train_wall": "14228"}
[2024-10-11 08:00:25,884][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 08:00:25,888][fairseq.trainer][INFO] - begin training epoch 1821
[2024-10-11 08:00:25,888][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 08:02:11,764][fairseq_cli.train][INFO] - end of epoch 1821 (average epoch stats below)
[2024-10-11 08:02:11,771][train][INFO] - {"epoch": 1821, "train_loss": "0.419", "train_ntokens": "260601", "train_nsentences": "1750.04", "train_wps": "118085", "train_ups": "0.45", "train_wpb": "260601", "train_bsz": "1750", "train_num_updates": "87369", "train_lr": "0.00042477", "train_gnorm": "0.353", "train_loss_scale": "2", "train_train_wall": "42", "train_gb_free": "40.6", "train_wall": "14334"}
[2024-10-11 08:02:11,872][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 08:02:11,884][fairseq.trainer][INFO] - begin training epoch 1822
[2024-10-11 08:02:11,885][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 08:03:48,010][train_inner][INFO] - {"epoch": 1822, "update": 1821.646, "loss": "0.424", "ntokens": "260607", "nsentences": "1766.97", "wps": "114633", "ups": "0.44", "wpb": "260607", "bsz": "1767", "num_updates": "87400", "lr": "0.000424728", "gnorm": "0.367", "loss_scale": "2", "train_wall": "179", "gb_free": "39.8", "wall": "14430"}
[2024-10-11 08:04:00,372][fairseq_cli.train][INFO] - end of epoch 1822 (average epoch stats below)
[2024-10-11 08:04:00,373][train][INFO] - {"epoch": 1822, "train_loss": "0.422", "train_ntokens": "260677", "train_nsentences": "1750.04", "train_wps": "115216", "train_ups": "0.44", "train_wpb": "260677", "train_bsz": "1750", "train_num_updates": "87417", "train_lr": "0.000424705", "train_gnorm": "0.352", "train_loss_scale": "2", "train_train_wall": "36", "train_gb_free": "39.7", "train_wall": "14443"}
[2024-10-11 08:04:00,446][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 08:04:00,448][fairseq.trainer][INFO] - begin training epoch 1823
[2024-10-11 08:04:00,448][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 08:05:49,843][fairseq_cli.train][INFO] - end of epoch 1823 (average epoch stats below)
[2024-10-11 08:05:49,846][train][INFO] - {"epoch": 1823, "train_loss": "0.41", "train_ntokens": "260553", "train_nsentences": "1750.04", "train_wps": "114247", "train_ups": "0.44", "train_wpb": "260553", "train_bsz": "1750", "train_num_updates": "87465", "train_lr": "0.00042464", "train_gnorm": "0.346", "train_loss_scale": "2", "train_train_wall": "35", "train_gb_free": "39.7", "train_wall": "14552"}
[2024-10-11 08:05:49,923][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 08:05:49,931][fairseq.trainer][INFO] - begin training epoch 1824
[2024-10-11 08:05:49,931][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 08:07:42,095][fairseq_cli.train][INFO] - end of epoch 1824 (average epoch stats below)
[2024-10-11 08:07:42,098][train][INFO] - {"epoch": 1824, "train_loss": "0.418", "train_ntokens": "260902", "train_nsentences": "1750.04", "train_wps": "111567", "train_ups": "0.43", "train_wpb": "260902", "train_bsz": "1750", "train_num_updates": "87513", "train_lr": "0.000424575", "train_gnorm": "0.371", "train_loss_scale": "2", "train_train_wall": "45", "train_gb_free": "40.1", "train_wall": "14665"}
[2024-10-11 08:07:42,152][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 08:07:42,155][fairseq.trainer][INFO] - begin training epoch 1825
[2024-10-11 08:07:42,155][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 08:09:33,256][fairseq_cli.train][INFO] - end of epoch 1825 (average epoch stats below)
[2024-10-11 08:09:33,263][train][INFO] - {"epoch": 1825, "train_loss": "0.421", "train_ntokens": "260458", "train_nsentences": "1750.04", "train_wps": "112466", "train_ups": "0.43", "train_wpb": "260458", "train_bsz": "1750", "train_num_updates": "87561", "train_lr": "0.00042451", "train_gnorm": "0.349", "train_loss_scale": "2", "train_train_wall": "47", "train_gb_free": "40.1", "train_wall": "14776"}
[2024-10-11 08:09:33,369][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 08:09:33,381][fairseq.trainer][INFO] - begin training epoch 1826
[2024-10-11 08:09:33,381][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 08:11:14,291][train_inner][INFO] - {"epoch": 1826, "update": 1825.812, "loss": "0.417", "ntokens": "260738", "nsentences": "1745.45", "wps": "116856", "ups": "0.45", "wpb": "260738", "bsz": "1745.5", "num_updates": "87600", "lr": "0.000424457", "gnorm": "0.357", "loss_scale": "2", "train_wall": "174", "gb_free": "40.1", "wall": "14877"}
[2024-10-11 08:11:23,324][fairseq_cli.train][INFO] - end of epoch 1826 (average epoch stats below)
[2024-10-11 08:11:23,330][train][INFO] - {"epoch": 1826, "train_loss": "0.42", "train_ntokens": "260988", "train_nsentences": "1750.04", "train_wps": "113824", "train_ups": "0.44", "train_wpb": "260988", "train_bsz": "1750", "train_num_updates": "87609", "train_lr": "0.000424444", "train_gnorm": "0.369", "train_loss_scale": "2", "train_train_wall": "45", "train_gb_free": "39.6", "train_wall": "14886"}
[2024-10-11 08:11:23,377][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 08:11:23,381][fairseq.trainer][INFO] - begin training epoch 1827
[2024-10-11 08:11:23,381][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 08:13:16,051][fairseq_cli.train][INFO] - end of epoch 1827 (average epoch stats below)
[2024-10-11 08:13:16,053][train][INFO] - {"epoch": 1827, "train_loss": "0.425", "train_ntokens": "260782", "train_nsentences": "1750.04", "train_wps": "111048", "train_ups": "0.43", "train_wpb": "260782", "train_bsz": "1750", "train_num_updates": "87657", "train_lr": "0.000424379", "train_gnorm": "0.341", "train_loss_scale": "2", "train_train_wall": "52", "train_gb_free": "40", "train_wall": "14998"}
[2024-10-11 08:13:16,105][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 08:13:16,108][fairseq.trainer][INFO] - begin training epoch 1828
[2024-10-11 08:13:16,108][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 08:15:06,174][fairseq_cli.train][INFO] - end of epoch 1828 (average epoch stats below)
[2024-10-11 08:15:06,185][train][INFO] - {"epoch": 1828, "train_loss": "0.427", "train_ntokens": "260614", "train_nsentences": "1750.04", "train_wps": "113598", "train_ups": "0.44", "train_wpb": "260614", "train_bsz": "1750", "train_num_updates": "87705", "train_lr": "0.000424314", "train_gnorm": "0.374", "train_loss_scale": "2", "train_train_wall": "41", "train_gb_free": "39.5", "train_wall": "15109"}
[2024-10-11 08:15:06,276][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 08:15:06,279][fairseq.trainer][INFO] - begin training epoch 1829
[2024-10-11 08:15:06,279][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 08:16:59,157][fairseq_cli.train][INFO] - end of epoch 1829 (average epoch stats below)
[2024-10-11 08:16:59,164][train][INFO] - {"epoch": 1829, "train_loss": "0.42", "train_ntokens": "261058", "train_nsentences": "1750.04", "train_wps": "110920", "train_ups": "0.42", "train_wpb": "261058", "train_bsz": "1750", "train_num_updates": "87753", "train_lr": "0.000424249", "train_gnorm": "0.374", "train_loss_scale": "2", "train_train_wall": "46", "train_gb_free": "39.7", "train_wall": "15222"}
[2024-10-11 08:16:59,230][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 08:16:59,234][fairseq.trainer][INFO] - begin training epoch 1830
[2024-10-11 08:16:59,234][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 08:18:47,300][train_inner][INFO] - {"epoch": 1830, "update": 1829.979, "loss": "0.424", "ntokens": "260809", "nsentences": "1753.23", "wps": "115149", "ups": "0.44", "wpb": "260809", "bsz": "1753.2", "num_updates": "87800", "lr": "0.000424185", "gnorm": "0.354", "loss_scale": "2", "train_wall": "189", "gb_free": "40", "wall": "15330"}
[2024-10-11 08:18:47,503][fairseq_cli.train][INFO] - end of epoch 1830 (average epoch stats below)
[2024-10-11 08:18:47,505][train][INFO] - {"epoch": 1830, "train_loss": "0.424", "train_ntokens": "260864", "train_nsentences": "1750.04", "train_wps": "115579", "train_ups": "0.44", "train_wpb": "260864", "train_bsz": "1750", "train_num_updates": "87801", "train_lr": "0.000424183", "train_gnorm": "0.329", "train_loss_scale": "2", "train_train_wall": "40", "train_gb_free": "40.1", "train_wall": "15330"}
[2024-10-11 08:18:47,558][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 08:18:47,560][fairseq.trainer][INFO] - begin training epoch 1831
[2024-10-11 08:18:47,561][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 08:20:37,713][fairseq_cli.train][INFO] - end of epoch 1831 (average epoch stats below)
[2024-10-11 08:20:37,716][train][INFO] - {"epoch": 1831, "train_loss": "0.423", "train_ntokens": "260516", "train_nsentences": "1750.04", "train_wps": "113465", "train_ups": "0.44", "train_wpb": "260516", "train_bsz": "1750", "train_num_updates": "87849", "train_lr": "0.000424118", "train_gnorm": "0.36", "train_loss_scale": "2", "train_train_wall": "49", "train_gb_free": "39.7", "train_wall": "15440"}
[2024-10-11 08:20:37,767][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 08:20:37,769][fairseq.trainer][INFO] - begin training epoch 1832
[2024-10-11 08:20:37,770][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 08:22:24,931][fairseq_cli.train][INFO] - end of epoch 1832 (average epoch stats below)
[2024-10-11 08:22:24,936][train][INFO] - {"epoch": 1832, "train_loss": "0.417", "train_ntokens": "260566", "train_nsentences": "1750.04", "train_wps": "116653", "train_ups": "0.45", "train_wpb": "260566", "train_bsz": "1750", "train_num_updates": "87897", "train_lr": "0.000424053", "train_gnorm": "0.4", "train_loss_scale": "2", "train_train_wall": "31", "train_gb_free": "39.8", "train_wall": "15547"}
[2024-10-11 08:22:25,030][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 08:22:25,032][fairseq.trainer][INFO] - begin training epoch 1833
[2024-10-11 08:22:25,032][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 08:24:14,379][fairseq_cli.train][INFO] - end of epoch 1833 (average epoch stats below)
[2024-10-11 08:24:14,391][train][INFO] - {"epoch": 1833, "train_loss": "0.408", "train_ntokens": "260872", "train_nsentences": "1750.04", "train_wps": "114405", "train_ups": "0.44", "train_wpb": "260872", "train_bsz": "1750", "train_num_updates": "87945", "train_lr": "0.000423988", "train_gnorm": "0.367", "train_loss_scale": "2", "train_train_wall": "44", "train_gb_free": "39.8", "train_wall": "15657"}
[2024-10-11 08:24:14,480][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 08:24:14,493][fairseq.trainer][INFO] - begin training epoch 1834
[2024-10-11 08:24:14,493][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 08:26:01,923][fairseq_cli.train][INFO] - end of epoch 1834 (average epoch stats below)
[2024-10-11 08:26:01,930][train][INFO] - {"epoch": 1834, "train_loss": "0.416", "train_ntokens": "260480", "train_nsentences": "1750.04", "train_wps": "116285", "train_ups": "0.45", "train_wpb": "260480", "train_bsz": "1750", "train_num_updates": "87993", "train_lr": "0.000423923", "train_gnorm": "0.341", "train_loss_scale": "2", "train_train_wall": "44", "train_gb_free": "39.7", "train_wall": "15764"}
[2024-10-11 08:26:02,016][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 08:26:02,033][fairseq.trainer][INFO] - begin training epoch 1835
[2024-10-11 08:26:02,033][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 08:27:21,944][train_inner][INFO] - {"epoch": 1835, "update": 1834.146, "loss": "0.417", "ntokens": "260446", "nsentences": "1760.47", "wps": "101215", "ups": "0.39", "wpb": "260446", "bsz": "1760.5", "num_updates": "88000", "lr": "0.000423913", "gnorm": "0.367", "loss_scale": "2", "train_wall": "180", "gb_free": "40.1", "wall": "15844"}
[2024-10-11 08:27:51,765][fairseq_cli.train][INFO] - end of epoch 1835 (average epoch stats below)
[2024-10-11 08:27:51,770][train][INFO] - {"epoch": 1835, "train_loss": "0.413", "train_ntokens": "260623", "train_nsentences": "1750.04", "train_wps": "113898", "train_ups": "0.44", "train_wpb": "260623", "train_bsz": "1750", "train_num_updates": "88041", "train_lr": "0.000423857", "train_gnorm": "0.353", "train_loss_scale": "2", "train_train_wall": "41", "train_gb_free": "40", "train_wall": "15874"}
[2024-10-11 08:27:51,846][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 08:27:51,858][fairseq.trainer][INFO] - begin training epoch 1836
[2024-10-11 08:27:51,859][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 08:29:40,808][fairseq_cli.train][INFO] - end of epoch 1836 (average epoch stats below)
[2024-10-11 08:29:40,815][train][INFO] - {"epoch": 1836, "train_loss": "0.421", "train_ntokens": "260759", "train_nsentences": "1750.04", "train_wps": "114786", "train_ups": "0.44", "train_wpb": "260759", "train_bsz": "1750", "train_num_updates": "88089", "train_lr": "0.000423792", "train_gnorm": "0.364", "train_loss_scale": "2", "train_train_wall": "44", "train_gb_free": "39.2", "train_wall": "15983"}
[2024-10-11 08:29:40,905][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 08:29:40,908][fairseq.trainer][INFO] - begin training epoch 1837
[2024-10-11 08:29:40,909][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 08:31:29,491][fairseq_cli.train][INFO] - end of epoch 1837 (average epoch stats below)
[2024-10-11 08:31:29,495][train][INFO] - {"epoch": 1837, "train_loss": "0.421", "train_ntokens": "260434", "train_nsentences": "1750.04", "train_wps": "115027", "train_ups": "0.44", "train_wpb": "260434", "train_bsz": "1750", "train_num_updates": "88137", "train_lr": "0.000423727", "train_gnorm": "0.349", "train_loss_scale": "2", "train_train_wall": "44", "train_gb_free": "40", "train_wall": "16092"}
[2024-10-11 08:31:29,557][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 08:31:29,562][fairseq.trainer][INFO] - begin training epoch 1838
[2024-10-11 08:31:29,562][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 08:33:17,487][fairseq_cli.train][INFO] - end of epoch 1838 (average epoch stats below)
[2024-10-11 08:33:17,502][train][INFO] - {"epoch": 1838, "train_loss": "0.422", "train_ntokens": "260763", "train_nsentences": "1750.04", "train_wps": "115891", "train_ups": "0.44", "train_wpb": "260763", "train_bsz": "1750", "train_num_updates": "88185", "train_lr": "0.000423662", "train_gnorm": "0.346", "train_loss_scale": "2", "train_train_wall": "31", "train_gb_free": "39.6", "train_wall": "16200"}
[2024-10-11 08:33:17,594][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 08:33:17,597][fairseq.trainer][INFO] - begin training epoch 1839
[2024-10-11 08:33:17,598][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 08:34:43,276][train_inner][INFO] - {"epoch": 1839, "update": 1838.312, "loss": "0.418", "ntokens": "260785", "nsentences": "1750.14", "wps": "118182", "ups": "0.45", "wpb": "260785", "bsz": "1750.1", "num_updates": "88200", "lr": "0.000423641", "gnorm": "0.354", "loss_scale": "2", "train_wall": "165", "gb_free": "39.7", "wall": "16286"}
[2024-10-11 08:35:06,816][fairseq_cli.train][INFO] - end of epoch 1839 (average epoch stats below)
[2024-10-11 08:35:06,820][train][INFO] - {"epoch": 1839, "train_loss": "0.419", "train_ntokens": "260443", "train_nsentences": "1750.04", "train_wps": "114362", "train_ups": "0.44", "train_wpb": "260443", "train_bsz": "1750", "train_num_updates": "88233", "train_lr": "0.000423596", "train_gnorm": "0.379", "train_loss_scale": "2", "train_train_wall": "33", "train_gb_free": "40.2", "train_wall": "16309"}
[2024-10-11 08:35:06,914][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 08:35:06,924][fairseq.trainer][INFO] - begin training epoch 1840
[2024-10-11 08:35:06,925][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 08:36:54,843][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 1840 @ 88281 updates
[2024-10-11 08:36:54,845][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-11 08:37:05,619][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-11 08:37:05,621][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 1840 @ 88281 updates, score None) (writing took 10.777634548954666 seconds)
[2024-10-11 08:37:05,621][fairseq_cli.train][INFO] - end of epoch 1840 (average epoch stats below)
[2024-10-11 08:37:05,623][train][INFO] - {"epoch": 1840, "train_loss": "0.425", "train_ntokens": "260580", "train_nsentences": "1750.04", "train_wps": "105286", "train_ups": "0.4", "train_wpb": "260580", "train_bsz": "1750", "train_num_updates": "88281", "train_lr": "0.000423531", "train_gnorm": "0.352", "train_loss_scale": "2", "train_train_wall": "45", "train_gb_free": "39.6", "train_wall": "16428"}
[2024-10-11 08:37:05,671][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 08:37:05,686][fairseq.trainer][INFO] - begin training epoch 1841
[2024-10-11 08:37:05,686][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 08:38:54,728][fairseq_cli.train][INFO] - end of epoch 1841 (average epoch stats below)
[2024-10-11 08:38:54,741][train][INFO] - {"epoch": 1841, "train_loss": "0.425", "train_ntokens": "260754", "train_nsentences": "1750.04", "train_wps": "114707", "train_ups": "0.44", "train_wpb": "260754", "train_bsz": "1750", "train_num_updates": "88329", "train_lr": "0.000423466", "train_gnorm": "0.357", "train_loss_scale": "2", "train_train_wall": "27", "train_gb_free": "40", "train_wall": "16537"}
[2024-10-11 08:38:54,824][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 08:38:54,832][fairseq.trainer][INFO] - begin training epoch 1842
[2024-10-11 08:38:54,833][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 08:40:45,777][fairseq_cli.train][INFO] - end of epoch 1842 (average epoch stats below)
[2024-10-11 08:40:45,782][train][INFO] - {"epoch": 1842, "train_loss": "0.423", "train_ntokens": "261275", "train_nsentences": "1750.04", "train_wps": "112946", "train_ups": "0.43", "train_wpb": "261275", "train_bsz": "1750", "train_num_updates": "88377", "train_lr": "0.000423401", "train_gnorm": "0.357", "train_loss_scale": "2", "train_train_wall": "45", "train_gb_free": "40.5", "train_wall": "16648"}
[2024-10-11 08:40:45,850][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 08:40:45,853][fairseq.trainer][INFO] - begin training epoch 1843
[2024-10-11 08:40:45,855][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 08:42:20,905][train_inner][INFO] - {"epoch": 1843, "update": 1842.479, "loss": "0.423", "ntokens": "260745", "nsentences": "1740.63", "wps": "113956", "ups": "0.44", "wpb": "260745", "bsz": "1740.6", "num_updates": "88400", "lr": "0.00042337", "gnorm": "0.363", "loss_scale": "2", "train_wall": "165", "gb_free": "40", "wall": "16743"}
[2024-10-11 08:42:38,901][fairseq_cli.train][INFO] - end of epoch 1843 (average epoch stats below)
[2024-10-11 08:42:38,915][train][INFO] - {"epoch": 1843, "train_loss": "0.409", "train_ntokens": "260768", "train_nsentences": "1750.04", "train_wps": "110655", "train_ups": "0.42", "train_wpb": "260768", "train_bsz": "1750", "train_num_updates": "88425", "train_lr": "0.000423336", "train_gnorm": "0.357", "train_loss_scale": "2", "train_train_wall": "48", "train_gb_free": "39.2", "train_wall": "16761"}
[2024-10-11 08:42:39,026][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 08:42:39,030][fairseq.trainer][INFO] - begin training epoch 1844
[2024-10-11 08:42:39,030][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 08:44:25,988][fairseq_cli.train][INFO] - end of epoch 1844 (average epoch stats below)
[2024-10-11 08:44:25,991][train][INFO] - {"epoch": 1844, "train_loss": "0.415", "train_ntokens": "260892", "train_nsentences": "1750.04", "train_wps": "116956", "train_ups": "0.45", "train_wpb": "260892", "train_bsz": "1750", "train_num_updates": "88473", "train_lr": "0.00042327", "train_gnorm": "0.372", "train_loss_scale": "2", "train_train_wall": "43", "train_gb_free": "39.8", "train_wall": "16868"}
[2024-10-11 08:44:26,083][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 08:44:26,095][fairseq.trainer][INFO] - begin training epoch 1845
[2024-10-11 08:44:26,095][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 08:46:12,397][fairseq_cli.train][INFO] - end of epoch 1845 (average epoch stats below)
[2024-10-11 08:46:12,420][train][INFO] - {"epoch": 1845, "train_loss": "0.411", "train_ntokens": "260415", "train_nsentences": "1750.04", "train_wps": "117452", "train_ups": "0.45", "train_wpb": "260415", "train_bsz": "1750", "train_num_updates": "88521", "train_lr": "0.000423205", "train_gnorm": "0.361", "train_loss_scale": "2", "train_train_wall": "41", "train_gb_free": "39.2", "train_wall": "16975"}
[2024-10-11 08:46:12,558][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 08:46:12,561][fairseq.trainer][INFO] - begin training epoch 1846
[2024-10-11 08:46:12,561][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 08:48:02,808][fairseq_cli.train][INFO] - end of epoch 1846 (average epoch stats below)
[2024-10-11 08:48:02,813][train][INFO] - {"epoch": 1846, "train_loss": "0.417", "train_ntokens": "260494", "train_nsentences": "1750.04", "train_wps": "113270", "train_ups": "0.43", "train_wpb": "260494", "train_bsz": "1750", "train_num_updates": "88569", "train_lr": "0.00042314", "train_gnorm": "0.355", "train_loss_scale": "2", "train_train_wall": "41", "train_gb_free": "39.8", "train_wall": "17085"}
[2024-10-11 08:48:02,866][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 08:48:02,869][fairseq.trainer][INFO] - begin training epoch 1847
[2024-10-11 08:48:02,869][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 08:49:42,548][train_inner][INFO] - {"epoch": 1847, "update": 1846.646, "loss": "0.414", "ntokens": "260889", "nsentences": "1742.78", "wps": "118146", "ups": "0.45", "wpb": "260889", "bsz": "1742.8", "num_updates": "88600", "lr": "0.000423098", "gnorm": "0.363", "loss_scale": "2", "train_wall": "169", "gb_free": "40.1", "wall": "17185"}
[2024-10-11 08:49:55,973][fairseq_cli.train][INFO] - end of epoch 1847 (average epoch stats below)
[2024-10-11 08:49:55,975][train][INFO] - {"epoch": 1847, "train_loss": "0.419", "train_ntokens": "260672", "train_nsentences": "1750.04", "train_wps": "110572", "train_ups": "0.42", "train_wpb": "260672", "train_bsz": "1750", "train_num_updates": "88617", "train_lr": "0.000423075", "train_gnorm": "0.379", "train_loss_scale": "2", "train_train_wall": "38", "train_gb_free": "39.3", "train_wall": "17198"}
[2024-10-11 08:49:56,026][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 08:49:56,030][fairseq.trainer][INFO] - begin training epoch 1848
[2024-10-11 08:49:56,030][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 08:51:49,784][fairseq_cli.train][INFO] - end of epoch 1848 (average epoch stats below)
[2024-10-11 08:51:49,787][train][INFO] - {"epoch": 1848, "train_loss": "0.424", "train_ntokens": "260939", "train_nsentences": "1750.04", "train_wps": "110054", "train_ups": "0.42", "train_wpb": "260939", "train_bsz": "1750", "train_num_updates": "88665", "train_lr": "0.00042301", "train_gnorm": "0.358", "train_loss_scale": "2", "train_train_wall": "55", "train_gb_free": "39.7", "train_wall": "17312"}
[2024-10-11 08:51:49,840][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 08:51:49,843][fairseq.trainer][INFO] - begin training epoch 1849
[2024-10-11 08:51:49,844][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 08:53:43,748][fairseq_cli.train][INFO] - end of epoch 1849 (average epoch stats below)
[2024-10-11 08:53:43,752][train][INFO] - {"epoch": 1849, "train_loss": "0.415", "train_ntokens": "260218", "train_nsentences": "1750.04", "train_wps": "109603", "train_ups": "0.42", "train_wpb": "260218", "train_bsz": "1750", "train_num_updates": "88713", "train_lr": "0.000422944", "train_gnorm": "0.362", "train_loss_scale": "2", "train_train_wall": "42", "train_gb_free": "40.2", "train_wall": "17426"}
[2024-10-11 08:53:43,806][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 08:53:43,808][fairseq.trainer][INFO] - begin training epoch 1850
[2024-10-11 08:53:43,809][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 08:55:34,542][fairseq_cli.train][INFO] - end of epoch 1850 (average epoch stats below)
[2024-10-11 08:55:34,548][train][INFO] - {"epoch": 1850, "train_loss": "0.415", "train_ntokens": "260547", "train_nsentences": "1750.04", "train_wps": "112882", "train_ups": "0.43", "train_wpb": "260547", "train_bsz": "1750", "train_num_updates": "88761", "train_lr": "0.000422879", "train_gnorm": "0.362", "train_loss_scale": "2", "train_train_wall": "41", "train_gb_free": "39.2", "train_wall": "17537"}
[2024-10-11 08:55:34,618][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 08:55:34,621][fairseq.trainer][INFO] - begin training epoch 1851
[2024-10-11 08:55:34,621][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 08:57:18,111][train_inner][INFO] - {"epoch": 1851, "update": 1850.812, "loss": "0.42", "ntokens": "260397", "nsentences": "1755.75", "wps": "114353", "ups": "0.44", "wpb": "260397", "bsz": "1755.8", "num_updates": "88800", "lr": "0.000422826", "gnorm": "0.358", "loss_scale": "2", "train_wall": "185", "gb_free": "39.4", "wall": "17640"}
[2024-10-11 08:57:24,775][fairseq_cli.train][INFO] - end of epoch 1851 (average epoch stats below)
[2024-10-11 08:57:24,777][train][INFO] - {"epoch": 1851, "train_loss": "0.424", "train_ntokens": "260662", "train_nsentences": "1750.04", "train_wps": "113511", "train_ups": "0.44", "train_wpb": "260662", "train_bsz": "1750", "train_num_updates": "88809", "train_lr": "0.000422814", "train_gnorm": "0.359", "train_loss_scale": "2", "train_train_wall": "42", "train_gb_free": "39.6", "train_wall": "17647"}
[2024-10-11 08:57:24,853][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 08:57:24,858][fairseq.trainer][INFO] - begin training epoch 1852
[2024-10-11 08:57:24,860][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 08:59:16,915][fairseq_cli.train][INFO] - end of epoch 1852 (average epoch stats below)
[2024-10-11 08:59:16,918][train][INFO] - {"epoch": 1852, "train_loss": "0.41", "train_ntokens": "261176", "train_nsentences": "1750.04", "train_wps": "111794", "train_ups": "0.43", "train_wpb": "261176", "train_bsz": "1750", "train_num_updates": "88857", "train_lr": "0.000422749", "train_gnorm": "0.371", "train_loss_scale": "2", "train_train_wall": "28", "train_gb_free": "39.8", "train_wall": "17759"}
[2024-10-11 08:59:17,023][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 08:59:17,026][fairseq.trainer][INFO] - begin training epoch 1853
[2024-10-11 08:59:17,026][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 09:01:05,884][fairseq_cli.train][INFO] - end of epoch 1853 (average epoch stats below)
[2024-10-11 09:01:05,890][train][INFO] - {"epoch": 1853, "train_loss": "0.413", "train_ntokens": "260797", "train_nsentences": "1750.04", "train_wps": "114881", "train_ups": "0.44", "train_wpb": "260797", "train_bsz": "1750", "train_num_updates": "88905", "train_lr": "0.000422683", "train_gnorm": "0.352", "train_loss_scale": "2", "train_train_wall": "46", "train_gb_free": "39.8", "train_wall": "17868"}
[2024-10-11 09:01:05,961][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 09:01:05,967][fairseq.trainer][INFO] - begin training epoch 1854
[2024-10-11 09:01:05,967][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 09:02:51,849][fairseq_cli.train][INFO] - end of epoch 1854 (average epoch stats below)
[2024-10-11 09:02:51,853][train][INFO] - {"epoch": 1854, "train_loss": "0.42", "train_ntokens": "261019", "train_nsentences": "1750.04", "train_wps": "118243", "train_ups": "0.45", "train_wpb": "261019", "train_bsz": "1750", "train_num_updates": "88953", "train_lr": "0.000422618", "train_gnorm": "0.366", "train_loss_scale": "2", "train_train_wall": "29", "train_gb_free": "40", "train_wall": "17974"}
[2024-10-11 09:02:51,948][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 09:02:51,961][fairseq.trainer][INFO] - begin training epoch 1855
[2024-10-11 09:02:51,962][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 09:04:40,323][train_inner][INFO] - {"epoch": 1855, "update": 1854.979, "loss": "0.415", "ntokens": "260850", "nsentences": "1749.02", "wps": "117983", "ups": "0.45", "wpb": "260850", "bsz": "1749", "num_updates": "89000", "lr": "0.000422554", "gnorm": "0.368", "loss_scale": "2", "train_wall": "151", "gb_free": "40.1", "wall": "18083"}
[2024-10-11 09:04:40,623][fairseq_cli.train][INFO] - end of epoch 1855 (average epoch stats below)
[2024-10-11 09:04:40,625][train][INFO] - {"epoch": 1855, "train_loss": "0.416", "train_ntokens": "260428", "train_nsentences": "1750.04", "train_wps": "114928", "train_ups": "0.44", "train_wpb": "260428", "train_bsz": "1750", "train_num_updates": "89001", "train_lr": "0.000422553", "train_gnorm": "0.378", "train_loss_scale": "2", "train_train_wall": "42", "train_gb_free": "39.4", "train_wall": "18083"}
[2024-10-11 09:04:40,718][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 09:04:40,727][fairseq.trainer][INFO] - begin training epoch 1856
[2024-10-11 09:04:40,728][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 09:06:30,923][fairseq_cli.train][INFO] - end of epoch 1856 (average epoch stats below)
[2024-10-11 09:06:30,939][train][INFO] - {"epoch": 1856, "train_loss": "0.41", "train_ntokens": "261060", "train_nsentences": "1750.04", "train_wps": "113608", "train_ups": "0.44", "train_wpb": "261060", "train_bsz": "1750", "train_num_updates": "89049", "train_lr": "0.000422488", "train_gnorm": "0.372", "train_loss_scale": "2", "train_train_wall": "30", "train_gb_free": "39.6", "train_wall": "18193"}
[2024-10-11 09:06:31,036][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 09:06:31,038][fairseq.trainer][INFO] - begin training epoch 1857
[2024-10-11 09:06:31,038][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 09:08:18,200][fairseq_cli.train][INFO] - end of epoch 1857 (average epoch stats below)
[2024-10-11 09:08:18,214][train][INFO] - {"epoch": 1857, "train_loss": "0.415", "train_ntokens": "260836", "train_nsentences": "1750.04", "train_wps": "116715", "train_ups": "0.45", "train_wpb": "260836", "train_bsz": "1750", "train_num_updates": "89097", "train_lr": "0.000422423", "train_gnorm": "0.365", "train_loss_scale": "2", "train_train_wall": "35", "train_gb_free": "40.3", "train_wall": "18301"}
[2024-10-11 09:08:18,304][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 09:08:18,311][fairseq.trainer][INFO] - begin training epoch 1858
[2024-10-11 09:08:18,312][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 09:10:09,359][fairseq_cli.train][INFO] - end of epoch 1858 (average epoch stats below)
[2024-10-11 09:10:09,364][train][INFO] - {"epoch": 1858, "train_loss": "0.415", "train_ntokens": "260887", "train_nsentences": "1750.04", "train_wps": "112669", "train_ups": "0.43", "train_wpb": "260887", "train_bsz": "1750", "train_num_updates": "89145", "train_lr": "0.000422357", "train_gnorm": "0.384", "train_loss_scale": "2", "train_train_wall": "36", "train_gb_free": "39.6", "train_wall": "18412"}
[2024-10-11 09:10:09,421][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 09:10:09,424][fairseq.trainer][INFO] - begin training epoch 1859
[2024-10-11 09:10:09,425][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 09:11:59,386][fairseq_cli.train][INFO] - end of epoch 1859 (average epoch stats below)
[2024-10-11 09:11:59,569][train][INFO] - {"epoch": 1859, "train_loss": "0.414", "train_ntokens": "260876", "train_nsentences": "1750.04", "train_wps": "113629", "train_ups": "0.44", "train_wpb": "260876", "train_bsz": "1750", "train_num_updates": "89193", "train_lr": "0.000422292", "train_gnorm": "0.363", "train_loss_scale": "2", "train_train_wall": "40", "train_gb_free": "39.7", "train_wall": "18522"}
[2024-10-11 09:11:59,638][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 09:11:59,642][fairseq.trainer][INFO] - begin training epoch 1860
[2024-10-11 09:11:59,642][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 09:13:24,295][train_inner][INFO] - {"epoch": 1860, "update": 1859.146, "loss": "0.413", "ntokens": "260957", "nsentences": "1743.98", "wps": "99608.1", "ups": "0.38", "wpb": "260957", "bsz": "1744", "num_updates": "89200", "lr": "0.000422283", "gnorm": "0.371", "loss_scale": "2", "train_wall": "145", "gb_free": "39.6", "wall": "18607"}
[2024-10-11 09:13:59,459][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 1860 @ 89241 updates
[2024-10-11 09:13:59,460][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-11 09:14:10,420][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-11 09:14:10,423][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 1860 @ 89241 updates, score None) (writing took 10.963556363247335 seconds)
[2024-10-11 09:14:10,423][fairseq_cli.train][INFO] - end of epoch 1860 (average epoch stats below)
[2024-10-11 09:14:10,425][train][INFO] - {"epoch": 1860, "train_loss": "0.426", "train_ntokens": "260739", "train_nsentences": "1750.04", "train_wps": "95645.9", "train_ups": "0.37", "train_wpb": "260739", "train_bsz": "1750", "train_num_updates": "89241", "train_lr": "0.000422227", "train_gnorm": "0.36", "train_loss_scale": "2", "train_train_wall": "39", "train_gb_free": "40.3", "train_wall": "18653"}
[2024-10-11 09:14:10,475][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 09:14:10,489][fairseq.trainer][INFO] - begin training epoch 1861
[2024-10-11 09:14:10,489][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 09:15:57,535][fairseq_cli.train][INFO] - end of epoch 1861 (average epoch stats below)
[2024-10-11 09:15:57,539][train][INFO] - {"epoch": 1861, "train_loss": "0.422", "train_ntokens": "260506", "train_nsentences": "1750.04", "train_wps": "116744", "train_ups": "0.45", "train_wpb": "260506", "train_bsz": "1750", "train_num_updates": "89289", "train_lr": "0.000422162", "train_gnorm": "0.352", "train_loss_scale": "4", "train_train_wall": "34", "train_gb_free": "40.6", "train_wall": "18760"}
[2024-10-11 09:15:57,652][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 09:15:57,654][fairseq.trainer][INFO] - begin training epoch 1862
[2024-10-11 09:15:57,655][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 09:17:46,534][fairseq_cli.train][INFO] - end of epoch 1862 (average epoch stats below)
[2024-10-11 09:17:46,537][train][INFO] - {"epoch": 1862, "train_loss": "0.416", "train_ntokens": "260650", "train_nsentences": "1750.04", "train_wps": "114789", "train_ups": "0.44", "train_wpb": "260650", "train_bsz": "1750", "train_num_updates": "89337", "train_lr": "0.000422096", "train_gnorm": "0.428", "train_loss_scale": "4", "train_train_wall": "42", "train_gb_free": "40.1", "train_wall": "18869"}
[2024-10-11 09:17:46,604][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 09:17:46,606][fairseq.trainer][INFO] - begin training epoch 1863
[2024-10-11 09:17:46,607][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 09:19:35,283][fairseq_cli.train][INFO] - end of epoch 1863 (average epoch stats below)
[2024-10-11 09:19:35,303][train][INFO] - {"epoch": 1863, "train_loss": "0.417", "train_ntokens": "260355", "train_nsentences": "1750.04", "train_wps": "114913", "train_ups": "0.44", "train_wpb": "260355", "train_bsz": "1750", "train_num_updates": "89385", "train_lr": "0.000422031", "train_gnorm": "0.343", "train_loss_scale": "4", "train_train_wall": "48", "train_gb_free": "40.1", "train_wall": "18978"}
[2024-10-11 09:19:35,389][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 09:19:35,391][fairseq.trainer][INFO] - begin training epoch 1864
[2024-10-11 09:19:35,392][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 09:20:56,966][train_inner][INFO] - {"epoch": 1864, "update": 1863.312, "loss": "0.419", "ntokens": "260763", "nsentences": "1739.74", "wps": "115213", "ups": "0.44", "wpb": "260763", "bsz": "1739.7", "num_updates": "89400", "lr": "0.000422011", "gnorm": "0.368", "loss_scale": "4", "train_wall": "173", "gb_free": "39.2", "wall": "19059"}
[2024-10-11 09:21:25,440][fairseq_cli.train][INFO] - end of epoch 1864 (average epoch stats below)
[2024-10-11 09:21:25,455][train][INFO] - {"epoch": 1864, "train_loss": "0.415", "train_ntokens": "260604", "train_nsentences": "1750.04", "train_wps": "113578", "train_ups": "0.44", "train_wpb": "260604", "train_bsz": "1750", "train_num_updates": "89433", "train_lr": "0.000421966", "train_gnorm": "0.341", "train_loss_scale": "4", "train_train_wall": "42", "train_gb_free": "39.3", "train_wall": "19088"}
[2024-10-11 09:21:25,537][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 09:21:25,545][fairseq.trainer][INFO] - begin training epoch 1865
[2024-10-11 09:21:25,545][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 09:23:14,802][fairseq_cli.train][INFO] - end of epoch 1865 (average epoch stats below)
[2024-10-11 09:23:14,823][train][INFO] - {"epoch": 1865, "train_loss": "0.413", "train_ntokens": "260798", "train_nsentences": "1750.04", "train_wps": "114478", "train_ups": "0.44", "train_wpb": "260798", "train_bsz": "1750", "train_num_updates": "89481", "train_lr": "0.000421901", "train_gnorm": "0.36", "train_loss_scale": "4", "train_train_wall": "46", "train_gb_free": "39.2", "train_wall": "19197"}
[2024-10-11 09:23:14,908][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 09:23:14,913][fairseq.trainer][INFO] - begin training epoch 1866
[2024-10-11 09:23:14,913][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 09:25:06,479][fairseq_cli.train][INFO] - end of epoch 1866 (average epoch stats below)
[2024-10-11 09:25:06,481][train][INFO] - {"epoch": 1866, "train_loss": "0.421", "train_ntokens": "260844", "train_nsentences": "1750.04", "train_wps": "112137", "train_ups": "0.43", "train_wpb": "260844", "train_bsz": "1750", "train_num_updates": "89529", "train_lr": "0.000421836", "train_gnorm": "0.388", "train_loss_scale": "4", "train_train_wall": "46", "train_gb_free": "39.4", "train_wall": "19309"}
[2024-10-11 09:25:06,568][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 09:25:06,571][fairseq.trainer][INFO] - begin training epoch 1867
[2024-10-11 09:25:06,572][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 09:26:55,922][fairseq_cli.train][INFO] - end of epoch 1867 (average epoch stats below)
[2024-10-11 09:26:55,925][train][INFO] - {"epoch": 1867, "train_loss": "0.414", "train_ntokens": "260918", "train_nsentences": "1750.04", "train_wps": "114438", "train_ups": "0.44", "train_wpb": "260918", "train_bsz": "1750", "train_num_updates": "89577", "train_lr": "0.00042177", "train_gnorm": "0.351", "train_loss_scale": "4", "train_train_wall": "37", "train_gb_free": "40.3", "train_wall": "19418"}
[2024-10-11 09:26:56,022][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 09:26:56,025][fairseq.trainer][INFO] - begin training epoch 1868
[2024-10-11 09:26:56,025][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 09:28:33,926][train_inner][INFO] - {"epoch": 1868, "update": 1867.479, "loss": "0.416", "ntokens": "260526", "nsentences": "1766.54", "wps": "114028", "ups": "0.44", "wpb": "260526", "bsz": "1766.5", "num_updates": "89600", "lr": "0.000421739", "gnorm": "0.362", "loss_scale": "4", "train_wall": "168", "gb_free": "41.1", "wall": "19516"}
[2024-10-11 09:28:52,640][fairseq_cli.train][INFO] - end of epoch 1868 (average epoch stats below)
[2024-10-11 09:28:52,643][train][INFO] - {"epoch": 1868, "train_loss": "0.414", "train_ntokens": "260416", "train_nsentences": "1750.04", "train_wps": "107099", "train_ups": "0.41", "train_wpb": "260416", "train_bsz": "1750", "train_num_updates": "89625", "train_lr": "0.000421705", "train_gnorm": "0.375", "train_loss_scale": "4", "train_train_wall": "25", "train_gb_free": "39.6", "train_wall": "19535"}
[2024-10-11 09:28:52,745][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 09:28:52,747][fairseq.trainer][INFO] - begin training epoch 1869
[2024-10-11 09:28:52,747][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 09:30:41,103][fairseq_cli.train][INFO] - end of epoch 1869 (average epoch stats below)
[2024-10-11 09:30:41,111][train][INFO] - {"epoch": 1869, "train_loss": "0.413", "train_ntokens": "260464", "train_nsentences": "1750.04", "train_wps": "115270", "train_ups": "0.44", "train_wpb": "260464", "train_bsz": "1750", "train_num_updates": "89673", "train_lr": "0.00042164", "train_gnorm": "0.369", "train_loss_scale": "4", "train_train_wall": "31", "train_gb_free": "39.6", "train_wall": "19644"}
[2024-10-11 09:30:41,205][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 09:30:41,220][fairseq.trainer][INFO] - begin training epoch 1870
[2024-10-11 09:30:41,221][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 09:32:36,330][fairseq_cli.train][INFO] - end of epoch 1870 (average epoch stats below)
[2024-10-11 09:32:36,333][train][INFO] - {"epoch": 1870, "train_loss": "0.413", "train_ntokens": "260662", "train_nsentences": "1750.04", "train_wps": "108592", "train_ups": "0.42", "train_wpb": "260662", "train_bsz": "1750", "train_num_updates": "89721", "train_lr": "0.000421575", "train_gnorm": "0.345", "train_loss_scale": "4", "train_train_wall": "26", "train_gb_free": "39.8", "train_wall": "19759"}
[2024-10-11 09:32:36,392][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 09:32:36,395][fairseq.trainer][INFO] - begin training epoch 1871
[2024-10-11 09:32:36,395][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 09:34:23,915][fairseq_cli.train][INFO] - end of epoch 1871 (average epoch stats below)
[2024-10-11 09:34:23,918][train][INFO] - {"epoch": 1871, "train_loss": "0.413", "train_ntokens": "260784", "train_nsentences": "1750.04", "train_wps": "116355", "train_ups": "0.45", "train_wpb": "260784", "train_bsz": "1750", "train_num_updates": "89769", "train_lr": "0.00042151", "train_gnorm": "0.362", "train_loss_scale": "4", "train_train_wall": "52", "train_gb_free": "39.3", "train_wall": "19866"}
[2024-10-11 09:34:24,040][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 09:34:24,043][fairseq.trainer][INFO] - begin training epoch 1872
[2024-10-11 09:34:24,043][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 09:35:58,631][train_inner][INFO] - {"epoch": 1872, "update": 1871.646, "loss": "0.414", "ntokens": "260604", "nsentences": "1740.85", "wps": "117205", "ups": "0.45", "wpb": "260604", "bsz": "1740.8", "num_updates": "89800", "lr": "0.000421467", "gnorm": "0.364", "loss_scale": "4", "train_wall": "157", "gb_free": "39.7", "wall": "19961"}
[2024-10-11 09:36:12,307][fairseq_cli.train][INFO] - end of epoch 1872 (average epoch stats below)
[2024-10-11 09:36:12,308][train][INFO] - {"epoch": 1872, "train_loss": "0.422", "train_ntokens": "260697", "train_nsentences": "1750.04", "train_wps": "115451", "train_ups": "0.44", "train_wpb": "260697", "train_bsz": "1750", "train_num_updates": "89817", "train_lr": "0.000421444", "train_gnorm": "0.379", "train_loss_scale": "4", "train_train_wall": "46", "train_gb_free": "40", "train_wall": "19975"}
[2024-10-11 09:36:12,378][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 09:36:12,380][fairseq.trainer][INFO] - begin training epoch 1873
[2024-10-11 09:36:12,381][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 09:38:02,597][fairseq_cli.train][INFO] - end of epoch 1873 (average epoch stats below)
[2024-10-11 09:38:02,611][train][INFO] - {"epoch": 1873, "train_loss": "0.408", "train_ntokens": "260795", "train_nsentences": "1750.04", "train_wps": "113498", "train_ups": "0.44", "train_wpb": "260795", "train_bsz": "1750", "train_num_updates": "89865", "train_lr": "0.000421379", "train_gnorm": "0.339", "train_loss_scale": "4", "train_train_wall": "45", "train_gb_free": "39.2", "train_wall": "20085"}
[2024-10-11 09:38:02,687][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 09:38:02,691][fairseq.trainer][INFO] - begin training epoch 1874
[2024-10-11 09:38:02,692][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 09:39:51,961][fairseq_cli.train][INFO] - end of epoch 1874 (average epoch stats below)
[2024-10-11 09:39:51,968][train][INFO] - {"epoch": 1874, "train_loss": "0.417", "train_ntokens": "260455", "train_nsentences": "1750.04", "train_wps": "114326", "train_ups": "0.44", "train_wpb": "260455", "train_bsz": "1750", "train_num_updates": "89913", "train_lr": "0.000421314", "train_gnorm": "0.37", "train_loss_scale": "4", "train_train_wall": "36", "train_gb_free": "39.9", "train_wall": "20194"}
[2024-10-11 09:39:52,195][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 09:39:52,204][fairseq.trainer][INFO] - begin training epoch 1875
[2024-10-11 09:39:52,204][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 09:41:39,450][fairseq_cli.train][INFO] - end of epoch 1875 (average epoch stats below)
[2024-10-11 09:41:39,455][train][INFO] - {"epoch": 1875, "train_loss": "0.412", "train_ntokens": "260658", "train_nsentences": "1750.04", "train_wps": "116406", "train_ups": "0.45", "train_wpb": "260658", "train_bsz": "1750", "train_num_updates": "89961", "train_lr": "0.000421249", "train_gnorm": "0.379", "train_loss_scale": "4", "train_train_wall": "47", "train_gb_free": "39.8", "train_wall": "20302"}
[2024-10-11 09:41:39,557][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 09:41:39,560][fairseq.trainer][INFO] - begin training epoch 1876
[2024-10-11 09:41:39,561][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 09:43:21,363][train_inner][INFO] - {"epoch": 1876, "update": 1875.812, "loss": "0.415", "ntokens": "260691", "nsentences": "1763.38", "wps": "117774", "ups": "0.45", "wpb": "260691", "bsz": "1763.4", "num_updates": "90000", "lr": "0.000421196", "gnorm": "0.362", "loss_scale": "4", "train_wall": "183", "gb_free": "40.3", "wall": "20404"}
[2024-10-11 09:43:29,025][fairseq_cli.train][INFO] - end of epoch 1876 (average epoch stats below)
[2024-10-11 09:43:29,028][train][INFO] - {"epoch": 1876, "train_loss": "0.415", "train_ntokens": "260510", "train_nsentences": "1750.04", "train_wps": "114123", "train_ups": "0.44", "train_wpb": "260510", "train_bsz": "1750", "train_num_updates": "90009", "train_lr": "0.000421183", "train_gnorm": "0.346", "train_loss_scale": "4", "train_train_wall": "48", "train_gb_free": "40.3", "train_wall": "20411"}
[2024-10-11 09:43:29,080][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 09:43:29,083][fairseq.trainer][INFO] - begin training epoch 1877
[2024-10-11 09:43:29,083][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 09:45:18,194][fairseq_cli.train][INFO] - end of epoch 1877 (average epoch stats below)
[2024-10-11 09:45:18,204][train][INFO] - {"epoch": 1877, "train_loss": "0.414", "train_ntokens": "261062", "train_nsentences": "1750.04", "train_wps": "114787", "train_ups": "0.44", "train_wpb": "261062", "train_bsz": "1750", "train_num_updates": "90057", "train_lr": "0.000421118", "train_gnorm": "0.353", "train_loss_scale": "4", "train_train_wall": "36", "train_gb_free": "39.8", "train_wall": "20521"}
[2024-10-11 09:45:18,295][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 09:45:18,298][fairseq.trainer][INFO] - begin training epoch 1878
[2024-10-11 09:45:18,298][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 09:47:08,419][fairseq_cli.train][INFO] - end of epoch 1878 (average epoch stats below)
[2024-10-11 09:47:08,423][train][INFO] - {"epoch": 1878, "train_loss": "0.416", "train_ntokens": "260277", "train_nsentences": "1750.04", "train_wps": "113354", "train_ups": "0.44", "train_wpb": "260277", "train_bsz": "1750", "train_num_updates": "90105", "train_lr": "0.000421053", "train_gnorm": "0.344", "train_loss_scale": "4", "train_train_wall": "36", "train_gb_free": "39.3", "train_wall": "20631"}
[2024-10-11 09:47:08,514][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 09:47:08,517][fairseq.trainer][INFO] - begin training epoch 1879
[2024-10-11 09:47:08,518][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 09:49:00,618][fairseq_cli.train][INFO] - end of epoch 1879 (average epoch stats below)
[2024-10-11 09:49:00,623][train][INFO] - {"epoch": 1879, "train_loss": "0.421", "train_ntokens": "260856", "train_nsentences": "1750.04", "train_wps": "111599", "train_ups": "0.43", "train_wpb": "260856", "train_bsz": "1750", "train_num_updates": "90153", "train_lr": "0.000420988", "train_gnorm": "0.366", "train_loss_scale": "4", "train_train_wall": "46", "train_gb_free": "39.9", "train_wall": "20743"}
[2024-10-11 09:49:00,710][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 09:49:00,715][fairseq.trainer][INFO] - begin training epoch 1880
[2024-10-11 09:49:00,718][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 09:51:15,167][train_inner][INFO] - {"epoch": 1880, "update": 1879.979, "loss": "0.415", "ntokens": "260642", "nsentences": "1744.94", "wps": "110029", "ups": "0.42", "wpb": "260642", "bsz": "1744.9", "num_updates": "90200", "lr": "0.000420924", "gnorm": "0.355", "loss_scale": "4", "train_wall": "169", "gb_free": "40.1", "wall": "20878"}
[2024-10-11 09:51:15,380][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 1880 @ 90201 updates
[2024-10-11 09:51:15,381][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-11 09:51:26,073][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-11 09:51:26,075][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 1880 @ 90201 updates, score None) (writing took 10.695046535693109 seconds)
[2024-10-11 09:51:26,075][fairseq_cli.train][INFO] - end of epoch 1880 (average epoch stats below)
[2024-10-11 09:51:26,077][train][INFO] - {"epoch": 1880, "train_loss": "0.409", "train_ntokens": "260755", "train_nsentences": "1750.04", "train_wps": "86051.3", "train_ups": "0.33", "train_wpb": "260755", "train_bsz": "1750", "train_num_updates": "90201", "train_lr": "0.000420923", "train_gnorm": "0.361", "train_loss_scale": "4", "train_train_wall": "43", "train_gb_free": "40.7", "train_wall": "20888"}
[2024-10-11 09:51:26,151][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 09:51:26,155][fairseq.trainer][INFO] - begin training epoch 1881
[2024-10-11 09:51:26,155][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 09:53:27,871][fairseq_cli.train][INFO] - end of epoch 1881 (average epoch stats below)
[2024-10-11 09:53:27,880][train][INFO] - {"epoch": 1881, "train_loss": "0.419", "train_ntokens": "260483", "train_nsentences": "1750.04", "train_wps": "102654", "train_ups": "0.39", "train_wpb": "260483", "train_bsz": "1750", "train_num_updates": "90249", "train_lr": "0.000420857", "train_gnorm": "0.335", "train_loss_scale": "4", "train_train_wall": "46", "train_gb_free": "39.8", "train_wall": "21010"}
[2024-10-11 09:53:28,002][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 09:53:28,022][fairseq.trainer][INFO] - begin training epoch 1882
[2024-10-11 09:53:28,023][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 09:55:38,108][fairseq_cli.train][INFO] - end of epoch 1882 (average epoch stats below)
[2024-10-11 09:55:38,111][train][INFO] - {"epoch": 1882, "train_loss": "0.42", "train_ntokens": "261131", "train_nsentences": "1750.04", "train_wps": "96248.8", "train_ups": "0.37", "train_wpb": "261131", "train_bsz": "1750", "train_num_updates": "90297", "train_lr": "0.000420792", "train_gnorm": "0.36", "train_loss_scale": "4", "train_train_wall": "37", "train_gb_free": "40", "train_wall": "21141"}
[2024-10-11 09:55:38,248][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 09:55:38,258][fairseq.trainer][INFO] - begin training epoch 1883
[2024-10-11 09:55:38,258][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 09:57:35,863][fairseq_cli.train][INFO] - end of epoch 1883 (average epoch stats below)
[2024-10-11 09:57:35,866][train][INFO] - {"epoch": 1883, "train_loss": "0.415", "train_ntokens": "260926", "train_nsentences": "1750.04", "train_wps": "106364", "train_ups": "0.41", "train_wpb": "260926", "train_bsz": "1750", "train_num_updates": "90345", "train_lr": "0.000420727", "train_gnorm": "0.358", "train_loss_scale": "4", "train_train_wall": "37", "train_gb_free": "40", "train_wall": "21258"}
[2024-10-11 09:57:35,954][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 09:57:35,962][fairseq.trainer][INFO] - begin training epoch 1884
[2024-10-11 09:57:35,962][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 09:59:38,575][fairseq_cli.train][INFO] - end of epoch 1884 (average epoch stats below)
[2024-10-11 09:59:38,595][train][INFO] - {"epoch": 1884, "train_loss": "0.422", "train_ntokens": "261226", "train_nsentences": "1750.04", "train_wps": "102182", "train_ups": "0.39", "train_wpb": "261226", "train_bsz": "1750", "train_num_updates": "90393", "train_lr": "0.000420662", "train_gnorm": "0.356", "train_loss_scale": "4", "train_train_wall": "34", "train_gb_free": "39.2", "train_wall": "21381"}
[2024-10-11 09:59:38,687][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 09:59:38,701][fairseq.trainer][INFO] - begin training epoch 1885
[2024-10-11 09:59:38,702][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 10:01:11,833][train_inner][INFO] - {"epoch": 1885, "update": 1884.146, "loss": "0.419", "ntokens": "260745", "nsentences": "1754.71", "wps": "87401.3", "ups": "0.34", "wpb": "260745", "bsz": "1754.7", "num_updates": "90400", "lr": "0.000420652", "gnorm": "0.352", "loss_scale": "4", "train_wall": "166", "gb_free": "39.2", "wall": "21474"}
[2024-10-11 10:01:34,642][fairseq_cli.train][INFO] - end of epoch 1885 (average epoch stats below)
[2024-10-11 10:01:34,645][train][INFO] - {"epoch": 1885, "train_loss": "0.41", "train_ntokens": "260819", "train_nsentences": "1750.04", "train_wps": "107882", "train_ups": "0.41", "train_wpb": "260819", "train_bsz": "1750", "train_num_updates": "90441", "train_lr": "0.000420596", "train_gnorm": "0.37", "train_loss_scale": "4", "train_train_wall": "33", "train_gb_free": "39.8", "train_wall": "21497"}
[2024-10-11 10:01:35,132][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 10:01:35,148][fairseq.trainer][INFO] - begin training epoch 1886
[2024-10-11 10:01:35,148][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 10:03:27,936][fairseq_cli.train][INFO] - end of epoch 1886 (average epoch stats below)
[2024-10-11 10:03:27,944][train][INFO] - {"epoch": 1886, "train_loss": "0.422", "train_ntokens": "260645", "train_nsentences": "1750.04", "train_wps": "110428", "train_ups": "0.42", "train_wpb": "260645", "train_bsz": "1750", "train_num_updates": "90489", "train_lr": "0.000420531", "train_gnorm": "0.372", "train_loss_scale": "4", "train_train_wall": "24", "train_gb_free": "40", "train_wall": "21610"}
[2024-10-11 10:03:28,048][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 10:03:28,069][fairseq.trainer][INFO] - begin training epoch 1887
[2024-10-11 10:03:28,069][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 10:05:23,167][fairseq_cli.train][INFO] - end of epoch 1887 (average epoch stats below)
[2024-10-11 10:05:23,175][train][INFO] - {"epoch": 1887, "train_loss": "0.412", "train_ntokens": "260903", "train_nsentences": "1750.04", "train_wps": "108684", "train_ups": "0.42", "train_wpb": "260903", "train_bsz": "1750", "train_num_updates": "90537", "train_lr": "0.000420466", "train_gnorm": "0.345", "train_loss_scale": "4", "train_train_wall": "41", "train_gb_free": "40.1", "train_wall": "21726"}
[2024-10-11 10:05:23,250][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 10:05:23,253][fairseq.trainer][INFO] - begin training epoch 1888
[2024-10-11 10:05:23,253][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 10:07:22,128][fairseq_cli.train][INFO] - end of epoch 1888 (average epoch stats below)
[2024-10-11 10:07:22,130][train][INFO] - {"epoch": 1888, "train_loss": "0.419", "train_ntokens": "260327", "train_nsentences": "1750.04", "train_wps": "105048", "train_ups": "0.4", "train_wpb": "260327", "train_bsz": "1750", "train_num_updates": "90585", "train_lr": "0.000420401", "train_gnorm": "0.365", "train_loss_scale": "4", "train_train_wall": "45", "train_gb_free": "39.6", "train_wall": "21845"}
[2024-10-11 10:07:22,218][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 10:07:22,222][fairseq.trainer][INFO] - begin training epoch 1889
[2024-10-11 10:07:22,223][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 10:08:52,407][train_inner][INFO] - {"epoch": 1889, "update": 1888.312, "loss": "0.416", "ntokens": "261011", "nsentences": "1738.58", "wps": "113345", "ups": "0.43", "wpb": "261011", "bsz": "1738.6", "num_updates": "90600", "lr": "0.00042038", "gnorm": "0.363", "loss_scale": "4", "train_wall": "145", "gb_free": "40.5", "wall": "21935"}
[2024-10-11 10:09:21,686][fairseq_cli.train][INFO] - end of epoch 1889 (average epoch stats below)
[2024-10-11 10:09:21,700][train][INFO] - {"epoch": 1889, "train_loss": "0.416", "train_ntokens": "260623", "train_nsentences": "1750.04", "train_wps": "104638", "train_ups": "0.4", "train_wpb": "260623", "train_bsz": "1750", "train_num_updates": "90633", "train_lr": "0.000420336", "train_gnorm": "0.346", "train_loss_scale": "4", "train_train_wall": "41", "train_gb_free": "39.3", "train_wall": "21964"}
[2024-10-11 10:09:21,802][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 10:09:21,805][fairseq.trainer][INFO] - begin training epoch 1890
[2024-10-11 10:09:21,805][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 10:11:19,672][fairseq_cli.train][INFO] - end of epoch 1890 (average epoch stats below)
[2024-10-11 10:11:19,680][train][INFO] - {"epoch": 1890, "train_loss": "0.415", "train_ntokens": "260107", "train_nsentences": "1750.04", "train_wps": "105828", "train_ups": "0.41", "train_wpb": "260107", "train_bsz": "1750", "train_num_updates": "90681", "train_lr": "0.00042027", "train_gnorm": "0.351", "train_loss_scale": "4", "train_train_wall": "40", "train_gb_free": "39.6", "train_wall": "22082"}
[2024-10-11 10:11:19,839][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 10:11:19,849][fairseq.trainer][INFO] - begin training epoch 1891
[2024-10-11 10:11:19,850][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 10:13:11,918][fairseq_cli.train][INFO] - end of epoch 1891 (average epoch stats below)
[2024-10-11 10:13:11,923][train][INFO] - {"epoch": 1891, "train_loss": "0.412", "train_ntokens": "260405", "train_nsentences": "1750.04", "train_wps": "111365", "train_ups": "0.43", "train_wpb": "260405", "train_bsz": "1750", "train_num_updates": "90729", "train_lr": "0.000420205", "train_gnorm": "0.364", "train_loss_scale": "4", "train_train_wall": "42", "train_gb_free": "39.6", "train_wall": "22194"}
[2024-10-11 10:13:12,058][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 10:13:12,060][fairseq.trainer][INFO] - begin training epoch 1892
[2024-10-11 10:13:12,061][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 10:15:05,096][fairseq_cli.train][INFO] - end of epoch 1892 (average epoch stats below)
[2024-10-11 10:15:05,099][train][INFO] - {"epoch": 1892, "train_loss": "0.416", "train_ntokens": "261103", "train_nsentences": "1750.04", "train_wps": "110742", "train_ups": "0.42", "train_wpb": "261103", "train_bsz": "1750", "train_num_updates": "90777", "train_lr": "0.00042014", "train_gnorm": "0.356", "train_loss_scale": "4", "train_train_wall": "42", "train_gb_free": "40.2", "train_wall": "22308"}
[2024-10-11 10:15:05,179][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 10:15:05,182][fairseq.trainer][INFO] - begin training epoch 1893
[2024-10-11 10:15:05,183][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 10:16:41,295][train_inner][INFO] - {"epoch": 1893, "update": 1892.479, "loss": "0.414", "ntokens": "260615", "nsentences": "1749.88", "wps": "111164", "ups": "0.43", "wpb": "260614", "bsz": "1749.9", "num_updates": "90800", "lr": "0.000420109", "gnorm": "0.354", "loss_scale": "4", "train_wall": "185", "gb_free": "39.6", "wall": "22404"}
[2024-10-11 10:16:59,773][fairseq_cli.train][INFO] - end of epoch 1893 (average epoch stats below)
[2024-10-11 10:16:59,774][train][INFO] - {"epoch": 1893, "train_loss": "0.411", "train_ntokens": "260696", "train_nsentences": "1750.04", "train_wps": "109131", "train_ups": "0.42", "train_wpb": "260696", "train_bsz": "1750", "train_num_updates": "90825", "train_lr": "0.000420075", "train_gnorm": "0.351", "train_loss_scale": "4", "train_train_wall": "49", "train_gb_free": "39.8", "train_wall": "22422"}
[2024-10-11 10:16:59,873][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 10:16:59,876][fairseq.trainer][INFO] - begin training epoch 1894
[2024-10-11 10:16:59,876][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 10:18:55,325][fairseq_cli.train][INFO] - end of epoch 1894 (average epoch stats below)
[2024-10-11 10:18:55,331][train][INFO] - {"epoch": 1894, "train_loss": "0.415", "train_ntokens": "260419", "train_nsentences": "1750.04", "train_wps": "108179", "train_ups": "0.42", "train_wpb": "260420", "train_bsz": "1750", "train_num_updates": "90873", "train_lr": "0.00042001", "train_gnorm": "0.368", "train_loss_scale": "4", "train_train_wall": "42", "train_gb_free": "40.2", "train_wall": "22538"}
[2024-10-11 10:18:55,440][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 10:18:55,442][fairseq.trainer][INFO] - begin training epoch 1895
[2024-10-11 10:18:55,442][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 10:20:52,793][fairseq_cli.train][INFO] - end of epoch 1895 (average epoch stats below)
[2024-10-11 10:20:52,810][train][INFO] - {"epoch": 1895, "train_loss": "0.417", "train_ntokens": "260581", "train_nsentences": "1750.04", "train_wps": "106473", "train_ups": "0.41", "train_wpb": "260581", "train_bsz": "1750", "train_num_updates": "90921", "train_lr": "0.000419944", "train_gnorm": "0.35", "train_loss_scale": "4", "train_train_wall": "48", "train_gb_free": "39.6", "train_wall": "22655"}
[2024-10-11 10:20:52,905][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 10:20:52,919][fairseq.trainer][INFO] - begin training epoch 1896
[2024-10-11 10:20:52,920][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 10:22:52,614][fairseq_cli.train][INFO] - end of epoch 1896 (average epoch stats below)
[2024-10-11 10:22:52,620][train][INFO] - {"epoch": 1896, "train_loss": "0.413", "train_ntokens": "260320", "train_nsentences": "1750.04", "train_wps": "104298", "train_ups": "0.4", "train_wpb": "260320", "train_bsz": "1750", "train_num_updates": "90969", "train_lr": "0.000419879", "train_gnorm": "0.363", "train_loss_scale": "4", "train_train_wall": "38", "train_gb_free": "39.1", "train_wall": "22775"}
[2024-10-11 10:22:52,705][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 10:22:52,708][fairseq.trainer][INFO] - begin training epoch 1897
[2024-10-11 10:22:52,708][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 10:24:30,549][train_inner][INFO] - {"epoch": 1897, "update": 1896.646, "loss": "0.414", "ntokens": "260519", "nsentences": "1750.09", "wps": "111037", "ups": "0.43", "wpb": "260520", "bsz": "1750.1", "num_updates": "91000", "lr": "0.000419837", "gnorm": "0.356", "loss_scale": "4", "train_wall": "182", "gb_free": "39.7", "wall": "22873"}
[2024-10-11 10:24:45,628][fairseq_cli.train][INFO] - end of epoch 1897 (average epoch stats below)
[2024-10-11 10:24:45,631][train][INFO] - {"epoch": 1897, "train_loss": "0.417", "train_ntokens": "260611", "train_nsentences": "1750.04", "train_wps": "110695", "train_ups": "0.42", "train_wpb": "260611", "train_bsz": "1750", "train_num_updates": "91017", "train_lr": "0.000419814", "train_gnorm": "0.358", "train_loss_scale": "4", "train_train_wall": "53", "train_gb_free": "40.1", "train_wall": "22888"}
[2024-10-11 10:24:45,685][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 10:24:45,691][fairseq.trainer][INFO] - begin training epoch 1898
[2024-10-11 10:24:45,692][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 10:26:32,822][fairseq_cli.train][INFO] - end of epoch 1898 (average epoch stats below)
[2024-10-11 10:26:32,825][train][INFO] - {"epoch": 1898, "train_loss": "0.406", "train_ntokens": "260796", "train_nsentences": "1750.04", "train_wps": "116785", "train_ups": "0.45", "train_wpb": "260796", "train_bsz": "1750", "train_num_updates": "91065", "train_lr": "0.000419749", "train_gnorm": "0.36", "train_loss_scale": "4", "train_train_wall": "45", "train_gb_free": "39.8", "train_wall": "22995"}
[2024-10-11 10:26:32,874][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 10:26:32,877][fairseq.trainer][INFO] - begin training epoch 1899
[2024-10-11 10:26:32,877][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 10:28:22,792][fairseq_cli.train][INFO] - end of epoch 1899 (average epoch stats below)
[2024-10-11 10:28:22,804][train][INFO] - {"epoch": 1899, "train_loss": "0.419", "train_ntokens": "260644", "train_nsentences": "1750.04", "train_wps": "113768", "train_ups": "0.44", "train_wpb": "260644", "train_bsz": "1750", "train_num_updates": "91113", "train_lr": "0.000419683", "train_gnorm": "0.378", "train_loss_scale": "4", "train_train_wall": "45", "train_gb_free": "41.1", "train_wall": "23105"}
[2024-10-11 10:28:22,883][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 10:28:22,886][fairseq.trainer][INFO] - begin training epoch 1900
[2024-10-11 10:28:22,887][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 10:30:11,220][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 1900 @ 91161 updates
[2024-10-11 10:30:11,221][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-11 10:30:22,342][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-11 10:30:22,345][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 1900 @ 91161 updates, score None) (writing took 11.125154420733452 seconds)
[2024-10-11 10:30:22,345][fairseq_cli.train][INFO] - end of epoch 1900 (average epoch stats below)
[2024-10-11 10:30:22,347][train][INFO] - {"epoch": 1900, "train_loss": "0.416", "train_ntokens": "260557", "train_nsentences": "1750.04", "train_wps": "104625", "train_ups": "0.4", "train_wpb": "260557", "train_bsz": "1750", "train_num_updates": "91161", "train_lr": "0.000419618", "train_gnorm": "0.37", "train_loss_scale": "4", "train_train_wall": "43", "train_gb_free": "39.6", "train_wall": "23225"}
[2024-10-11 10:30:22,397][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 10:30:22,401][fairseq.trainer][INFO] - begin training epoch 1901
[2024-10-11 10:30:22,401][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 10:32:07,075][train_inner][INFO] - {"epoch": 1901, "update": 1900.812, "loss": "0.415", "ntokens": "260436", "nsentences": "1759.13", "wps": "114105", "ups": "0.44", "wpb": "260436", "bsz": "1759.1", "num_updates": "91200", "lr": "0.000419565", "gnorm": "0.368", "loss_scale": "4", "train_wall": "165", "gb_free": "39.6", "wall": "23329"}
[2024-10-11 10:32:15,234][fairseq_cli.train][INFO] - end of epoch 1901 (average epoch stats below)
[2024-10-11 10:32:15,251][train][INFO] - {"epoch": 1901, "train_loss": "0.412", "train_ntokens": "260684", "train_nsentences": "1750.04", "train_wps": "110846", "train_ups": "0.43", "train_wpb": "260684", "train_bsz": "1750", "train_num_updates": "91209", "train_lr": "0.000419553", "train_gnorm": "0.357", "train_loss_scale": "4", "train_train_wall": "26", "train_gb_free": "40.3", "train_wall": "23338"}
[2024-10-11 10:32:15,341][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 10:32:15,364][fairseq.trainer][INFO] - begin training epoch 1902
[2024-10-11 10:32:15,365][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 10:34:04,637][fairseq_cli.train][INFO] - end of epoch 1902 (average epoch stats below)
[2024-10-11 10:34:04,655][train][INFO] - {"epoch": 1902, "train_loss": "0.417", "train_ntokens": "260579", "train_nsentences": "1750.04", "train_wps": "114334", "train_ups": "0.44", "train_wpb": "260579", "train_bsz": "1750", "train_num_updates": "91257", "train_lr": "0.000419488", "train_gnorm": "0.375", "train_loss_scale": "4", "train_train_wall": "42", "train_gb_free": "39.6", "train_wall": "23447"}
[2024-10-11 10:34:04,802][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 10:34:04,805][fairseq.trainer][INFO] - begin training epoch 1903
[2024-10-11 10:34:04,805][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 10:35:53,264][fairseq_cli.train][INFO] - end of epoch 1903 (average epoch stats below)
[2024-10-11 10:35:53,272][train][INFO] - {"epoch": 1903, "train_loss": "0.415", "train_ntokens": "260828", "train_nsentences": "1750.04", "train_wps": "115284", "train_ups": "0.44", "train_wpb": "260828", "train_bsz": "1750", "train_num_updates": "91305", "train_lr": "0.000419423", "train_gnorm": "0.35", "train_loss_scale": "4", "train_train_wall": "34", "train_gb_free": "39.3", "train_wall": "23556"}
[2024-10-11 10:35:53,381][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 10:35:53,393][fairseq.trainer][INFO] - begin training epoch 1904
[2024-10-11 10:35:53,393][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 10:37:28,383][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
[2024-10-11 10:37:41,833][fairseq_cli.train][INFO] - end of epoch 1904 (average epoch stats below)
[2024-10-11 10:37:41,835][train][INFO] - {"epoch": 1904, "train_loss": "0.411", "train_ntokens": "260601", "train_nsentences": "1755.53", "train_wps": "112825", "train_ups": "0.43", "train_wpb": "260601", "train_bsz": "1755.5", "train_num_updates": "91352", "train_lr": "0.000419359", "train_gnorm": "0.385", "train_loss_scale": "4", "train_train_wall": "33", "train_gb_free": "39.6", "train_wall": "23664"}
[2024-10-11 10:37:41,935][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 10:37:41,944][fairseq.trainer][INFO] - begin training epoch 1905
[2024-10-11 10:37:41,945][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 10:39:38,723][train_inner][INFO] - {"epoch": 1905, "update": 1905.0, "loss": "0.414", "ntokens": "260717", "nsentences": "1749.71", "wps": "115463", "ups": "0.44", "wpb": "260717", "bsz": "1749.7", "num_updates": "91400", "lr": "0.000419293", "gnorm": "0.374", "loss_scale": "4", "train_wall": "162", "gb_free": "40", "wall": "23781"}
[2024-10-11 10:39:38,729][fairseq_cli.train][INFO] - end of epoch 1905 (average epoch stats below)
[2024-10-11 10:39:38,731][train][INFO] - {"epoch": 1905, "train_loss": "0.413", "train_ntokens": "260755", "train_nsentences": "1750.04", "train_wps": "107076", "train_ups": "0.41", "train_wpb": "260755", "train_bsz": "1750", "train_num_updates": "91400", "train_lr": "0.000419293", "train_gnorm": "0.391", "train_loss_scale": "4", "train_train_wall": "45", "train_gb_free": "40", "train_wall": "23781"}
[2024-10-11 10:39:38,823][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 10:39:38,837][fairseq.trainer][INFO] - begin training epoch 1906
[2024-10-11 10:39:38,837][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 10:41:33,423][fairseq_cli.train][INFO] - end of epoch 1906 (average epoch stats below)
[2024-10-11 10:41:33,427][train][INFO] - {"epoch": 1906, "train_loss": "0.408", "train_ntokens": "260768", "train_nsentences": "1750.04", "train_wps": "109139", "train_ups": "0.42", "train_wpb": "260768", "train_bsz": "1750", "train_num_updates": "91448", "train_lr": "0.000419228", "train_gnorm": "0.365", "train_loss_scale": "4", "train_train_wall": "44", "train_gb_free": "39.8", "train_wall": "23896"}
[2024-10-11 10:41:33,522][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 10:41:33,524][fairseq.trainer][INFO] - begin training epoch 1907
[2024-10-11 10:41:33,525][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 10:43:26,460][fairseq_cli.train][INFO] - end of epoch 1907 (average epoch stats below)
[2024-10-11 10:43:26,467][train][INFO] - {"epoch": 1907, "train_loss": "0.412", "train_ntokens": "260585", "train_nsentences": "1750.04", "train_wps": "110655", "train_ups": "0.42", "train_wpb": "260585", "train_bsz": "1750", "train_num_updates": "91496", "train_lr": "0.000419163", "train_gnorm": "0.345", "train_loss_scale": "4", "train_train_wall": "41", "train_gb_free": "39.6", "train_wall": "24009"}
[2024-10-11 10:43:26,558][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 10:43:26,561][fairseq.trainer][INFO] - begin training epoch 1908
[2024-10-11 10:43:26,561][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 10:45:15,560][fairseq_cli.train][INFO] - end of epoch 1908 (average epoch stats below)
[2024-10-11 10:45:15,576][train][INFO] - {"epoch": 1908, "train_loss": "0.409", "train_ntokens": "260612", "train_nsentences": "1750.04", "train_wps": "114654", "train_ups": "0.44", "train_wpb": "260612", "train_bsz": "1750", "train_num_updates": "91544", "train_lr": "0.000419098", "train_gnorm": "0.364", "train_loss_scale": "4", "train_train_wall": "42", "train_gb_free": "40.5", "train_wall": "24118"}
[2024-10-11 10:45:15,676][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 10:45:15,679][fairseq.trainer][INFO] - begin training epoch 1909
[2024-10-11 10:45:15,683][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 10:47:05,103][fairseq_cli.train][INFO] - end of epoch 1909 (average epoch stats below)
[2024-10-11 10:47:05,120][train][INFO] - {"epoch": 1909, "train_loss": "0.417", "train_ntokens": "260911", "train_nsentences": "1750.04", "train_wps": "114330", "train_ups": "0.44", "train_wpb": "260911", "train_bsz": "1750", "train_num_updates": "91592", "train_lr": "0.000419033", "train_gnorm": "0.354", "train_loss_scale": "4", "train_train_wall": "40", "train_gb_free": "40.5", "train_wall": "24228"}
[2024-10-11 10:47:05,216][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 10:47:05,218][fairseq.trainer][INFO] - begin training epoch 1910
[2024-10-11 10:47:05,218][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 10:48:27,014][train_inner][INFO] - {"epoch": 1910, "update": 1909.167, "loss": "0.411", "ntokens": "260691", "nsentences": "1750.57", "wps": "98693.6", "ups": "0.38", "wpb": "260691", "bsz": "1750.6", "num_updates": "91600", "lr": "0.000419022", "gnorm": "0.358", "loss_scale": "4", "train_wall": "180", "gb_free": "39.6", "wall": "24309"}
[2024-10-11 10:48:55,356][fairseq_cli.train][INFO] - end of epoch 1910 (average epoch stats below)
[2024-10-11 10:48:55,357][train][INFO] - {"epoch": 1910, "train_loss": "0.41", "train_ntokens": "260922", "train_nsentences": "1750.04", "train_wps": "113615", "train_ups": "0.44", "train_wpb": "260922", "train_bsz": "1750", "train_num_updates": "91640", "train_lr": "0.000418967", "train_gnorm": "0.361", "train_loss_scale": "4", "train_train_wall": "39", "train_gb_free": "39.2", "train_wall": "24338"}
[2024-10-11 10:48:55,421][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 10:48:55,423][fairseq.trainer][INFO] - begin training epoch 1911
[2024-10-11 10:48:55,424][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 10:50:42,116][fairseq_cli.train][INFO] - end of epoch 1911 (average epoch stats below)
[2024-10-11 10:50:42,119][train][INFO] - {"epoch": 1911, "train_loss": "0.415", "train_ntokens": "260827", "train_nsentences": "1750.04", "train_wps": "117272", "train_ups": "0.45", "train_wpb": "260827", "train_bsz": "1750", "train_num_updates": "91688", "train_lr": "0.000418902", "train_gnorm": "0.339", "train_loss_scale": "4", "train_train_wall": "40", "train_gb_free": "39.5", "train_wall": "24445"}
[2024-10-11 10:50:42,221][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 10:50:42,236][fairseq.trainer][INFO] - begin training epoch 1912
[2024-10-11 10:50:42,237][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 10:52:32,067][fairseq_cli.train][INFO] - end of epoch 1912 (average epoch stats below)
[2024-10-11 10:52:32,069][train][INFO] - {"epoch": 1912, "train_loss": "0.409", "train_ntokens": "260636", "train_nsentences": "1750.04", "train_wps": "113788", "train_ups": "0.44", "train_wpb": "260636", "train_bsz": "1750", "train_num_updates": "91736", "train_lr": "0.000418837", "train_gnorm": "0.349", "train_loss_scale": "4", "train_train_wall": "49", "train_gb_free": "39.8", "train_wall": "24554"}
[2024-10-11 10:52:32,121][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 10:52:32,124][fairseq.trainer][INFO] - begin training epoch 1913
[2024-10-11 10:52:32,124][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 10:54:21,953][fairseq_cli.train][INFO] - end of epoch 1913 (average epoch stats below)
[2024-10-11 10:54:21,959][train][INFO] - {"epoch": 1913, "train_loss": "0.414", "train_ntokens": "260918", "train_nsentences": "1750.04", "train_wps": "113974", "train_ups": "0.44", "train_wpb": "260918", "train_bsz": "1750", "train_num_updates": "91784", "train_lr": "0.000418772", "train_gnorm": "0.363", "train_loss_scale": "4", "train_train_wall": "31", "train_gb_free": "39.4", "train_wall": "24664"}
[2024-10-11 10:54:22,047][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 10:54:22,050][fairseq.trainer][INFO] - begin training epoch 1914
[2024-10-11 10:54:22,053][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 10:55:54,828][train_inner][INFO] - {"epoch": 1914, "update": 1913.333, "loss": "0.413", "ntokens": "260742", "nsentences": "1749.12", "wps": "116452", "ups": "0.45", "wpb": "260742", "bsz": "1749.1", "num_updates": "91800", "lr": "0.00041875", "gnorm": "0.35", "loss_scale": "4", "train_wall": "173", "gb_free": "39.6", "wall": "24757"}
[2024-10-11 10:56:17,620][fairseq_cli.train][INFO] - end of epoch 1914 (average epoch stats below)
[2024-10-11 10:56:17,624][train][INFO] - {"epoch": 1914, "train_loss": "0.416", "train_ntokens": "260442", "train_nsentences": "1750.04", "train_wps": "108087", "train_ups": "0.42", "train_wpb": "260442", "train_bsz": "1750", "train_num_updates": "91832", "train_lr": "0.000418707", "train_gnorm": "0.355", "train_loss_scale": "4", "train_train_wall": "48", "train_gb_free": "39.3", "train_wall": "24780"}
[2024-10-11 10:56:17,718][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 10:56:17,736][fairseq.trainer][INFO] - begin training epoch 1915
[2024-10-11 10:56:17,737][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 10:58:08,961][fairseq_cli.train][INFO] - end of epoch 1915 (average epoch stats below)
[2024-10-11 10:58:08,963][train][INFO] - {"epoch": 1915, "train_loss": "0.412", "train_ntokens": "260920", "train_nsentences": "1750.04", "train_wps": "112490", "train_ups": "0.43", "train_wpb": "260920", "train_bsz": "1750", "train_num_updates": "91880", "train_lr": "0.000418641", "train_gnorm": "0.391", "train_loss_scale": "4", "train_train_wall": "42", "train_gb_free": "39.6", "train_wall": "24891"}
[2024-10-11 10:58:09,021][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 10:58:09,024][fairseq.trainer][INFO] - begin training epoch 1916
[2024-10-11 10:58:09,024][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 11:00:01,256][fairseq_cli.train][INFO] - end of epoch 1916 (average epoch stats below)
[2024-10-11 11:00:01,272][train][INFO] - {"epoch": 1916, "train_loss": "0.411", "train_ntokens": "260940", "train_nsentences": "1750.04", "train_wps": "111539", "train_ups": "0.43", "train_wpb": "260940", "train_bsz": "1750", "train_num_updates": "91928", "train_lr": "0.000418576", "train_gnorm": "0.356", "train_loss_scale": "4", "train_train_wall": "48", "train_gb_free": "39.8", "train_wall": "25004"}
[2024-10-11 11:00:01,360][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 11:00:01,364][fairseq.trainer][INFO] - begin training epoch 1917
[2024-10-11 11:00:01,365][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 11:01:53,184][fairseq_cli.train][INFO] - end of epoch 1917 (average epoch stats below)
[2024-10-11 11:01:53,186][train][INFO] - {"epoch": 1917, "train_loss": "0.409", "train_ntokens": "260957", "train_nsentences": "1750.04", "train_wps": "111928", "train_ups": "0.43", "train_wpb": "260957", "train_bsz": "1750", "train_num_updates": "91976", "train_lr": "0.000418511", "train_gnorm": "0.356", "train_loss_scale": "4", "train_train_wall": "38", "train_gb_free": "40", "train_wall": "25116"}
[2024-10-11 11:01:53,237][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 11:01:53,239][fairseq.trainer][INFO] - begin training epoch 1918
[2024-10-11 11:01:53,239][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 11:03:24,645][train_inner][INFO] - {"epoch": 1918, "update": 1917.5, "loss": "0.411", "ntokens": "260765", "nsentences": "1763.4", "wps": "115944", "ups": "0.44", "wpb": "260765", "bsz": "1763.4", "num_updates": "92000", "lr": "0.000418478", "gnorm": "0.367", "loss_scale": "4", "train_wall": "173", "gb_free": "39.8", "wall": "25207"}
[2024-10-11 11:03:39,840][fairseq_cli.train][INFO] - end of epoch 1918 (average epoch stats below)
[2024-10-11 11:03:39,844][train][INFO] - {"epoch": 1918, "train_loss": "0.412", "train_ntokens": "260945", "train_nsentences": "1750.04", "train_wps": "117441", "train_ups": "0.45", "train_wpb": "260945", "train_bsz": "1750", "train_num_updates": "92024", "train_lr": "0.000418446", "train_gnorm": "0.354", "train_loss_scale": "4", "train_train_wall": "37", "train_gb_free": "39.7", "train_wall": "25222"}
[2024-10-11 11:03:39,930][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 11:03:39,937][fairseq.trainer][INFO] - begin training epoch 1919
[2024-10-11 11:03:39,937][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 11:05:31,303][fairseq_cli.train][INFO] - end of epoch 1919 (average epoch stats below)
[2024-10-11 11:05:31,306][train][INFO] - {"epoch": 1919, "train_loss": "0.406", "train_ntokens": "260978", "train_nsentences": "1750.04", "train_wps": "112391", "train_ups": "0.43", "train_wpb": "260978", "train_bsz": "1750", "train_num_updates": "92072", "train_lr": "0.00041838", "train_gnorm": "0.365", "train_loss_scale": "4", "train_train_wall": "45", "train_gb_free": "39.4", "train_wall": "25334"}
[2024-10-11 11:05:31,384][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 11:05:31,386][fairseq.trainer][INFO] - begin training epoch 1920
[2024-10-11 11:05:31,387][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 11:07:23,489][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 1920 @ 92120 updates
[2024-10-11 11:07:23,492][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-11 11:07:34,238][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-11 11:07:34,241][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 1920 @ 92120 updates, score None) (writing took 10.752197463996708 seconds)
[2024-10-11 11:07:34,241][fairseq_cli.train][INFO] - end of epoch 1920 (average epoch stats below)
[2024-10-11 11:07:34,242][train][INFO] - {"epoch": 1920, "train_loss": "0.416", "train_ntokens": "260797", "train_nsentences": "1750.04", "train_wps": "101830", "train_ups": "0.39", "train_wpb": "260797", "train_bsz": "1750", "train_num_updates": "92120", "train_lr": "0.000418315", "train_gnorm": "0.347", "train_loss_scale": "4", "train_train_wall": "43", "train_gb_free": "39.9", "train_wall": "25457"}
[2024-10-11 11:07:34,306][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 11:07:34,317][fairseq.trainer][INFO] - begin training epoch 1921
[2024-10-11 11:07:34,317][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 11:09:21,486][fairseq_cli.train][INFO] - end of epoch 1921 (average epoch stats below)
[2024-10-11 11:09:21,503][train][INFO] - {"epoch": 1921, "train_loss": "0.408", "train_ntokens": "261133", "train_nsentences": "1750.04", "train_wps": "116878", "train_ups": "0.45", "train_wpb": "261133", "train_bsz": "1750", "train_num_updates": "92168", "train_lr": "0.00041825", "train_gnorm": "0.364", "train_loss_scale": "4", "train_train_wall": "37", "train_gb_free": "39.9", "train_wall": "25564"}
[2024-10-11 11:09:21,590][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 11:09:21,601][fairseq.trainer][INFO] - begin training epoch 1922
[2024-10-11 11:09:21,601][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 11:11:00,935][train_inner][INFO] - {"epoch": 1922, "update": 1921.667, "loss": "0.41", "ntokens": "260920", "nsentences": "1737.23", "wps": "114367", "ups": "0.44", "wpb": "260920", "bsz": "1737.2", "num_updates": "92200", "lr": "0.000418207", "gnorm": "0.353", "loss_scale": "4", "train_wall": "174", "gb_free": "39.7", "wall": "25663"}
[2024-10-11 11:11:16,040][fairseq_cli.train][INFO] - end of epoch 1922 (average epoch stats below)
[2024-10-11 11:11:16,042][train][INFO] - {"epoch": 1922, "train_loss": "0.405", "train_ntokens": "260218", "train_nsentences": "1750.04", "train_wps": "109054", "train_ups": "0.42", "train_wpb": "260218", "train_bsz": "1750", "train_num_updates": "92216", "train_lr": "0.000418185", "train_gnorm": "0.35", "train_loss_scale": "4", "train_train_wall": "49", "train_gb_free": "39.3", "train_wall": "25678"}
[2024-10-11 11:11:16,154][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 11:11:16,158][fairseq.trainer][INFO] - begin training epoch 1923
[2024-10-11 11:11:16,159][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 11:12:52,582][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2024-10-11 11:13:07,080][fairseq_cli.train][INFO] - end of epoch 1923 (average epoch stats below)
[2024-10-11 11:13:07,087][train][INFO] - {"epoch": 1923, "train_loss": "0.409", "train_ntokens": "260542", "train_nsentences": "1754.51", "train_wps": "110283", "train_ups": "0.42", "train_wpb": "260542", "train_bsz": "1754.5", "train_num_updates": "92263", "train_lr": "0.000418121", "train_gnorm": "0.374", "train_loss_scale": "2", "train_train_wall": "42", "train_gb_free": "40.1", "train_wall": "25790"}
[2024-10-11 11:13:07,165][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 11:13:07,173][fairseq.trainer][INFO] - begin training epoch 1924
[2024-10-11 11:13:07,173][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 11:14:57,933][fairseq_cli.train][INFO] - end of epoch 1924 (average epoch stats below)
[2024-10-11 11:14:57,936][train][INFO] - {"epoch": 1924, "train_loss": "0.413", "train_ntokens": "260556", "train_nsentences": "1750.04", "train_wps": "112830", "train_ups": "0.43", "train_wpb": "260556", "train_bsz": "1750", "train_num_updates": "92311", "train_lr": "0.000418056", "train_gnorm": "0.366", "train_loss_scale": "2", "train_train_wall": "47", "train_gb_free": "39.7", "train_wall": "25900"}
[2024-10-11 11:14:58,080][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 11:14:58,084][fairseq.trainer][INFO] - begin training epoch 1925
[2024-10-11 11:14:58,085][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 11:16:49,746][fairseq_cli.train][INFO] - end of epoch 1925 (average epoch stats below)
[2024-10-11 11:16:49,756][train][INFO] - {"epoch": 1925, "train_loss": "0.409", "train_ntokens": "260777", "train_nsentences": "1750.04", "train_wps": "111945", "train_ups": "0.43", "train_wpb": "260777", "train_bsz": "1750", "train_num_updates": "92359", "train_lr": "0.00041799", "train_gnorm": "0.347", "train_loss_scale": "2", "train_train_wall": "52", "train_gb_free": "40.1", "train_wall": "26012"}
[2024-10-11 11:16:49,837][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 11:16:49,839][fairseq.trainer][INFO] - begin training epoch 1926
[2024-10-11 11:16:49,840][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 11:18:36,478][train_inner][INFO] - {"epoch": 1926, "update": 1925.854, "loss": "0.412", "ntokens": "260685", "nsentences": "1747.29", "wps": "114456", "ups": "0.44", "wpb": "260685", "bsz": "1747.3", "num_updates": "92400", "lr": "0.000417935", "gnorm": "0.361", "loss_scale": "2", "train_wall": "197", "gb_free": "40.1", "wall": "26119"}
[2024-10-11 11:18:38,724][fairseq_cli.train][INFO] - end of epoch 1926 (average epoch stats below)
[2024-10-11 11:18:38,730][train][INFO] - {"epoch": 1926, "train_loss": "0.416", "train_ntokens": "260665", "train_nsentences": "1750.04", "train_wps": "114824", "train_ups": "0.44", "train_wpb": "260665", "train_bsz": "1750", "train_num_updates": "92407", "train_lr": "0.000417925", "train_gnorm": "0.354", "train_loss_scale": "2", "train_train_wall": "44", "train_gb_free": "39.3", "train_wall": "26121"}
[2024-10-11 11:18:38,854][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 11:18:38,860][fairseq.trainer][INFO] - begin training epoch 1927
[2024-10-11 11:18:38,861][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 11:20:29,485][fairseq_cli.train][INFO] - end of epoch 1927 (average epoch stats below)
[2024-10-11 11:20:29,501][train][INFO] - {"epoch": 1927, "train_loss": "0.414", "train_ntokens": "260620", "train_nsentences": "1750.04", "train_wps": "112940", "train_ups": "0.43", "train_wpb": "260620", "train_bsz": "1750", "train_num_updates": "92455", "train_lr": "0.00041786", "train_gnorm": "0.362", "train_loss_scale": "2", "train_train_wall": "28", "train_gb_free": "39.8", "train_wall": "26232"}
[2024-10-11 11:20:29,581][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 11:20:29,587][fairseq.trainer][INFO] - begin training epoch 1928
[2024-10-11 11:20:29,587][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 11:22:20,654][fairseq_cli.train][INFO] - end of epoch 1928 (average epoch stats below)
[2024-10-11 11:22:20,671][train][INFO] - {"epoch": 1928, "train_loss": "0.404", "train_ntokens": "260683", "train_nsentences": "1750.04", "train_wps": "112573", "train_ups": "0.43", "train_wpb": "260683", "train_bsz": "1750", "train_num_updates": "92503", "train_lr": "0.000417795", "train_gnorm": "0.358", "train_loss_scale": "2", "train_train_wall": "39", "train_gb_free": "39.6", "train_wall": "26343"}
[2024-10-11 11:22:20,746][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 11:22:20,749][fairseq.trainer][INFO] - begin training epoch 1929
[2024-10-11 11:22:20,749][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 11:24:12,220][fairseq_cli.train][INFO] - end of epoch 1929 (average epoch stats below)
[2024-10-11 11:24:12,224][train][INFO] - {"epoch": 1929, "train_loss": "0.41", "train_ntokens": "260320", "train_nsentences": "1750.04", "train_wps": "112017", "train_ups": "0.43", "train_wpb": "260320", "train_bsz": "1750", "train_num_updates": "92551", "train_lr": "0.00041773", "train_gnorm": "0.375", "train_loss_scale": "2", "train_train_wall": "42", "train_gb_free": "39.1", "train_wall": "26455"}
[2024-10-11 11:24:12,303][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 11:24:12,305][fairseq.trainer][INFO] - begin training epoch 1930
[2024-10-11 11:24:12,306][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 11:26:01,908][fairseq_cli.train][INFO] - end of epoch 1930 (average epoch stats below)
[2024-10-11 11:26:01,911][train][INFO] - {"epoch": 1930, "train_loss": "0.412", "train_ntokens": "260582", "train_nsentences": "1750.04", "train_wps": "114036", "train_ups": "0.44", "train_wpb": "260582", "train_bsz": "1750", "train_num_updates": "92599", "train_lr": "0.000417664", "train_gnorm": "0.364", "train_loss_scale": "2", "train_train_wall": "42", "train_gb_free": "39.2", "train_wall": "26564"}
[2024-10-11 11:26:01,994][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 11:26:01,996][fairseq.trainer][INFO] - begin training epoch 1931
[2024-10-11 11:26:01,996][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 11:27:12,803][train_inner][INFO] - {"epoch": 1931, "update": 1930.021, "loss": "0.409", "ntokens": "260574", "nsentences": "1754.5", "wps": "100937", "ups": "0.39", "wpb": "260574", "bsz": "1754.5", "num_updates": "92600", "lr": "0.000417663", "gnorm": "0.365", "loss_scale": "2", "train_wall": "159", "gb_free": "39.8", "wall": "26635"}
[2024-10-11 11:27:49,237][fairseq_cli.train][INFO] - end of epoch 1931 (average epoch stats below)
[2024-10-11 11:27:49,259][train][INFO] - {"epoch": 1931, "train_loss": "0.421", "train_ntokens": "260912", "train_nsentences": "1750.04", "train_wps": "116691", "train_ups": "0.45", "train_wpb": "260912", "train_bsz": "1750", "train_num_updates": "92647", "train_lr": "0.000417599", "train_gnorm": "0.348", "train_loss_scale": "2", "train_train_wall": "41", "train_gb_free": "39.6", "train_wall": "26672"}
[2024-10-11 11:27:49,346][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 11:27:49,354][fairseq.trainer][INFO] - begin training epoch 1932
[2024-10-11 11:27:49,355][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 11:29:39,660][fairseq_cli.train][INFO] - end of epoch 1932 (average epoch stats below)
[2024-10-11 11:29:39,666][train][INFO] - {"epoch": 1932, "train_loss": "0.414", "train_ntokens": "260931", "train_nsentences": "1750.04", "train_wps": "113446", "train_ups": "0.43", "train_wpb": "260931", "train_bsz": "1750", "train_num_updates": "92695", "train_lr": "0.000417534", "train_gnorm": "0.351", "train_loss_scale": "2", "train_train_wall": "38", "train_gb_free": "40.1", "train_wall": "26782"}
[2024-10-11 11:29:39,762][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 11:29:39,771][fairseq.trainer][INFO] - begin training epoch 1933
[2024-10-11 11:29:39,771][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 11:31:28,707][fairseq_cli.train][INFO] - end of epoch 1933 (average epoch stats below)
[2024-10-11 11:31:28,712][train][INFO] - {"epoch": 1933, "train_loss": "0.409", "train_ntokens": "260781", "train_nsentences": "1750.04", "train_wps": "114796", "train_ups": "0.44", "train_wpb": "260781", "train_bsz": "1750", "train_num_updates": "92743", "train_lr": "0.000417469", "train_gnorm": "0.358", "train_loss_scale": "2", "train_train_wall": "29", "train_gb_free": "40.1", "train_wall": "26891"}
[2024-10-11 11:31:28,832][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 11:31:28,842][fairseq.trainer][INFO] - begin training epoch 1934
[2024-10-11 11:31:28,842][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 11:33:18,611][fairseq_cli.train][INFO] - end of epoch 1934 (average epoch stats below)
[2024-10-11 11:33:18,626][train][INFO] - {"epoch": 1934, "train_loss": "0.407", "train_ntokens": "260846", "train_nsentences": "1750.04", "train_wps": "113918", "train_ups": "0.44", "train_wpb": "260846", "train_bsz": "1750", "train_num_updates": "92791", "train_lr": "0.000417404", "train_gnorm": "0.39", "train_loss_scale": "2", "train_train_wall": "51", "train_gb_free": "40.1", "train_wall": "27001"}
[2024-10-11 11:33:18,741][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 11:33:18,756][fairseq.trainer][INFO] - begin training epoch 1935
[2024-10-11 11:33:18,757][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 11:34:44,077][train_inner][INFO] - {"epoch": 1935, "update": 1934.188, "loss": "0.413", "ntokens": "260643", "nsentences": "1759.24", "wps": "115516", "ups": "0.44", "wpb": "260643", "bsz": "1759.2", "num_updates": "92800", "lr": "0.000417391", "gnorm": "0.361", "loss_scale": "2", "train_wall": "168", "gb_free": "39.8", "wall": "27086"}
[2024-10-11 11:35:10,009][fairseq_cli.train][INFO] - end of epoch 1935 (average epoch stats below)
[2024-10-11 11:35:10,011][train][INFO] - {"epoch": 1935, "train_loss": "0.416", "train_ntokens": "260742", "train_nsentences": "1750.04", "train_wps": "112368", "train_ups": "0.43", "train_wpb": "260742", "train_bsz": "1750", "train_num_updates": "92839", "train_lr": "0.000417338", "train_gnorm": "0.358", "train_loss_scale": "2", "train_train_wall": "40", "train_gb_free": "40.3", "train_wall": "27112"}
[2024-10-11 11:35:10,092][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 11:35:10,096][fairseq.trainer][INFO] - begin training epoch 1936
[2024-10-11 11:35:10,096][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 11:36:59,059][fairseq_cli.train][INFO] - end of epoch 1936 (average epoch stats below)
[2024-10-11 11:36:59,063][train][INFO] - {"epoch": 1936, "train_loss": "0.417", "train_ntokens": "260853", "train_nsentences": "1750.04", "train_wps": "114820", "train_ups": "0.44", "train_wpb": "260853", "train_bsz": "1750", "train_num_updates": "92887", "train_lr": "0.000417273", "train_gnorm": "0.365", "train_loss_scale": "2", "train_train_wall": "37", "train_gb_free": "40.6", "train_wall": "27221"}
[2024-10-11 11:36:59,147][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 11:36:59,169][fairseq.trainer][INFO] - begin training epoch 1937
[2024-10-11 11:36:59,169][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 11:38:48,021][fairseq_cli.train][INFO] - end of epoch 1937 (average epoch stats below)
[2024-10-11 11:38:48,028][train][INFO] - {"epoch": 1937, "train_loss": "0.411", "train_ntokens": "260753", "train_nsentences": "1750.04", "train_wps": "114870", "train_ups": "0.44", "train_wpb": "260753", "train_bsz": "1750", "train_num_updates": "92935", "train_lr": "0.000417208", "train_gnorm": "0.356", "train_loss_scale": "2", "train_train_wall": "42", "train_gb_free": "39.3", "train_wall": "27330"}
[2024-10-11 11:38:48,137][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 11:38:48,164][fairseq.trainer][INFO] - begin training epoch 1938
[2024-10-11 11:38:48,164][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 11:40:37,558][fairseq_cli.train][INFO] - end of epoch 1938 (average epoch stats below)
[2024-10-11 11:40:37,563][train][INFO] - {"epoch": 1938, "train_loss": "0.412", "train_ntokens": "260875", "train_nsentences": "1750.04", "train_wps": "114325", "train_ups": "0.44", "train_wpb": "260874", "train_bsz": "1750", "train_num_updates": "92983", "train_lr": "0.000417143", "train_gnorm": "0.378", "train_loss_scale": "2", "train_train_wall": "40", "train_gb_free": "39.9", "train_wall": "27440"}
[2024-10-11 11:40:37,650][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 11:40:37,668][fairseq.trainer][INFO] - begin training epoch 1939
[2024-10-11 11:40:37,669][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 11:42:06,103][train_inner][INFO] - {"epoch": 1939, "update": 1938.354, "loss": "0.414", "ntokens": "261083", "nsentences": "1743.15", "wps": "118132", "ups": "0.45", "wpb": "261083", "bsz": "1743.2", "num_updates": "93000", "lr": "0.00041712", "gnorm": "0.365", "loss_scale": "2", "train_wall": "153", "gb_free": "40", "wall": "27529"}
[2024-10-11 11:42:25,077][fairseq_cli.train][INFO] - end of epoch 1939 (average epoch stats below)
[2024-10-11 11:42:25,087][train][INFO] - {"epoch": 1939, "train_loss": "0.42", "train_ntokens": "260765", "train_nsentences": "1750.04", "train_wps": "116421", "train_ups": "0.45", "train_wpb": "260765", "train_bsz": "1750", "train_num_updates": "93031", "train_lr": "0.000417077", "train_gnorm": "0.359", "train_loss_scale": "2", "train_train_wall": "27", "train_gb_free": "40.1", "train_wall": "27548"}
[2024-10-11 11:42:25,194][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 11:42:25,196][fairseq.trainer][INFO] - begin training epoch 1940
[2024-10-11 11:42:25,197][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 11:44:15,197][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 1940 @ 93079 updates
[2024-10-11 11:44:15,197][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-11 11:44:25,984][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-11 11:44:25,986][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 1940 @ 93079 updates, score None) (writing took 10.789496365003288 seconds)
[2024-10-11 11:44:25,986][fairseq_cli.train][INFO] - end of epoch 1940 (average epoch stats below)
[2024-10-11 11:44:25,988][train][INFO] - {"epoch": 1940, "train_loss": "0.414", "train_ntokens": "260456", "train_nsentences": "1750.04", "train_wps": "103410", "train_ups": "0.4", "train_wpb": "260456", "train_bsz": "1750", "train_num_updates": "93079", "train_lr": "0.000417012", "train_gnorm": "0.38", "train_loss_scale": "2", "train_train_wall": "39", "train_gb_free": "40.6", "train_wall": "27668"}
[2024-10-11 11:44:26,038][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 11:44:26,050][fairseq.trainer][INFO] - begin training epoch 1941
[2024-10-11 11:44:26,051][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 11:46:13,358][fairseq_cli.train][INFO] - end of epoch 1941 (average epoch stats below)
[2024-10-11 11:46:13,367][train][INFO] - {"epoch": 1941, "train_loss": "0.405", "train_ntokens": "260415", "train_nsentences": "1750.04", "train_wps": "116414", "train_ups": "0.45", "train_wpb": "260415", "train_bsz": "1750", "train_num_updates": "93127", "train_lr": "0.000416947", "train_gnorm": "0.349", "train_loss_scale": "2", "train_train_wall": "26", "train_gb_free": "39.1", "train_wall": "27776"}
[2024-10-11 11:46:13,452][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 11:46:13,457][fairseq.trainer][INFO] - begin training epoch 1942
[2024-10-11 11:46:13,457][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 11:48:03,871][fairseq_cli.train][INFO] - end of epoch 1942 (average epoch stats below)
[2024-10-11 11:48:03,876][train][INFO] - {"epoch": 1942, "train_loss": "0.41", "train_ntokens": "260239", "train_nsentences": "1750.04", "train_wps": "113040", "train_ups": "0.43", "train_wpb": "260239", "train_bsz": "1750", "train_num_updates": "93175", "train_lr": "0.000416882", "train_gnorm": "0.361", "train_loss_scale": "2", "train_train_wall": "44", "train_gb_free": "39.2", "train_wall": "27886"}
[2024-10-11 11:48:03,939][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 11:48:03,941][fairseq.trainer][INFO] - begin training epoch 1943
[2024-10-11 11:48:03,942][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 11:49:36,873][train_inner][INFO] - {"epoch": 1943, "update": 1942.521, "loss": "0.41", "ntokens": "260540", "nsentences": "1749.28", "wps": "115599", "ups": "0.44", "wpb": "260540", "bsz": "1749.3", "num_updates": "93200", "lr": "0.000416848", "gnorm": "0.362", "loss_scale": "2", "train_wall": "152", "gb_free": "39.2", "wall": "27979"}
[2024-10-11 11:49:52,612][fairseq_cli.train][INFO] - end of epoch 1943 (average epoch stats below)
[2024-10-11 11:49:52,619][train][INFO] - {"epoch": 1943, "train_loss": "0.407", "train_ntokens": "260372", "train_nsentences": "1750.04", "train_wps": "114936", "train_ups": "0.44", "train_wpb": "260372", "train_bsz": "1750", "train_num_updates": "93223", "train_lr": "0.000416817", "train_gnorm": "0.351", "train_loss_scale": "2", "train_train_wall": "41", "train_gb_free": "40.2", "train_wall": "27995"}
[2024-10-11 11:49:52,682][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 11:49:52,684][fairseq.trainer][INFO] - begin training epoch 1944
[2024-10-11 11:49:52,685][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 11:51:41,757][fairseq_cli.train][INFO] - end of epoch 1944 (average epoch stats below)
[2024-10-11 11:51:41,766][train][INFO] - {"epoch": 1944, "train_loss": "0.406", "train_ntokens": "260761", "train_nsentences": "1750.04", "train_wps": "114681", "train_ups": "0.44", "train_wpb": "260761", "train_bsz": "1750", "train_num_updates": "93271", "train_lr": "0.000416751", "train_gnorm": "0.348", "train_loss_scale": "2", "train_train_wall": "46", "train_gb_free": "40.1", "train_wall": "28104"}
[2024-10-11 11:51:41,861][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 11:51:41,864][fairseq.trainer][INFO] - begin training epoch 1945
[2024-10-11 11:51:41,864][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 11:53:29,975][fairseq_cli.train][INFO] - end of epoch 1945 (average epoch stats below)
[2024-10-11 11:53:29,978][train][INFO] - {"epoch": 1945, "train_loss": "0.412", "train_ntokens": "260571", "train_nsentences": "1750.04", "train_wps": "115586", "train_ups": "0.44", "train_wpb": "260571", "train_bsz": "1750", "train_num_updates": "93319", "train_lr": "0.000416686", "train_gnorm": "0.365", "train_loss_scale": "2", "train_train_wall": "35", "train_gb_free": "39.8", "train_wall": "28212"}
[2024-10-11 11:53:30,057][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 11:53:30,068][fairseq.trainer][INFO] - begin training epoch 1946
[2024-10-11 11:53:30,070][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 11:55:19,123][fairseq_cli.train][INFO] - end of epoch 1946 (average epoch stats below)
[2024-10-11 11:55:19,127][train][INFO] - {"epoch": 1946, "train_loss": "0.407", "train_ntokens": "260811", "train_nsentences": "1750.04", "train_wps": "114701", "train_ups": "0.44", "train_wpb": "260811", "train_bsz": "1750", "train_num_updates": "93367", "train_lr": "0.000416621", "train_gnorm": "0.373", "train_loss_scale": "2", "train_train_wall": "44", "train_gb_free": "40.7", "train_wall": "28322"}
[2024-10-11 11:55:19,190][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 11:55:19,193][fairseq.trainer][INFO] - begin training epoch 1947
[2024-10-11 11:55:19,193][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 11:56:56,242][train_inner][INFO] - {"epoch": 1947, "update": 1946.688, "loss": "0.407", "ntokens": "260423", "nsentences": "1744.92", "wps": "118546", "ups": "0.46", "wpb": "260423", "bsz": "1744.9", "num_updates": "93400", "lr": "0.000416576", "gnorm": "0.358", "loss_scale": "2", "train_wall": "165", "gb_free": "40.1", "wall": "28419"}
[2024-10-11 11:57:09,331][fairseq_cli.train][INFO] - end of epoch 1947 (average epoch stats below)
[2024-10-11 11:57:09,332][train][INFO] - {"epoch": 1947, "train_loss": "0.406", "train_ntokens": "260657", "train_nsentences": "1750.04", "train_wps": "113533", "train_ups": "0.44", "train_wpb": "260657", "train_bsz": "1750", "train_num_updates": "93415", "train_lr": "0.000416556", "train_gnorm": "0.351", "train_loss_scale": "2", "train_train_wall": "35", "train_gb_free": "39.4", "train_wall": "28432"}
[2024-10-11 11:57:09,412][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 11:57:09,422][fairseq.trainer][INFO] - begin training epoch 1948
[2024-10-11 11:57:09,429][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 11:58:56,634][fairseq_cli.train][INFO] - end of epoch 1948 (average epoch stats below)
[2024-10-11 11:58:56,637][train][INFO] - {"epoch": 1948, "train_loss": "0.409", "train_ntokens": "260723", "train_nsentences": "1750.04", "train_wps": "116632", "train_ups": "0.45", "train_wpb": "260723", "train_bsz": "1750", "train_num_updates": "93463", "train_lr": "0.00041649", "train_gnorm": "0.362", "train_loss_scale": "2", "train_train_wall": "41", "train_gb_free": "39.9", "train_wall": "28539"}
[2024-10-11 11:58:56,750][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 11:58:56,753][fairseq.trainer][INFO] - begin training epoch 1949
[2024-10-11 11:58:56,753][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-11 12:00:46,094][fairseq_cli.train][INFO] - end of epoch 1949 (average epoch stats below)
[2024-10-11 12:00:46,096][train][INFO] - {"epoch": 1949, "train_loss": "0.401", "train_ntokens": "260516", "train_nsentences": "1750.04", "train_wps": "114249", "train_ups": "0.44", "train_wpb": "260516", "train_bsz": "1750", "train_num_updates": "93511", "train_lr": "0.000416425", "train_gnorm": "0.362", "train_loss_scale": "2", "train_train_wall": "37", "train_gb_free": "39.8", "train_wall": "28649"}
[2024-10-11 12:00:46,157][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-11 12:00:46,160][fairseq.trainer][INFO] - begin training epoch 1950
[2024-10-11 12:00:46,160][fairseq_cli.train][INFO] - Start iterating over samples
