[2024-10-07 08:21:56,975][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:15414', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-07 08:22:01,600][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:13125', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-07 08:22:03,503][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:15911', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-07 08:22:05,890][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-07 08:22:05,892][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-07 08:22:05,892][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-07 08:22:05,892][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-07 08:22:05,893][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-07 08:22:05,894][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-07 08:22:06,290][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-07 08:22:09,311][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:15136', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-07 08:22:11,737][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:10198', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-07 08:22:12,439][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:19107', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-07 08:22:12,751][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-07 08:22:12,753][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-07 08:22:12,753][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-07 08:22:12,753][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-07 08:22:12,754][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-07 08:22:12,754][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-07 08:22:13,148][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-07 08:22:13,875][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:18795', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-07 08:22:15,163][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:17672', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-07 08:22:16,210][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-07 08:22:16,214][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-07 08:22:16,214][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-07 08:22:16,214][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-07 08:22:16,215][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-07 08:22:16,215][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-07 08:22:16,486][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-07 08:22:20,731][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-07 08:22:20,739][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-07 08:22:20,741][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-07 08:22:20,741][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-07 08:22:20,742][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-07 08:22:20,743][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-07 08:22:21,224][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-07 08:22:29,574][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-07 08:22:29,577][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-07 08:22:29,577][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-07 08:22:29,577][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-07 08:22:29,927][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-07 08:22:29,928][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-07 08:22:34,794][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-07 08:22:35,169][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-07 08:22:35,169][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-07 08:22:35,169][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-07 08:22:35,170][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-07 08:22:35,171][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-07 08:22:43,260][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-07 08:22:43,262][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-07 08:22:43,262][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-07 08:22:43,262][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-07 08:22:43,263][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-07 08:22:43,264][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-07 08:22:43,316][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-07 08:22:45,037][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-07 08:22:49,607][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-07 08:22:51,346][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-07 08:22:51,404][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-07 08:22:51,404][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-07 08:22:51,404][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-07 08:22:51,405][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-07 08:22:51,406][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-07 08:22:56,501][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-07 08:23:07,946][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-07 08:23:07,947][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-07 08:23:07,947][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-07 08:23:07,947][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-07 08:23:07,947][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-07 08:23:07,947][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-07 08:23:07,947][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-07 08:23:07,947][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-07 08:23:07,947][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-07 08:23:07,947][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-07 08:23:07,948][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-07 08:23:07,948][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-07 08:23:07,973][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-07 08:23:22,446][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-07 08:23:22,446][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-07 08:23:22,446][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-07 08:23:22,446][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-07 08:23:22,446][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-07 08:23:22,446][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-07 08:23:22,446][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-07 08:23:22,446][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-07 08:23:22,446][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-07 08:23:22,446][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-07 08:23:22,446][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-07 08:23:22,454][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-07 08:23:22,529][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-07 08:23:50,404][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-07 08:23:50,405][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-07 08:23:50,405][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-07 08:23:50,405][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-07 08:23:50,405][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-07 08:23:50,405][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-07 08:23:50,405][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-07 08:23:50,405][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-07 08:23:50,405][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-07 08:23:50,405][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-07 08:23:50,405][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-07 08:23:50,406][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-07 08:23:50,406][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-07 08:24:01,450][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-07 08:24:01,451][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-07 08:24:01,451][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-07 08:24:01,451][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-07 08:24:01,451][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-07 08:24:01,451][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-07 08:24:01,451][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-07 08:24:01,451][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-07 08:24:01,451][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-07 08:24:01,451][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-07 08:24:01,452][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-07 08:24:01,452][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-07 08:24:01,453][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-07 08:24:05,334][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 601 @ 28785 updates)
[2024-10-07 08:24:05,342][fairseq.trainer][INFO] - loading train data for epoch 601
[2024-10-07 08:24:05,962][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-07 08:24:06,992][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 601 @ 28785 updates)
[2024-10-07 08:24:07,119][fairseq.trainer][INFO] - loading train data for epoch 601
[2024-10-07 08:24:09,163][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-07 08:24:17,873][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 08:24:17,881][fairseq.trainer][INFO] - begin training epoch 601
[2024-10-07 08:24:17,881][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 08:24:22,010][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 08:24:22,018][fairseq.trainer][INFO] - begin training epoch 601
[2024-10-07 08:24:22,019][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 08:24:44,128][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 601 @ 28785 updates)
[2024-10-07 08:24:44,130][fairseq.trainer][INFO] - loading train data for epoch 601
[2024-10-07 08:24:45,284][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-07 08:25:00,033][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 08:25:00,042][fairseq.trainer][INFO] - begin training epoch 601
[2024-10-07 08:25:00,043][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 08:25:02,479][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 601 @ 28785 updates)
[2024-10-07 08:25:02,481][fairseq.trainer][INFO] - loading train data for epoch 601
[2024-10-07 08:25:02,765][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-07 08:25:06,388][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-07 08:25:06,389][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-07 08:25:06,389][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-07 08:25:06,389][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-07 08:25:06,389][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-07 08:25:06,389][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-07 08:25:06,389][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-07 08:25:06,389][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-07 08:25:06,389][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-07 08:25:06,389][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-07 08:25:06,389][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-07 08:25:06,389][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-07 08:25:06,468][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-07 08:25:10,381][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 08:25:10,400][fairseq.trainer][INFO] - begin training epoch 601
[2024-10-07 08:25:10,401][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 08:25:12,388][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-07 08:25:12,388][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-07 08:25:12,388][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-07 08:25:12,388][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-07 08:25:12,388][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-07 08:25:12,388][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-07 08:25:12,388][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-07 08:25:12,388][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-07 08:25:12,388][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-07 08:25:12,388][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-07 08:25:12,388][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-07 08:25:12,389][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-07 08:25:12,389][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-07 08:25:38,254][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 601 @ 28785 updates)
[2024-10-07 08:25:38,257][fairseq.trainer][INFO] - loading train data for epoch 601
[2024-10-07 08:25:38,539][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-07 08:25:42,296][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-07 08:25:42,296][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-07 08:25:42,296][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-07 08:25:42,296][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-07 08:25:42,296][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-07 08:25:42,296][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-07 08:25:42,296][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-07 08:25:42,296][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-07 08:25:42,296][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-07 08:25:42,296][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-07 08:25:42,296][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-07 08:25:42,297][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-07 08:25:42,297][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-07 08:25:47,600][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-07 08:25:47,601][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-07 08:25:47,601][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-07 08:25:47,601][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-07 08:25:47,602][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-07 08:25:47,602][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-07 08:25:47,602][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-07 08:25:47,602][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-07 08:25:47,602][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-07 08:25:47,602][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-07 08:25:47,602][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-07 08:25:47,603][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-07 08:25:47,604][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-07 08:25:53,874][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 08:25:53,909][fairseq.trainer][INFO] - begin training epoch 601
[2024-10-07 08:25:53,912][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 08:25:54,087][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 601 @ 28785 updates)
[2024-10-07 08:25:54,089][fairseq.trainer][INFO] - loading train data for epoch 601
[2024-10-07 08:25:54,664][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-07 08:26:00,523][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 08:26:00,549][fairseq.trainer][INFO] - begin training epoch 601
[2024-10-07 08:26:00,549][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 08:26:16,318][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 601 @ 28785 updates)
[2024-10-07 08:26:16,330][fairseq.trainer][INFO] - loading train data for epoch 601
[2024-10-07 08:26:16,621][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-07 08:26:22,418][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 601 @ 28785 updates)
[2024-10-07 08:26:22,419][fairseq.trainer][INFO] - loading train data for epoch 601
[2024-10-07 08:26:24,731][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-07 08:26:30,990][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 08:26:30,998][fairseq.trainer][INFO] - begin training epoch 601
[2024-10-07 08:26:30,999][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 08:26:35,142][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 08:26:35,146][fairseq.trainer][INFO] - begin training epoch 601
[2024-10-07 08:26:35,147][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 08:31:07,597][train_inner][INFO] - {"epoch": 601, "update": 600.312, "loss": "0.649", "ntokens": "263898", "nsentences": "1596.6", "wps": "42529.6", "ups": "0.16", "wpb": "263898", "bsz": "1596.6", "num_updates": "28800", "lr": "0.00045", "gnorm": "0.377", "loss_scale": "2", "train_wall": "87", "gb_free": "39.6", "wall": "479"}
[2024-10-07 08:32:34,272][fairseq_cli.train][INFO] - end of epoch 601 (average epoch stats below)
[2024-10-07 08:32:36,344][train][INFO] - {"epoch": 601, "train_loss": "0.658", "train_ntokens": "261195", "train_nsentences": "1750.04", "train_wps": "70149", "train_ups": "0.27", "train_wpb": "261195", "train_bsz": "1750", "train_num_updates": "28833", "train_lr": "0.000450516", "train_gnorm": "0.442", "train_loss_scale": "2", "train_train_wall": "171", "train_gb_free": "39.6", "train_wall": "567"}
[2024-10-07 08:32:53,219][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 08:32:53,745][fairseq.trainer][INFO] - begin training epoch 602
[2024-10-07 08:32:53,745][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 08:39:13,193][fairseq.trainer][WARNING] - OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 88.00 MiB. GPU 
[2024-10-07 08:39:13,355][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 1            |        cudaMalloc retries: 1         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   2662 MiB |   3014 MiB |   3641 MiB |    978 MiB |
|       from large pool |   2623 MiB |   2975 MiB |   3520 MiB |    897 MiB |
|       from small pool |     39 MiB |    103 MiB |    120 MiB |     80 MiB |
|---------------------------------------------------------------------------|
| Active memory         |   2662 MiB |   3014 MiB |   3641 MiB |    978 MiB |
|       from large pool |   2623 MiB |   2975 MiB |   3520 MiB |    897 MiB |
|       from small pool |     39 MiB |    103 MiB |    120 MiB |     80 MiB |
|---------------------------------------------------------------------------|
| Requested memory      |   2652 MiB |   3003 MiB |   3627 MiB |    975 MiB |
|       from large pool |   2612 MiB |   2963 MiB |   3507 MiB |    894 MiB |
|       from small pool |     39 MiB |    102 MiB |    120 MiB |     80 MiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   3122 MiB |   3122 MiB |   3122 MiB |      0 B   |
|       from large pool |   3018 MiB |   3018 MiB |   3018 MiB |      0 B   |
|       from small pool |    104 MiB |    104 MiB |    104 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory | 109582 KiB | 279485 KiB | 720243 KiB | 610661 KiB |
|       from large pool |  43987 KiB | 213073 KiB | 533341 KiB | 489354 KiB |
|       from small pool |  65595 KiB |  69238 KiB | 186902 KiB | 121307 KiB |
|---------------------------------------------------------------------------|
| Allocations           |    2304    |    3233    |    3339    |    1035    |
|       from large pool |     123    |     125    |     139    |      16    |
|       from small pool |    2181    |    3155    |    3200    |    1019    |
|---------------------------------------------------------------------------|
| Active allocs         |    2304    |    3233    |    3339    |    1035    |
|       from large pool |     123    |     125    |     139    |      16    |
|       from small pool |    2181    |    3155    |    3200    |    1019    |
|---------------------------------------------------------------------------|
| GPU reserved segments |     100    |     100    |     100    |       0    |
|       from large pool |      48    |      48    |      48    |       0    |
|       from small pool |      52    |      52    |      52    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     314    |     316    |     581    |     267    |
|       from large pool |      18    |      20    |      32    |      14    |
|       from small pool |     296    |     298    |     549    |     253    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-07 08:39:13,356][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-07 08:39:13,356][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-07 08:39:13,356][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-07 08:39:13,357][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 4                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-07 08:39:13,357][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 5                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-07 08:39:13,358][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 6                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-07 08:39:13,358][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 7                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-07 08:39:13,358][fairseq.trainer][WARNING] - attempting to recover from OOM in forward/backward pass
[2024-10-07 08:39:27,729][fairseq.trainer][WARNING] - OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 
[2024-10-07 08:39:27,730][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 1            |        cudaMalloc retries: 1         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   1718 MiB |   1778 MiB |   1849 MiB | 133911 KiB |
|       from large pool |   1680 MiB |   1740 MiB |   1740 MiB |  61440 KiB |
|       from small pool |     38 MiB |    103 MiB |    108 MiB |  72471 KiB |
|---------------------------------------------------------------------------|
| Active memory         |   1718 MiB |   1778 MiB |   1849 MiB | 133911 KiB |
|       from large pool |   1680 MiB |   1740 MiB |   1740 MiB |  61440 KiB |
|       from small pool |     38 MiB |    103 MiB |    108 MiB |  72471 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |   1710 MiB |   1768 MiB |   1839 MiB | 132416 KiB |
|       from large pool |   1672 MiB |   1730 MiB |   1730 MiB |  60000 KiB |
|       from small pool |     38 MiB |    102 MiB |    108 MiB |  72416 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   1868 MiB |   1868 MiB |   1868 MiB |      0 B   |
|       from large pool |   1764 MiB |   1764 MiB |   1764 MiB |      0 B   |
|       from small pool |    104 MiB |    104 MiB |    104 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  91414 KiB |  98973 KiB | 375784 KiB | 284369 KiB |
|       from large pool |  23998 KiB |  31498 KiB | 198742 KiB | 174744 KiB |
|       from small pool |  67416 KiB |  69238 KiB | 177042 KiB | 109625 KiB |
|---------------------------------------------------------------------------|
| Allocations           |    2261    |    3233    |    3268    |    1007    |
|       from large pool |      91    |      93    |      93    |       2    |
|       from small pool |    2170    |    3155    |    3175    |    1005    |
|---------------------------------------------------------------------------|
| Active allocs         |    2261    |    3233    |    3268    |    1007    |
|       from large pool |      91    |      93    |      93    |       2    |
|       from small pool |    2170    |    3155    |    3175    |    1005    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      77    |      77    |      77    |       0    |
|       from large pool |      25    |      25    |      25    |       0    |
|       from small pool |      52    |      52    |      52    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     309    |     309    |     570    |     261    |
|       from large pool |      12    |      12    |      21    |       9    |
|       from small pool |     297    |     298    |     549    |     252    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-07 08:39:27,730][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-07 08:39:27,735][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-07 08:39:27,735][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-07 08:39:27,736][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 4                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-07 08:39:27,736][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 5                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-07 08:39:27,736][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 6                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-07 08:39:27,737][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 7                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-07 08:39:27,737][fairseq.trainer][WARNING] - attempting to recover from OOM in forward/backward pass
[2024-10-07 08:44:54,784][fairseq_cli.train][INFO] - end of epoch 602 (average epoch stats below)
[2024-10-07 08:44:54,801][train][INFO] - {"epoch": 602, "train_loss": "0.649", "train_ntokens": "260858", "train_nsentences": "1750.04", "train_wps": "16956.1", "train_ups": "0.07", "train_wpb": "260858", "train_bsz": "1750", "train_num_updates": "28881", "train_lr": "0.000451266", "train_gnorm": "0.399", "train_loss_scale": "2", "train_train_wall": "129", "train_gb_free": "39.9", "train_wall": "1307"}
[2024-10-07 08:44:55,037][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 08:44:55,047][fairseq.trainer][INFO] - begin training epoch 603
[2024-10-07 08:44:55,047][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 08:48:12,677][fairseq_cli.train][INFO] - end of epoch 603 (average epoch stats below)
[2024-10-07 08:48:12,684][train][INFO] - {"epoch": 603, "train_loss": "0.652", "train_ntokens": "260441", "train_nsentences": "1750.04", "train_wps": "63175.9", "train_ups": "0.24", "train_wpb": "260441", "train_bsz": "1750", "train_num_updates": "28929", "train_lr": "0.000452016", "train_gnorm": "0.417", "train_loss_scale": "2", "train_train_wall": "102", "train_gb_free": "39.6", "train_wall": "1505"}
[2024-10-07 08:48:12,999][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 08:48:13,032][fairseq.trainer][INFO] - begin training epoch 604
[2024-10-07 08:48:13,033][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 08:51:12,360][fairseq_cli.train][INFO] - end of epoch 604 (average epoch stats below)
[2024-10-07 08:51:12,364][train][INFO] - {"epoch": 604, "train_loss": "0.653", "train_ntokens": "260998", "train_nsentences": "1750.04", "train_wps": "69724.4", "train_ups": "0.27", "train_wpb": "260998", "train_bsz": "1750", "train_num_updates": "28977", "train_lr": "0.000452766", "train_gnorm": "0.469", "train_loss_scale": "2", "train_train_wall": "79", "train_gb_free": "39.6", "train_wall": "1684"}
[2024-10-07 08:51:12,506][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 08:51:12,511][fairseq.trainer][INFO] - begin training epoch 605
[2024-10-07 08:51:12,511][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 08:54:18,821][train_inner][INFO] - {"epoch": 605, "update": 604.479, "loss": "0.653", "ntokens": "260525", "nsentences": "1764.69", "wps": "37459.6", "ups": "0.14", "wpb": "260525", "bsz": "1764.7", "num_updates": "29000", "lr": "0.000453125", "gnorm": "0.435", "loss_scale": "2", "train_wall": "473", "gb_free": "40.5", "wall": "1871"}
[2024-10-07 08:54:34,840][fairseq_cli.train][INFO] - end of epoch 605 (average epoch stats below)
[2024-10-07 08:54:34,842][train][INFO] - {"epoch": 605, "train_loss": "0.649", "train_ntokens": "260607", "train_nsentences": "1750.04", "train_wps": "61780.9", "train_ups": "0.24", "train_wpb": "260607", "train_bsz": "1750", "train_num_updates": "29025", "train_lr": "0.000453516", "train_gnorm": "0.388", "train_loss_scale": "2", "train_train_wall": "94", "train_gb_free": "40.3", "train_wall": "1887"}
[2024-10-07 08:54:35,284][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 08:54:35,296][fairseq.trainer][INFO] - begin training epoch 606
[2024-10-07 08:54:35,296][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 08:59:15,891][fairseq_cli.train][INFO] - end of epoch 606 (average epoch stats below)
[2024-10-07 08:59:15,905][train][INFO] - {"epoch": 606, "train_loss": "0.65", "train_ntokens": "260389", "train_nsentences": "1750.04", "train_wps": "44469.9", "train_ups": "0.17", "train_wpb": "260389", "train_bsz": "1750", "train_num_updates": "29073", "train_lr": "0.000454266", "train_gnorm": "0.444", "train_loss_scale": "2", "train_train_wall": "98", "train_gb_free": "39.7", "train_wall": "2168"}
[2024-10-07 08:59:16,147][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 08:59:16,151][fairseq.trainer][INFO] - begin training epoch 607
[2024-10-07 08:59:16,151][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 09:02:21,188][fairseq_cli.train][INFO] - end of epoch 607 (average epoch stats below)
[2024-10-07 09:02:21,201][train][INFO] - {"epoch": 607, "train_loss": "0.65", "train_ntokens": "260379", "train_nsentences": "1750.04", "train_wps": "67450.9", "train_ups": "0.26", "train_wpb": "260379", "train_bsz": "1750", "train_num_updates": "29121", "train_lr": "0.000455016", "train_gnorm": "0.394", "train_loss_scale": "2", "train_train_wall": "32", "train_gb_free": "39.7", "train_wall": "2353"}
[2024-10-07 09:02:21,390][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 09:02:21,394][fairseq.trainer][INFO] - begin training epoch 608
[2024-10-07 09:02:21,395][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 09:05:20,766][fairseq_cli.train][INFO] - end of epoch 608 (average epoch stats below)
[2024-10-07 09:05:20,771][train][INFO] - {"epoch": 608, "train_loss": "0.656", "train_ntokens": "260878", "train_nsentences": "1750.04", "train_wps": "69735", "train_ups": "0.27", "train_wpb": "260878", "train_bsz": "1750", "train_num_updates": "29169", "train_lr": "0.000455766", "train_gnorm": "0.415", "train_loss_scale": "2", "train_train_wall": "43", "train_gb_free": "39.9", "train_wall": "2533"}
[2024-10-07 09:05:20,945][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 09:05:20,950][fairseq.trainer][INFO] - begin training epoch 609
[2024-10-07 09:05:20,950][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 09:08:40,412][train_inner][INFO] - {"epoch": 609, "update": 608.646, "loss": "0.65", "ntokens": "260710", "nsentences": "1739.49", "wps": "60518.7", "ups": "0.23", "wpb": "260710", "bsz": "1739.5", "num_updates": "29200", "lr": "0.00045625", "gnorm": "0.419", "loss_scale": "2", "train_wall": "256", "gb_free": "40", "wall": "2732"}
[2024-10-07 09:08:53,560][fairseq_cli.train][INFO] - end of epoch 609 (average epoch stats below)
[2024-10-07 09:08:53,567][train][INFO] - {"epoch": 609, "train_loss": "0.648", "train_ntokens": "260944", "train_nsentences": "1750.04", "train_wps": "58862.3", "train_ups": "0.23", "train_wpb": "260944", "train_bsz": "1750", "train_num_updates": "29217", "train_lr": "0.000456516", "train_gnorm": "0.45", "train_loss_scale": "2", "train_train_wall": "80", "train_gb_free": "40", "train_wall": "2746"}
[2024-10-07 09:08:53,802][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 09:08:53,810][fairseq.trainer][INFO] - begin training epoch 610
[2024-10-07 09:08:53,811][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 09:12:47,804][fairseq_cli.train][INFO] - end of epoch 610 (average epoch stats below)
[2024-10-07 09:12:47,807][train][INFO] - {"epoch": 610, "train_loss": "0.656", "train_ntokens": "260426", "train_nsentences": "1750.04", "train_wps": "53366.4", "train_ups": "0.2", "train_wpb": "260426", "train_bsz": "1750", "train_num_updates": "29265", "train_lr": "0.000457266", "train_gnorm": "0.45", "train_loss_scale": "2", "train_train_wall": "79", "train_gb_free": "40.3", "train_wall": "2980"}
[2024-10-07 09:12:47,863][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 09:12:47,886][fairseq.trainer][INFO] - begin training epoch 611
[2024-10-07 09:12:47,886][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 09:15:27,292][fairseq_cli.train][INFO] - end of epoch 611 (average epoch stats below)
[2024-10-07 09:15:27,301][train][INFO] - {"epoch": 611, "train_loss": "0.648", "train_ntokens": "260894", "train_nsentences": "1750.04", "train_wps": "78517.4", "train_ups": "0.3", "train_wpb": "260894", "train_bsz": "1750", "train_num_updates": "29313", "train_lr": "0.000458016", "train_gnorm": "0.397", "train_loss_scale": "2", "train_train_wall": "76", "train_gb_free": "39.8", "train_wall": "3139"}
[2024-10-07 09:15:27,458][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 09:15:27,464][fairseq.trainer][INFO] - begin training epoch 612
[2024-10-07 09:15:27,464][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 09:17:52,944][fairseq_cli.train][INFO] - end of epoch 612 (average epoch stats below)
[2024-10-07 09:17:52,952][train][INFO] - {"epoch": 612, "train_loss": "0.651", "train_ntokens": "260681", "train_nsentences": "1750.04", "train_wps": "85910.9", "train_ups": "0.33", "train_wpb": "260681", "train_bsz": "1750", "train_num_updates": "29361", "train_lr": "0.000458766", "train_gnorm": "0.471", "train_loss_scale": "2", "train_train_wall": "64", "train_gb_free": "40.5", "train_wall": "3285"}
[2024-10-07 09:17:53,118][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 09:17:53,128][fairseq.trainer][INFO] - begin training epoch 613
[2024-10-07 09:17:53,129][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 09:20:11,139][train_inner][INFO] - {"epoch": 613, "update": 612.812, "loss": "0.651", "ntokens": "260992", "nsentences": "1742.81", "wps": "75572.1", "ups": "0.29", "wpb": "260992", "bsz": "1742.8", "num_updates": "29400", "lr": "0.000459375", "gnorm": "0.433", "loss_scale": "2", "train_wall": "282", "gb_free": "41", "wall": "3423"}
[2024-10-07 09:20:19,818][fairseq_cli.train][INFO] - end of epoch 613 (average epoch stats below)
[2024-10-07 09:20:19,822][train][INFO] - {"epoch": 613, "train_loss": "0.646", "train_ntokens": "261012", "train_nsentences": "1750.04", "train_wps": "85306", "train_ups": "0.33", "train_wpb": "261012", "train_bsz": "1750", "train_num_updates": "29409", "train_lr": "0.000459516", "train_gnorm": "0.44", "train_loss_scale": "2", "train_train_wall": "60", "train_gb_free": "39.3", "train_wall": "3432"}
[2024-10-07 09:20:19,909][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 09:20:19,917][fairseq.trainer][INFO] - begin training epoch 614
[2024-10-07 09:20:19,917][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 09:22:55,397][fairseq_cli.train][INFO] - end of epoch 614 (average epoch stats below)
[2024-10-07 09:22:55,415][train][INFO] - {"epoch": 614, "train_loss": "0.65", "train_ntokens": "260842", "train_nsentences": "1750.04", "train_wps": "80472.2", "train_ups": "0.31", "train_wpb": "260842", "train_bsz": "1750", "train_num_updates": "29457", "train_lr": "0.000460266", "train_gnorm": "0.4", "train_loss_scale": "2", "train_train_wall": "47", "train_gb_free": "39.1", "train_wall": "3587"}
[2024-10-07 09:22:55,591][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 09:22:55,596][fairseq.trainer][INFO] - begin training epoch 615
[2024-10-07 09:22:55,597][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 09:25:22,865][fairseq_cli.train][INFO] - end of epoch 615 (average epoch stats below)
[2024-10-07 09:25:22,872][train][INFO] - {"epoch": 615, "train_loss": "0.648", "train_ntokens": "260822", "train_nsentences": "1750.04", "train_wps": "84904.2", "train_ups": "0.33", "train_wpb": "260822", "train_bsz": "1750", "train_num_updates": "29505", "train_lr": "0.000461016", "train_gnorm": "0.43", "train_loss_scale": "2", "train_train_wall": "49", "train_gb_free": "40.1", "train_wall": "3735"}
[2024-10-07 09:25:23,053][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 09:25:23,081][fairseq.trainer][INFO] - begin training epoch 616
[2024-10-07 09:25:23,082][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 09:28:03,602][fairseq_cli.train][INFO] - end of epoch 616 (average epoch stats below)
[2024-10-07 09:28:03,605][train][INFO] - {"epoch": 616, "train_loss": "0.646", "train_ntokens": "260495", "train_nsentences": "1750.04", "train_wps": "77793.4", "train_ups": "0.3", "train_wpb": "260495", "train_bsz": "1750", "train_num_updates": "29553", "train_lr": "0.000461766", "train_gnorm": "0.457", "train_loss_scale": "2", "train_train_wall": "70", "train_gb_free": "39.3", "train_wall": "3896"}
[2024-10-07 09:28:03,666][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 09:28:03,678][fairseq.trainer][INFO] - begin training epoch 617
[2024-10-07 09:28:03,678][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 09:30:38,579][train_inner][INFO] - {"epoch": 617, "update": 616.979, "loss": "0.648", "ntokens": "260505", "nsentences": "1762.23", "wps": "83040.3", "ups": "0.32", "wpb": "260505", "bsz": "1762.2", "num_updates": "29600", "lr": "0.0004625", "gnorm": "0.435", "loss_scale": "2", "train_wall": "203", "gb_free": "39.7", "wall": "4051"}
[2024-10-07 09:30:38,849][fairseq_cli.train][INFO] - end of epoch 617 (average epoch stats below)
[2024-10-07 09:30:38,850][train][INFO] - {"epoch": 617, "train_loss": "0.645", "train_ntokens": "260703", "train_nsentences": "1750.04", "train_wps": "80607.2", "train_ups": "0.31", "train_wpb": "260703", "train_bsz": "1750", "train_num_updates": "29601", "train_lr": "0.000462516", "train_gnorm": "0.436", "train_loss_scale": "2", "train_train_wall": "28", "train_gb_free": "39.3", "train_wall": "4051"}
[2024-10-07 09:30:38,986][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 09:30:38,991][fairseq.trainer][INFO] - begin training epoch 618
[2024-10-07 09:30:38,992][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 09:33:13,789][fairseq_cli.train][INFO] - end of epoch 618 (average epoch stats below)
[2024-10-07 09:33:13,803][train][INFO] - {"epoch": 618, "train_loss": "0.645", "train_ntokens": "260426", "train_nsentences": "1750.04", "train_wps": "80674.1", "train_ups": "0.31", "train_wpb": "260426", "train_bsz": "1750", "train_num_updates": "29649", "train_lr": "0.000463266", "train_gnorm": "0.439", "train_loss_scale": "2", "train_train_wall": "79", "train_gb_free": "39.6", "train_wall": "4206"}
[2024-10-07 09:33:13,951][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 09:33:13,955][fairseq.trainer][INFO] - begin training epoch 619
[2024-10-07 09:33:13,955][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 09:35:30,917][fairseq_cli.train][INFO] - end of epoch 619 (average epoch stats below)
[2024-10-07 09:35:30,920][train][INFO] - {"epoch": 619, "train_loss": "0.646", "train_ntokens": "260553", "train_nsentences": "1750.04", "train_wps": "91212.4", "train_ups": "0.35", "train_wpb": "260553", "train_bsz": "1750", "train_num_updates": "29697", "train_lr": "0.000464016", "train_gnorm": "0.46", "train_loss_scale": "2", "train_train_wall": "62", "train_gb_free": "39.7", "train_wall": "4343"}
[2024-10-07 09:35:31,035][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 09:35:31,049][fairseq.trainer][INFO] - begin training epoch 620
[2024-10-07 09:35:31,050][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 09:37:39,741][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 620 @ 29745 updates
[2024-10-07 09:37:39,742][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-07 09:37:43,041][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-07 09:37:43,043][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 620 @ 29745 updates, score None) (writing took 3.3022340144962072 seconds)
[2024-10-07 09:37:43,043][fairseq_cli.train][INFO] - end of epoch 620 (average epoch stats below)
[2024-10-07 09:37:43,046][train][INFO] - {"epoch": 620, "train_loss": "0.648", "train_ntokens": "261129", "train_nsentences": "1750.04", "train_wps": "94867.3", "train_ups": "0.36", "train_wpb": "261128", "train_bsz": "1750", "train_num_updates": "29745", "train_lr": "0.000464766", "train_gnorm": "0.416", "train_loss_scale": "2", "train_train_wall": "55", "train_gb_free": "39.7", "train_wall": "4475"}
[2024-10-07 09:37:43,102][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 09:37:43,106][fairseq.trainer][INFO] - begin training epoch 621
[2024-10-07 09:37:43,106][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 09:39:47,226][fairseq_cli.train][INFO] - end of epoch 621 (average epoch stats below)
[2024-10-07 09:39:47,229][train][INFO] - {"epoch": 621, "train_loss": "0.646", "train_ntokens": "260853", "train_nsentences": "1750.04", "train_wps": "100829", "train_ups": "0.39", "train_wpb": "260853", "train_bsz": "1750", "train_num_updates": "29793", "train_lr": "0.000465516", "train_gnorm": "0.428", "train_loss_scale": "2", "train_train_wall": "53", "train_gb_free": "40", "train_wall": "4599"}
[2024-10-07 09:39:47,292][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 09:39:47,296][fairseq.trainer][INFO] - begin training epoch 622
[2024-10-07 09:39:47,296][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 09:41:17,220][train_inner][INFO] - {"epoch": 622, "update": 621.146, "loss": "0.646", "ntokens": "260769", "nsentences": "1750.29", "wps": "81664.5", "ups": "0.31", "wpb": "260768", "bsz": "1750.3", "num_updates": "29800", "lr": "0.000465625", "gnorm": "0.436", "loss_scale": "2", "train_wall": "266", "gb_free": "40.1", "wall": "4689"}
[2024-10-07 09:41:55,741][fairseq_cli.train][INFO] - end of epoch 622 (average epoch stats below)
[2024-10-07 09:41:55,750][train][INFO] - {"epoch": 622, "train_loss": "0.649", "train_ntokens": "260498", "train_nsentences": "1750.04", "train_wps": "97297.2", "train_ups": "0.37", "train_wpb": "260498", "train_bsz": "1750", "train_num_updates": "29841", "train_lr": "0.000466266", "train_gnorm": "0.411", "train_loss_scale": "2", "train_train_wall": "51", "train_gb_free": "40.3", "train_wall": "4728"}
[2024-10-07 09:41:56,299][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 09:41:56,308][fairseq.trainer][INFO] - begin training epoch 623
[2024-10-07 09:41:56,308][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 09:47:11,851][fairseq_cli.train][INFO] - end of epoch 623 (average epoch stats below)
[2024-10-07 09:47:11,861][train][INFO] - {"epoch": 623, "train_loss": "0.643", "train_ntokens": "260912", "train_nsentences": "1750.04", "train_wps": "39618.7", "train_ups": "0.15", "train_wpb": "260912", "train_bsz": "1750", "train_num_updates": "29889", "train_lr": "0.000467016", "train_gnorm": "0.392", "train_loss_scale": "2", "train_train_wall": "53", "train_gb_free": "40.1", "train_wall": "5044"}
[2024-10-07 09:47:12,118][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 09:47:12,133][fairseq.trainer][INFO] - begin training epoch 624
[2024-10-07 09:47:12,133][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 09:50:36,290][fairseq_cli.train][INFO] - end of epoch 624 (average epoch stats below)
[2024-10-07 09:50:36,309][train][INFO] - {"epoch": 624, "train_loss": "0.643", "train_ntokens": "260697", "train_nsentences": "1750.04", "train_wps": "61206.7", "train_ups": "0.23", "train_wpb": "260697", "train_bsz": "1750", "train_num_updates": "29937", "train_lr": "0.000467766", "train_gnorm": "0.436", "train_loss_scale": "2", "train_train_wall": "82", "train_gb_free": "39.1", "train_wall": "5248"}
[2024-10-07 09:50:36,534][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 09:50:36,562][fairseq.trainer][INFO] - begin training epoch 625
[2024-10-07 09:50:36,562][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 09:53:29,729][fairseq_cli.train][INFO] - end of epoch 625 (average epoch stats below)
[2024-10-07 09:53:29,739][train][INFO] - {"epoch": 625, "train_loss": "0.647", "train_ntokens": "260781", "train_nsentences": "1750.04", "train_wps": "72178.2", "train_ups": "0.28", "train_wpb": "260781", "train_bsz": "1750", "train_num_updates": "29985", "train_lr": "0.000468516", "train_gnorm": "0.407", "train_loss_scale": "2", "train_train_wall": "76", "train_gb_free": "39.8", "train_wall": "5422"}
[2024-10-07 09:53:29,917][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 09:53:29,923][fairseq.trainer][INFO] - begin training epoch 626
[2024-10-07 09:53:29,923][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 09:56:08,263][train_inner][INFO] - {"epoch": 626, "update": 625.312, "loss": "0.645", "ntokens": "260784", "nsentences": "1748.26", "wps": "58534.9", "ups": "0.22", "wpb": "260784", "bsz": "1748.3", "num_updates": "30000", "lr": "0.00046875", "gnorm": "0.41", "loss_scale": "2", "train_wall": "256", "gb_free": "40.5", "wall": "5580"}
[2024-10-07 09:56:25,928][fairseq_cli.train][INFO] - end of epoch 626 (average epoch stats below)
[2024-10-07 09:56:25,932][train][INFO] - {"epoch": 626, "train_loss": "0.648", "train_ntokens": "260955", "train_nsentences": "1750.04", "train_wps": "71093.1", "train_ups": "0.27", "train_wpb": "260955", "train_bsz": "1750", "train_num_updates": "30033", "train_lr": "0.000469266", "train_gnorm": "0.398", "train_loss_scale": "2", "train_train_wall": "28", "train_gb_free": "39.3", "train_wall": "5598"}
[2024-10-07 09:56:26,109][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 09:56:26,114][fairseq.trainer][INFO] - begin training epoch 627
[2024-10-07 09:56:26,114][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 09:59:21,223][fairseq_cli.train][INFO] - end of epoch 627 (average epoch stats below)
[2024-10-07 09:59:21,231][train][INFO] - {"epoch": 627, "train_loss": "0.644", "train_ntokens": "260811", "train_nsentences": "1750.04", "train_wps": "71416", "train_ups": "0.27", "train_wpb": "260811", "train_bsz": "1750", "train_num_updates": "30081", "train_lr": "0.000470016", "train_gnorm": "0.415", "train_loss_scale": "2", "train_train_wall": "45", "train_gb_free": "39.9", "train_wall": "5773"}
[2024-10-07 09:59:21,414][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 09:59:21,423][fairseq.trainer][INFO] - begin training epoch 628
[2024-10-07 09:59:21,423][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 10:02:22,851][fairseq_cli.train][INFO] - end of epoch 628 (average epoch stats below)
[2024-10-07 10:02:22,869][train][INFO] - {"epoch": 628, "train_loss": "0.643", "train_ntokens": "260775", "train_nsentences": "1750.04", "train_wps": "68913.7", "train_ups": "0.26", "train_wpb": "260775", "train_bsz": "1750", "train_num_updates": "30129", "train_lr": "0.000470766", "train_gnorm": "0.453", "train_loss_scale": "2", "train_train_wall": "74", "train_gb_free": "40", "train_wall": "5955"}
[2024-10-07 10:02:23,035][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 10:02:23,040][fairseq.trainer][INFO] - begin training epoch 629
[2024-10-07 10:02:23,041][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 10:05:18,185][fairseq_cli.train][INFO] - end of epoch 629 (average epoch stats below)
[2024-10-07 10:05:18,197][train][INFO] - {"epoch": 629, "train_loss": "0.646", "train_ntokens": "260807", "train_nsentences": "1750.04", "train_wps": "71403", "train_ups": "0.27", "train_wpb": "260807", "train_bsz": "1750", "train_num_updates": "30177", "train_lr": "0.000471516", "train_gnorm": "0.434", "train_loss_scale": "2", "train_train_wall": "100", "train_gb_free": "39.6", "train_wall": "6130"}
[2024-10-07 10:05:18,366][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 10:05:18,385][fairseq.trainer][INFO] - begin training epoch 630
[2024-10-07 10:05:18,386][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 10:07:37,053][train_inner][INFO] - {"epoch": 630, "update": 629.479, "loss": "0.645", "ntokens": "260735", "nsentences": "1754.44", "wps": "75709", "ups": "0.29", "wpb": "260735", "bsz": "1754.4", "num_updates": "30200", "lr": "0.000471875", "gnorm": "0.434", "loss_scale": "2", "train_wall": "267", "gb_free": "39.3", "wall": "6269"}
[2024-10-07 10:07:56,123][fairseq_cli.train][INFO] - end of epoch 630 (average epoch stats below)
[2024-10-07 10:07:56,126][train][INFO] - {"epoch": 630, "train_loss": "0.643", "train_ntokens": "261077", "train_nsentences": "1750.04", "train_wps": "79351.9", "train_ups": "0.3", "train_wpb": "261077", "train_bsz": "1750", "train_num_updates": "30225", "train_lr": "0.000472266", "train_gnorm": "0.445", "train_loss_scale": "2", "train_train_wall": "50", "train_gb_free": "39.3", "train_wall": "6288"}
[2024-10-07 10:07:56,280][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 10:07:56,285][fairseq.trainer][INFO] - begin training epoch 631
[2024-10-07 10:07:56,286][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 10:10:36,332][fairseq_cli.train][INFO] - end of epoch 631 (average epoch stats below)
[2024-10-07 10:10:36,334][train][INFO] - {"epoch": 631, "train_loss": "0.646", "train_ntokens": "260720", "train_nsentences": "1750.04", "train_wps": "78115.2", "train_ups": "0.3", "train_wpb": "260720", "train_bsz": "1750", "train_num_updates": "30273", "train_lr": "0.000473016", "train_gnorm": "0.407", "train_loss_scale": "2", "train_train_wall": "58", "train_gb_free": "40", "train_wall": "6448"}
[2024-10-07 10:10:36,414][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 10:10:36,429][fairseq.trainer][INFO] - begin training epoch 632
[2024-10-07 10:10:36,430][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 10:12:59,085][fairseq_cli.train][INFO] - end of epoch 632 (average epoch stats below)
[2024-10-07 10:12:59,102][train][INFO] - {"epoch": 632, "train_loss": "0.642", "train_ntokens": "260502", "train_nsentences": "1750.04", "train_wps": "87585.4", "train_ups": "0.34", "train_wpb": "260502", "train_bsz": "1750", "train_num_updates": "30321", "train_lr": "0.000473766", "train_gnorm": "0.396", "train_loss_scale": "2", "train_train_wall": "62", "train_gb_free": "40.1", "train_wall": "6591"}
[2024-10-07 10:12:59,236][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 10:12:59,247][fairseq.trainer][INFO] - begin training epoch 633
[2024-10-07 10:12:59,248][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 10:15:12,865][fairseq_cli.train][INFO] - end of epoch 633 (average epoch stats below)
[2024-10-07 10:15:12,886][train][INFO] - {"epoch": 633, "train_loss": "0.644", "train_ntokens": "260620", "train_nsentences": "1750.04", "train_wps": "93512.7", "train_ups": "0.36", "train_wpb": "260620", "train_bsz": "1750", "train_num_updates": "30369", "train_lr": "0.000474516", "train_gnorm": "0.413", "train_loss_scale": "2", "train_train_wall": "52", "train_gb_free": "39.2", "train_wall": "6725"}
[2024-10-07 10:15:13,030][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 10:15:13,033][fairseq.trainer][INFO] - begin training epoch 634
[2024-10-07 10:15:13,034][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 10:17:15,077][train_inner][INFO] - {"epoch": 634, "update": 633.646, "loss": "0.643", "ntokens": "260784", "nsentences": "1751.62", "wps": "90233.4", "ups": "0.35", "wpb": "260784", "bsz": "1751.6", "num_updates": "30400", "lr": "0.000475", "gnorm": "0.406", "loss_scale": "2", "train_wall": "238", "gb_free": "40.2", "wall": "6847"}
[2024-10-07 10:17:30,373][fairseq_cli.train][INFO] - end of epoch 634 (average epoch stats below)
[2024-10-07 10:17:30,376][train][INFO] - {"epoch": 634, "train_loss": "0.643", "train_ntokens": "260603", "train_nsentences": "1750.04", "train_wps": "90982.6", "train_ups": "0.35", "train_wpb": "260603", "train_bsz": "1750", "train_num_updates": "30417", "train_lr": "0.000475266", "train_gnorm": "0.405", "train_loss_scale": "2", "train_train_wall": "61", "train_gb_free": "40", "train_wall": "6862"}
[2024-10-07 10:17:30,536][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 10:17:30,543][fairseq.trainer][INFO] - begin training epoch 635
[2024-10-07 10:17:30,544][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 10:19:50,609][fairseq_cli.train][INFO] - end of epoch 635 (average epoch stats below)
[2024-10-07 10:19:50,623][train][INFO] - {"epoch": 635, "train_loss": "0.643", "train_ntokens": "261006", "train_nsentences": "1750.04", "train_wps": "89335.6", "train_ups": "0.34", "train_wpb": "261006", "train_bsz": "1750", "train_num_updates": "30465", "train_lr": "0.000476016", "train_gnorm": "0.443", "train_loss_scale": "2", "train_train_wall": "51", "train_gb_free": "40.2", "train_wall": "7003"}
[2024-10-07 10:19:50,749][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 10:19:50,769][fairseq.trainer][INFO] - begin training epoch 636
[2024-10-07 10:19:50,770][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 10:21:59,361][fairseq_cli.train][INFO] - end of epoch 636 (average epoch stats below)
[2024-10-07 10:21:59,365][train][INFO] - {"epoch": 636, "train_loss": "0.65", "train_ntokens": "261050", "train_nsentences": "1750.04", "train_wps": "97332.2", "train_ups": "0.37", "train_wpb": "261050", "train_bsz": "1750", "train_num_updates": "30513", "train_lr": "0.000476766", "train_gnorm": "0.441", "train_loss_scale": "2", "train_train_wall": "51", "train_gb_free": "39.4", "train_wall": "7131"}
[2024-10-07 10:21:59,421][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 10:21:59,426][fairseq.trainer][INFO] - begin training epoch 637
[2024-10-07 10:21:59,426][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 10:25:11,791][fairseq_cli.train][INFO] - end of epoch 637 (average epoch stats below)
[2024-10-07 10:25:11,800][train][INFO] - {"epoch": 637, "train_loss": "0.649", "train_ntokens": "260919", "train_nsentences": "1750.04", "train_wps": "65084", "train_ups": "0.25", "train_wpb": "260919", "train_bsz": "1750", "train_num_updates": "30561", "train_lr": "0.000477516", "train_gnorm": "0.443", "train_loss_scale": "2", "train_train_wall": "91", "train_gb_free": "39.8", "train_wall": "7324"}
[2024-10-07 10:25:12,136][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 10:25:12,147][fairseq.trainer][INFO] - begin training epoch 638
[2024-10-07 10:25:12,148][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 10:30:04,235][train_inner][INFO] - {"epoch": 638, "update": 637.812, "loss": "0.645", "ntokens": "260825", "nsentences": "1743.68", "wps": "67822.9", "ups": "0.26", "wpb": "260825", "bsz": "1743.7", "num_updates": "30600", "lr": "0.000478125", "gnorm": "0.439", "loss_scale": "2", "train_wall": "272", "gb_free": "40", "wall": "7616"}
[2024-10-07 10:30:07,920][fairseq_cli.train][INFO] - end of epoch 638 (average epoch stats below)
[2024-10-07 10:30:07,927][train][INFO] - {"epoch": 638, "train_loss": "0.639", "train_ntokens": "260714", "train_nsentences": "1750.04", "train_wps": "42260.9", "train_ups": "0.16", "train_wpb": "260714", "train_bsz": "1750", "train_num_updates": "30609", "train_lr": "0.000478266", "train_gnorm": "0.439", "train_loss_scale": "2", "train_train_wall": "69", "train_gb_free": "39.8", "train_wall": "7620"}
[2024-10-07 10:30:08,020][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 10:30:08,040][fairseq.trainer][INFO] - begin training epoch 639
[2024-10-07 10:30:08,042][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 10:33:18,406][fairseq_cli.train][INFO] - end of epoch 639 (average epoch stats below)
[2024-10-07 10:33:18,410][train][INFO] - {"epoch": 639, "train_loss": "0.645", "train_ntokens": "260876", "train_nsentences": "1750.04", "train_wps": "65739.3", "train_ups": "0.25", "train_wpb": "260876", "train_bsz": "1750", "train_num_updates": "30657", "train_lr": "0.000479016", "train_gnorm": "0.431", "train_loss_scale": "2", "train_train_wall": "93", "train_gb_free": "39.7", "train_wall": "7810"}
[2024-10-07 10:33:18,596][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 10:33:18,601][fairseq.trainer][INFO] - begin training epoch 640
[2024-10-07 10:33:18,602][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 10:35:51,056][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 640 @ 30705 updates
[2024-10-07 10:35:51,058][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-07 10:35:55,509][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-07 10:35:55,511][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 640 @ 30705 updates, score None) (writing took 4.455337558872998 seconds)
[2024-10-07 10:35:55,512][fairseq_cli.train][INFO] - end of epoch 640 (average epoch stats below)
[2024-10-07 10:35:55,515][train][INFO] - {"epoch": 640, "train_loss": "0.64", "train_ntokens": "260523", "train_nsentences": "1750.04", "train_wps": "79599.1", "train_ups": "0.31", "train_wpb": "260523", "train_bsz": "1750", "train_num_updates": "30705", "train_lr": "0.000479766", "train_gnorm": "0.435", "train_loss_scale": "2", "train_train_wall": "53", "train_gb_free": "40.1", "train_wall": "7968"}
[2024-10-07 10:35:55,803][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 10:35:55,829][fairseq.trainer][INFO] - begin training epoch 641
[2024-10-07 10:35:55,829][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 10:39:11,651][fairseq_cli.train][INFO] - end of epoch 641 (average epoch stats below)
[2024-10-07 10:39:11,657][train][INFO] - {"epoch": 641, "train_loss": "0.639", "train_ntokens": "260680", "train_nsentences": "1750.04", "train_wps": "63794.7", "train_ups": "0.24", "train_wpb": "260680", "train_bsz": "1750", "train_num_updates": "30753", "train_lr": "0.000480516", "train_gnorm": "0.378", "train_loss_scale": "2", "train_train_wall": "86", "train_gb_free": "39.7", "train_wall": "8164"}
[2024-10-07 10:39:12,068][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 10:39:12,108][fairseq.trainer][INFO] - begin training epoch 642
[2024-10-07 10:39:12,109][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 10:41:31,944][train_inner][INFO] - {"epoch": 642, "update": 641.979, "loss": "0.641", "ntokens": "260753", "nsentences": "1750.42", "wps": "75833.8", "ups": "0.29", "wpb": "260753", "bsz": "1750.4", "num_updates": "30800", "lr": "0.00048125", "gnorm": "0.422", "loss_scale": "2", "train_wall": "285", "gb_free": "39.3", "wall": "8304"}
[2024-10-07 10:41:32,176][fairseq_cli.train][INFO] - end of epoch 642 (average epoch stats below)
[2024-10-07 10:41:32,179][train][INFO] - {"epoch": 642, "train_loss": "0.641", "train_ntokens": "260859", "train_nsentences": "1750.04", "train_wps": "89106.7", "train_ups": "0.34", "train_wpb": "260860", "train_bsz": "1750", "train_num_updates": "30801", "train_lr": "0.000481266", "train_gnorm": "0.442", "train_loss_scale": "2", "train_train_wall": "50", "train_gb_free": "39.6", "train_wall": "8304"}
[2024-10-07 10:41:32,235][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 10:41:32,239][fairseq.trainer][INFO] - begin training epoch 643
[2024-10-07 10:41:32,239][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 10:43:40,337][fairseq_cli.train][INFO] - end of epoch 643 (average epoch stats below)
[2024-10-07 10:43:40,346][train][INFO] - {"epoch": 643, "train_loss": "0.644", "train_ntokens": "260813", "train_nsentences": "1750.04", "train_wps": "97682", "train_ups": "0.37", "train_wpb": "260813", "train_bsz": "1750", "train_num_updates": "30849", "train_lr": "0.000482016", "train_gnorm": "0.446", "train_loss_scale": "4", "train_train_wall": "60", "train_gb_free": "39.3", "train_wall": "8432"}
[2024-10-07 10:43:40,428][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 10:43:40,432][fairseq.trainer][INFO] - begin training epoch 644
[2024-10-07 10:43:40,433][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 10:45:49,323][fairseq_cli.train][INFO] - end of epoch 644 (average epoch stats below)
[2024-10-07 10:45:49,333][train][INFO] - {"epoch": 644, "train_loss": "0.64", "train_ntokens": "260822", "train_nsentences": "1750.04", "train_wps": "97063.4", "train_ups": "0.37", "train_wpb": "260822", "train_bsz": "1750", "train_num_updates": "30897", "train_lr": "0.000482766", "train_gnorm": "0.37", "train_loss_scale": "4", "train_train_wall": "44", "train_gb_free": "39.7", "train_wall": "8561"}
[2024-10-07 10:45:49,525][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 10:45:49,537][fairseq.trainer][INFO] - begin training epoch 645
[2024-10-07 10:45:49,538][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 10:49:27,350][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2024-10-07 10:49:31,451][fairseq_cli.train][INFO] - end of epoch 645 (average epoch stats below)
[2024-10-07 10:49:31,455][train][INFO] - {"epoch": 645, "train_loss": "0.642", "train_ntokens": "260899", "train_nsentences": "1765.87", "train_wps": "55205.5", "train_ups": "0.21", "train_wpb": "260899", "train_bsz": "1765.9", "train_num_updates": "30944", "train_lr": "0.0004835", "train_gnorm": "0.44", "train_loss_scale": "2", "train_train_wall": "22", "train_gb_free": "40.1", "train_wall": "8784"}
[2024-10-07 10:49:31,679][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 10:49:31,693][fairseq.trainer][INFO] - begin training epoch 646
[2024-10-07 10:49:31,693][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 10:52:48,829][fairseq_cli.train][INFO] - end of epoch 646 (average epoch stats below)
[2024-10-07 10:52:48,832][train][INFO] - {"epoch": 646, "train_loss": "0.639", "train_ntokens": "260853", "train_nsentences": "1750.04", "train_wps": "63437.7", "train_ups": "0.24", "train_wpb": "260853", "train_bsz": "1750", "train_num_updates": "30992", "train_lr": "0.00048425", "train_gnorm": "0.398", "train_loss_scale": "2", "train_train_wall": "55", "train_gb_free": "39.7", "train_wall": "8981"}
[2024-10-07 10:52:51,133][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 10:52:51,152][fairseq.trainer][INFO] - begin training epoch 647
[2024-10-07 10:52:51,153][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 10:56:59,112][train_inner][INFO] - {"epoch": 647, "update": 646.167, "loss": "0.641", "ntokens": "260934", "nsentences": "1754.25", "wps": "56286.5", "ups": "0.22", "wpb": "260934", "bsz": "1754.2", "num_updates": "31000", "lr": "0.000484375", "gnorm": "0.421", "loss_scale": "2", "train_wall": "258", "gb_free": "40.4", "wall": "9231"}
[2024-10-07 10:57:34,960][fairseq_cli.train][INFO] - end of epoch 647 (average epoch stats below)
[2024-10-07 10:57:34,962][train][INFO] - {"epoch": 647, "train_loss": "0.642", "train_ntokens": "260727", "train_nsentences": "1750.04", "train_wps": "43738.9", "train_ups": "0.17", "train_wpb": "260727", "train_bsz": "1750", "train_num_updates": "31040", "train_lr": "0.000485", "train_gnorm": "0.498", "train_loss_scale": "2", "train_train_wall": "112", "train_gb_free": "40", "train_wall": "9267"}
[2024-10-07 10:57:35,274][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 10:57:35,282][fairseq.trainer][INFO] - begin training epoch 648
[2024-10-07 10:57:35,283][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 11:02:15,728][fairseq_cli.train][INFO] - end of epoch 648 (average epoch stats below)
[2024-10-07 11:02:15,746][train][INFO] - {"epoch": 648, "train_loss": "0.637", "train_ntokens": "261046", "train_nsentences": "1750.04", "train_wps": "44626.3", "train_ups": "0.17", "train_wpb": "261046", "train_bsz": "1750", "train_num_updates": "31088", "train_lr": "0.00048575", "train_gnorm": "0.422", "train_loss_scale": "2", "train_train_wall": "123", "train_gb_free": "39.7", "train_wall": "9548"}
[2024-10-07 11:02:16,008][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 11:02:16,018][fairseq.trainer][INFO] - begin training epoch 649
[2024-10-07 11:02:16,018][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 11:06:41,628][fairseq_cli.train][INFO] - end of epoch 649 (average epoch stats below)
[2024-10-07 11:06:41,640][train][INFO] - {"epoch": 649, "train_loss": "0.635", "train_ntokens": "260622", "train_nsentences": "1750.04", "train_wps": "47048.8", "train_ups": "0.18", "train_wpb": "260622", "train_bsz": "1750", "train_num_updates": "31136", "train_lr": "0.0004865", "train_gnorm": "0.445", "train_loss_scale": "2", "train_train_wall": "77", "train_gb_free": "40", "train_wall": "9814"}
[2024-10-07 11:06:41,993][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 11:06:42,033][fairseq.trainer][INFO] - begin training epoch 650
[2024-10-07 11:06:42,033][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 11:13:17,189][fairseq_cli.train][INFO] - end of epoch 650 (average epoch stats below)
[2024-10-07 11:13:17,199][train][INFO] - {"epoch": 650, "train_loss": "0.638", "train_ntokens": "260703", "train_nsentences": "1750.04", "train_wps": "31636.1", "train_ups": "0.12", "train_wpb": "260703", "train_bsz": "1750", "train_num_updates": "31184", "train_lr": "0.00048725", "train_gnorm": "0.393", "train_loss_scale": "2", "train_train_wall": "93", "train_gb_free": "39.6", "train_wall": "10209"}
[2024-10-07 11:13:17,520][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 11:13:17,528][fairseq.trainer][INFO] - begin training epoch 651
[2024-10-07 11:13:17,531][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 11:16:56,833][train_inner][INFO] - {"epoch": 651, "update": 650.333, "loss": "0.639", "ntokens": "260781", "nsentences": "1753.93", "wps": "43548.1", "ups": "0.17", "wpb": "260781", "bsz": "1753.9", "num_updates": "31200", "lr": "0.0004875", "gnorm": "0.429", "loss_scale": "2", "train_wall": "395", "gb_free": "39.7", "wall": "10429"}
[2024-10-07 11:17:42,443][fairseq_cli.train][INFO] - end of epoch 651 (average epoch stats below)
[2024-10-07 11:17:42,445][train][INFO] - {"epoch": 651, "train_loss": "0.642", "train_ntokens": "260709", "train_nsentences": "1750.04", "train_wps": "47179.3", "train_ups": "0.18", "train_wpb": "260709", "train_bsz": "1750", "train_num_updates": "31232", "train_lr": "0.000488", "train_gnorm": "0.433", "train_loss_scale": "2", "train_train_wall": "109", "train_gb_free": "39.6", "train_wall": "10474"}
[2024-10-07 11:17:42,702][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 11:17:42,706][fairseq.trainer][INFO] - begin training epoch 652
[2024-10-07 11:17:42,707][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 11:21:54,049][fairseq_cli.train][INFO] - end of epoch 652 (average epoch stats below)
[2024-10-07 11:21:54,053][train][INFO] - {"epoch": 652, "train_loss": "0.643", "train_ntokens": "260819", "train_nsentences": "1750.04", "train_wps": "49758", "train_ups": "0.19", "train_wpb": "260819", "train_bsz": "1750", "train_num_updates": "31280", "train_lr": "0.00048875", "train_gnorm": "0.465", "train_loss_scale": "2", "train_train_wall": "135", "train_gb_free": "39.3", "train_wall": "10726"}
[2024-10-07 11:21:54,273][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 11:21:54,301][fairseq.trainer][INFO] - begin training epoch 653
[2024-10-07 11:21:54,301][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 11:26:22,766][fairseq_cli.train][INFO] - end of epoch 653 (average epoch stats below)
[2024-10-07 11:26:22,772][train][INFO] - {"epoch": 653, "train_loss": "0.634", "train_ntokens": "260498", "train_nsentences": "1750.04", "train_wps": "46532", "train_ups": "0.18", "train_wpb": "260498", "train_bsz": "1750", "train_num_updates": "31328", "train_lr": "0.0004895", "train_gnorm": "0.414", "train_loss_scale": "2", "train_train_wall": "157", "train_gb_free": "39.7", "train_wall": "10995"}
[2024-10-07 11:26:23,190][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 11:26:23,250][fairseq.trainer][INFO] - begin training epoch 654
[2024-10-07 11:26:23,250][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 11:31:21,841][fairseq_cli.train][INFO] - end of epoch 654 (average epoch stats below)
[2024-10-07 11:31:21,848][train][INFO] - {"epoch": 654, "train_loss": "0.634", "train_ntokens": "260993", "train_nsentences": "1750.04", "train_wps": "41888.4", "train_ups": "0.16", "train_wpb": "260993", "train_bsz": "1750", "train_num_updates": "31376", "train_lr": "0.00049025", "train_gnorm": "0.408", "train_loss_scale": "2", "train_train_wall": "127", "train_gb_free": "40", "train_wall": "11294"}
[2024-10-07 11:31:22,123][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 11:31:22,159][fairseq.trainer][INFO] - begin training epoch 655
[2024-10-07 11:31:22,159][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 11:35:43,273][train_inner][INFO] - {"epoch": 655, "update": 654.5, "loss": "0.637", "ntokens": "260589", "nsentences": "1757.4", "wps": "46271.2", "ups": "0.18", "wpb": "260590", "bsz": "1757.4", "num_updates": "31400", "lr": "0.000490625", "gnorm": "0.437", "loss_scale": "2", "train_wall": "527", "gb_free": "39.3", "wall": "11555"}
[2024-10-07 11:35:58,347][fairseq_cli.train][INFO] - end of epoch 655 (average epoch stats below)
[2024-10-07 11:35:58,350][train][INFO] - {"epoch": 655, "train_loss": "0.638", "train_ntokens": "260739", "train_nsentences": "1750.04", "train_wps": "45263.9", "train_ups": "0.17", "train_wpb": "260739", "train_bsz": "1750", "train_num_updates": "31424", "train_lr": "0.000491", "train_gnorm": "0.436", "train_loss_scale": "2", "train_train_wall": "80", "train_gb_free": "39.6", "train_wall": "11570"}
[2024-10-07 11:35:58,622][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 11:35:58,626][fairseq.trainer][INFO] - begin training epoch 656
[2024-10-07 11:35:58,627][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 11:39:43,472][fairseq_cli.train][INFO] - end of epoch 656 (average epoch stats below)
[2024-10-07 11:39:43,482][train][INFO] - {"epoch": 656, "train_loss": "0.639", "train_ntokens": "261350", "train_nsentences": "1750.04", "train_wps": "55722.7", "train_ups": "0.21", "train_wpb": "261350", "train_bsz": "1750", "train_num_updates": "31472", "train_lr": "0.00049175", "train_gnorm": "0.477", "train_loss_scale": "2", "train_train_wall": "83", "train_gb_free": "40.6", "train_wall": "11796"}
[2024-10-07 11:39:43,784][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 11:39:43,821][fairseq.trainer][INFO] - begin training epoch 657
[2024-10-07 11:39:43,821][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 11:46:39,873][fairseq_cli.train][INFO] - end of epoch 657 (average epoch stats below)
[2024-10-07 11:46:39,893][train][INFO] - {"epoch": 657, "train_loss": "0.633", "train_ntokens": "260702", "train_nsentences": "1750.04", "train_wps": "30051.6", "train_ups": "0.12", "train_wpb": "260702", "train_bsz": "1750", "train_num_updates": "31520", "train_lr": "0.0004925", "train_gnorm": "0.439", "train_loss_scale": "2", "train_train_wall": "127", "train_gb_free": "39.8", "train_wall": "12212"}
[2024-10-07 11:46:40,342][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 11:46:40,365][fairseq.trainer][INFO] - begin training epoch 658
[2024-10-07 11:46:40,366][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 11:51:48,313][fairseq_cli.train][INFO] - end of epoch 658 (average epoch stats below)
[2024-10-07 11:51:48,317][train][INFO] - {"epoch": 658, "train_loss": "0.635", "train_ntokens": "260711", "train_nsentences": "1750.04", "train_wps": "40574.7", "train_ups": "0.16", "train_wpb": "260710", "train_bsz": "1750", "train_num_updates": "31568", "train_lr": "0.00049325", "train_gnorm": "0.38", "train_loss_scale": "2", "train_train_wall": "88", "train_gb_free": "39.2", "train_wall": "12520"}
[2024-10-07 11:51:48,668][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 11:51:48,701][fairseq.trainer][INFO] - begin training epoch 659
[2024-10-07 11:51:48,701][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 11:56:25,272][train_inner][INFO] - {"epoch": 659, "update": 658.667, "loss": "0.637", "ntokens": "260895", "nsentences": "1744.9", "wps": "42012.4", "ups": "0.16", "wpb": "260895", "bsz": "1744.9", "num_updates": "31600", "lr": "0.00049375", "gnorm": "0.432", "loss_scale": "2", "train_wall": "369", "gb_free": "39.3", "wall": "12797"}
[2024-10-07 11:56:37,446][fairseq_cli.train][INFO] - end of epoch 659 (average epoch stats below)
[2024-10-07 11:56:37,459][train][INFO] - {"epoch": 659, "train_loss": "0.639", "train_ntokens": "260688", "train_nsentences": "1750.04", "train_wps": "43278.4", "train_ups": "0.17", "train_wpb": "260688", "train_bsz": "1750", "train_num_updates": "31616", "train_lr": "0.000494", "train_gnorm": "0.451", "train_loss_scale": "2", "train_train_wall": "68", "train_gb_free": "39.6", "train_wall": "12809"}
[2024-10-07 11:56:37,815][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 11:56:37,840][fairseq.trainer][INFO] - begin training epoch 660
[2024-10-07 11:56:37,840][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 12:01:41,543][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 660 @ 31664 updates
[2024-10-07 12:01:41,546][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-07 12:01:45,580][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-07 12:01:45,582][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 660 @ 31664 updates, score None) (writing took 4.039597378112376 seconds)
[2024-10-07 12:01:45,583][fairseq_cli.train][INFO] - end of epoch 660 (average epoch stats below)
[2024-10-07 12:01:45,585][train][INFO] - {"epoch": 660, "train_loss": "0.644", "train_ntokens": "260321", "train_nsentences": "1750.04", "train_wps": "40553.3", "train_ups": "0.16", "train_wpb": "260321", "train_bsz": "1750", "train_num_updates": "31664", "train_lr": "0.00049475", "train_gnorm": "0.401", "train_loss_scale": "2", "train_train_wall": "139", "train_gb_free": "39.8", "train_wall": "13118"}
[2024-10-07 12:01:45,668][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 12:01:45,685][fairseq.trainer][INFO] - begin training epoch 661
[2024-10-07 12:01:45,686][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 12:07:06,403][fairseq_cli.train][INFO] - end of epoch 661 (average epoch stats below)
[2024-10-07 12:07:06,421][train][INFO] - {"epoch": 661, "train_loss": "0.642", "train_ntokens": "260810", "train_nsentences": "1750.04", "train_wps": "39020.8", "train_ups": "0.15", "train_wpb": "260810", "train_bsz": "1750", "train_num_updates": "31712", "train_lr": "0.0004955", "train_gnorm": "0.422", "train_loss_scale": "2", "train_train_wall": "102", "train_gb_free": "40.1", "train_wall": "13438"}
[2024-10-07 12:07:06,758][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 12:07:06,773][fairseq.trainer][INFO] - begin training epoch 662
[2024-10-07 12:07:06,773][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 12:10:07,911][fairseq_cli.train][INFO] - end of epoch 662 (average epoch stats below)
[2024-10-07 12:10:07,916][train][INFO] - {"epoch": 662, "train_loss": "0.638", "train_ntokens": "260982", "train_nsentences": "1750.04", "train_wps": "69023.3", "train_ups": "0.26", "train_wpb": "260982", "train_bsz": "1750", "train_num_updates": "31760", "train_lr": "0.00049625", "train_gnorm": "0.38", "train_loss_scale": "2", "train_train_wall": "67", "train_gb_free": "39.1", "train_wall": "13620"}
[2024-10-07 12:10:08,008][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 12:10:08,021][fairseq.trainer][INFO] - begin training epoch 663
[2024-10-07 12:10:08,022][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 12:12:09,559][train_inner][INFO] - {"epoch": 663, "update": 662.833, "loss": "0.64", "ntokens": "260760", "nsentences": "1735.21", "wps": "55230.3", "ups": "0.21", "wpb": "260760", "bsz": "1735.2", "num_updates": "31800", "lr": "0.000496875", "gnorm": "0.425", "loss_scale": "2", "train_wall": "367", "gb_free": "39.6", "wall": "13742"}
[2024-10-07 12:12:15,113][fairseq_cli.train][INFO] - end of epoch 663 (average epoch stats below)
[2024-10-07 12:12:15,115][train][INFO] - {"epoch": 663, "train_loss": "0.633", "train_ntokens": "260741", "train_nsentences": "1750.04", "train_wps": "98395.2", "train_ups": "0.38", "train_wpb": "260741", "train_bsz": "1750", "train_num_updates": "31808", "train_lr": "0.000497", "train_gnorm": "0.479", "train_loss_scale": "2", "train_train_wall": "52", "train_gb_free": "39.8", "train_wall": "13747"}
[2024-10-07 12:12:15,228][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 12:12:15,249][fairseq.trainer][INFO] - begin training epoch 664
[2024-10-07 12:12:15,250][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 12:14:22,780][fairseq_cli.train][INFO] - end of epoch 664 (average epoch stats below)
[2024-10-07 12:14:22,785][train][INFO] - {"epoch": 664, "train_loss": "0.633", "train_ntokens": "260736", "train_nsentences": "1750.04", "train_wps": "98031.6", "train_ups": "0.38", "train_wpb": "260736", "train_bsz": "1750", "train_num_updates": "31856", "train_lr": "0.00049775", "train_gnorm": "0.379", "train_loss_scale": "2", "train_train_wall": "41", "train_gb_free": "40.1", "train_wall": "13875"}
[2024-10-07 12:14:22,880][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 12:14:22,901][fairseq.trainer][INFO] - begin training epoch 665
[2024-10-07 12:14:22,902][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 12:17:27,094][fairseq_cli.train][INFO] - end of epoch 665 (average epoch stats below)
[2024-10-07 12:17:27,102][train][INFO] - {"epoch": 665, "train_loss": "0.639", "train_ntokens": "260707", "train_nsentences": "1750.04", "train_wps": "67894.4", "train_ups": "0.26", "train_wpb": "260707", "train_bsz": "1750", "train_num_updates": "31904", "train_lr": "0.0004985", "train_gnorm": "0.428", "train_loss_scale": "2", "train_train_wall": "76", "train_gb_free": "39.7", "train_wall": "14059"}
[2024-10-07 12:17:27,306][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 12:17:27,329][fairseq.trainer][INFO] - begin training epoch 666
[2024-10-07 12:17:27,330][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 12:20:44,847][fairseq_cli.train][INFO] - end of epoch 666 (average epoch stats below)
[2024-10-07 12:20:44,854][train][INFO] - {"epoch": 666, "train_loss": "0.638", "train_ntokens": "260707", "train_nsentences": "1750.04", "train_wps": "63281.8", "train_ups": "0.24", "train_wpb": "260707", "train_bsz": "1750", "train_num_updates": "31952", "train_lr": "0.00049925", "train_gnorm": "0.445", "train_loss_scale": "2", "train_train_wall": "83", "train_gb_free": "40.1", "train_wall": "14257"}
[2024-10-07 12:20:45,024][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 12:20:45,030][fairseq.trainer][INFO] - begin training epoch 667
[2024-10-07 12:20:45,030][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 12:23:49,633][train_inner][INFO] - {"epoch": 667, "update": 667.0, "loss": "0.638", "ntokens": "260615", "nsentences": "1762.27", "wps": "74455.8", "ups": "0.29", "wpb": "260615", "bsz": "1762.3", "num_updates": "32000", "lr": "0.0005", "gnorm": "0.411", "loss_scale": "2", "train_wall": "304", "gb_free": "39.3", "wall": "14442"}
[2024-10-07 12:23:49,639][fairseq_cli.train][INFO] - end of epoch 667 (average epoch stats below)
[2024-10-07 12:23:49,641][train][INFO] - {"epoch": 667, "train_loss": "0.639", "train_ntokens": "260447", "train_nsentences": "1750.04", "train_wps": "67656.2", "train_ups": "0.26", "train_wpb": "260447", "train_bsz": "1750", "train_num_updates": "32000", "train_lr": "0.0005", "train_gnorm": "0.395", "train_loss_scale": "2", "train_train_wall": "100", "train_gb_free": "39.3", "train_wall": "14442"}
[2024-10-07 12:23:49,847][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 12:23:49,852][fairseq.trainer][INFO] - begin training epoch 668
[2024-10-07 12:23:49,852][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 12:27:45,694][fairseq_cli.train][INFO] - end of epoch 668 (average epoch stats below)
[2024-10-07 12:27:45,698][train][INFO] - {"epoch": 668, "train_loss": "0.637", "train_ntokens": "260319", "train_nsentences": "1750.04", "train_wps": "52934.2", "train_ups": "0.2", "train_wpb": "260320", "train_bsz": "1750", "train_num_updates": "32048", "train_lr": "0.000499935", "train_gnorm": "0.409", "train_loss_scale": "2", "train_train_wall": "88", "train_gb_free": "39.6", "train_wall": "14678"}
[2024-10-07 12:27:45,922][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 12:27:45,929][fairseq.trainer][INFO] - begin training epoch 669
[2024-10-07 12:27:45,929][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 12:31:37,424][fairseq_cli.train][INFO] - end of epoch 669 (average epoch stats below)
[2024-10-07 12:31:37,432][train][INFO] - {"epoch": 669, "train_loss": "0.642", "train_ntokens": "260518", "train_nsentences": "1750.04", "train_wps": "53962.8", "train_ups": "0.21", "train_wpb": "260518", "train_bsz": "1750", "train_num_updates": "32096", "train_lr": "0.00049987", "train_gnorm": "0.457", "train_loss_scale": "2", "train_train_wall": "111", "train_gb_free": "40.6", "train_wall": "14909"}
[2024-10-07 12:31:37,647][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 12:31:37,657][fairseq.trainer][INFO] - begin training epoch 670
[2024-10-07 12:31:37,658][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 12:35:36,528][fairseq_cli.train][INFO] - end of epoch 670 (average epoch stats below)
[2024-10-07 12:35:36,540][train][INFO] - {"epoch": 670, "train_loss": "0.638", "train_ntokens": "260803", "train_nsentences": "1750.04", "train_wps": "52356", "train_ups": "0.2", "train_wpb": "260803", "train_bsz": "1750", "train_num_updates": "32144", "train_lr": "0.000499804", "train_gnorm": "0.472", "train_loss_scale": "2", "train_train_wall": "92", "train_gb_free": "39.3", "train_wall": "15149"}
[2024-10-07 12:35:36,786][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 12:35:36,789][fairseq.trainer][INFO] - begin training epoch 671
[2024-10-07 12:35:36,789][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 12:39:42,189][fairseq_cli.train][INFO] - end of epoch 671 (average epoch stats below)
[2024-10-07 12:39:42,192][train][INFO] - {"epoch": 671, "train_loss": "0.642", "train_ntokens": "260736", "train_nsentences": "1750.04", "train_wps": "50948.2", "train_ups": "0.2", "train_wpb": "260736", "train_bsz": "1750", "train_num_updates": "32192", "train_lr": "0.000499739", "train_gnorm": "0.412", "train_loss_scale": "2", "train_train_wall": "151", "train_gb_free": "39.7", "train_wall": "15394"}
[2024-10-07 12:39:42,481][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 12:39:42,504][fairseq.trainer][INFO] - begin training epoch 672
[2024-10-07 12:39:42,505][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 12:43:26,076][train_inner][INFO] - {"epoch": 672, "update": 671.167, "loss": "0.639", "ntokens": "260737", "nsentences": "1745.94", "wps": "44326.6", "ups": "0.17", "wpb": "260737", "bsz": "1745.9", "num_updates": "32200", "lr": "0.000499728", "gnorm": "0.434", "loss_scale": "2", "train_wall": "538", "gb_free": "39.3", "wall": "15618"}
[2024-10-07 12:43:53,570][fairseq_cli.train][INFO] - end of epoch 672 (average epoch stats below)
[2024-10-07 12:43:53,571][train][INFO] - {"epoch": 672, "train_loss": "0.634", "train_ntokens": "260652", "train_nsentences": "1750.04", "train_wps": "49771.1", "train_ups": "0.19", "train_wpb": "260652", "train_bsz": "1750", "train_num_updates": "32240", "train_lr": "0.000499674", "train_gnorm": "0.402", "train_loss_scale": "2", "train_train_wall": "122", "train_gb_free": "40.3", "train_wall": "15646"}
[2024-10-07 12:43:53,859][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 12:43:53,865][fairseq.trainer][INFO] - begin training epoch 673
[2024-10-07 12:43:53,865][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 12:47:47,134][fairseq_cli.train][INFO] - end of epoch 673 (average epoch stats below)
[2024-10-07 12:47:47,144][train][INFO] - {"epoch": 673, "train_loss": "0.632", "train_ntokens": "260954", "train_nsentences": "1750.04", "train_wps": "53627.6", "train_ups": "0.21", "train_wpb": "260954", "train_bsz": "1750", "train_num_updates": "32288", "train_lr": "0.000499609", "train_gnorm": "0.408", "train_loss_scale": "2", "train_train_wall": "124", "train_gb_free": "39.6", "train_wall": "15879"}
[2024-10-07 12:47:47,439][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 12:47:47,449][fairseq.trainer][INFO] - begin training epoch 674
[2024-10-07 12:47:47,449][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 12:51:38,848][fairseq_cli.train][INFO] - end of epoch 674 (average epoch stats below)
[2024-10-07 12:51:38,853][train][INFO] - {"epoch": 674, "train_loss": "0.635", "train_ntokens": "260602", "train_nsentences": "1750.04", "train_wps": "53986", "train_ups": "0.21", "train_wpb": "260602", "train_bsz": "1750", "train_num_updates": "32336", "train_lr": "0.000499543", "train_gnorm": "0.393", "train_loss_scale": "2", "train_train_wall": "152", "train_gb_free": "39.3", "train_wall": "16111"}
[2024-10-07 12:51:39,160][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 12:51:39,167][fairseq.trainer][INFO] - begin training epoch 675
[2024-10-07 12:51:39,168][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 12:55:44,190][fairseq_cli.train][INFO] - end of epoch 675 (average epoch stats below)
[2024-10-07 12:55:44,201][train][INFO] - {"epoch": 675, "train_loss": "0.638", "train_ntokens": "260443", "train_nsentences": "1750.04", "train_wps": "50953.9", "train_ups": "0.2", "train_wpb": "260443", "train_bsz": "1750", "train_num_updates": "32384", "train_lr": "0.000499478", "train_gnorm": "0.409", "train_loss_scale": "2", "train_train_wall": "84", "train_gb_free": "39.7", "train_wall": "16356"}
[2024-10-07 12:55:44,438][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 12:55:44,452][fairseq.trainer][INFO] - begin training epoch 676
[2024-10-07 12:55:44,452][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 12:59:43,841][train_inner][INFO] - {"epoch": 676, "update": 675.333, "loss": "0.634", "ntokens": "260528", "nsentences": "1751.9", "wps": "53293", "ups": "0.2", "wpb": "260528", "bsz": "1751.9", "num_updates": "32400", "lr": "0.000499457", "gnorm": "0.407", "loss_scale": "2", "train_wall": "463", "gb_free": "39.6", "wall": "16596"}
[2024-10-07 13:00:03,364][fairseq_cli.train][INFO] - end of epoch 676 (average epoch stats below)
[2024-10-07 13:00:03,367][train][INFO] - {"epoch": 676, "train_loss": "0.633", "train_ntokens": "260682", "train_nsentences": "1750.04", "train_wps": "48281.2", "train_ups": "0.19", "train_wpb": "260682", "train_bsz": "1750", "train_num_updates": "32432", "train_lr": "0.000499413", "train_gnorm": "0.383", "train_loss_scale": "2", "train_train_wall": "94", "train_gb_free": "39.2", "train_wall": "16615"}
[2024-10-07 13:00:03,714][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 13:00:03,737][fairseq.trainer][INFO] - begin training epoch 677
[2024-10-07 13:00:03,738][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 13:03:21,285][fairseq_cli.train][INFO] - end of epoch 677 (average epoch stats below)
[2024-10-07 13:03:21,288][train][INFO] - {"epoch": 677, "train_loss": "0.635", "train_ntokens": "260532", "train_nsentences": "1750.04", "train_wps": "63185.3", "train_ups": "0.24", "train_wpb": "260532", "train_bsz": "1750", "train_num_updates": "32480", "train_lr": "0.000499348", "train_gnorm": "0.424", "train_loss_scale": "2", "train_train_wall": "95", "train_gb_free": "39.3", "train_wall": "16813"}
[2024-10-07 13:03:21,476][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 13:03:21,482][fairseq.trainer][INFO] - begin training epoch 678
[2024-10-07 13:03:21,483][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 13:06:33,626][fairseq_cli.train][INFO] - end of epoch 678 (average epoch stats below)
[2024-10-07 13:06:33,640][train][INFO] - {"epoch": 678, "train_loss": "0.634", "train_ntokens": "260943", "train_nsentences": "1750.04", "train_wps": "65117.2", "train_ups": "0.25", "train_wpb": "260943", "train_bsz": "1750", "train_num_updates": "32528", "train_lr": "0.000499283", "train_gnorm": "0.417", "train_loss_scale": "2", "train_train_wall": "43", "train_gb_free": "39.2", "train_wall": "17006"}
[2024-10-07 13:06:33,905][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 13:06:33,940][fairseq.trainer][INFO] - begin training epoch 679
[2024-10-07 13:06:33,940][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 13:10:07,052][fairseq_cli.train][INFO] - end of epoch 679 (average epoch stats below)
[2024-10-07 13:10:07,068][train][INFO] - {"epoch": 679, "train_loss": "0.635", "train_ntokens": "260427", "train_nsentences": "1750.04", "train_wps": "58571", "train_ups": "0.22", "train_wpb": "260427", "train_bsz": "1750", "train_num_updates": "32576", "train_lr": "0.000499217", "train_gnorm": "0.486", "train_loss_scale": "2", "train_train_wall": "122", "train_gb_free": "39.3", "train_wall": "17219"}
[2024-10-07 13:10:07,321][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 13:10:07,343][fairseq.trainer][INFO] - begin training epoch 680
[2024-10-07 13:10:07,343][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 13:13:22,610][train_inner][INFO] - {"epoch": 680, "update": 679.5, "loss": "0.635", "ntokens": "260528", "nsentences": "1757.93", "wps": "63639.4", "ups": "0.24", "wpb": "260528", "bsz": "1757.9", "num_updates": "32600", "lr": "0.000499185", "gnorm": "0.432", "loss_scale": "2", "train_wall": "352", "gb_free": "40.5", "wall": "17415"}
[2024-10-07 13:13:37,411][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 680 @ 32624 updates
[2024-10-07 13:13:37,412][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-07 13:13:41,691][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-07 13:13:41,694][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 680 @ 32624 updates, score None) (writing took 4.2823876505717635 seconds)
[2024-10-07 13:13:41,694][fairseq_cli.train][INFO] - end of epoch 680 (average epoch stats below)
[2024-10-07 13:13:41,696][train][INFO] - {"epoch": 680, "train_loss": "0.636", "train_ntokens": "260739", "train_nsentences": "1750.04", "train_wps": "58313.5", "train_ups": "0.22", "train_wpb": "260739", "train_bsz": "1750", "train_num_updates": "32624", "train_lr": "0.000499152", "train_gnorm": "0.465", "train_loss_scale": "2", "train_train_wall": "88", "train_gb_free": "39.3", "train_wall": "17434"}
[2024-10-07 13:13:42,028][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 13:13:42,049][fairseq.trainer][INFO] - begin training epoch 681
[2024-10-07 13:13:42,050][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 13:15:58,038][fairseq_cli.train][INFO] - end of epoch 681 (average epoch stats below)
[2024-10-07 13:15:58,042][train][INFO] - {"epoch": 681, "train_loss": "0.639", "train_ntokens": "260594", "train_nsentences": "1750.04", "train_wps": "91743.5", "train_ups": "0.35", "train_wpb": "260594", "train_bsz": "1750", "train_num_updates": "32672", "train_lr": "0.000499087", "train_gnorm": "0.414", "train_loss_scale": "2", "train_train_wall": "56", "train_gb_free": "39.6", "train_wall": "17570"}
[2024-10-07 13:15:58,155][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 13:15:58,160][fairseq.trainer][INFO] - begin training epoch 682
[2024-10-07 13:15:58,160][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 13:18:05,217][fairseq_cli.train][INFO] - end of epoch 682 (average epoch stats below)
[2024-10-07 13:18:05,220][train][INFO] - {"epoch": 682, "train_loss": "0.634", "train_ntokens": "260826", "train_nsentences": "1750.04", "train_wps": "98444.9", "train_ups": "0.38", "train_wpb": "260826", "train_bsz": "1750", "train_num_updates": "32720", "train_lr": "0.000499022", "train_gnorm": "0.414", "train_loss_scale": "2", "train_train_wall": "50", "train_gb_free": "40", "train_wall": "17697"}
[2024-10-07 13:18:05,276][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 13:18:05,280][fairseq.trainer][INFO] - begin training epoch 683
[2024-10-07 13:18:05,280][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 13:20:13,069][fairseq_cli.train][INFO] - end of epoch 683 (average epoch stats below)
[2024-10-07 13:20:13,072][train][INFO] - {"epoch": 683, "train_loss": "0.629", "train_ntokens": "260171", "train_nsentences": "1750.04", "train_wps": "97678.9", "train_ups": "0.38", "train_wpb": "260171", "train_bsz": "1750", "train_num_updates": "32768", "train_lr": "0.000498957", "train_gnorm": "0.388", "train_loss_scale": "2", "train_train_wall": "48", "train_gb_free": "39.8", "train_wall": "17825"}
[2024-10-07 13:20:13,274][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 13:20:13,286][fairseq.trainer][INFO] - begin training epoch 684
[2024-10-07 13:20:13,286][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 13:23:33,440][train_inner][INFO] - {"epoch": 684, "update": 683.667, "loss": "0.634", "ntokens": "260684", "nsentences": "1752.5", "wps": "85355.3", "ups": "0.33", "wpb": "260684", "bsz": "1752.5", "num_updates": "32800", "lr": "0.000498913", "gnorm": "0.424", "loss_scale": "2", "train_wall": "204", "gb_free": "39.6", "wall": "18025"}
[2024-10-07 13:23:45,718][fairseq_cli.train][INFO] - end of epoch 684 (average epoch stats below)
[2024-10-07 13:23:45,722][train][INFO] - {"epoch": 684, "train_loss": "0.633", "train_ntokens": "260142", "train_nsentences": "1750.04", "train_wps": "58721.6", "train_ups": "0.23", "train_wpb": "260142", "train_bsz": "1750", "train_num_updates": "32816", "train_lr": "0.000498891", "train_gnorm": "0.451", "train_loss_scale": "2", "train_train_wall": "46", "train_gb_free": "39.6", "train_wall": "18038"}
[2024-10-07 13:23:45,911][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 13:23:45,923][fairseq.trainer][INFO] - begin training epoch 685
[2024-10-07 13:23:45,923][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 13:26:30,841][fairseq_cli.train][INFO] - end of epoch 685 (average epoch stats below)
[2024-10-07 13:26:30,852][train][INFO] - {"epoch": 685, "train_loss": "0.637", "train_ntokens": "260616", "train_nsentences": "1750.04", "train_wps": "75757.4", "train_ups": "0.29", "train_wpb": "260616", "train_bsz": "1750", "train_num_updates": "32864", "train_lr": "0.000498826", "train_gnorm": "0.397", "train_loss_scale": "2", "train_train_wall": "45", "train_gb_free": "39.7", "train_wall": "18203"}
[2024-10-07 13:26:31,053][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 13:26:31,057][fairseq.trainer][INFO] - begin training epoch 686
[2024-10-07 13:26:31,058][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 13:29:19,764][fairseq_cli.train][INFO] - end of epoch 686 (average epoch stats below)
[2024-10-07 13:29:19,769][train][INFO] - {"epoch": 686, "train_loss": "0.628", "train_ntokens": "260616", "train_nsentences": "1750.04", "train_wps": "74058.9", "train_ups": "0.28", "train_wpb": "260616", "train_bsz": "1750", "train_num_updates": "32912", "train_lr": "0.000498761", "train_gnorm": "0.41", "train_loss_scale": "2", "train_train_wall": "58", "train_gb_free": "40.1", "train_wall": "18372"}
[2024-10-07 13:29:19,939][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 13:29:19,945][fairseq.trainer][INFO] - begin training epoch 687
[2024-10-07 13:29:19,945][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 13:32:00,647][fairseq_cli.train][INFO] - end of epoch 687 (average epoch stats below)
[2024-10-07 13:32:00,655][train][INFO] - {"epoch": 687, "train_loss": "0.637", "train_ntokens": "260575", "train_nsentences": "1750.04", "train_wps": "77743.8", "train_ups": "0.3", "train_wpb": "260574", "train_bsz": "1750", "train_num_updates": "32960", "train_lr": "0.000498696", "train_gnorm": "0.425", "train_loss_scale": "2", "train_train_wall": "67", "train_gb_free": "39.6", "train_wall": "18533"}
[2024-10-07 13:32:00,875][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 13:32:00,884][fairseq.trainer][INFO] - begin training epoch 688
[2024-10-07 13:32:00,885][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 13:34:27,971][train_inner][INFO] - {"epoch": 688, "update": 687.833, "loss": "0.634", "ntokens": "260611", "nsentences": "1733.53", "wps": "79635.3", "ups": "0.31", "wpb": "260611", "bsz": "1733.5", "num_updates": "33000", "lr": "0.000498641", "gnorm": "0.41", "loss_scale": "4", "train_wall": "239", "gb_free": "39.8", "wall": "18680"}
[2024-10-07 13:34:31,407][fairseq_cli.train][INFO] - end of epoch 688 (average epoch stats below)
[2024-10-07 13:34:31,409][train][INFO] - {"epoch": 688, "train_loss": "0.637", "train_ntokens": "260910", "train_nsentences": "1750.04", "train_wps": "83075.4", "train_ups": "0.32", "train_wpb": "260910", "train_bsz": "1750", "train_num_updates": "33008", "train_lr": "0.00049863", "train_gnorm": "0.405", "train_loss_scale": "4", "train_train_wall": "62", "train_gb_free": "39.2", "train_wall": "18683"}
[2024-10-07 13:34:31,611][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 13:34:31,624][fairseq.trainer][INFO] - begin training epoch 689
[2024-10-07 13:34:31,624][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 13:36:18,131][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2024-10-07 13:36:49,553][fairseq_cli.train][INFO] - end of epoch 689 (average epoch stats below)
[2024-10-07 13:36:49,560][train][INFO] - {"epoch": 689, "train_loss": "0.632", "train_ntokens": "260893", "train_nsentences": "1750.57", "train_wps": "88760", "train_ups": "0.34", "train_wpb": "260893", "train_bsz": "1750.6", "train_num_updates": "33055", "train_lr": "0.000498567", "train_gnorm": "0.445", "train_loss_scale": "2", "train_train_wall": "47", "train_gb_free": "39.3", "train_wall": "18822"}
[2024-10-07 13:36:49,775][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 13:36:49,782][fairseq.trainer][INFO] - begin training epoch 690
[2024-10-07 13:36:49,782][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 13:39:15,754][fairseq_cli.train][INFO] - end of epoch 690 (average epoch stats below)
[2024-10-07 13:39:15,759][train][INFO] - {"epoch": 690, "train_loss": "0.625", "train_ntokens": "260687", "train_nsentences": "1750.04", "train_wps": "85591", "train_ups": "0.33", "train_wpb": "260687", "train_bsz": "1750", "train_num_updates": "33103", "train_lr": "0.000498501", "train_gnorm": "0.478", "train_loss_scale": "2", "train_train_wall": "47", "train_gb_free": "39.5", "train_wall": "18968"}
[2024-10-07 13:39:15,925][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 13:39:15,954][fairseq.trainer][INFO] - begin training epoch 691
[2024-10-07 13:39:15,955][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 13:41:53,648][fairseq_cli.train][INFO] - end of epoch 691 (average epoch stats below)
[2024-10-07 13:41:53,662][train][INFO] - {"epoch": 691, "train_loss": "0.621", "train_ntokens": "260458", "train_nsentences": "1750.04", "train_wps": "79177.2", "train_ups": "0.3", "train_wpb": "260458", "train_bsz": "1750", "train_num_updates": "33151", "train_lr": "0.000498436", "train_gnorm": "0.393", "train_loss_scale": "2", "train_train_wall": "47", "train_gb_free": "39.8", "train_wall": "19126"}
[2024-10-07 13:41:53,823][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 13:41:53,827][fairseq.trainer][INFO] - begin training epoch 692
[2024-10-07 13:41:53,828][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 13:44:16,182][fairseq_cli.train][INFO] - end of epoch 692 (average epoch stats below)
[2024-10-07 13:44:16,201][train][INFO] - {"epoch": 692, "train_loss": "0.627", "train_ntokens": "260816", "train_nsentences": "1750.04", "train_wps": "87832.3", "train_ups": "0.34", "train_wpb": "260816", "train_bsz": "1750", "train_num_updates": "33199", "train_lr": "0.000498371", "train_gnorm": "0.425", "train_loss_scale": "2", "train_train_wall": "67", "train_gb_free": "39.7", "train_wall": "19268"}
[2024-10-07 13:44:16,425][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 13:44:16,431][fairseq.trainer][INFO] - begin training epoch 693
[2024-10-07 13:44:16,432][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 13:46:15,559][train_inner][INFO] - {"epoch": 693, "update": 692.021, "loss": "0.628", "ntokens": "260626", "nsentences": "1764.16", "wps": "73667.7", "ups": "0.28", "wpb": "260626", "bsz": "1764.2", "num_updates": "33200", "lr": "0.00049837", "gnorm": "0.433", "loss_scale": "2", "train_wall": "250", "gb_free": "39.3", "wall": "19388"}
[2024-10-07 13:46:44,508][fairseq_cli.train][INFO] - end of epoch 693 (average epoch stats below)
[2024-10-07 13:46:44,511][train][INFO] - {"epoch": 693, "train_loss": "0.628", "train_ntokens": "260682", "train_nsentences": "1750.04", "train_wps": "84371", "train_ups": "0.32", "train_wpb": "260682", "train_bsz": "1750", "train_num_updates": "33247", "train_lr": "0.000498306", "train_gnorm": "0.394", "train_loss_scale": "2", "train_train_wall": "67", "train_gb_free": "39.8", "train_wall": "19417"}
[2024-10-07 13:46:44,717][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 13:46:44,739][fairseq.trainer][INFO] - begin training epoch 694
[2024-10-07 13:46:44,740][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 13:49:17,688][fairseq_cli.train][INFO] - end of epoch 694 (average epoch stats below)
[2024-10-07 13:49:17,699][train][INFO] - {"epoch": 694, "train_loss": "0.62", "train_ntokens": "260308", "train_nsentences": "1750.04", "train_wps": "81566.7", "train_ups": "0.31", "train_wpb": "260308", "train_bsz": "1750", "train_num_updates": "33295", "train_lr": "0.00049824", "train_gnorm": "0.405", "train_loss_scale": "2", "train_train_wall": "59", "train_gb_free": "39.6", "train_wall": "19570"}
[2024-10-07 13:49:17,914][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 13:49:17,920][fairseq.trainer][INFO] - begin training epoch 695
[2024-10-07 13:49:17,920][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 13:53:27,405][fairseq_cli.train][INFO] - end of epoch 695 (average epoch stats below)
[2024-10-07 13:53:27,417][train][INFO] - {"epoch": 695, "train_loss": "0.626", "train_ntokens": "260449", "train_nsentences": "1750.04", "train_wps": "50063.4", "train_ups": "0.19", "train_wpb": "260449", "train_bsz": "1750", "train_num_updates": "33343", "train_lr": "0.000498175", "train_gnorm": "0.409", "train_loss_scale": "2", "train_train_wall": "139", "train_gb_free": "39.3", "train_wall": "19819"}
[2024-10-07 13:53:27,774][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 13:53:27,853][fairseq.trainer][INFO] - begin training epoch 696
[2024-10-07 13:53:27,853][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 13:58:28,054][fairseq_cli.train][INFO] - end of epoch 696 (average epoch stats below)
[2024-10-07 13:58:28,062][train][INFO] - {"epoch": 696, "train_loss": "0.622", "train_ntokens": "260751", "train_nsentences": "1750.04", "train_wps": "41631.1", "train_ups": "0.16", "train_wpb": "260751", "train_bsz": "1750", "train_num_updates": "33391", "train_lr": "0.00049811", "train_gnorm": "0.45", "train_loss_scale": "2", "train_train_wall": "91", "train_gb_free": "40", "train_wall": "20120"}
[2024-10-07 13:58:28,395][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 13:58:28,408][fairseq.trainer][INFO] - begin training epoch 697
[2024-10-07 13:58:28,408][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 14:02:19,367][train_inner][INFO] - {"epoch": 697, "update": 696.188, "loss": "0.623", "ntokens": "260713", "nsentences": "1739.24", "wps": "54103.8", "ups": "0.21", "wpb": "260713", "bsz": "1739.2", "num_updates": "33400", "lr": "0.000498098", "gnorm": "0.413", "loss_scale": "2", "train_wall": "383", "gb_free": "39.8", "wall": "20351"}
[2024-10-07 14:02:46,721][fairseq_cli.train][INFO] - end of epoch 697 (average epoch stats below)
[2024-10-07 14:02:46,724][train][INFO] - {"epoch": 697, "train_loss": "0.622", "train_ntokens": "260717", "train_nsentences": "1750.04", "train_wps": "48382.1", "train_ups": "0.19", "train_wpb": "260717", "train_bsz": "1750", "train_num_updates": "33439", "train_lr": "0.000498045", "train_gnorm": "0.412", "train_loss_scale": "2", "train_train_wall": "92", "train_gb_free": "39.3", "train_wall": "20379"}
[2024-10-07 14:02:46,955][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 14:02:46,960][fairseq.trainer][INFO] - begin training epoch 698
[2024-10-07 14:02:46,960][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 14:05:13,469][fairseq_cli.train][INFO] - end of epoch 698 (average epoch stats below)
[2024-10-07 14:05:13,481][train][INFO] - {"epoch": 698, "train_loss": "0.624", "train_ntokens": "260856", "train_nsentences": "1750.04", "train_wps": "85321.2", "train_ups": "0.33", "train_wpb": "260856", "train_bsz": "1750", "train_num_updates": "33487", "train_lr": "0.00049798", "train_gnorm": "0.421", "train_loss_scale": "2", "train_train_wall": "63", "train_gb_free": "40", "train_wall": "20526"}
[2024-10-07 14:05:13,779][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 14:05:13,790][fairseq.trainer][INFO] - begin training epoch 699
[2024-10-07 14:05:13,790][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 14:08:49,305][fairseq_cli.train][INFO] - end of epoch 699 (average epoch stats below)
[2024-10-07 14:08:49,312][train][INFO] - {"epoch": 699, "train_loss": "0.62", "train_ntokens": "260700", "train_nsentences": "1750.04", "train_wps": "57979.5", "train_ups": "0.22", "train_wpb": "260700", "train_bsz": "1750", "train_num_updates": "33535", "train_lr": "0.000497914", "train_gnorm": "0.429", "train_loss_scale": "2", "train_train_wall": "64", "train_gb_free": "39.8", "train_wall": "20741"}
[2024-10-07 14:08:49,560][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 14:08:49,590][fairseq.trainer][INFO] - begin training epoch 700
[2024-10-07 14:08:49,590][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 14:11:50,706][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 700 @ 33583 updates
[2024-10-07 14:11:50,711][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-07 14:11:55,535][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-07 14:11:55,538][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 700 @ 33583 updates, score None) (writing took 4.831231282092631 seconds)
[2024-10-07 14:11:55,538][fairseq_cli.train][INFO] - end of epoch 700 (average epoch stats below)
[2024-10-07 14:11:55,540][train][INFO] - {"epoch": 700, "train_loss": "0.62", "train_ntokens": "260701", "train_nsentences": "1750.04", "train_wps": "67197.4", "train_ups": "0.26", "train_wpb": "260701", "train_bsz": "1750", "train_num_updates": "33583", "train_lr": "0.000497849", "train_gnorm": "0.394", "train_loss_scale": "2", "train_train_wall": "66", "train_gb_free": "40", "train_wall": "20928"}
[2024-10-07 14:11:55,692][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 14:11:55,705][fairseq.trainer][INFO] - begin training epoch 701
[2024-10-07 14:11:55,705][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 14:15:06,177][train_inner][INFO] - {"epoch": 701, "update": 700.354, "loss": "0.621", "ntokens": "260452", "nsentences": "1755.35", "wps": "67935.8", "ups": "0.26", "wpb": "260452", "bsz": "1755.4", "num_updates": "33600", "lr": "0.000497826", "gnorm": "0.415", "loss_scale": "2", "train_wall": "269", "gb_free": "39.3", "wall": "21118"}
[2024-10-07 14:15:24,124][fairseq_cli.train][INFO] - end of epoch 701 (average epoch stats below)
[2024-10-07 14:15:24,128][train][INFO] - {"epoch": 701, "train_loss": "0.619", "train_ntokens": "260833", "train_nsentences": "1750.04", "train_wps": "60023.7", "train_ups": "0.23", "train_wpb": "260833", "train_bsz": "1750", "train_num_updates": "33631", "train_lr": "0.000497784", "train_gnorm": "0.393", "train_loss_scale": "2", "train_train_wall": "67", "train_gb_free": "39.7", "train_wall": "21136"}
[2024-10-07 14:15:24,385][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 14:15:24,392][fairseq.trainer][INFO] - begin training epoch 702
[2024-10-07 14:15:24,396][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 14:17:50,089][fairseq_cli.train][INFO] - end of epoch 702 (average epoch stats below)
[2024-10-07 14:17:50,096][train][INFO] - {"epoch": 702, "train_loss": "0.614", "train_ntokens": "260971", "train_nsentences": "1750.04", "train_wps": "85821.1", "train_ups": "0.33", "train_wpb": "260971", "train_bsz": "1750", "train_num_updates": "33679", "train_lr": "0.000497719", "train_gnorm": "0.447", "train_loss_scale": "2", "train_train_wall": "44", "train_gb_free": "39.8", "train_wall": "21282"}
[2024-10-07 14:17:50,236][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 14:17:50,246][fairseq.trainer][INFO] - begin training epoch 703
[2024-10-07 14:17:50,247][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 14:19:55,323][fairseq_cli.train][INFO] - end of epoch 703 (average epoch stats below)
[2024-10-07 14:19:55,333][train][INFO] - {"epoch": 703, "train_loss": "0.623", "train_ntokens": "260829", "train_nsentences": "1750.04", "train_wps": "99971.9", "train_ups": "0.38", "train_wpb": "260829", "train_bsz": "1750", "train_num_updates": "33727", "train_lr": "0.000497654", "train_gnorm": "0.402", "train_loss_scale": "2", "train_train_wall": "48", "train_gb_free": "39.3", "train_wall": "21407"}
[2024-10-07 14:19:55,432][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 14:19:55,448][fairseq.trainer][INFO] - begin training epoch 704
[2024-10-07 14:19:55,448][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 14:22:02,147][fairseq_cli.train][INFO] - end of epoch 704 (average epoch stats below)
[2024-10-07 14:22:02,150][train][INFO] - {"epoch": 704, "train_loss": "0.622", "train_ntokens": "260858", "train_nsentences": "1750.04", "train_wps": "98736.5", "train_ups": "0.38", "train_wpb": "260858", "train_bsz": "1750", "train_num_updates": "33775", "train_lr": "0.000497588", "train_gnorm": "0.422", "train_loss_scale": "2", "train_train_wall": "51", "train_gb_free": "40.1", "train_wall": "21534"}
[2024-10-07 14:22:02,205][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 14:22:02,208][fairseq.trainer][INFO] - begin training epoch 705
[2024-10-07 14:22:02,209][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 14:24:58,268][train_inner][INFO] - {"epoch": 705, "update": 704.521, "loss": "0.619", "ntokens": "261156", "nsentences": "1741.91", "wps": "88215.9", "ups": "0.34", "wpb": "261156", "bsz": "1741.9", "num_updates": "33800", "lr": "0.000497554", "gnorm": "0.419", "loss_scale": "2", "train_wall": "236", "gb_free": "39.8", "wall": "21710"}
[2024-10-07 14:25:14,380][fairseq_cli.train][INFO] - end of epoch 705 (average epoch stats below)
[2024-10-07 14:25:14,382][train][INFO] - {"epoch": 705, "train_loss": "0.618", "train_ntokens": "261118", "train_nsentences": "1750.04", "train_wps": "65201.9", "train_ups": "0.25", "train_wpb": "261118", "train_bsz": "1750", "train_num_updates": "33823", "train_lr": "0.000497523", "train_gnorm": "0.431", "train_loss_scale": "2", "train_train_wall": "92", "train_gb_free": "39.3", "train_wall": "21726"}
[2024-10-07 14:25:14,550][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 14:25:14,564][fairseq.trainer][INFO] - begin training epoch 706
[2024-10-07 14:25:14,564][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 14:28:22,041][fairseq_cli.train][INFO] - end of epoch 706 (average epoch stats below)
[2024-10-07 14:28:22,055][train][INFO] - {"epoch": 706, "train_loss": "0.616", "train_ntokens": "260829", "train_nsentences": "1750.04", "train_wps": "66712", "train_ups": "0.26", "train_wpb": "260829", "train_bsz": "1750", "train_num_updates": "33871", "train_lr": "0.000497458", "train_gnorm": "0.467", "train_loss_scale": "2", "train_train_wall": "55", "train_gb_free": "40", "train_wall": "21914"}
[2024-10-07 14:28:22,286][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 14:28:22,303][fairseq.trainer][INFO] - begin training epoch 707
[2024-10-07 14:28:22,303][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 14:31:38,195][fairseq_cli.train][INFO] - end of epoch 707 (average epoch stats below)
[2024-10-07 14:31:38,219][train][INFO] - {"epoch": 707, "train_loss": "0.626", "train_ntokens": "260959", "train_nsentences": "1750.04", "train_wps": "63857.4", "train_ups": "0.24", "train_wpb": "260959", "train_bsz": "1750", "train_num_updates": "33919", "train_lr": "0.000497393", "train_gnorm": "0.418", "train_loss_scale": "2", "train_train_wall": "43", "train_gb_free": "39.7", "train_wall": "22110"}
[2024-10-07 14:31:38,398][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 14:31:38,433][fairseq.trainer][INFO] - begin training epoch 708
[2024-10-07 14:31:38,433][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 14:34:56,109][fairseq_cli.train][INFO] - end of epoch 708 (average epoch stats below)
[2024-10-07 14:34:56,125][train][INFO] - {"epoch": 708, "train_loss": "0.622", "train_ntokens": "260717", "train_nsentences": "1750.04", "train_wps": "63235.1", "train_ups": "0.24", "train_wpb": "260716", "train_bsz": "1750", "train_num_updates": "33967", "train_lr": "0.000497327", "train_gnorm": "0.387", "train_loss_scale": "2", "train_train_wall": "103", "train_gb_free": "39.7", "train_wall": "22308"}
[2024-10-07 14:34:56,333][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 14:34:56,339][fairseq.trainer][INFO] - begin training epoch 709
[2024-10-07 14:34:56,340][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 14:37:58,748][train_inner][INFO] - {"epoch": 709, "update": 708.688, "loss": "0.62", "ntokens": "260886", "nsentences": "1754.29", "wps": "66853.3", "ups": "0.26", "wpb": "260886", "bsz": "1754.3", "num_updates": "34000", "lr": "0.000497283", "gnorm": "0.417", "loss_scale": "2", "train_wall": "311", "gb_free": "39.6", "wall": "22491"}
[2024-10-07 14:38:11,905][fairseq_cli.train][INFO] - end of epoch 709 (average epoch stats below)
[2024-10-07 14:38:11,907][train][INFO] - {"epoch": 709, "train_loss": "0.616", "train_ntokens": "260865", "train_nsentences": "1750.04", "train_wps": "63957.7", "train_ups": "0.25", "train_wpb": "260865", "train_bsz": "1750", "train_num_updates": "34015", "train_lr": "0.000497262", "train_gnorm": "0.37", "train_loss_scale": "2", "train_train_wall": "107", "train_gb_free": "39.3", "train_wall": "22504"}
[2024-10-07 14:38:12,071][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 14:38:12,083][fairseq.trainer][INFO] - begin training epoch 710
[2024-10-07 14:38:12,084][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 14:40:48,164][fairseq_cli.train][INFO] - end of epoch 710 (average epoch stats below)
[2024-10-07 14:40:48,170][train][INFO] - {"epoch": 710, "train_loss": "0.618", "train_ntokens": "260792", "train_nsentences": "1750.04", "train_wps": "80110", "train_ups": "0.31", "train_wpb": "260792", "train_bsz": "1750", "train_num_updates": "34063", "train_lr": "0.000497197", "train_gnorm": "0.424", "train_loss_scale": "2", "train_train_wall": "53", "train_gb_free": "40.6", "train_wall": "22660"}
[2024-10-07 14:40:48,425][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 14:40:48,445][fairseq.trainer][INFO] - begin training epoch 711
[2024-10-07 14:40:48,446][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 14:43:35,977][fairseq_cli.train][INFO] - end of epoch 711 (average epoch stats below)
[2024-10-07 14:43:35,982][train][INFO] - {"epoch": 711, "train_loss": "0.621", "train_ntokens": "260065", "train_nsentences": "1750.04", "train_wps": "74389.2", "train_ups": "0.29", "train_wpb": "260066", "train_bsz": "1750", "train_num_updates": "34111", "train_lr": "0.000497132", "train_gnorm": "0.447", "train_loss_scale": "2", "train_train_wall": "81", "train_gb_free": "39.7", "train_wall": "22828"}
[2024-10-07 14:43:36,118][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 14:43:36,129][fairseq.trainer][INFO] - begin training epoch 712
[2024-10-07 14:43:36,129][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 14:46:04,394][fairseq_cli.train][INFO] - end of epoch 712 (average epoch stats below)
[2024-10-07 14:46:04,401][train][INFO] - {"epoch": 712, "train_loss": "0.616", "train_ntokens": "260592", "train_nsentences": "1750.04", "train_wps": "84280.4", "train_ups": "0.32", "train_wpb": "260592", "train_bsz": "1750", "train_num_updates": "34159", "train_lr": "0.000497067", "train_gnorm": "0.403", "train_loss_scale": "2", "train_train_wall": "52", "train_gb_free": "39.8", "train_wall": "22976"}
[2024-10-07 14:46:04,562][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 14:46:04,571][fairseq.trainer][INFO] - begin training epoch 713
[2024-10-07 14:46:04,572][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 14:48:50,921][train_inner][INFO] - {"epoch": 713, "update": 712.854, "loss": "0.618", "ntokens": "260516", "nsentences": "1756.8", "wps": "79893.2", "ups": "0.31", "wpb": "260516", "bsz": "1756.8", "num_updates": "34200", "lr": "0.000497011", "gnorm": "0.413", "loss_scale": "2", "train_wall": "245", "gb_free": "39.6", "wall": "23143"}
[2024-10-07 14:48:53,200][fairseq_cli.train][INFO] - end of epoch 713 (average epoch stats below)
[2024-10-07 14:48:53,202][train][INFO] - {"epoch": 713, "train_loss": "0.613", "train_ntokens": "260442", "train_nsentences": "1750.04", "train_wps": "74060.2", "train_ups": "0.28", "train_wpb": "260442", "train_bsz": "1750", "train_num_updates": "34207", "train_lr": "0.000497001", "train_gnorm": "0.397", "train_loss_scale": "2", "train_train_wall": "49", "train_gb_free": "39.6", "train_wall": "23145"}
[2024-10-07 14:48:53,489][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 14:48:53,532][fairseq.trainer][INFO] - begin training epoch 714
[2024-10-07 14:48:53,532][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 14:51:17,996][fairseq_cli.train][INFO] - end of epoch 714 (average epoch stats below)
[2024-10-07 14:51:18,005][train][INFO] - {"epoch": 714, "train_loss": "0.617", "train_ntokens": "260821", "train_nsentences": "1750.04", "train_wps": "86460", "train_ups": "0.33", "train_wpb": "260821", "train_bsz": "1750", "train_num_updates": "34255", "train_lr": "0.000496936", "train_gnorm": "0.426", "train_loss_scale": "2", "train_train_wall": "49", "train_gb_free": "39.6", "train_wall": "23290"}
[2024-10-07 14:51:18,274][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 14:51:18,310][fairseq.trainer][INFO] - begin training epoch 715
[2024-10-07 14:51:18,311][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 14:54:07,314][fairseq_cli.train][INFO] - end of epoch 715 (average epoch stats below)
[2024-10-07 14:54:07,329][train][INFO] - {"epoch": 715, "train_loss": "0.618", "train_ntokens": "260799", "train_nsentences": "1750.04", "train_wps": "73932.7", "train_ups": "0.28", "train_wpb": "260799", "train_bsz": "1750", "train_num_updates": "34303", "train_lr": "0.000496871", "train_gnorm": "0.397", "train_loss_scale": "2", "train_train_wall": "70", "train_gb_free": "40.8", "train_wall": "23459"}
[2024-10-07 14:54:07,562][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 14:54:07,567][fairseq.trainer][INFO] - begin training epoch 716
[2024-10-07 14:54:07,567][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 14:57:41,969][fairseq_cli.train][INFO] - end of epoch 716 (average epoch stats below)
[2024-10-07 14:57:41,980][train][INFO] - {"epoch": 716, "train_loss": "0.615", "train_ntokens": "260496", "train_nsentences": "1750.04", "train_wps": "58252.8", "train_ups": "0.22", "train_wpb": "260496", "train_bsz": "1750", "train_num_updates": "34351", "train_lr": "0.000496806", "train_gnorm": "0.421", "train_loss_scale": "2", "train_train_wall": "101", "train_gb_free": "40.1", "train_wall": "23674"}
[2024-10-07 14:57:42,270][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 14:57:42,285][fairseq.trainer][INFO] - begin training epoch 717
[2024-10-07 14:57:42,288][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 15:01:20,055][fairseq_cli.train][INFO] - end of epoch 717 (average epoch stats below)
[2024-10-07 15:01:20,064][train][INFO] - {"epoch": 717, "train_loss": "0.617", "train_ntokens": "260815", "train_nsentences": "1750.04", "train_wps": "57406", "train_ups": "0.22", "train_wpb": "260815", "train_bsz": "1750", "train_num_updates": "34399", "train_lr": "0.00049674", "train_gnorm": "0.395", "train_loss_scale": "2", "train_train_wall": "69", "train_gb_free": "39.7", "train_wall": "23892"}
[2024-10-07 15:01:20,273][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 15:01:20,298][fairseq.trainer][INFO] - begin training epoch 718
[2024-10-07 15:01:20,299][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 15:03:39,305][train_inner][INFO] - {"epoch": 718, "update": 717.021, "loss": "0.617", "ntokens": "260665", "nsentences": "1744.33", "wps": "58683.8", "ups": "0.23", "wpb": "260665", "bsz": "1744.3", "num_updates": "34400", "lr": "0.000496739", "gnorm": "0.409", "loss_scale": "2", "train_wall": "327", "gb_free": "40.3", "wall": "24031"}
[2024-10-07 15:04:32,070][fairseq_cli.train][INFO] - end of epoch 718 (average epoch stats below)
[2024-10-07 15:04:32,074][train][INFO] - {"epoch": 718, "train_loss": "0.617", "train_ntokens": "260615", "train_nsentences": "1750.04", "train_wps": "65151.8", "train_ups": "0.25", "train_wpb": "260615", "train_bsz": "1750", "train_num_updates": "34447", "train_lr": "0.000496675", "train_gnorm": "0.387", "train_loss_scale": "2", "train_train_wall": "89", "train_gb_free": "39.7", "train_wall": "24084"}
[2024-10-07 15:04:32,309][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 15:04:32,337][fairseq.trainer][INFO] - begin training epoch 719
[2024-10-07 15:04:32,337][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 15:07:56,710][fairseq_cli.train][INFO] - end of epoch 719 (average epoch stats below)
[2024-10-07 15:07:56,717][train][INFO] - {"epoch": 719, "train_loss": "0.62", "train_ntokens": "260738", "train_nsentences": "1750.04", "train_wps": "61158.6", "train_ups": "0.23", "train_wpb": "260738", "train_bsz": "1750", "train_num_updates": "34495", "train_lr": "0.00049661", "train_gnorm": "0.409", "train_loss_scale": "2", "train_train_wall": "87", "train_gb_free": "39.3", "train_wall": "24289"}
[2024-10-07 15:07:56,975][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 15:07:56,998][fairseq.trainer][INFO] - begin training epoch 720
[2024-10-07 15:07:56,998][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 15:11:47,799][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 720 @ 34543 updates
[2024-10-07 15:11:47,802][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-07 15:11:52,051][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-07 15:11:52,053][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 720 @ 34543 updates, score None) (writing took 4.254166514612734 seconds)
[2024-10-07 15:11:52,054][fairseq_cli.train][INFO] - end of epoch 720 (average epoch stats below)
[2024-10-07 15:11:52,057][train][INFO] - {"epoch": 720, "train_loss": "0.623", "train_ntokens": "260549", "train_nsentences": "1750.04", "train_wps": "53142.5", "train_ups": "0.2", "train_wpb": "260549", "train_bsz": "1750", "train_num_updates": "34543", "train_lr": "0.000496545", "train_gnorm": "0.429", "train_loss_scale": "2", "train_train_wall": "63", "train_gb_free": "40.2", "train_wall": "24524"}
[2024-10-07 15:11:52,218][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 15:11:52,235][fairseq.trainer][INFO] - begin training epoch 721
[2024-10-07 15:11:52,236][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 15:14:26,677][fairseq_cli.train][INFO] - end of epoch 721 (average epoch stats below)
[2024-10-07 15:14:26,685][train][INFO] - {"epoch": 721, "train_loss": "0.616", "train_ntokens": "260635", "train_nsentences": "1750.04", "train_wps": "80909.3", "train_ups": "0.31", "train_wpb": "260636", "train_bsz": "1750", "train_num_updates": "34591", "train_lr": "0.00049648", "train_gnorm": "0.406", "train_loss_scale": "2", "train_train_wall": "51", "train_gb_free": "39.8", "train_wall": "24679"}
[2024-10-07 15:14:26,973][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 15:14:27,010][fairseq.trainer][INFO] - begin training epoch 722
[2024-10-07 15:14:27,011][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 15:16:23,152][train_inner][INFO] - {"epoch": 722, "update": 721.188, "loss": "0.618", "ntokens": "260709", "nsentences": "1747.61", "wps": "68266.5", "ups": "0.26", "wpb": "260709", "bsz": "1747.6", "num_updates": "34600", "lr": "0.000496467", "gnorm": "0.409", "loss_scale": "2", "train_wall": "260", "gb_free": "39.9", "wall": "24795"}
[2024-10-07 15:16:58,146][fairseq_cli.train][INFO] - end of epoch 722 (average epoch stats below)
[2024-10-07 15:16:58,151][train][INFO] - {"epoch": 722, "train_loss": "0.611", "train_ntokens": "260472", "train_nsentences": "1750.04", "train_wps": "82547.8", "train_ups": "0.32", "train_wpb": "260472", "train_bsz": "1750", "train_num_updates": "34639", "train_lr": "0.000496414", "train_gnorm": "0.404", "train_loss_scale": "2", "train_train_wall": "39", "train_gb_free": "39.6", "train_wall": "24830"}
[2024-10-07 15:16:58,239][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 15:16:58,243][fairseq.trainer][INFO] - begin training epoch 723
[2024-10-07 15:16:58,243][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 15:19:04,741][fairseq_cli.train][INFO] - end of epoch 723 (average epoch stats below)
[2024-10-07 15:19:04,759][train][INFO] - {"epoch": 723, "train_loss": "0.62", "train_ntokens": "260948", "train_nsentences": "1750.04", "train_wps": "98945", "train_ups": "0.38", "train_wpb": "260948", "train_bsz": "1750", "train_num_updates": "34687", "train_lr": "0.000496349", "train_gnorm": "0.395", "train_loss_scale": "2", "train_train_wall": "41", "train_gb_free": "40.3", "train_wall": "24957"}
[2024-10-07 15:19:04,840][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 15:19:04,843][fairseq.trainer][INFO] - begin training epoch 724
[2024-10-07 15:19:04,844][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 15:21:10,656][fairseq_cli.train][INFO] - end of epoch 724 (average epoch stats below)
[2024-10-07 15:21:10,659][train][INFO] - {"epoch": 724, "train_loss": "0.611", "train_ntokens": "260637", "train_nsentences": "1750.04", "train_wps": "99372.2", "train_ups": "0.38", "train_wpb": "260637", "train_bsz": "1750", "train_num_updates": "34735", "train_lr": "0.000496284", "train_gnorm": "0.413", "train_loss_scale": "2", "train_train_wall": "52", "train_gb_free": "39.8", "train_wall": "25083"}
[2024-10-07 15:21:10,746][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 15:21:10,755][fairseq.trainer][INFO] - begin training epoch 725
[2024-10-07 15:21:10,755][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 15:25:17,401][fairseq_cli.train][INFO] - end of epoch 725 (average epoch stats below)
[2024-10-07 15:25:17,419][train][INFO] - {"epoch": 725, "train_loss": "0.618", "train_ntokens": "261023", "train_nsentences": "1750.04", "train_wps": "50777.3", "train_ups": "0.19", "train_wpb": "261023", "train_bsz": "1750", "train_num_updates": "34783", "train_lr": "0.000496219", "train_gnorm": "0.382", "train_loss_scale": "2", "train_train_wall": "97", "train_gb_free": "39.3", "train_wall": "25329"}
[2024-10-07 15:25:17,628][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 15:25:17,631][fairseq.trainer][INFO] - begin training epoch 726
[2024-10-07 15:25:17,631][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 15:27:50,002][train_inner][INFO] - {"epoch": 726, "update": 725.354, "loss": "0.615", "ntokens": "260570", "nsentences": "1761.8", "wps": "75874.4", "ups": "0.29", "wpb": "260570", "bsz": "1761.8", "num_updates": "34800", "lr": "0.000496196", "gnorm": "0.398", "loss_scale": "2", "train_wall": "286", "gb_free": "39.3", "wall": "25482"}
[2024-10-07 15:28:06,640][fairseq_cli.train][INFO] - end of epoch 726 (average epoch stats below)
[2024-10-07 15:28:06,642][train][INFO] - {"epoch": 726, "train_loss": "0.615", "train_ntokens": "260273", "train_nsentences": "1750.04", "train_wps": "73827.6", "train_ups": "0.28", "train_wpb": "260272", "train_bsz": "1750", "train_num_updates": "34831", "train_lr": "0.000496154", "train_gnorm": "0.418", "train_loss_scale": "2", "train_train_wall": "79", "train_gb_free": "39.9", "train_wall": "25499"}
[2024-10-07 15:28:06,816][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 15:28:06,824][fairseq.trainer][INFO] - begin training epoch 727
[2024-10-07 15:28:06,825][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 15:31:28,785][fairseq_cli.train][INFO] - end of epoch 727 (average epoch stats below)
[2024-10-07 15:31:28,788][train][INFO] - {"epoch": 727, "train_loss": "0.609", "train_ntokens": "260669", "train_nsentences": "1750.04", "train_wps": "61897.4", "train_ups": "0.24", "train_wpb": "260669", "train_bsz": "1750", "train_num_updates": "34879", "train_lr": "0.000496088", "train_gnorm": "0.421", "train_loss_scale": "2", "train_train_wall": "43", "train_gb_free": "40.3", "train_wall": "25701"}
[2024-10-07 15:31:28,859][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 15:31:28,888][fairseq.trainer][INFO] - begin training epoch 728
[2024-10-07 15:31:28,888][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 15:35:09,004][fairseq_cli.train][INFO] - end of epoch 728 (average epoch stats below)
[2024-10-07 15:35:09,007][train][INFO] - {"epoch": 728, "train_loss": "0.608", "train_ntokens": "260712", "train_nsentences": "1750.04", "train_wps": "56826.7", "train_ups": "0.22", "train_wpb": "260712", "train_bsz": "1750", "train_num_updates": "34927", "train_lr": "0.000496023", "train_gnorm": "0.421", "train_loss_scale": "2", "train_train_wall": "111", "train_gb_free": "40.7", "train_wall": "25921"}
[2024-10-07 15:35:09,212][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 15:35:09,237][fairseq.trainer][INFO] - begin training epoch 729
[2024-10-07 15:35:09,237][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 15:38:46,670][fairseq_cli.train][INFO] - end of epoch 729 (average epoch stats below)
[2024-10-07 15:38:46,690][train][INFO] - {"epoch": 729, "train_loss": "0.614", "train_ntokens": "260871", "train_nsentences": "1750.04", "train_wps": "57524.5", "train_ups": "0.22", "train_wpb": "260871", "train_bsz": "1750", "train_num_updates": "34975", "train_lr": "0.000495958", "train_gnorm": "0.414", "train_loss_scale": "2", "train_train_wall": "111", "train_gb_free": "40.5", "train_wall": "26139"}
[2024-10-07 15:38:46,894][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 15:38:46,918][fairseq.trainer][INFO] - begin training epoch 730
[2024-10-07 15:38:46,919][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 15:42:20,093][train_inner][INFO] - {"epoch": 730, "update": 729.521, "loss": "0.611", "ntokens": "260740", "nsentences": "1738.54", "wps": "59934.5", "ups": "0.23", "wpb": "260740", "bsz": "1738.5", "num_updates": "35000", "lr": "0.000495924", "gnorm": "0.419", "loss_scale": "2", "train_wall": "332", "gb_free": "39.3", "wall": "26352"}
[2024-10-07 15:42:33,849][fairseq_cli.train][INFO] - end of epoch 730 (average epoch stats below)
[2024-10-07 15:42:33,852][train][INFO] - {"epoch": 730, "train_loss": "0.61", "train_ntokens": "260577", "train_nsentences": "1750.04", "train_wps": "55061.6", "train_ups": "0.21", "train_wpb": "260577", "train_bsz": "1750", "train_num_updates": "35023", "train_lr": "0.000495893", "train_gnorm": "0.431", "train_loss_scale": "2", "train_train_wall": "63", "train_gb_free": "39.8", "train_wall": "26366"}
[2024-10-07 15:42:34,174][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 15:42:34,180][fairseq.trainer][INFO] - begin training epoch 731
[2024-10-07 15:42:34,181][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 15:46:33,180][fairseq_cli.train][INFO] - end of epoch 731 (average epoch stats below)
[2024-10-07 15:46:33,196][train][INFO] - {"epoch": 731, "train_loss": "0.613", "train_ntokens": "260468", "train_nsentences": "1750.04", "train_wps": "52236.8", "train_ups": "0.2", "train_wpb": "260468", "train_bsz": "1750", "train_num_updates": "35071", "train_lr": "0.000495827", "train_gnorm": "0.464", "train_loss_scale": "4", "train_train_wall": "96", "train_gb_free": "39.6", "train_wall": "26605"}
[2024-10-07 15:46:33,424][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 15:46:33,466][fairseq.trainer][INFO] - begin training epoch 732
[2024-10-07 15:46:33,466][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 15:50:41,343][fairseq_cli.train][INFO] - end of epoch 732 (average epoch stats below)
[2024-10-07 15:50:41,355][train][INFO] - {"epoch": 732, "train_loss": "0.607", "train_ntokens": "260565", "train_nsentences": "1750.04", "train_wps": "50400.5", "train_ups": "0.19", "train_wpb": "260565", "train_bsz": "1750", "train_num_updates": "35119", "train_lr": "0.000495762", "train_gnorm": "0.372", "train_loss_scale": "4", "train_train_wall": "125", "train_gb_free": "40.2", "train_wall": "26853"}
[2024-10-07 15:50:41,680][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 15:50:41,684][fairseq.trainer][INFO] - begin training epoch 733
[2024-10-07 15:50:41,684][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 15:54:22,617][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2024-10-07 15:54:56,312][fairseq_cli.train][INFO] - end of epoch 733 (average epoch stats below)
[2024-10-07 15:54:56,315][train][INFO] - {"epoch": 733, "train_loss": "0.611", "train_ntokens": "261048", "train_nsentences": "1742.77", "train_wps": "48122.9", "train_ups": "0.18", "train_wpb": "261048", "train_bsz": "1742.8", "train_num_updates": "35166", "train_lr": "0.000495698", "train_gnorm": "0.438", "train_loss_scale": "2", "train_train_wall": "142", "train_gb_free": "39.2", "train_wall": "27108"}
[2024-10-07 15:54:56,574][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 15:54:56,595][fairseq.trainer][INFO] - begin training epoch 734
[2024-10-07 15:54:56,595][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 15:58:48,298][train_inner][INFO] - {"epoch": 734, "update": 733.708, "loss": "0.61", "ntokens": "260845", "nsentences": "1749.63", "wps": "52792.3", "ups": "0.2", "wpb": "260845", "bsz": "1749.6", "num_updates": "35200", "lr": "0.000495652", "gnorm": "0.423", "loss_scale": "2", "train_wall": "492", "gb_free": "39.2", "wall": "27340"}
[2024-10-07 15:59:01,407][fairseq_cli.train][INFO] - end of epoch 734 (average epoch stats below)
[2024-10-07 15:59:01,409][train][INFO] - {"epoch": 734, "train_loss": "0.607", "train_ntokens": "260826", "train_nsentences": "1750.04", "train_wps": "51081.6", "train_ups": "0.2", "train_wpb": "260826", "train_bsz": "1750", "train_num_updates": "35214", "train_lr": "0.000495633", "train_gnorm": "0.387", "train_loss_scale": "2", "train_train_wall": "128", "train_gb_free": "39.7", "train_wall": "27353"}
[2024-10-07 15:59:01,640][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 15:59:01,660][fairseq.trainer][INFO] - begin training epoch 735
[2024-10-07 15:59:01,661][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 16:02:25,196][fairseq_cli.train][INFO] - end of epoch 735 (average epoch stats below)
[2024-10-07 16:02:25,218][train][INFO] - {"epoch": 735, "train_loss": "0.61", "train_ntokens": "260721", "train_nsentences": "1750.04", "train_wps": "61404.7", "train_ups": "0.24", "train_wpb": "260721", "train_bsz": "1750", "train_num_updates": "35262", "train_lr": "0.000495568", "train_gnorm": "0.402", "train_loss_scale": "2", "train_train_wall": "85", "train_gb_free": "39.7", "train_wall": "27557"}
[2024-10-07 16:02:25,545][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 16:02:25,552][fairseq.trainer][INFO] - begin training epoch 736
[2024-10-07 16:02:25,552][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 16:06:06,598][fairseq_cli.train][INFO] - end of epoch 736 (average epoch stats below)
[2024-10-07 16:06:06,610][train][INFO] - {"epoch": 736, "train_loss": "0.608", "train_ntokens": "260742", "train_nsentences": "1750.04", "train_wps": "56532.5", "train_ups": "0.22", "train_wpb": "260742", "train_bsz": "1750", "train_num_updates": "35310", "train_lr": "0.000495503", "train_gnorm": "0.388", "train_loss_scale": "2", "train_train_wall": "100", "train_gb_free": "39.7", "train_wall": "27779"}
[2024-10-07 16:06:06,835][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 16:06:06,840][fairseq.trainer][INFO] - begin training epoch 737
[2024-10-07 16:06:06,840][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 16:09:36,297][fairseq_cli.train][INFO] - end of epoch 737 (average epoch stats below)
[2024-10-07 16:09:36,302][train][INFO] - {"epoch": 737, "train_loss": "0.615", "train_ntokens": "260607", "train_nsentences": "1750.04", "train_wps": "59656.5", "train_ups": "0.23", "train_wpb": "260607", "train_bsz": "1750", "train_num_updates": "35358", "train_lr": "0.000495438", "train_gnorm": "0.402", "train_loss_scale": "2", "train_train_wall": "100", "train_gb_free": "39.3", "train_wall": "27988"}
[2024-10-07 16:09:36,456][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 16:09:36,489][fairseq.trainer][INFO] - begin training epoch 738
[2024-10-07 16:09:36,490][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 16:13:47,511][train_inner][INFO] - {"epoch": 738, "update": 737.875, "loss": "0.61", "ntokens": "260399", "nsentences": "1755.52", "wps": "57918.5", "ups": "0.22", "wpb": "260399", "bsz": "1755.5", "num_updates": "35400", "lr": "0.00049538", "gnorm": "0.398", "loss_scale": "2", "train_wall": "442", "gb_free": "39.7", "wall": "28240"}
[2024-10-07 16:13:49,347][fairseq_cli.train][INFO] - end of epoch 738 (average epoch stats below)
[2024-10-07 16:13:49,349][train][INFO] - {"epoch": 738, "train_loss": "0.607", "train_ntokens": "260411", "train_nsentences": "1750.04", "train_wps": "49397.4", "train_ups": "0.19", "train_wpb": "260411", "train_bsz": "1750", "train_num_updates": "35406", "train_lr": "0.000495372", "train_gnorm": "0.417", "train_loss_scale": "2", "train_train_wall": "147", "train_gb_free": "39.8", "train_wall": "28241"}
[2024-10-07 16:13:49,563][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 16:13:49,589][fairseq.trainer][INFO] - begin training epoch 739
[2024-10-07 16:13:49,589][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 16:16:39,783][fairseq_cli.train][INFO] - end of epoch 739 (average epoch stats below)
[2024-10-07 16:16:39,786][train][INFO] - {"epoch": 739, "train_loss": "0.608", "train_ntokens": "260594", "train_nsentences": "1750.04", "train_wps": "73392.3", "train_ups": "0.28", "train_wpb": "260594", "train_bsz": "1750", "train_num_updates": "35454", "train_lr": "0.000495307", "train_gnorm": "0.403", "train_loss_scale": "2", "train_train_wall": "81", "train_gb_free": "39.7", "train_wall": "28412"}
[2024-10-07 16:16:39,847][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 16:16:39,855][fairseq.trainer][INFO] - begin training epoch 740
[2024-10-07 16:16:39,855][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 16:20:12,409][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 740 @ 35502 updates
[2024-10-07 16:20:12,415][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-07 16:20:16,867][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-07 16:20:16,869][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 740 @ 35502 updates, score None) (writing took 4.460069393739104 seconds)
[2024-10-07 16:20:16,870][fairseq_cli.train][INFO] - end of epoch 740 (average epoch stats below)
[2024-10-07 16:20:16,872][train][INFO] - {"epoch": 740, "train_loss": "0.604", "train_ntokens": "260595", "train_nsentences": "1750.04", "train_wps": "57621.3", "train_ups": "0.22", "train_wpb": "260595", "train_bsz": "1750", "train_num_updates": "35502", "train_lr": "0.000495242", "train_gnorm": "0.398", "train_loss_scale": "2", "train_train_wall": "60", "train_gb_free": "39.3", "train_wall": "28629"}
[2024-10-07 16:20:16,976][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 16:20:17,013][fairseq.trainer][INFO] - begin training epoch 741
[2024-10-07 16:20:17,013][fairseq_cli.train][INFO] - Start iterating over samples
