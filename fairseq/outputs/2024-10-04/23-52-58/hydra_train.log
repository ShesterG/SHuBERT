[2024-10-04 23:53:45,013][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:19508', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'channel'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_800k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-04 23:53:46,229][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:14395', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'channel'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_800k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-04 23:53:46,341][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:18879', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'channel'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_800k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-04 23:53:46,361][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:19840', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'channel'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_800k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-04 23:53:47,902][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:16618', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'channel'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_800k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-04 23:53:48,366][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:12676', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'channel'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_800k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-04 23:53:48,849][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-04 23:53:48,862][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-04 23:53:48,862][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-04 23:53:48,862][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-04 23:53:48,863][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-04 23:53:48,864][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-04 23:53:49,357][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:12561', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'channel'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_800k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-04 23:53:50,701][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-04 23:53:50,707][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-04 23:53:50,707][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-04 23:53:50,707][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-04 23:53:50,708][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-04 23:53:50,714][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-04 23:53:52,161][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:19411', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'channel'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_800k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-04 23:53:53,156][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-04 23:53:53,158][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-04 23:53:53,158][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-04 23:53:53,158][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-04 23:53:53,160][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-04 23:53:53,160][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-04 23:53:53,326][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-04 23:53:53,328][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-04 23:53:53,328][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-04 23:53:53,328][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-04 23:53:53,328][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-04 23:53:53,329][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-04 23:53:56,535][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-04 23:53:57,660][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-04 23:53:58,263][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-04 23:54:01,481][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-04 23:54:07,381][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-04 23:54:07,403][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-04 23:54:07,403][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-04 23:54:07,403][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-04 23:54:07,404][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-04 23:54:07,404][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-04 23:54:07,699][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-04 23:54:07,701][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-04 23:54:07,701][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-04 23:54:07,701][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-04 23:54:07,810][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-04 23:54:07,810][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-04 23:54:11,544][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-04 23:54:11,547][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-04 23:54:11,547][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-04 23:54:11,547][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-04 23:54:11,548][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-04 23:54:11,549][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-04 23:54:19,428][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-04 23:54:19,523][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-04 23:54:19,523][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-04 23:54:19,523][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-04 23:54:19,524][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-04 23:54:19,524][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-04 23:54:45,757][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-04 23:54:50,902][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-04 23:54:59,591][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-04 23:55:13,379][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-04 23:56:54,179][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-04 23:56:54,182][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 23:56:54,182][fairseq.utils][INFO] - rank   1: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 23:56:54,182][fairseq.utils][INFO] - rank   2: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 23:56:54,182][fairseq.utils][INFO] - rank   3: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 23:56:54,182][fairseq.utils][INFO] - rank   4: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 23:56:54,182][fairseq.utils][INFO] - rank   5: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 23:56:54,182][fairseq.utils][INFO] - rank   6: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 23:56:54,183][fairseq.utils][INFO] - rank   7: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 23:56:54,183][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-04 23:56:54,184][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-04 23:56:54,202][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-04 23:56:54,203][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-04 23:57:12,914][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-04 23:57:12,916][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 23:57:12,917][fairseq.utils][INFO] - rank   1: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 23:57:12,917][fairseq.utils][INFO] - rank   2: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 23:57:12,917][fairseq.utils][INFO] - rank   3: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 23:57:12,917][fairseq.utils][INFO] - rank   4: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 23:57:12,917][fairseq.utils][INFO] - rank   5: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 23:57:12,917][fairseq.utils][INFO] - rank   6: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 23:57:12,917][fairseq.utils][INFO] - rank   7: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 23:57:12,917][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-04 23:57:12,917][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-04 23:57:12,918][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-04 23:57:12,919][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-04 23:57:32,430][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt (epoch 19 @ 8613 updates)
[2024-10-04 23:57:32,431][fairseq.trainer][INFO] - loading train data for epoch 19
[2024-10-04 23:57:38,047][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt (epoch 19 @ 8613 updates)
[2024-10-04 23:57:38,048][fairseq.trainer][INFO] - loading train data for epoch 19
[2024-10-04 23:57:42,033][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-04 23:57:42,996][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-04 23:58:49,413][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-04 23:58:49,423][fairseq.trainer][INFO] - begin training epoch 19
[2024-10-04 23:58:49,424][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-04 23:59:09,971][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-04 23:59:09,972][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 23:59:09,972][fairseq.utils][INFO] - rank   1: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 23:59:09,972][fairseq.utils][INFO] - rank   2: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 23:59:09,972][fairseq.utils][INFO] - rank   3: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 23:59:09,972][fairseq.utils][INFO] - rank   4: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 23:59:09,972][fairseq.utils][INFO] - rank   5: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 23:59:09,972][fairseq.utils][INFO] - rank   6: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 23:59:09,972][fairseq.utils][INFO] - rank   7: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 23:59:09,972][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-04 23:59:09,972][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-04 23:59:09,972][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-04 23:59:09,973][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-05 00:00:06,061][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 00:00:06,072][fairseq.trainer][INFO] - begin training epoch 19
[2024-10-05 00:00:06,074][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 00:00:36,203][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt (epoch 19 @ 8613 updates)
[2024-10-05 00:00:36,205][fairseq.trainer][INFO] - loading train data for epoch 19
[2024-10-05 00:00:54,524][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-05 00:00:54,525][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 00:00:54,525][fairseq.utils][INFO] - rank   1: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 00:00:54,525][fairseq.utils][INFO] - rank   2: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 00:00:54,525][fairseq.utils][INFO] - rank   3: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 00:00:54,525][fairseq.utils][INFO] - rank   4: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 00:00:54,525][fairseq.utils][INFO] - rank   5: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 00:00:54,525][fairseq.utils][INFO] - rank   6: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 00:00:54,525][fairseq.utils][INFO] - rank   7: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 00:00:54,525][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-05 00:00:54,525][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-05 00:00:54,545][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-05 00:00:54,546][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-05 00:00:59,045][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-05 00:01:10,364][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt (epoch 19 @ 8613 updates)
[2024-10-05 00:01:10,767][fairseq.trainer][INFO] - loading train data for epoch 19
[2024-10-05 00:01:15,261][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-05 00:02:17,651][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-05 00:02:17,651][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 00:02:17,652][fairseq.utils][INFO] - rank   1: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 00:02:17,652][fairseq.utils][INFO] - rank   2: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 00:02:17,652][fairseq.utils][INFO] - rank   3: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 00:02:17,652][fairseq.utils][INFO] - rank   4: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 00:02:17,652][fairseq.utils][INFO] - rank   5: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 00:02:17,652][fairseq.utils][INFO] - rank   6: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 00:02:17,652][fairseq.utils][INFO] - rank   7: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 00:02:17,652][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-05 00:02:17,652][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-05 00:02:17,653][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-05 00:02:17,654][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-05 00:02:25,527][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-05 00:02:25,527][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 00:02:25,527][fairseq.utils][INFO] - rank   1: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 00:02:25,527][fairseq.utils][INFO] - rank   2: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 00:02:25,527][fairseq.utils][INFO] - rank   3: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 00:02:25,527][fairseq.utils][INFO] - rank   4: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 00:02:25,527][fairseq.utils][INFO] - rank   5: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 00:02:25,527][fairseq.utils][INFO] - rank   6: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 00:02:25,527][fairseq.utils][INFO] - rank   7: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 00:02:25,527][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-05 00:02:25,528][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-05 00:02:25,534][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-05 00:02:25,535][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-05 00:02:35,445][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 00:02:35,451][fairseq.trainer][INFO] - begin training epoch 19
[2024-10-05 00:02:35,451][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 00:02:52,497][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt (epoch 19 @ 8613 updates)
[2024-10-05 00:02:52,790][fairseq.trainer][INFO] - loading train data for epoch 19
[2024-10-05 00:03:04,429][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-05 00:03:05,098][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt (epoch 19 @ 8613 updates)
[2024-10-05 00:03:05,162][fairseq.trainer][INFO] - loading train data for epoch 19
[2024-10-05 00:03:09,113][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-05 00:03:22,205][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-05 00:03:22,206][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 00:03:22,208][fairseq.utils][INFO] - rank   1: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 00:03:22,208][fairseq.utils][INFO] - rank   2: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 00:03:22,208][fairseq.utils][INFO] - rank   3: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 00:03:22,208][fairseq.utils][INFO] - rank   4: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 00:03:22,209][fairseq.utils][INFO] - rank   5: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 00:03:22,209][fairseq.utils][INFO] - rank   6: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 00:03:22,209][fairseq.utils][INFO] - rank   7: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 00:03:22,209][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-05 00:03:22,209][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-05 00:03:22,209][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-05 00:03:22,222][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-05 00:03:26,013][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 00:03:26,027][fairseq.trainer][INFO] - begin training epoch 19
[2024-10-05 00:03:26,028][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 00:03:34,580][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-05 00:03:34,598][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 00:03:34,598][fairseq.utils][INFO] - rank   1: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 00:03:34,598][fairseq.utils][INFO] - rank   2: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 00:03:34,598][fairseq.utils][INFO] - rank   3: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 00:03:34,598][fairseq.utils][INFO] - rank   4: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 00:03:34,598][fairseq.utils][INFO] - rank   5: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 00:03:34,598][fairseq.utils][INFO] - rank   6: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 00:03:34,598][fairseq.utils][INFO] - rank   7: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-05 00:03:34,598][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-05 00:03:34,599][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-05 00:03:34,599][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-05 00:03:34,600][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-05 00:03:57,095][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt (epoch 19 @ 8613 updates)
[2024-10-05 00:03:57,158][fairseq.trainer][INFO] - loading train data for epoch 19
[2024-10-05 00:03:59,639][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-05 00:04:13,379][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt (epoch 19 @ 8613 updates)
[2024-10-05 00:04:13,456][fairseq.trainer][INFO] - loading train data for epoch 19
[2024-10-05 00:04:16,856][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-05 00:04:25,229][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 00:04:25,247][fairseq.trainer][INFO] - begin training epoch 19
[2024-10-05 00:04:25,248][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 00:05:10,389][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 00:05:10,402][fairseq.trainer][INFO] - begin training epoch 19
[2024-10-05 00:05:10,403][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 00:05:21,625][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 00:05:21,635][fairseq.trainer][INFO] - begin training epoch 19
[2024-10-05 00:05:21,636][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 00:05:30,843][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 00:05:30,857][fairseq.trainer][INFO] - begin training epoch 19
[2024-10-05 00:05:30,857][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 01:01:01,378][train_inner][INFO] - {"epoch": 19, "update": 18.39, "loss": "2.931", "ntokens": "358464", "nsentences": "1753.05", "wps": "34896", "ups": "0.1", "wpb": "358464", "bsz": "1753.1", "num_updates": "8800", "lr": "0.0001375", "gnorm": "2.27", "loss_scale": "1", "train_wall": "2182", "gb_free": "39.2", "wall": "3459"}
[2024-10-05 01:01:04,358][train_inner][INFO] - {"epoch": 19, "update": 18.39, "loss": "2.931", "ntokens": "358464", "nsentences": "1753.05", "wps": "34838.8", "ups": "0.1", "wpb": "358464", "bsz": "1753.1", "num_updates": "8800", "lr": "0.0001375", "gnorm": "2.27", "loss_scale": "1", "train_wall": "2100", "gb_free": "39.2", "wall": "3526"}
[2024-10-05 01:19:21,515][train_inner][INFO] - {"epoch": 19, "update": 18.808, "loss": "2.915", "ntokens": "358166", "nsentences": "1736.77", "wps": "65119.5", "ups": "0.18", "wpb": "358166", "bsz": "1736.8", "num_updates": "9000", "lr": "0.000140625", "gnorm": "2.205", "loss_scale": "1", "train_wall": "1080", "gb_free": "39.8", "wall": "4559"}
[2024-10-05 01:19:31,818][train_inner][INFO] - {"epoch": 19, "update": 18.808, "loss": "2.915", "ntokens": "358166", "nsentences": "1736.77", "wps": "64683.2", "ups": "0.18", "wpb": "358166", "bsz": "1736.8", "num_updates": "9000", "lr": "0.000140625", "gnorm": "2.205", "loss_scale": "1", "train_wall": "1084", "gb_free": "39.8", "wall": "4634"}
[2024-10-05 01:28:26,110][fairseq_cli.train][INFO] - end of epoch 19 (average epoch stats below)
[2024-10-05 01:28:26,210][train][INFO] - {"epoch": 19, "train_loss": "2.924", "train_ntokens": "357674", "train_nsentences": "1753.71", "train_wps": "48081.3", "train_ups": "0.13", "train_wpb": "357674", "train_bsz": "1753.7", "train_num_updates": "9092", "train_lr": "0.000142063", "train_gnorm": "2.226", "train_loss_scale": "1", "train_train_wall": "3750", "train_gb_free": "39.2", "train_wall": "5104"}
[2024-10-05 01:28:26,727][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 01:28:26,783][fairseq.trainer][INFO] - begin training epoch 20
[2024-10-05 01:28:26,784][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 01:28:42,440][fairseq_cli.train][INFO] - end of epoch 19 (average epoch stats below)
[2024-10-05 01:28:42,457][train][INFO] - {"epoch": 19, "train_loss": "2.924", "train_ntokens": "357674", "train_nsentences": "1753.71", "train_wps": "47859.9", "train_ups": "0.13", "train_wpb": "357674", "train_bsz": "1753.7", "train_num_updates": "9092", "train_lr": "0.000142063", "train_gnorm": "2.226", "train_loss_scale": "1", "train_train_wall": "3685", "train_gb_free": "39.2", "train_wall": "5185"}
[2024-10-05 01:28:42,645][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 01:28:42,653][fairseq.trainer][INFO] - begin training epoch 20
[2024-10-05 01:28:42,654][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 01:49:44,311][train_inner][INFO] - {"epoch": 20, "update": 19.225, "loss": "2.907", "ntokens": "356787", "nsentences": "1791.72", "wps": "39150", "ups": "0.11", "wpb": "356787", "bsz": "1791.7", "num_updates": "9200", "lr": "0.00014375", "gnorm": "2.116", "loss_scale": "1", "train_wall": "1171", "gb_free": "39.6", "wall": "6382"}
[2024-10-05 01:49:44,566][train_inner][INFO] - {"epoch": 20, "update": 19.225, "loss": "2.907", "ntokens": "356787", "nsentences": "1791.72", "wps": "39364.3", "ups": "0.11", "wpb": "356787", "bsz": "1791.7", "num_updates": "9200", "lr": "0.00014375", "gnorm": "2.118", "loss_scale": "1", "train_wall": "1194", "gb_free": "39.6", "wall": "6447"}
[2024-10-05 02:02:19,479][train_inner][INFO] - {"epoch": 20, "update": 19.643, "loss": "2.894", "ntokens": "358413", "nsentences": "1743.79", "wps": "94940", "ups": "0.26", "wpb": "358413", "bsz": "1743.8", "num_updates": "9400", "lr": "0.000146875", "gnorm": "2.167", "loss_scale": "1", "train_wall": "681", "gb_free": "39.6", "wall": "7137"}
[2024-10-05 02:02:19,931][train_inner][INFO] - {"epoch": 20, "update": 19.643, "loss": "2.894", "ntokens": "358413", "nsentences": "1743.79", "wps": "94900.6", "ups": "0.26", "wpb": "358413", "bsz": "1743.8", "num_updates": "9400", "lr": "0.000146875", "gnorm": "2.171", "loss_scale": "1", "train_wall": "689", "gb_free": "39.6", "wall": "7202"}
[2024-10-05 02:13:37,531][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 20 @ 9571 updates
[2024-10-05 02:13:37,533][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-05 02:13:39,792][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 20 @ 9571 updates
[2024-10-05 02:13:39,793][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-05 02:13:40,994][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-05 02:13:40,996][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt (epoch 20 @ 9571 updates, score None) (writing took 3.4650294315069914 seconds)
[2024-10-05 02:13:40,997][fairseq_cli.train][INFO] - end of epoch 20 (average epoch stats below)
[2024-10-05 02:13:41,051][train][INFO] - {"epoch": 20, "train_loss": "2.89", "train_ntokens": "357674", "train_nsentences": "1753.71", "train_wps": "63108.4", "train_ups": "0.18", "train_wpb": "357674", "train_bsz": "1753.7", "train_num_updates": "9571", "train_lr": "0.000149547", "train_gnorm": "2.106", "train_loss_scale": "1", "train_train_wall": "1954", "train_gb_free": "39.7", "train_wall": "7819"}
[2024-10-05 02:13:41,114][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 02:13:41,129][fairseq.trainer][INFO] - begin training epoch 21
[2024-10-05 02:13:41,130][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 02:23:26,647][train_inner][INFO] - {"epoch": 21, "update": 20.061, "loss": "2.883", "ntokens": "356651", "nsentences": "1745.95", "wps": "56293.8", "ups": "0.16", "wpb": "356651", "bsz": "1746", "num_updates": "9600", "lr": "0.00015", "gnorm": "2.069", "loss_scale": "1", "train_wall": "832", "gb_free": "39.3", "wall": "8404"}
[2024-10-05 02:41:57,293][train_inner][INFO] - {"epoch": 21, "update": 20.478, "loss": "2.872", "ntokens": "358199", "nsentences": "1762.8", "wps": "64504.7", "ups": "0.18", "wpb": "358199", "bsz": "1762.8", "num_updates": "9800", "lr": "0.000153125", "gnorm": "2.095", "loss_scale": "1", "train_wall": "1101", "gb_free": "41.1", "wall": "9515"}
[2024-10-05 03:08:52,828][train_inner][INFO] - {"epoch": 21, "update": 20.896, "loss": "2.862", "ntokens": "358495", "nsentences": "1747.43", "wps": "44382.8", "ups": "0.12", "wpb": "358495", "bsz": "1747.4", "num_updates": "10000", "lr": "0.00015625", "gnorm": "1.967", "loss_scale": "1", "train_wall": "1421", "gb_free": "40.5", "wall": "11131"}
[2024-10-05 03:11:06,486][fairseq_cli.train][INFO] - end of epoch 21 (average epoch stats below)
[2024-10-05 03:11:06,501][train][INFO] - {"epoch": 21, "train_loss": "2.867", "train_ntokens": "357674", "train_nsentences": "1753.71", "train_wps": "49725.3", "train_ups": "0.14", "train_wpb": "357674", "train_bsz": "1753.7", "train_num_updates": "10050", "train_lr": "0.000157031", "train_gnorm": "2.056", "train_loss_scale": "1", "train_train_wall": "2896", "train_gb_free": "39.8", "train_wall": "11264"}
[2024-10-05 03:11:09,913][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 03:11:09,983][fairseq.trainer][INFO] - begin training epoch 22
[2024-10-05 03:11:09,984][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 03:23:51,519][train_inner][INFO] - {"epoch": 22, "update": 21.313, "loss": "2.855", "ntokens": "356765", "nsentences": "1753.72", "wps": "79400.1", "ups": "0.22", "wpb": "356765", "bsz": "1753.7", "num_updates": "10200", "lr": "0.000159375", "gnorm": "2.057", "loss_scale": "1", "train_wall": "519", "gb_free": "39.6", "wall": "12029"}
[2024-10-05 03:30:36,545][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.5
[2024-10-05 03:31:34,358][train_inner][INFO] - {"epoch": 22, "update": 21.733, "loss": "2.837", "ntokens": "358392", "nsentences": "1723.3", "wps": "154890", "ups": "0.43", "wpb": "358392", "bsz": "1723.3", "num_updates": "10400", "lr": "0.0001625", "gnorm": "1.986", "loss_scale": "0.5", "train_wall": "441", "gb_free": "39.6", "wall": "12492"}
[2024-10-05 03:36:12,872][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 22 @ 10528 updates
[2024-10-05 03:36:12,875][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-05 03:36:18,683][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-05 03:36:18,687][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt (epoch 22 @ 10528 updates, score None) (writing took 5.8151127099990845 seconds)
[2024-10-05 03:36:18,688][fairseq_cli.train][INFO] - end of epoch 22 (average epoch stats below)
[2024-10-05 03:36:18,690][train][INFO] - {"epoch": 22, "train_loss": "2.844", "train_ntokens": "357679", "train_nsentences": "1754.13", "train_wps": "113064", "train_ups": "0.32", "train_wpb": "357679", "train_bsz": "1754.1", "train_num_updates": "10528", "train_lr": "0.0001645", "train_gnorm": "1.983", "train_loss_scale": "0.5", "train_train_wall": "1025", "train_gb_free": "39.3", "train_wall": "12776"}
[2024-10-05 03:36:18,819][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 03:36:18,837][fairseq.trainer][INFO] - begin training epoch 23
[2024-10-05 03:36:18,838][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 03:43:33,034][train_inner][INFO] - {"epoch": 23, "update": 22.15, "loss": "2.842", "ntokens": "356727", "nsentences": "1791.3", "wps": "99277.7", "ups": "0.28", "wpb": "356727", "bsz": "1791.3", "num_updates": "10600", "lr": "0.000165625", "gnorm": "1.995", "loss_scale": "0.5", "train_wall": "298", "gb_free": "39.2", "wall": "13211"}
[2024-10-05 03:48:06,818][train_inner][INFO] - {"epoch": 23, "update": 22.568, "loss": "2.82", "ntokens": "358440", "nsentences": "1758.86", "wps": "261852", "ups": "0.73", "wpb": "358440", "bsz": "1758.9", "num_updates": "10800", "lr": "0.00016875", "gnorm": "2.02", "loss_scale": "0.5", "train_wall": "270", "gb_free": "39.3", "wall": "13485"}
[2024-10-05 03:52:55,675][train_inner][INFO] - {"epoch": 23, "update": 22.985, "loss": "2.815", "ntokens": "358190", "nsentences": "1743.9", "wps": "248054", "ups": "0.69", "wpb": "358190", "bsz": "1743.9", "num_updates": "11000", "lr": "0.000171875", "gnorm": "1.922", "loss_scale": "0.5", "train_wall": "285", "gb_free": "39.3", "wall": "13773"}
[2024-10-05 03:53:19,185][fairseq_cli.train][INFO] - end of epoch 23 (average epoch stats below)
[2024-10-05 03:53:19,194][train][INFO] - {"epoch": 23, "train_loss": "2.819", "train_ntokens": "357674", "train_nsentences": "1753.71", "train_wps": "167885", "train_ups": "0.47", "train_wpb": "357674", "train_bsz": "1753.7", "train_num_updates": "11007", "train_lr": "0.000171984", "train_gnorm": "1.994", "train_loss_scale": "0.5", "train_train_wall": "679", "train_gb_free": "40.1", "train_wall": "13797"}
[2024-10-05 03:53:19,425][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 03:53:19,456][fairseq.trainer][INFO] - begin training epoch 24
[2024-10-05 03:53:19,457][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 04:04:01,620][train_inner][INFO] - {"epoch": 24, "update": 23.403, "loss": "2.807", "ntokens": "356724", "nsentences": "1775.58", "wps": "107137", "ups": "0.3", "wpb": "356724", "bsz": "1775.6", "num_updates": "11200", "lr": "0.000175", "gnorm": "1.947", "loss_scale": "0.5", "train_wall": "357", "gb_free": "39.8", "wall": "14439"}
[2024-10-05 04:09:04,883][train_inner][INFO] - {"epoch": 24, "update": 23.82, "loss": "2.802", "ntokens": "358335", "nsentences": "1757.04", "wps": "236323", "ups": "0.66", "wpb": "358335", "bsz": "1757", "num_updates": "11400", "lr": "0.000178125", "gnorm": "1.894", "loss_scale": "0.5", "train_wall": "300", "gb_free": "40.1", "wall": "14743"}
[2024-10-05 04:11:15,055][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 24 @ 11486 updates
[2024-10-05 04:11:15,056][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-05 04:11:19,063][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-05 04:11:19,066][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt (epoch 24 @ 11486 updates, score None) (writing took 4.011188940145075 seconds)
[2024-10-05 04:11:19,067][fairseq_cli.train][INFO] - end of epoch 24 (average epoch stats below)
[2024-10-05 04:11:19,071][train][INFO] - {"epoch": 24, "train_loss": "2.801", "train_ntokens": "357674", "train_nsentences": "1753.71", "train_wps": "158654", "train_ups": "0.44", "train_wpb": "357674", "train_bsz": "1753.7", "train_num_updates": "11486", "train_lr": "0.000179469", "train_gnorm": "1.906", "train_loss_scale": "0.5", "train_train_wall": "763", "train_gb_free": "39.3", "train_wall": "14877"}
[2024-10-05 04:11:19,172][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 04:11:19,180][fairseq.trainer][INFO] - begin training epoch 25
[2024-10-05 04:11:19,180][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 04:19:21,815][train_inner][INFO] - {"epoch": 25, "update": 24.238, "loss": "2.78", "ntokens": "356800", "nsentences": "1692.36", "wps": "115670", "ups": "0.32", "wpb": "356800", "bsz": "1692.4", "num_updates": "11600", "lr": "0.00018125", "gnorm": "1.858", "loss_scale": "0.5", "train_wall": "288", "gb_free": "40.1", "wall": "15360"}
[2024-10-05 04:24:44,170][train_inner][INFO] - {"epoch": 25, "update": 24.656, "loss": "2.782", "ntokens": "358484", "nsentences": "1759.43", "wps": "222420", "ups": "0.62", "wpb": "358484", "bsz": "1759.4", "num_updates": "11800", "lr": "0.000184375", "gnorm": "1.837", "loss_scale": "0.5", "train_wall": "319", "gb_free": "40.3", "wall": "15682"}
[2024-10-05 04:29:11,306][fairseq_cli.train][INFO] - end of epoch 25 (average epoch stats below)
[2024-10-05 04:29:11,323][train][INFO] - {"epoch": 25, "train_loss": "2.779", "train_ntokens": "357674", "train_nsentences": "1753.71", "train_wps": "159782", "train_ups": "0.45", "train_wpb": "357674", "train_bsz": "1753.7", "train_num_updates": "11965", "train_lr": "0.000186953", "train_gnorm": "1.829", "train_loss_scale": "0.5", "train_train_wall": "742", "train_gb_free": "39.6", "train_wall": "15949"}
[2024-10-05 04:29:11,431][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 04:29:11,435][fairseq.trainer][INFO] - begin training epoch 26
[2024-10-05 04:29:11,435][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 04:35:46,378][train_inner][INFO] - {"epoch": 26, "update": 25.073, "loss": "2.777", "ntokens": "356589", "nsentences": "1784.07", "wps": "107698", "ups": "0.3", "wpb": "356589", "bsz": "1784.1", "num_updates": "12000", "lr": "0.0001875", "gnorm": "1.84", "loss_scale": "0.5", "train_wall": "339", "gb_free": "39.6", "wall": "16344"}
[2024-10-05 04:40:02,783][train_inner][INFO] - {"epoch": 26, "update": 25.491, "loss": "2.75", "ntokens": "358359", "nsentences": "1735.82", "wps": "279530", "ups": "0.78", "wpb": "358359", "bsz": "1735.8", "num_updates": "12200", "lr": "0.000190625", "gnorm": "1.728", "loss_scale": "0.5", "train_wall": "253", "gb_free": "39.3", "wall": "16601"}
[2024-10-05 04:44:46,668][train_inner][INFO] - {"epoch": 26, "update": 25.908, "loss": "2.767", "ntokens": "358441", "nsentences": "1768.27", "wps": "252529", "ups": "0.7", "wpb": "358441", "bsz": "1768.3", "num_updates": "12400", "lr": "0.00019375", "gnorm": "1.769", "loss_scale": "0.5", "train_wall": "280", "gb_free": "40.1", "wall": "16884"}
[2024-10-05 04:45:16,665][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.5
[2024-10-05 04:46:05,687][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 26 @ 12443 updates
[2024-10-05 04:46:05,688][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-05 04:46:09,636][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-05 04:46:09,639][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt (epoch 26 @ 12443 updates, score None) (writing took 3.9520210521295667 seconds)
[2024-10-05 04:46:09,639][fairseq_cli.train][INFO] - end of epoch 26 (average epoch stats below)
[2024-10-05 04:46:09,645][train][INFO] - {"epoch": 26, "train_loss": "2.761", "train_ntokens": "357673", "train_nsentences": "1753.73", "train_wps": "167893", "train_ups": "0.47", "train_wpb": "357673", "train_bsz": "1753.7", "train_num_updates": "12443", "train_lr": "0.000194422", "train_gnorm": "1.765", "train_loss_scale": "0.5", "train_train_wall": "685", "train_gb_free": "39.8", "train_wall": "16967"}
[2024-10-05 04:46:09,724][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 04:46:09,731][fairseq.trainer][INFO] - begin training epoch 27
[2024-10-05 04:46:09,731][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 04:55:11,988][train_inner][INFO] - {"epoch": 27, "update": 26.328, "loss": "2.747", "ntokens": "356621", "nsentences": "1747.97", "wps": "114061", "ups": "0.32", "wpb": "356621", "bsz": "1748", "num_updates": "12600", "lr": "0.000196875", "gnorm": "1.753", "loss_scale": "0.5", "train_wall": "307", "gb_free": "40.2", "wall": "17510"}
[2024-10-05 05:00:16,442][train_inner][INFO] - {"epoch": 27, "update": 26.745, "loss": "2.738", "ntokens": "358240", "nsentences": "1732.54", "wps": "235336", "ups": "0.66", "wpb": "358240", "bsz": "1732.5", "num_updates": "12800", "lr": "0.0002", "gnorm": "1.693", "loss_scale": "0.5", "train_wall": "301", "gb_free": "39.3", "wall": "17814"}
[2024-10-05 05:03:03,458][fairseq_cli.train][INFO] - end of epoch 27 (average epoch stats below)
[2024-10-05 05:03:03,486][train][INFO] - {"epoch": 27, "train_loss": "2.742", "train_ntokens": "357674", "train_nsentences": "1753.71", "train_wps": "168988", "train_ups": "0.47", "train_wpb": "357674", "train_bsz": "1753.7", "train_num_updates": "12922", "train_lr": "0.000201906", "train_gnorm": "1.72", "train_loss_scale": "0.5", "train_train_wall": "694", "train_gb_free": "39.6", "train_wall": "17981"}
[2024-10-05 05:03:04,071][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 05:03:04,110][fairseq.trainer][INFO] - begin training epoch 28
[2024-10-05 05:03:04,111][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 05:11:05,811][train_inner][INFO] - {"epoch": 28, "update": 27.163, "loss": "2.744", "ntokens": "356702", "nsentences": "1785.72", "wps": "109862", "ups": "0.31", "wpb": "356702", "bsz": "1785.7", "num_updates": "13000", "lr": "0.000203125", "gnorm": "1.725", "loss_scale": "0.5", "train_wall": "320", "gb_free": "39.6", "wall": "18464"}
[2024-10-05 05:16:08,052][train_inner][INFO] - {"epoch": 28, "update": 27.58, "loss": "2.721", "ntokens": "358357", "nsentences": "1722.03", "wps": "237138", "ups": "0.66", "wpb": "358357", "bsz": "1722", "num_updates": "13200", "lr": "0.00020625", "gnorm": "1.698", "loss_scale": "0.5", "train_wall": "299", "gb_free": "39.6", "wall": "18766"}
[2024-10-05 05:21:31,716][train_inner][INFO] - {"epoch": 28, "update": 27.998, "loss": "2.734", "ntokens": "358460", "nsentences": "1783.44", "wps": "221521", "ups": "0.62", "wpb": "358460", "bsz": "1783.4", "num_updates": "13400", "lr": "0.000209375", "gnorm": "1.69", "loss_scale": "0.5", "train_wall": "320", "gb_free": "39.3", "wall": "19089"}
[2024-10-05 05:21:34,174][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 28 @ 13401 updates
[2024-10-05 05:21:34,175][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-05 05:21:39,781][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-05 05:21:39,822][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt (epoch 28 @ 13401 updates, score None) (writing took 5.6472850991413 seconds)
[2024-10-05 05:21:39,822][fairseq_cli.train][INFO] - end of epoch 28 (average epoch stats below)
[2024-10-05 05:21:39,834][train][INFO] - {"epoch": 28, "train_loss": "2.728", "train_ntokens": "357674", "train_nsentences": "1753.71", "train_wps": "153472", "train_ups": "0.43", "train_wpb": "357674", "train_bsz": "1753.7", "train_num_updates": "13401", "train_lr": "0.000209391", "train_gnorm": "1.701", "train_loss_scale": "0.5", "train_train_wall": "776", "train_gb_free": "40.2", "train_wall": "19098"}
[2024-10-05 05:21:39,938][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 05:21:39,945][fairseq.trainer][INFO] - begin training epoch 29
[2024-10-05 05:21:39,946][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 05:32:07,582][train_inner][INFO] - {"epoch": 29, "update": 28.415, "loss": "2.711", "ntokens": "356780", "nsentences": "1747.63", "wps": "112223", "ups": "0.31", "wpb": "356780", "bsz": "1747.6", "num_updates": "13600", "lr": "0.0002125", "gnorm": "1.67", "loss_scale": "0.5", "train_wall": "297", "gb_free": "39.6", "wall": "19725"}
[2024-10-05 05:36:49,065][train_inner][INFO] - {"epoch": 29, "update": 28.833, "loss": "2.715", "ntokens": "358371", "nsentences": "1774.62", "wps": "254634", "ups": "0.71", "wpb": "358371", "bsz": "1774.6", "num_updates": "13800", "lr": "0.000215625", "gnorm": "1.672", "loss_scale": "0.5", "train_wall": "278", "gb_free": "39.4", "wall": "20007"}
[2024-10-05 05:38:46,459][fairseq_cli.train][INFO] - end of epoch 29 (average epoch stats below)
[2024-10-05 05:38:46,492][train][INFO] - {"epoch": 29, "train_loss": "2.711", "train_ntokens": "357674", "train_nsentences": "1753.71", "train_wps": "166878", "train_ups": "0.47", "train_wpb": "357674", "train_bsz": "1753.7", "train_num_updates": "13880", "train_lr": "0.000216875", "train_gnorm": "1.669", "train_loss_scale": "0.5", "train_train_wall": "687", "train_gb_free": "40.1", "train_wall": "20124"}
[2024-10-05 05:38:46,642][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 05:38:46,646][fairseq.trainer][INFO] - begin training epoch 30
[2024-10-05 05:38:46,646][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 05:47:10,847][train_inner][INFO] - {"epoch": 30, "update": 29.251, "loss": "2.696", "ntokens": "356602", "nsentences": "1718.01", "wps": "114706", "ups": "0.32", "wpb": "356602", "bsz": "1718", "num_updates": "14000", "lr": "0.00021875", "gnorm": "1.657", "loss_scale": "0.5", "train_wall": "315", "gb_free": "39.7", "wall": "20629"}
[2024-10-05 05:51:41,888][train_inner][INFO] - {"epoch": 30, "update": 29.668, "loss": "2.7", "ntokens": "358426", "nsentences": "1796.39", "wps": "264486", "ups": "0.74", "wpb": "358426", "bsz": "1796.4", "num_updates": "14200", "lr": "0.000221875", "gnorm": "1.627", "loss_scale": "0.5", "train_wall": "268", "gb_free": "39.8", "wall": "20900"}
[2024-10-05 05:55:34,572][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 30 @ 14359 updates
[2024-10-05 05:55:34,574][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-05 05:55:38,435][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-05 05:55:38,438][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt (epoch 30 @ 14359 updates, score None) (writing took 3.865541729144752 seconds)
[2024-10-05 05:55:38,439][fairseq_cli.train][INFO] - end of epoch 30 (average epoch stats below)
[2024-10-05 05:55:38,441][train][INFO] - {"epoch": 30, "train_loss": "2.697", "train_ntokens": "357674", "train_nsentences": "1753.71", "train_wps": "169304", "train_ups": "0.47", "train_wpb": "357674", "train_bsz": "1753.7", "train_num_updates": "14359", "train_lr": "0.000224359", "train_gnorm": "1.636", "train_loss_scale": "0.5", "train_train_wall": "697", "train_gb_free": "40.1", "train_wall": "21136"}
[2024-10-05 05:55:38,494][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 05:55:38,499][fairseq.trainer][INFO] - begin training epoch 31
[2024-10-05 05:55:38,499][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 06:02:35,240][train_inner][INFO] - {"epoch": 31, "update": 30.086, "loss": "2.696", "ntokens": "356798", "nsentences": "1720.87", "wps": "109222", "ups": "0.31", "wpb": "356798", "bsz": "1720.9", "num_updates": "14400", "lr": "0.000225", "gnorm": "1.62", "loss_scale": "0.5", "train_wall": "312", "gb_free": "39.8", "wall": "21553"}
[2024-10-05 06:03:58,538][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.5
[2024-10-05 06:07:29,137][train_inner][INFO] - {"epoch": 31, "update": 30.505, "loss": "2.675", "ntokens": "358290", "nsentences": "1763.14", "wps": "243823", "ups": "0.68", "wpb": "358290", "bsz": "1763.1", "num_updates": "14600", "lr": "0.000228125", "gnorm": "1.542", "loss_scale": "0.5", "train_wall": "291", "gb_free": "40.3", "wall": "21847"}
[2024-10-05 06:12:58,335][train_inner][INFO] - {"epoch": 31, "update": 30.923, "loss": "2.687", "ntokens": "358337", "nsentences": "1767.02", "wps": "217706", "ups": "0.61", "wpb": "358337", "bsz": "1767", "num_updates": "14800", "lr": "0.00023125", "gnorm": "1.645", "loss_scale": "0.5", "train_wall": "325", "gb_free": "39.6", "wall": "22176"}
[2024-10-05 06:13:51,911][fairseq_cli.train][INFO] - end of epoch 31 (average epoch stats below)
[2024-10-05 06:13:51,970][train][INFO] - {"epoch": 31, "train_loss": "2.682", "train_ntokens": "357673", "train_nsentences": "1755.04", "train_wps": "156353", "train_ups": "0.44", "train_wpb": "357673", "train_bsz": "1755", "train_num_updates": "14837", "train_lr": "0.000231828", "train_gnorm": "1.595", "train_loss_scale": "0.5", "train_train_wall": "751", "train_gb_free": "39.2", "train_wall": "22230"}
[2024-10-05 06:13:52,586][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 06:13:52,627][fairseq.trainer][INFO] - begin training epoch 32
[2024-10-05 06:13:52,628][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 06:23:20,113][train_inner][INFO] - {"epoch": 32, "update": 31.34, "loss": "2.671", "ntokens": "356646", "nsentences": "1777.66", "wps": "114719", "ups": "0.32", "wpb": "356646", "bsz": "1777.7", "num_updates": "15000", "lr": "0.000234375", "gnorm": "1.615", "loss_scale": "0.5", "train_wall": "287", "gb_free": "41.3", "wall": "22798"}
[2024-10-05 06:28:05,048][train_inner][INFO] - {"epoch": 32, "update": 31.758, "loss": "2.673", "ntokens": "358463", "nsentences": "1755.92", "wps": "251618", "ups": "0.7", "wpb": "358463", "bsz": "1755.9", "num_updates": "15200", "lr": "0.0002375", "gnorm": "1.607", "loss_scale": "0.5", "train_wall": "281", "gb_free": "40.1", "wall": "23083"}
[2024-10-05 06:31:03,587][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 32 @ 15316 updates
[2024-10-05 06:31:03,588][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-05 06:31:07,055][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-05 06:31:07,058][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt (epoch 32 @ 15316 updates, score None) (writing took 3.471395851112902 seconds)
[2024-10-05 06:31:07,059][fairseq_cli.train][INFO] - end of epoch 32 (average epoch stats below)
[2024-10-05 06:31:07,062][train][INFO] - {"epoch": 32, "train_loss": "2.671", "train_ntokens": "357674", "train_nsentences": "1753.71", "train_wps": "165518", "train_ups": "0.46", "train_wpb": "357674", "train_bsz": "1753.7", "train_num_updates": "15316", "train_lr": "0.000239313", "train_gnorm": "1.608", "train_loss_scale": "0.5", "train_train_wall": "692", "train_gb_free": "39.2", "train_wall": "23265"}
[2024-10-05 06:31:07,129][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 06:31:07,134][fairseq.trainer][INFO] - begin training epoch 33
[2024-10-05 06:31:07,134][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 06:38:43,462][train_inner][INFO] - {"epoch": 33, "update": 32.175, "loss": "2.663", "ntokens": "356785", "nsentences": "1737.97", "wps": "111773", "ups": "0.31", "wpb": "356786", "bsz": "1738", "num_updates": "15400", "lr": "0.000240625", "gnorm": "1.582", "loss_scale": "0.5", "train_wall": "307", "gb_free": "39.6", "wall": "23721"}
[2024-10-05 06:43:18,927][train_inner][INFO] - {"epoch": 33, "update": 32.593, "loss": "2.656", "ntokens": "358338", "nsentences": "1752.7", "wps": "260175", "ups": "0.73", "wpb": "358338", "bsz": "1752.7", "num_updates": "15600", "lr": "0.00024375", "gnorm": "1.525", "loss_scale": "0.5", "train_wall": "271", "gb_free": "39.4", "wall": "23997"}
[2024-10-05 06:47:46,537][fairseq_cli.train][INFO] - end of epoch 33 (average epoch stats below)
[2024-10-05 06:47:46,560][train][INFO] - {"epoch": 33, "train_loss": "2.659", "train_ntokens": "357674", "train_nsentences": "1753.71", "train_wps": "171413", "train_ups": "0.48", "train_wpb": "357674", "train_bsz": "1753.7", "train_num_updates": "15795", "train_lr": "0.000246797", "train_gnorm": "1.543", "train_loss_scale": "0.5", "train_train_wall": "666", "train_gb_free": "39.2", "train_wall": "24264"}
[2024-10-05 06:47:47,033][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 06:47:47,076][fairseq.trainer][INFO] - begin training epoch 34
[2024-10-05 06:47:47,076][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 06:53:57,404][train_inner][INFO] - {"epoch": 34, "update": 33.01, "loss": "2.661", "ntokens": "356706", "nsentences": "1724.62", "wps": "111737", "ups": "0.31", "wpb": "356706", "bsz": "1724.6", "num_updates": "15800", "lr": "0.000246875", "gnorm": "1.579", "loss_scale": "0.5", "train_wall": "304", "gb_free": "39.6", "wall": "24635"}
[2024-10-05 06:58:55,525][train_inner][INFO] - {"epoch": 34, "update": 33.428, "loss": "2.642", "ntokens": "358329", "nsentences": "1778.52", "wps": "240396", "ups": "0.67", "wpb": "358329", "bsz": "1778.5", "num_updates": "16000", "lr": "0.00025", "gnorm": "1.559", "loss_scale": "0.5", "train_wall": "295", "gb_free": "39.6", "wall": "24933"}
[2024-10-05 07:03:59,385][train_inner][INFO] - {"epoch": 34, "update": 33.846, "loss": "2.655", "ntokens": "358241", "nsentences": "1739.59", "wps": "235797", "ups": "0.66", "wpb": "358241", "bsz": "1739.6", "num_updates": "16200", "lr": "0.000253125", "gnorm": "1.517", "loss_scale": "0.5", "train_wall": "301", "gb_free": "40.2", "wall": "25237"}
[2024-10-05 07:05:48,654][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 34 @ 16274 updates
[2024-10-05 07:05:48,655][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-05 07:05:52,602][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-05 07:05:52,607][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt (epoch 34 @ 16274 updates, score None) (writing took 3.953652792610228 seconds)
[2024-10-05 07:05:52,608][fairseq_cli.train][INFO] - end of epoch 34 (average epoch stats below)
[2024-10-05 07:05:52,611][train][INFO] - {"epoch": 34, "train_loss": "2.649", "train_ntokens": "357674", "train_nsentences": "1753.71", "train_wps": "157752", "train_ups": "0.44", "train_wpb": "357674", "train_bsz": "1753.7", "train_num_updates": "16274", "train_lr": "0.000254281", "train_gnorm": "1.56", "train_loss_scale": "0.5", "train_train_wall": "744", "train_gb_free": "39.8", "train_wall": "25350"}
[2024-10-05 07:05:52,669][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 07:05:52,677][fairseq.trainer][INFO] - begin training epoch 35
[2024-10-05 07:05:52,677][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 07:14:37,350][train_inner][INFO] - {"epoch": 35, "update": 34.263, "loss": "2.641", "ntokens": "356816", "nsentences": "1747.19", "wps": "111861", "ups": "0.31", "wpb": "356816", "bsz": "1747.2", "num_updates": "16400", "lr": "0.00025625", "gnorm": "1.527", "loss_scale": "0.5", "train_wall": "301", "gb_free": "39.6", "wall": "25875"}
[2024-10-05 07:17:53,183][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.5
[2024-10-05 07:19:47,041][train_inner][INFO] - {"epoch": 35, "update": 34.683, "loss": "2.637", "ntokens": "358342", "nsentences": "1756.22", "wps": "231432", "ups": "0.65", "wpb": "358342", "bsz": "1756.2", "num_updates": "16600", "lr": "0.000259375", "gnorm": "1.509", "loss_scale": "0.5", "train_wall": "306", "gb_free": "39.3", "wall": "26185"}
[2024-10-05 07:23:30,189][fairseq_cli.train][INFO] - end of epoch 35 (average epoch stats below)
[2024-10-05 07:23:30,209][train][INFO] - {"epoch": 35, "train_loss": "2.637", "train_ntokens": "357671", "train_nsentences": "1753.82", "train_wps": "161657", "train_ups": "0.45", "train_wpb": "357671", "train_bsz": "1753.8", "train_num_updates": "16752", "train_lr": "0.00026175", "train_gnorm": "1.493", "train_loss_scale": "0.5", "train_train_wall": "720", "train_gb_free": "40.1", "train_wall": "26408"}
[2024-10-05 07:23:30,417][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 07:23:30,444][fairseq.trainer][INFO] - begin training epoch 36
[2024-10-05 07:23:30,445][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 07:30:13,840][train_inner][INFO] - {"epoch": 36, "update": 35.1, "loss": "2.638", "ntokens": "356827", "nsentences": "1749.15", "wps": "113858", "ups": "0.32", "wpb": "356827", "bsz": "1749.2", "num_updates": "16800", "lr": "0.0002625", "gnorm": "1.514", "loss_scale": "0.5", "train_wall": "278", "gb_free": "39.3", "wall": "26812"}
[2024-10-05 07:35:11,924][train_inner][INFO] - {"epoch": 36, "update": 35.518, "loss": "2.624", "ntokens": "358212", "nsentences": "1779.39", "wps": "240346", "ups": "0.67", "wpb": "358212", "bsz": "1779.4", "num_updates": "17000", "lr": "0.000265625", "gnorm": "1.436", "loss_scale": "0.5", "train_wall": "295", "gb_free": "40.1", "wall": "27110"}
[2024-10-05 07:40:00,634][train_inner][INFO] - {"epoch": 36, "update": 35.935, "loss": "2.627", "ntokens": "358433", "nsentences": "1732.43", "wps": "248304", "ups": "0.69", "wpb": "358433", "bsz": "1732.4", "num_updates": "17200", "lr": "0.00026875", "gnorm": "1.457", "loss_scale": "0.5", "train_wall": "285", "gb_free": "39.3", "wall": "27398"}
[2024-10-05 07:40:45,263][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 36 @ 17231 updates
[2024-10-05 07:40:45,264][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-05 07:40:52,835][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-05 07:40:52,863][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt (epoch 36 @ 17231 updates, score None) (writing took 7.599860559217632 seconds)
[2024-10-05 07:40:52,863][fairseq_cli.train][INFO] - end of epoch 36 (average epoch stats below)
[2024-10-05 07:40:52,890][train][INFO] - {"epoch": 36, "train_loss": "2.627", "train_ntokens": "357674", "train_nsentences": "1753.71", "train_wps": "164317", "train_ups": "0.46", "train_wpb": "357674", "train_bsz": "1753.7", "train_num_updates": "17231", "train_lr": "0.000269234", "train_gnorm": "1.465", "train_loss_scale": "0.5", "train_train_wall": "682", "train_gb_free": "40.5", "train_wall": "27451"}
[2024-10-05 07:40:53,095][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-05 07:40:53,114][fairseq.trainer][INFO] - begin training epoch 37
[2024-10-05 07:40:53,115][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-05 07:50:47,998][train_inner][INFO] - {"epoch": 37, "update": 36.353, "loss": "2.616", "ntokens": "356758", "nsentences": "1750.46", "wps": "110219", "ups": "0.31", "wpb": "356758", "bsz": "1750.5", "num_updates": "17400", "lr": "0.000271875", "gnorm": "1.456", "loss_scale": "0.5", "train_wall": "305", "gb_free": "40.1", "wall": "28046"}
