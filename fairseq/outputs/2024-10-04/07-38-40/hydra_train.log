[2024-10-04 07:39:21,367][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:17808', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'channel'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_800k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-04 07:39:23,562][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-04 07:39:23,564][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-04 07:39:23,564][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-04 07:39:23,564][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-04 07:39:23,565][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-04 07:39:23,566][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-04 07:39:27,320][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:19929', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'channel'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_800k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-04 07:39:27,949][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:19685', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'channel'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_800k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-04 07:39:28,469][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:13463', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'channel'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_800k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-04 07:39:28,756][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-04 07:39:28,909][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:14895', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'channel'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_800k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-04 07:39:29,110][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:19515', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'channel'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_800k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-04 07:39:31,937][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:11166', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'channel'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_800k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-04 07:39:31,942][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-04 07:39:31,958][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-04 07:39:31,958][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-04 07:39:31,958][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-04 07:39:31,959][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-04 07:39:31,960][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-04 07:39:31,983][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-04 07:39:31,985][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-04 07:39:31,985][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-04 07:39:31,985][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-04 07:39:31,990][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-04 07:39:31,990][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-04 07:39:32,993][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:11922', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'channel'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_800k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-04 07:39:34,174][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-04 07:39:34,176][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-04 07:39:34,176][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-04 07:39:34,176][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-04 07:39:34,177][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-04 07:39:34,178][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-04 07:39:36,448][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-04 07:39:36,548][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-04 07:39:36,550][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-04 07:39:36,557][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-04 07:39:36,558][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-04 07:39:36,559][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-04 07:39:36,560][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-04 07:39:39,354][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-04 07:39:42,127][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-04 07:39:42,196][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-04 07:39:43,720][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-04 07:39:43,735][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-04 07:39:43,735][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-04 07:39:43,735][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-04 07:39:43,736][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-04 07:39:43,746][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-04 07:39:51,232][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-04 07:39:51,235][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-04 07:39:51,235][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-04 07:39:51,235][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-04 07:39:51,236][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-04 07:39:51,242][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-04 07:39:53,226][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-04 07:39:53,228][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-04 07:39:53,228][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-04 07:39:53,228][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-04 07:39:53,229][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-04 07:39:53,262][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-04 07:40:13,161][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-04 07:40:28,761][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-04 07:40:28,981][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-04 07:44:13,071][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-04 07:44:13,167][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 07:44:13,167][fairseq.utils][INFO] - rank   1: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 07:44:13,167][fairseq.utils][INFO] - rank   2: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 07:44:13,167][fairseq.utils][INFO] - rank   3: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 07:44:13,167][fairseq.utils][INFO] - rank   4: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 07:44:13,167][fairseq.utils][INFO] - rank   5: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 07:44:13,167][fairseq.utils][INFO] - rank   6: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 07:44:13,167][fairseq.utils][INFO] - rank   7: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 07:44:13,167][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-04 07:44:13,168][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-04 07:44:13,168][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-04 07:44:13,169][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-04 07:44:13,169][fairseq.trainer][INFO] - No existing checkpoint found /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-04 07:44:13,169][fairseq.trainer][INFO] - loading train data for epoch 1
[2024-10-04 07:44:21,360][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-04 07:46:01,971][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-04 07:46:01,978][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 07:46:01,978][fairseq.utils][INFO] - rank   1: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 07:46:01,978][fairseq.utils][INFO] - rank   2: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 07:46:01,978][fairseq.utils][INFO] - rank   3: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 07:46:01,978][fairseq.utils][INFO] - rank   4: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 07:46:01,978][fairseq.utils][INFO] - rank   5: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 07:46:01,978][fairseq.utils][INFO] - rank   6: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 07:46:01,978][fairseq.utils][INFO] - rank   7: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 07:46:01,978][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-04 07:46:01,979][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-04 07:46:01,979][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-04 07:46:01,980][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-04 07:46:01,980][fairseq.trainer][INFO] - No existing checkpoint found /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-04 07:46:01,980][fairseq.trainer][INFO] - loading train data for epoch 1
[2024-10-04 07:46:02,207][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-04 07:46:02,207][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 07:46:02,207][fairseq.utils][INFO] - rank   1: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 07:46:02,207][fairseq.utils][INFO] - rank   2: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 07:46:02,207][fairseq.utils][INFO] - rank   3: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 07:46:02,207][fairseq.utils][INFO] - rank   4: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 07:46:02,208][fairseq.utils][INFO] - rank   5: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 07:46:02,208][fairseq.utils][INFO] - rank   6: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 07:46:02,208][fairseq.utils][INFO] - rank   7: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 07:46:02,208][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-04 07:46:02,208][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-04 07:46:02,208][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-04 07:46:02,209][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-04 07:46:02,210][fairseq.trainer][INFO] - No existing checkpoint found /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-04 07:46:02,210][fairseq.trainer][INFO] - loading train data for epoch 1
[2024-10-04 07:46:06,105][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-04 07:46:07,573][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-04 07:46:14,076][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-04 07:46:14,076][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 07:46:14,076][fairseq.utils][INFO] - rank   1: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 07:46:14,076][fairseq.utils][INFO] - rank   2: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 07:46:14,076][fairseq.utils][INFO] - rank   3: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 07:46:14,076][fairseq.utils][INFO] - rank   4: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 07:46:14,076][fairseq.utils][INFO] - rank   5: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 07:46:14,076][fairseq.utils][INFO] - rank   6: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 07:46:14,077][fairseq.utils][INFO] - rank   7: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 07:46:14,077][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-04 07:46:14,077][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-04 07:46:14,077][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-04 07:46:14,078][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-04 07:46:14,078][fairseq.trainer][INFO] - No existing checkpoint found /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-04 07:46:14,078][fairseq.trainer][INFO] - loading train data for epoch 1
[2024-10-04 07:46:38,800][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-04 07:46:48,498][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-04 07:46:48,511][fairseq.trainer][INFO] - begin training epoch 1
[2024-10-04 07:46:48,511][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-04 07:46:59,222][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-04 07:46:59,229][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 07:46:59,230][fairseq.utils][INFO] - rank   1: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 07:46:59,230][fairseq.utils][INFO] - rank   2: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 07:46:59,230][fairseq.utils][INFO] - rank   3: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 07:46:59,230][fairseq.utils][INFO] - rank   4: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 07:46:59,230][fairseq.utils][INFO] - rank   5: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 07:46:59,230][fairseq.utils][INFO] - rank   6: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 07:46:59,230][fairseq.utils][INFO] - rank   7: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 07:46:59,230][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-04 07:46:59,230][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-04 07:46:59,231][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-04 07:46:59,232][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-04 07:46:59,232][fairseq.trainer][INFO] - No existing checkpoint found /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-04 07:46:59,232][fairseq.trainer][INFO] - loading train data for epoch 1
[2024-10-04 07:47:02,427][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-04 07:47:17,459][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-04 07:47:17,459][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 07:47:17,459][fairseq.utils][INFO] - rank   1: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 07:47:17,459][fairseq.utils][INFO] - rank   2: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 07:47:17,459][fairseq.utils][INFO] - rank   3: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 07:47:17,459][fairseq.utils][INFO] - rank   4: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 07:47:17,459][fairseq.utils][INFO] - rank   5: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 07:47:17,459][fairseq.utils][INFO] - rank   6: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 07:47:17,459][fairseq.utils][INFO] - rank   7: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 07:47:17,459][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-04 07:47:17,459][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-04 07:47:17,460][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-04 07:47:17,460][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-04 07:47:17,460][fairseq.trainer][INFO] - No existing checkpoint found /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-04 07:47:17,461][fairseq.trainer][INFO] - loading train data for epoch 1
[2024-10-04 07:47:21,663][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-04 07:47:24,423][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-04 07:47:24,423][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 07:47:24,424][fairseq.utils][INFO] - rank   1: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 07:47:24,424][fairseq.utils][INFO] - rank   2: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 07:47:24,424][fairseq.utils][INFO] - rank   3: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 07:47:24,424][fairseq.utils][INFO] - rank   4: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 07:47:24,424][fairseq.utils][INFO] - rank   5: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 07:47:24,424][fairseq.utils][INFO] - rank   6: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 07:47:24,424][fairseq.utils][INFO] - rank   7: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 07:47:24,424][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-04 07:47:24,424][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-04 07:47:24,424][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-04 07:47:24,425][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-04 07:47:24,425][fairseq.trainer][INFO] - No existing checkpoint found /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-04 07:47:24,425][fairseq.trainer][INFO] - loading train data for epoch 1
[2024-10-04 07:47:40,733][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-04 07:47:56,514][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-04 07:47:56,532][fairseq.trainer][INFO] - begin training epoch 1
[2024-10-04 07:47:56,533][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-04 07:48:38,559][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-04 07:48:38,559][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 07:48:38,559][fairseq.utils][INFO] - rank   1: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 07:48:38,559][fairseq.utils][INFO] - rank   2: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 07:48:38,559][fairseq.utils][INFO] - rank   3: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 07:48:38,559][fairseq.utils][INFO] - rank   4: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 07:48:38,559][fairseq.utils][INFO] - rank   5: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 07:48:38,559][fairseq.utils][INFO] - rank   6: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 07:48:38,560][fairseq.utils][INFO] - rank   7: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-04 07:48:38,560][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-04 07:48:38,560][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-04 07:48:38,560][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-04 07:48:38,561][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-04 07:48:38,561][fairseq.trainer][INFO] - No existing checkpoint found /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-04 07:48:38,561][fairseq.trainer][INFO] - loading train data for epoch 1
[2024-10-04 07:48:44,405][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-04 07:48:58,209][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-04 07:48:58,227][fairseq.trainer][INFO] - begin training epoch 1
[2024-10-04 07:48:58,234][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-04 07:49:03,118][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-04 07:49:03,137][fairseq.trainer][INFO] - begin training epoch 1
[2024-10-04 07:49:03,141][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-04 07:49:08,302][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-04 07:49:08,311][fairseq.trainer][INFO] - begin training epoch 1
[2024-10-04 07:49:08,311][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-04 07:49:19,054][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-04 07:49:19,059][fairseq.trainer][INFO] - begin training epoch 1
[2024-10-04 07:49:19,060][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-04 07:50:01,666][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-04 07:50:01,675][fairseq.trainer][INFO] - begin training epoch 1
[2024-10-04 07:50:01,675][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-04 07:50:07,174][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-04 07:50:07,177][fairseq.trainer][INFO] - begin training epoch 1
[2024-10-04 07:50:07,178][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-04 08:14:59,527][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
[2024-10-04 08:14:59,871][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
[2024-10-04 08:16:03,888][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
[2024-10-04 08:16:04,927][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
[2024-10-04 08:16:11,753][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
[2024-10-04 08:16:14,289][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
[2024-10-04 08:16:14,579][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
[2024-10-04 08:16:15,285][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
[2024-10-04 08:16:44,663][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
[2024-10-04 08:16:53,888][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
[2024-10-04 08:35:32,428][train_inner][INFO] - {"epoch": 1, "update": 0.428, "loss": "5.56", "ntokens": "358464", "nsentences": "1773.44", "wps": "61698.9", "ups": "0.17", "wpb": "358464", "bsz": "1773.4", "num_updates": "200", "lr": "3.125e-06", "gnorm": "0.888", "loss_scale": "4", "train_wall": "1191", "gb_free": "39.6", "wall": "2958"}
[2024-10-04 08:35:32,573][train_inner][INFO] - {"epoch": 1, "update": 0.428, "loss": "5.56", "ntokens": "358464", "nsentences": "1773.44", "wps": "61675.3", "ups": "0.17", "wpb": "358464", "bsz": "1773.4", "num_updates": "200", "lr": "3.125e-06", "gnorm": "0.888", "loss_scale": "4", "train_wall": "1212", "gb_free": "39.6", "wall": "2888"}
[2024-10-04 08:54:22,428][train_inner][INFO] - {"epoch": 1, "update": 0.846, "loss": "5.344", "ntokens": "358155", "nsentences": "1717.45", "wps": "63401.7", "ups": "0.18", "wpb": "358155", "bsz": "1717.5", "num_updates": "400", "lr": "6.25e-06", "gnorm": "0.719", "loss_scale": "4", "train_wall": "687", "gb_free": "40.1", "wall": "4088"}
[2024-10-04 08:54:24,025][train_inner][INFO] - {"epoch": 1, "update": 0.846, "loss": "5.344", "ntokens": "358155", "nsentences": "1717.45", "wps": "63309.3", "ups": "0.18", "wpb": "358155", "bsz": "1717.5", "num_updates": "400", "lr": "6.25e-06", "gnorm": "0.719", "loss_scale": "4", "train_wall": "694", "gb_free": "40.1", "wall": "4020"}
[2024-10-04 08:59:27,571][fairseq_cli.train][INFO] - end of epoch 1 (average epoch stats below)
[2024-10-04 08:59:28,092][train][INFO] - {"epoch": 1, "train_loss": "5.392", "train_ntokens": "357663", "train_nsentences": "1751.18", "train_wps": "65272.8", "train_ups": "0.18", "train_wpb": "357663", "train_bsz": "1751.2", "train_num_updates": "474", "train_lr": "7.40625e-06", "train_gnorm": "0.888", "train_loss_scale": "4", "train_train_wall": "2180", "train_gb_free": "39.6", "train_wall": "4394"}
[2024-10-04 08:59:29,295][fairseq_cli.train][INFO] - end of epoch 1 (average epoch stats below)
[2024-10-04 08:59:29,896][train][INFO] - {"epoch": 1, "train_loss": "5.392", "train_ntokens": "357663", "train_nsentences": "1751.18", "train_wps": "65230", "train_ups": "0.18", "train_wpb": "357663", "train_bsz": "1751.2", "train_num_updates": "474", "train_lr": "7.40625e-06", "train_gnorm": "0.888", "train_loss_scale": "4", "train_train_wall": "2207", "train_gb_free": "39.6", "train_wall": "4325"}
[2024-10-04 08:59:30,939][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-04 08:59:31,061][fairseq.trainer][INFO] - begin training epoch 2
[2024-10-04 08:59:31,070][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-04 08:59:31,067][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-04 08:59:31,111][fairseq.trainer][INFO] - begin training epoch 2
[2024-10-04 08:59:31,130][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-04 09:23:11,278][train_inner][INFO] - {"epoch": 2, "update": 1.263, "loss": "4.942", "ntokens": "356827", "nsentences": "1773.64", "wps": "41318.3", "ups": "0.12", "wpb": "356827", "bsz": "1773.6", "num_updates": "600", "lr": "9.375e-06", "gnorm": "1.725", "loss_scale": "4", "train_wall": "1314", "gb_free": "39.2", "wall": "5747"}
[2024-10-04 09:23:11,556][train_inner][INFO] - {"epoch": 2, "update": 1.263, "loss": "4.942", "ntokens": "356827", "nsentences": "1773.64", "wps": "41274", "ups": "0.12", "wpb": "356827", "bsz": "1773.6", "num_updates": "600", "lr": "9.375e-06", "gnorm": "1.725", "loss_scale": "4", "train_wall": "1221", "gb_free": "39.2", "wall": "5817"}
[2024-10-04 09:28:00,392][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2024-10-04 09:28:08,266][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2024-10-04 09:44:34,622][train_inner][INFO] - {"epoch": 2, "update": 1.683, "loss": "4.624", "ntokens": "358220", "nsentences": "1755.14", "wps": "55830.7", "ups": "0.16", "wpb": "358220", "bsz": "1755.1", "num_updates": "800", "lr": "1.25e-05", "gnorm": "2.548", "loss_scale": "2", "train_wall": "592", "gb_free": "40.3", "wall": "7030"}
[2024-10-04 09:44:35,709][train_inner][INFO] - {"epoch": 2, "update": 1.683, "loss": "4.624", "ntokens": "358220", "nsentences": "1755.14", "wps": "55791.9", "ups": "0.16", "wpb": "358220", "bsz": "1755.1", "num_updates": "800", "lr": "1.25e-05", "gnorm": "2.548", "loss_scale": "2", "train_wall": "602", "gb_free": "40.3", "wall": "7102"}
[2024-10-04 09:49:44,115][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
[2024-10-04 09:49:57,143][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
[2024-10-04 10:01:20,971][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 2 @ 951 updates
[2024-10-04 10:01:20,974][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-04 10:01:21,242][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 2 @ 951 updates
[2024-10-04 10:01:21,243][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-04 10:01:25,594][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-04 10:01:25,598][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt (epoch 2 @ 951 updates, score None) (writing took 4.627084939740598 seconds)
[2024-10-04 10:01:25,599][fairseq_cli.train][INFO] - end of epoch 2 (average epoch stats below)
[2024-10-04 10:01:25,634][train][INFO] - {"epoch": 2, "train_loss": "4.622", "train_ntokens": "357670", "train_nsentences": "1753.18", "train_wps": "45893.3", "train_ups": "0.13", "train_wpb": "357670", "train_bsz": "1753.2", "train_num_updates": "951", "train_lr": "1.48594e-05", "train_gnorm": "2.605", "train_loss_scale": "1", "train_train_wall": "2421", "train_gb_free": "39.3", "train_wall": "8112"}
[2024-10-04 10:01:25,729][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-04 10:01:25,782][fairseq.trainer][INFO] - begin training epoch 3
[2024-10-04 10:01:25,783][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-04 10:14:12,768][train_inner][INFO] - {"epoch": 3, "update": 2.102, "loss": "4.388", "ntokens": "356879", "nsentences": "1786", "wps": "40165.3", "ups": "0.11", "wpb": "356879", "bsz": "1786", "num_updates": "1000", "lr": "1.5625e-05", "gnorm": "3.349", "loss_scale": "1", "train_wall": "1255", "gb_free": "39.8", "wall": "8879"}
[2024-10-04 10:38:35,221][train_inner][INFO] - {"epoch": 3, "update": 2.52, "loss": "4.194", "ntokens": "358234", "nsentences": "1723.83", "wps": "48991.6", "ups": "0.14", "wpb": "358234", "bsz": "1723.8", "num_updates": "1200", "lr": "1.875e-05", "gnorm": "3.439", "loss_scale": "1", "train_wall": "1331", "gb_free": "39.6", "wall": "10341"}
[2024-10-04 11:04:49,340][train_inner][INFO] - {"epoch": 3, "update": 2.937, "loss": "4.045", "ntokens": "358413", "nsentences": "1737.67", "wps": "45539.9", "ups": "0.13", "wpb": "358413", "bsz": "1737.7", "num_updates": "1400", "lr": "2.1875e-05", "gnorm": "3.568", "loss_scale": "1", "train_wall": "1572", "gb_free": "39.8", "wall": "11915"}
[2024-10-04 11:06:29,695][fairseq_cli.train][INFO] - end of epoch 3 (average epoch stats below)
[2024-10-04 11:06:29,747][train][INFO] - {"epoch": 3, "train_loss": "4.131", "train_ntokens": "357674", "train_nsentences": "1753.71", "train_wps": "43883.5", "train_ups": "0.12", "train_wpb": "357674", "train_bsz": "1753.7", "train_num_updates": "1430", "train_lr": "2.23438e-05", "train_gnorm": "3.539", "train_loss_scale": "1", "train_train_wall": "3356", "train_gb_free": "39.3", "train_wall": "12016"}
[2024-10-04 11:06:33,998][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-04 11:06:34,104][fairseq.trainer][INFO] - begin training epoch 4
[2024-10-04 11:06:34,104][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-04 11:16:48,494][train_inner][INFO] - {"epoch": 4, "update": 3.355, "loss": "3.94", "ntokens": "356630", "nsentences": "1808.34", "wps": "99186.4", "ups": "0.28", "wpb": "356630", "bsz": "1808.3", "num_updates": "1600", "lr": "2.5e-05", "gnorm": "3.717", "loss_scale": "1", "train_wall": "381", "gb_free": "39.3", "wall": "12634"}
[2024-10-04 11:21:14,639][train_inner][INFO] - {"epoch": 4, "update": 3.772, "loss": "3.838", "ntokens": "358352", "nsentences": "1722.77", "wps": "269372", "ups": "0.75", "wpb": "358352", "bsz": "1722.8", "num_updates": "1800", "lr": "2.8125e-05", "gnorm": "3.992", "loss_scale": "1", "train_wall": "203", "gb_free": "39.7", "wall": "12901"}
[2024-10-04 11:24:15,543][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 4 @ 1909 updates
[2024-10-04 11:24:15,550][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-04 11:24:19,709][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-04 11:24:19,712][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt (epoch 4 @ 1909 updates, score None) (writing took 4.168298514559865 seconds)
[2024-10-04 11:24:19,712][fairseq_cli.train][INFO] - end of epoch 4 (average epoch stats below)
[2024-10-04 11:24:19,714][train][INFO] - {"epoch": 4, "train_loss": "3.858", "train_ntokens": "357674", "train_nsentences": "1753.71", "train_wps": "160123", "train_ups": "0.45", "train_wpb": "357674", "train_bsz": "1753.7", "train_num_updates": "1909", "train_lr": "2.98281e-05", "train_gnorm": "3.884", "train_loss_scale": "1", "train_train_wall": "646", "train_gb_free": "40.6", "train_wall": "13086"}
[2024-10-04 11:24:19,917][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-04 11:24:19,950][fairseq.trainer][INFO] - begin training epoch 5
[2024-10-04 11:24:19,951][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-04 11:32:29,885][train_inner][INFO] - {"epoch": 5, "update": 4.19, "loss": "3.765", "ntokens": "356818", "nsentences": "1741.66", "wps": "105700", "ups": "0.3", "wpb": "356818", "bsz": "1741.7", "num_updates": "2000", "lr": "3.125e-05", "gnorm": "3.876", "loss_scale": "1", "train_wall": "320", "gb_free": "39.8", "wall": "13576"}
[2024-10-04 11:33:27,839][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.5
[2024-10-04 11:37:31,849][train_inner][INFO] - {"epoch": 5, "update": 4.61, "loss": "3.695", "ntokens": "358272", "nsentences": "1791.33", "wps": "237341", "ups": "0.66", "wpb": "358272", "bsz": "1791.3", "num_updates": "2200", "lr": "3.4375e-05", "gnorm": "3.961", "loss_scale": "0.5", "train_wall": "289", "gb_free": "40.3", "wall": "13878"}
[2024-10-04 11:41:32,162][fairseq_cli.train][INFO] - end of epoch 5 (average epoch stats below)
[2024-10-04 11:41:32,215][train][INFO] - {"epoch": 5, "train_loss": "3.684", "train_ntokens": "357673", "train_nsentences": "1754.28", "train_wps": "165588", "train_ups": "0.46", "train_wpb": "357673", "train_bsz": "1754.3", "train_num_updates": "2387", "train_lr": "3.72969e-05", "train_gnorm": "3.88", "train_loss_scale": "0.5", "train_train_wall": "679", "train_gb_free": "39.7", "train_wall": "14118"}
[2024-10-04 11:41:33,214][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-04 11:41:33,247][fairseq.trainer][INFO] - begin training epoch 6
[2024-10-04 11:41:33,248][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-04 11:48:15,062][train_inner][INFO] - {"epoch": 6, "update": 5.027, "loss": "3.638", "ntokens": "356856", "nsentences": "1709.74", "wps": "110963", "ups": "0.31", "wpb": "356856", "bsz": "1709.7", "num_updates": "2400", "lr": "3.75e-05", "gnorm": "3.897", "loss_scale": "0.5", "train_wall": "312", "gb_free": "39.8", "wall": "14521"}
[2024-10-04 11:52:29,657][train_inner][INFO] - {"epoch": 6, "update": 5.445, "loss": "3.592", "ntokens": "358289", "nsentences": "1762.36", "wps": "281464", "ups": "0.79", "wpb": "358289", "bsz": "1762.4", "num_updates": "2600", "lr": "4.0625e-05", "gnorm": "3.893", "loss_scale": "0.5", "train_wall": "227", "gb_free": "39.6", "wall": "14776"}
[2024-10-04 11:57:40,562][train_inner][INFO] - {"epoch": 6, "update": 5.862, "loss": "3.556", "ntokens": "358290", "nsentences": "1760.05", "wps": "230509", "ups": "0.64", "wpb": "358290", "bsz": "1760", "num_updates": "2800", "lr": "4.375e-05", "gnorm": "3.885", "loss_scale": "0.5", "train_wall": "297", "gb_free": "39.6", "wall": "15086"}
[2024-10-04 11:59:36,052][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 6 @ 2866 updates
[2024-10-04 11:59:36,056][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-04 11:59:40,352][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-04 11:59:40,356][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt (epoch 6 @ 2866 updates, score None) (writing took 4.303870612755418 seconds)
[2024-10-04 11:59:40,356][fairseq_cli.train][INFO] - end of epoch 6 (average epoch stats below)
[2024-10-04 11:59:40,359][train][INFO] - {"epoch": 6, "train_loss": "3.567", "train_ntokens": "357674", "train_nsentences": "1753.71", "train_wps": "157448", "train_ups": "0.44", "train_wpb": "357674", "train_bsz": "1753.7", "train_num_updates": "2866", "train_lr": "4.47813e-05", "train_gnorm": "3.891", "train_loss_scale": "0.5", "train_train_wall": "720", "train_gb_free": "39.4", "train_wall": "15206"}
[2024-10-04 11:59:40,475][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-04 11:59:40,482][fairseq.trainer][INFO] - begin training epoch 7
[2024-10-04 11:59:40,483][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-04 12:08:22,780][train_inner][INFO] - {"epoch": 7, "update": 6.28, "loss": "3.507", "ntokens": "356906", "nsentences": "1735.95", "wps": "111152", "ups": "0.31", "wpb": "356906", "bsz": "1736", "num_updates": "3000", "lr": "4.6875e-05", "gnorm": "3.665", "loss_scale": "0.5", "train_wall": "307", "gb_free": "39.3", "wall": "15729"}
[2024-10-04 12:13:04,443][train_inner][INFO] - {"epoch": 7, "update": 6.697, "loss": "3.47", "ntokens": "358379", "nsentences": "1716.22", "wps": "254494", "ups": "0.71", "wpb": "358379", "bsz": "1716.2", "num_updates": "3200", "lr": "5e-05", "gnorm": "3.69", "loss_scale": "0.5", "train_wall": "278", "gb_free": "40.6", "wall": "16010"}
[2024-10-04 12:16:55,033][fairseq_cli.train][INFO] - end of epoch 7 (average epoch stats below)
[2024-10-04 12:16:55,054][train][INFO] - {"epoch": 7, "train_loss": "3.472", "train_ntokens": "357674", "train_nsentences": "1753.71", "train_wps": "165582", "train_ups": "0.46", "train_wpb": "357674", "train_bsz": "1753.7", "train_num_updates": "3345", "train_lr": "5.22656e-05", "train_gnorm": "3.667", "train_loss_scale": "0.5", "train_train_wall": "698", "train_gb_free": "39.8", "train_wall": "16241"}
[2024-10-04 12:16:55,394][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-04 12:16:55,412][fairseq.trainer][INFO] - begin training epoch 8
[2024-10-04 12:16:55,413][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-04 12:24:01,267][train_inner][INFO] - {"epoch": 8, "update": 7.115, "loss": "3.443", "ntokens": "356607", "nsentences": "1779.12", "wps": "108586", "ups": "0.3", "wpb": "356607", "bsz": "1779.1", "num_updates": "3400", "lr": "5.3125e-05", "gnorm": "3.696", "loss_scale": "0.5", "train_wall": "339", "gb_free": "39.3", "wall": "16667"}
[2024-10-04 12:28:37,914][train_inner][INFO] - {"epoch": 8, "update": 7.532, "loss": "3.406", "ntokens": "358287", "nsentences": "1741.72", "wps": "259039", "ups": "0.72", "wpb": "358287", "bsz": "1741.7", "num_updates": "3600", "lr": "5.625e-05", "gnorm": "3.472", "loss_scale": "0.5", "train_wall": "273", "gb_free": "39.6", "wall": "16944"}
[2024-10-04 12:33:49,405][train_inner][INFO] - {"epoch": 8, "update": 7.95, "loss": "3.384", "ntokens": "358372", "nsentences": "1756.68", "wps": "230104", "ups": "0.64", "wpb": "358372", "bsz": "1756.7", "num_updates": "3800", "lr": "5.9375e-05", "gnorm": "3.565", "loss_scale": "0.5", "train_wall": "308", "gb_free": "39.3", "wall": "17255"}
[2024-10-04 12:34:43,099][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 8 @ 3824 updates
[2024-10-04 12:34:43,100][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-04 12:34:49,827][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-04 12:34:49,833][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt (epoch 8 @ 3824 updates, score None) (writing took 6.734304482117295 seconds)
[2024-10-04 12:34:49,834][fairseq_cli.train][INFO] - end of epoch 8 (average epoch stats below)
[2024-10-04 12:34:49,836][train][INFO] - {"epoch": 8, "train_loss": "3.4", "train_ntokens": "357674", "train_nsentences": "1753.71", "train_wps": "159406", "train_ups": "0.45", "train_wpb": "357674", "train_bsz": "1753.7", "train_num_updates": "3824", "train_lr": "5.975e-05", "train_gnorm": "3.531", "train_loss_scale": "0.5", "train_train_wall": "743", "train_gb_free": "39.3", "train_wall": "17316"}
[2024-10-04 12:34:49,905][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-04 12:34:49,933][fairseq.trainer][INFO] - begin training epoch 9
[2024-10-04 12:34:49,934][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-04 12:44:59,556][train_inner][INFO] - {"epoch": 9, "update": 8.367, "loss": "3.36", "ntokens": "356659", "nsentences": "1784.45", "wps": "106442", "ups": "0.3", "wpb": "356659", "bsz": "1784.5", "num_updates": "4000", "lr": "6.25e-05", "gnorm": "3.377", "loss_scale": "0.5", "train_wall": "327", "gb_free": "39.6", "wall": "17925"}
[2024-10-04 12:50:11,960][train_inner][INFO] - {"epoch": 9, "update": 8.785, "loss": "3.331", "ntokens": "358416", "nsentences": "1760.65", "wps": "229459", "ups": "0.64", "wpb": "358416", "bsz": "1760.7", "num_updates": "4200", "lr": "6.5625e-05", "gnorm": "3.273", "loss_scale": "1", "train_wall": "310", "gb_free": "39.1", "wall": "18238"}
[2024-10-04 12:53:01,887][fairseq_cli.train][INFO] - end of epoch 9 (average epoch stats below)
[2024-10-04 12:53:01,897][train][INFO] - {"epoch": 9, "train_loss": "3.334", "train_ntokens": "357674", "train_nsentences": "1753.71", "train_wps": "156884", "train_ups": "0.44", "train_wpb": "357674", "train_bsz": "1753.7", "train_num_updates": "4303", "train_lr": "6.72344e-05", "train_gnorm": "3.295", "train_loss_scale": "1", "train_train_wall": "754", "train_gb_free": "39.6", "train_wall": "18408"}
[2024-10-04 12:53:02,068][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-04 12:53:02,074][fairseq.trainer][INFO] - begin training epoch 10
[2024-10-04 12:53:02,075][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-04 13:00:56,027][train_inner][INFO] - {"epoch": 10, "update": 9.203, "loss": "3.305", "ntokens": "356853", "nsentences": "1720.38", "wps": "110816", "ups": "0.31", "wpb": "356853", "bsz": "1720.4", "num_updates": "4400", "lr": "6.875e-05", "gnorm": "3.298", "loss_scale": "1", "train_wall": "336", "gb_free": "40.1", "wall": "18882"}
[2024-10-04 13:05:36,212][train_inner][INFO] - {"epoch": 10, "update": 9.62, "loss": "3.281", "ntokens": "358394", "nsentences": "1745.93", "wps": "255839", "ups": "0.71", "wpb": "358394", "bsz": "1745.9", "num_updates": "4600", "lr": "7.1875e-05", "gnorm": "3.2", "loss_scale": "1", "train_wall": "276", "gb_free": "39.4", "wall": "19162"}
[2024-10-04 13:09:53,123][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 10 @ 4782 updates
[2024-10-04 13:09:53,136][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-04 13:10:07,254][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-04 13:10:07,575][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt (epoch 10 @ 4782 updates, score None) (writing took 14.451917222701013 seconds)
[2024-10-04 13:10:07,576][fairseq_cli.train][INFO] - end of epoch 10 (average epoch stats below)
[2024-10-04 13:10:07,578][train][INFO] - {"epoch": 10, "train_loss": "3.28", "train_ntokens": "357674", "train_nsentences": "1753.71", "train_wps": "167036", "train_ups": "0.47", "train_wpb": "357674", "train_bsz": "1753.7", "train_num_updates": "4782", "train_lr": "7.47187e-05", "train_gnorm": "3.219", "train_loss_scale": "1", "train_train_wall": "695", "train_gb_free": "40.1", "train_wall": "19433"}
[2024-10-04 13:10:07,711][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-04 13:10:07,746][fairseq.trainer][INFO] - begin training epoch 11
[2024-10-04 13:10:07,746][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-04 13:16:47,745][train_inner][INFO] - {"epoch": 11, "update": 10.038, "loss": "3.267", "ntokens": "356640", "nsentences": "1779.87", "wps": "106217", "ups": "0.3", "wpb": "356640", "bsz": "1779.9", "num_updates": "4800", "lr": "7.5e-05", "gnorm": "3.119", "loss_scale": "1", "train_wall": "342", "gb_free": "40.1", "wall": "19834"}
[2024-10-04 13:21:00,812][train_inner][INFO] - {"epoch": 11, "update": 10.455, "loss": "3.238", "ntokens": "358385", "nsentences": "1785.12", "wps": "283239", "ups": "0.79", "wpb": "358385", "bsz": "1785.1", "num_updates": "5000", "lr": "7.8125e-05", "gnorm": "3.045", "loss_scale": "1", "train_wall": "250", "gb_free": "40.5", "wall": "20087"}
[2024-10-04 13:26:03,739][train_inner][INFO] - {"epoch": 11, "update": 10.873, "loss": "3.219", "ntokens": "358228", "nsentences": "1723.88", "wps": "236523", "ups": "0.66", "wpb": "358228", "bsz": "1723.9", "num_updates": "5200", "lr": "8.125e-05", "gnorm": "3.069", "loss_scale": "1", "train_wall": "300", "gb_free": "40.1", "wall": "20390"}
[2024-10-04 13:27:33,681][fairseq_cli.train][INFO] - end of epoch 11 (average epoch stats below)
[2024-10-04 13:27:33,723][train][INFO] - {"epoch": 11, "train_loss": "3.226", "train_ntokens": "357674", "train_nsentences": "1753.71", "train_wps": "163769", "train_ups": "0.46", "train_wpb": "357674", "train_bsz": "1753.7", "train_num_updates": "5261", "train_lr": "8.22031e-05", "train_gnorm": "3.025", "train_loss_scale": "1", "train_train_wall": "728", "train_gb_free": "39.3", "train_wall": "20480"}
[2024-10-04 13:27:34,109][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-04 13:27:34,140][fairseq.trainer][INFO] - begin training epoch 12
[2024-10-04 13:27:34,141][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-04 13:36:39,623][train_inner][INFO] - {"epoch": 12, "update": 11.29, "loss": "3.189", "ntokens": "356811", "nsentences": "1717.66", "wps": "112228", "ups": "0.31", "wpb": "356811", "bsz": "1717.7", "num_updates": "5400", "lr": "8.4375e-05", "gnorm": "2.912", "loss_scale": "1", "train_wall": "273", "gb_free": "39.2", "wall": "21026"}
[2024-10-04 13:40:58,076][train_inner][INFO] - {"epoch": 12, "update": 11.708, "loss": "3.183", "ntokens": "358458", "nsentences": "1755.53", "wps": "277393", "ups": "0.77", "wpb": "358458", "bsz": "1755.5", "num_updates": "5600", "lr": "8.75e-05", "gnorm": "2.982", "loss_scale": "1", "train_wall": "255", "gb_free": "39.6", "wall": "21284"}
[2024-10-04 13:41:16,541][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.5
[2024-10-04 13:44:16,497][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 12 @ 5739 updates
[2024-10-04 13:44:16,512][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-04 13:44:28,969][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-04 13:44:31,163][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt (epoch 12 @ 5739 updates, score None) (writing took 14.666160319000483 seconds)
[2024-10-04 13:44:31,170][fairseq_cli.train][INFO] - end of epoch 12 (average epoch stats below)
[2024-10-04 13:44:31,172][train][INFO] - {"epoch": 12, "train_loss": "3.18", "train_ntokens": "357673", "train_nsentences": "1753.2", "train_wps": "168036", "train_ups": "0.47", "train_wpb": "357673", "train_bsz": "1753.2", "train_num_updates": "5739", "train_lr": "8.96719e-05", "train_gnorm": "2.958", "train_loss_scale": "0.5", "train_train_wall": "634", "train_gb_free": "39.9", "train_wall": "21497"}
[2024-10-04 13:44:31,287][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-04 13:44:31,323][fairseq.trainer][INFO] - begin training epoch 13
[2024-10-04 13:44:31,324][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-04 13:51:39,830][train_inner][INFO] - {"epoch": 13, "update": 12.127, "loss": "3.169", "ntokens": "356549", "nsentences": "1792.76", "wps": "111119", "ups": "0.31", "wpb": "356549", "bsz": "1792.8", "num_updates": "5800", "lr": "9.0625e-05", "gnorm": "2.903", "loss_scale": "0.5", "train_wall": "303", "gb_free": "40.6", "wall": "21926"}
[2024-10-04 13:56:21,632][train_inner][INFO] - {"epoch": 13, "update": 12.545, "loss": "3.131", "ntokens": "358402", "nsentences": "1745.71", "wps": "254390", "ups": "0.71", "wpb": "358402", "bsz": "1745.7", "num_updates": "6000", "lr": "9.375e-05", "gnorm": "2.813", "loss_scale": "0.5", "train_wall": "277", "gb_free": "39.6", "wall": "22208"}
[2024-10-04 14:00:27,685][train_inner][INFO] - {"epoch": 13, "update": 12.962, "loss": "3.126", "ntokens": "358284", "nsentences": "1767.87", "wps": "291254", "ups": "0.81", "wpb": "358284", "bsz": "1767.9", "num_updates": "6200", "lr": "9.6875e-05", "gnorm": "2.747", "loss_scale": "0.5", "train_wall": "240", "gb_free": "39.6", "wall": "22454"}
[2024-10-04 14:00:49,958][fairseq_cli.train][INFO] - end of epoch 13 (average epoch stats below)
[2024-10-04 14:00:49,982][train][INFO] - {"epoch": 13, "train_loss": "3.133", "train_ntokens": "357674", "train_nsentences": "1753.71", "train_wps": "175039", "train_ups": "0.49", "train_wpb": "357674", "train_bsz": "1753.7", "train_num_updates": "6218", "train_lr": "9.71563e-05", "train_gnorm": "2.798", "train_loss_scale": "0.5", "train_train_wall": "648", "train_gb_free": "39.8", "train_wall": "22476"}
[2024-10-04 14:00:50,198][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-04 14:00:50,214][fairseq.trainer][INFO] - begin training epoch 14
[2024-10-04 14:00:50,214][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-04 14:11:03,759][train_inner][INFO] - {"epoch": 14, "update": 13.38, "loss": "3.099", "ntokens": "356800", "nsentences": "1742.93", "wps": "112190", "ups": "0.31", "wpb": "356800", "bsz": "1742.9", "num_updates": "6400", "lr": "0.0001", "gnorm": "2.778", "loss_scale": "0.5", "train_wall": "264", "gb_free": "39.6", "wall": "23090"}
[2024-10-04 14:15:28,586][train_inner][INFO] - {"epoch": 14, "update": 13.797, "loss": "3.093", "ntokens": "358304", "nsentences": "1769.07", "wps": "270599", "ups": "0.76", "wpb": "358304", "bsz": "1769.1", "num_updates": "6600", "lr": "0.000103125", "gnorm": "2.7", "loss_scale": "0.5", "train_wall": "261", "gb_free": "40.1", "wall": "23355"}
[2024-10-04 14:17:03,112][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 14 @ 6697 updates
[2024-10-04 14:17:03,118][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-04 14:17:17,718][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-04 14:17:18,282][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt (epoch 14 @ 6697 updates, score None) (writing took 15.17024810705334 seconds)
[2024-10-04 14:17:18,290][fairseq_cli.train][INFO] - end of epoch 14 (average epoch stats below)
[2024-10-04 14:17:18,298][train][INFO] - {"epoch": 14, "train_loss": "3.092", "train_ntokens": "357674", "train_nsentences": "1753.71", "train_wps": "173353", "train_ups": "0.48", "train_wpb": "357674", "train_bsz": "1753.7", "train_num_updates": "6697", "train_lr": "0.000104641", "train_gnorm": "2.689", "train_loss_scale": "0.5", "train_train_wall": "596", "train_gb_free": "39.1", "train_wall": "23464"}
[2024-10-04 14:17:18,644][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-04 14:17:18,690][fairseq.trainer][INFO] - begin training epoch 15
[2024-10-04 14:17:18,690][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-04 14:25:40,583][train_inner][INFO] - {"epoch": 15, "update": 14.215, "loss": "3.069", "ntokens": "356675", "nsentences": "1725.01", "wps": "116562", "ups": "0.33", "wpb": "356675", "bsz": "1725", "num_updates": "6800", "lr": "0.00010625", "gnorm": "2.61", "loss_scale": "0.5", "train_wall": "267", "gb_free": "39.7", "wall": "23967"}
[2024-10-04 14:30:03,286][train_inner][INFO] - {"epoch": 15, "update": 14.633, "loss": "3.058", "ntokens": "358362", "nsentences": "1774.79", "wps": "272835", "ups": "0.76", "wpb": "358362", "bsz": "1774.8", "num_updates": "7000", "lr": "0.000109375", "gnorm": "2.558", "loss_scale": "0.5", "train_wall": "258", "gb_free": "39.8", "wall": "24229"}
[2024-10-04 14:33:47,900][fairseq_cli.train][INFO] - end of epoch 15 (average epoch stats below)
[2024-10-04 14:33:47,942][train][INFO] - {"epoch": 15, "train_loss": "3.053", "train_ntokens": "357674", "train_nsentences": "1753.71", "train_wps": "173120", "train_ups": "0.48", "train_wpb": "357674", "train_bsz": "1753.7", "train_num_updates": "7176", "train_lr": "0.000112125", "train_gnorm": "2.583", "train_loss_scale": "0.5", "train_train_wall": "651", "train_gb_free": "40.5", "train_wall": "24454"}
[2024-10-04 14:33:48,543][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-04 14:33:48,577][fairseq.trainer][INFO] - begin training epoch 16
[2024-10-04 14:33:48,578][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-04 14:40:32,932][train_inner][INFO] - {"epoch": 16, "update": 15.05, "loss": "3.041", "ntokens": "356697", "nsentences": "1740.59", "wps": "113303", "ups": "0.32", "wpb": "356697", "bsz": "1740.6", "num_updates": "7200", "lr": "0.0001125", "gnorm": "2.572", "loss_scale": "0.5", "train_wall": "295", "gb_free": "39.8", "wall": "24859"}
[2024-10-04 14:44:54,585][train_inner][INFO] - {"epoch": 16, "update": 15.468, "loss": "3.016", "ntokens": "358471", "nsentences": "1743.61", "wps": "274027", "ups": "0.76", "wpb": "358471", "bsz": "1743.6", "num_updates": "7400", "lr": "0.000115625", "gnorm": "2.489", "loss_scale": "0.5", "train_wall": "259", "gb_free": "39.6", "wall": "25121"}
[2024-10-04 14:49:24,535][train_inner][INFO] - {"epoch": 16, "update": 15.885, "loss": "3.019", "ntokens": "358318", "nsentences": "1769.42", "wps": "265477", "ups": "0.74", "wpb": "358318", "bsz": "1769.4", "num_updates": "7600", "lr": "0.00011875", "gnorm": "2.468", "loss_scale": "0.5", "train_wall": "266", "gb_free": "39.6", "wall": "25390"}
[2024-10-04 14:50:23,618][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 16 @ 7655 updates
[2024-10-04 14:50:23,619][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-04 14:50:39,261][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-04 14:50:39,838][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt (epoch 16 @ 7655 updates, score None) (writing took 16.220056219026446 seconds)
[2024-10-04 14:50:39,849][fairseq_cli.train][INFO] - end of epoch 16 (average epoch stats below)
[2024-10-04 14:50:39,852][train][INFO] - {"epoch": 16, "train_loss": "3.017", "train_ntokens": "357674", "train_nsentences": "1753.71", "train_wps": "169311", "train_ups": "0.47", "train_wpb": "357674", "train_bsz": "1753.7", "train_num_updates": "7655", "train_lr": "0.000119609", "train_gnorm": "2.475", "train_loss_scale": "0.5", "train_train_wall": "659", "train_gb_free": "39.8", "train_wall": "25466"}
[2024-10-04 14:50:40,010][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-04 14:50:40,037][fairseq.trainer][INFO] - begin training epoch 17
[2024-10-04 14:50:40,038][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-04 15:00:35,304][train_inner][INFO] - {"epoch": 17, "update": 16.303, "loss": "2.992", "ntokens": "356843", "nsentences": "1749.96", "wps": "106399", "ups": "0.3", "wpb": "356843", "bsz": "1750", "num_updates": "7800", "lr": "0.000121875", "gnorm": "2.352", "loss_scale": "1", "train_wall": "275", "gb_free": "39.6", "wall": "26061"}
[2024-10-04 15:04:45,886][train_inner][INFO] - {"epoch": 17, "update": 16.72, "loss": "2.982", "ntokens": "358275", "nsentences": "1743.18", "wps": "285985", "ups": "0.8", "wpb": "358274", "bsz": "1743.2", "num_updates": "8000", "lr": "0.000125", "gnorm": "2.448", "loss_scale": "1", "train_wall": "247", "gb_free": "39.1", "wall": "26312"}
[2024-10-04 15:07:49,525][fairseq_cli.train][INFO] - end of epoch 17 (average epoch stats below)
[2024-10-04 15:07:49,544][train][INFO] - {"epoch": 17, "train_loss": "2.981", "train_ntokens": "357674", "train_nsentences": "1753.71", "train_wps": "166386", "train_ups": "0.47", "train_wpb": "357674", "train_bsz": "1753.7", "train_num_updates": "8134", "train_lr": "0.000127094", "train_gnorm": "2.381", "train_loss_scale": "1", "train_train_wall": "645", "train_gb_free": "39.2", "train_wall": "26495"}
[2024-10-04 15:07:49,732][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-04 15:07:49,742][fairseq.trainer][INFO] - begin training epoch 18
[2024-10-04 15:07:49,743][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-04 15:15:33,600][train_inner][INFO] - {"epoch": 18, "update": 17.138, "loss": "2.966", "ntokens": "356763", "nsentences": "1746.17", "wps": "110161", "ups": "0.31", "wpb": "356762", "bsz": "1746.2", "num_updates": "8200", "lr": "0.000128125", "gnorm": "2.303", "loss_scale": "1", "train_wall": "284", "gb_free": "39.4", "wall": "26960"}
[2024-10-04 15:20:40,346][train_inner][INFO] - {"epoch": 18, "update": 17.555, "loss": "2.962", "ntokens": "358193", "nsentences": "1784", "wps": "233558", "ups": "0.65", "wpb": "358193", "bsz": "1784", "num_updates": "8400", "lr": "0.00013125", "gnorm": "2.359", "loss_scale": "1", "train_wall": "303", "gb_free": "40.3", "wall": "27266"}
[2024-10-04 15:24:35,380][train_inner][INFO] - {"epoch": 18, "update": 17.973, "loss": "2.95", "ntokens": "358437", "nsentences": "1748.78", "wps": "305061", "ups": "0.85", "wpb": "358437", "bsz": "1748.8", "num_updates": "8600", "lr": "0.000134375", "gnorm": "2.257", "loss_scale": "1", "train_wall": "231", "gb_free": "39.6", "wall": "27501"}
[2024-10-04 15:24:55,747][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 18 @ 8613 updates
[2024-10-04 15:24:55,748][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-04 15:25:09,959][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt
[2024-10-04 15:25:10,490][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_channel_400/ckpt/checkpoint_last.pt (epoch 18 @ 8613 updates, score None) (writing took 14.743220242671669 seconds)
[2024-10-04 15:25:10,491][fairseq_cli.train][INFO] - end of epoch 18 (average epoch stats below)
[2024-10-04 15:25:10,506][train][INFO] - {"epoch": 18, "train_loss": "2.954", "train_ntokens": "357674", "train_nsentences": "1753.71", "train_wps": "164586", "train_ups": "0.46", "train_wpb": "357674", "train_bsz": "1753.7", "train_num_updates": "8613", "train_lr": "0.000134578", "train_gnorm": "2.302", "train_loss_scale": "1", "train_train_wall": "656", "train_gb_free": "39.8", "train_wall": "27536"}
[2024-10-04 15:25:10,672][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-04 15:25:10,697][fairseq.trainer][INFO] - begin training epoch 19
[2024-10-04 15:25:10,698][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-04 15:36:15,274][train_inner][INFO] - {"epoch": 19, "update": 18.39, "loss": "2.929", "ntokens": "356886", "nsentences": "1745.22", "wps": "101984", "ups": "0.29", "wpb": "356886", "bsz": "1745.2", "num_updates": "8800", "lr": "0.0001375", "gnorm": "2.266", "loss_scale": "1", "train_wall": "299", "gb_free": "39.2", "wall": "28201"}
