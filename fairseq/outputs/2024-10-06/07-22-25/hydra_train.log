[2024-10-06 07:22:57,688][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:15839', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-06 07:22:58,600][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:19698', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-06 07:22:58,682][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:16526', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-06 07:23:01,067][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:16342', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-06 07:23:01,144][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:15846', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-06 07:23:02,153][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-06 07:23:02,159][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-06 07:23:02,159][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-06 07:23:02,159][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-06 07:23:02,160][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-06 07:23:02,161][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-06 07:23:02,399][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-06 07:23:02,400][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-06 07:23:02,402][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-06 07:23:02,402][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-06 07:23:02,402][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-06 07:23:02,407][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-06 07:23:02,408][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-06 07:23:03,021][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-06 07:23:03,299][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:10114', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-06 07:23:03,599][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:19781', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-06 07:23:05,197][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-06 07:23:05,199][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-06 07:23:05,199][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-06 07:23:05,200][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-06 07:23:05,200][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-06 07:23:05,201][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-06 07:23:05,850][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-06 07:23:06,471][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-06 07:23:06,473][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-06 07:23:06,473][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-06 07:23:06,473][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-06 07:23:06,474][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-06 07:23:06,474][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-06 07:23:07,685][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-06 07:23:07,907][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:18914', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-06 07:23:13,067][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-06 07:23:13,079][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-06 07:23:13,079][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-06 07:23:13,079][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-06 07:23:13,080][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-06 07:23:13,087][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-06 07:23:14,191][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-06 07:23:27,735][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-06 07:23:28,059][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-06 07:23:28,059][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-06 07:23:28,059][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-06 07:23:28,062][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-06 07:23:28,062][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-06 07:23:36,423][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-06 07:23:36,424][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-06 07:23:36,425][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-06 07:23:36,425][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-06 07:23:36,425][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-06 07:23:36,426][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-06 07:23:41,529][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-06 07:23:44,503][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-06 07:23:52,629][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-06 07:23:52,630][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-06 07:23:52,630][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-06 07:23:52,630][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-06 07:23:52,632][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-06 07:23:52,632][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-06 07:23:57,920][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-06 07:24:00,692][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 07:24:00,693][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 07:24:00,693][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 07:24:00,693][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 07:24:00,693][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 07:24:00,693][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 07:24:00,693][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 07:24:00,693][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 07:24:00,694][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 07:24:00,694][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 07:24:00,694][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-06 07:24:00,698][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-06 07:24:00,699][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-06 07:24:16,496][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 07:24:16,496][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 07:24:16,497][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 07:24:16,497][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 07:24:16,497][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 07:24:16,497][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 07:24:16,497][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 07:24:16,497][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 07:24:16,497][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 07:24:16,497][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 07:24:16,497][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-06 07:24:16,498][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-06 07:24:16,503][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-06 07:24:32,573][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 201 @ 9591 updates)
[2024-10-06 07:24:32,584][fairseq.trainer][INFO] - loading train data for epoch 201
[2024-10-06 07:24:33,105][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-06 07:24:35,087][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 07:24:35,088][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 07:24:35,088][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 07:24:35,088][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 07:24:35,088][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 07:24:35,088][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 07:24:35,088][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 07:24:35,088][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 07:24:35,088][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 07:24:35,088][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 07:24:35,088][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-06 07:24:35,088][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-06 07:24:35,089][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-06 07:24:41,146][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 07:24:41,156][fairseq.trainer][INFO] - begin training epoch 201
[2024-10-06 07:24:41,157][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 07:24:51,660][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 201 @ 9591 updates)
[2024-10-06 07:24:51,673][fairseq.trainer][INFO] - loading train data for epoch 201
[2024-10-06 07:24:52,581][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-06 07:25:00,913][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 07:25:00,921][fairseq.trainer][INFO] - begin training epoch 201
[2024-10-06 07:25:00,925][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 07:25:11,797][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 201 @ 9591 updates)
[2024-10-06 07:25:11,809][fairseq.trainer][INFO] - loading train data for epoch 201
[2024-10-06 07:25:12,298][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-06 07:25:19,195][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 07:25:19,207][fairseq.trainer][INFO] - begin training epoch 201
[2024-10-06 07:25:19,207][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 07:25:22,624][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 07:25:22,625][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 07:25:22,625][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 07:25:22,625][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 07:25:22,625][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 07:25:22,625][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 07:25:22,625][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 07:25:22,625][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 07:25:22,625][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 07:25:22,625][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 07:25:22,625][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-06 07:25:22,625][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-06 07:25:23,578][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-06 07:25:38,349][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 07:25:38,350][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 07:25:38,350][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 07:25:38,350][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 07:25:38,350][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 07:25:38,350][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 07:25:38,350][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 07:25:38,350][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 07:25:38,350][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 07:25:38,350][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 07:25:38,351][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-06 07:25:38,351][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-06 07:25:38,352][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-06 07:25:40,347][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 07:25:40,348][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 07:25:40,348][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 07:25:40,348][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 07:25:40,349][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 07:25:40,349][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 07:25:40,349][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 07:25:40,349][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 07:25:40,350][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 07:25:40,350][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 07:25:40,351][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-06 07:25:40,354][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-06 07:25:40,355][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-06 07:25:43,921][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 07:25:43,923][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 07:25:43,923][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 07:25:43,923][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 07:25:43,923][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 07:25:43,923][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 07:25:43,923][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 07:25:43,924][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 07:25:43,924][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 07:25:43,924][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 07:25:43,924][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-06 07:25:43,924][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-06 07:25:43,925][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-06 07:26:09,444][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 201 @ 9591 updates)
[2024-10-06 07:26:09,445][fairseq.trainer][INFO] - loading train data for epoch 201
[2024-10-06 07:26:09,786][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-06 07:26:13,806][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 201 @ 9591 updates)
[2024-10-06 07:26:13,849][fairseq.trainer][INFO] - loading train data for epoch 201
[2024-10-06 07:26:16,266][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-06 07:26:24,014][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 07:26:24,073][fairseq.trainer][INFO] - begin training epoch 201
[2024-10-06 07:26:24,079][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 07:26:25,460][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 07:26:25,461][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 07:26:25,461][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 07:26:25,461][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 07:26:25,461][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 07:26:25,461][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 07:26:25,461][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 07:26:25,461][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 07:26:25,461][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 07:26:25,461][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 07:26:25,461][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-06 07:26:25,467][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-06 07:26:25,469][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-06 07:26:30,620][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 07:26:30,632][fairseq.trainer][INFO] - begin training epoch 201
[2024-10-06 07:26:30,632][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 07:26:58,533][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 201 @ 9591 updates)
[2024-10-06 07:26:58,535][fairseq.trainer][INFO] - loading train data for epoch 201
[2024-10-06 07:27:01,325][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-06 07:27:04,389][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 201 @ 9591 updates)
[2024-10-06 07:27:04,391][fairseq.trainer][INFO] - loading train data for epoch 201
[2024-10-06 07:27:04,700][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-06 07:27:06,368][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 201 @ 9591 updates)
[2024-10-06 07:27:06,370][fairseq.trainer][INFO] - loading train data for epoch 201
[2024-10-06 07:27:07,022][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-06 07:27:10,089][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 07:27:10,105][fairseq.trainer][INFO] - begin training epoch 201
[2024-10-06 07:27:10,110][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 07:27:13,090][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 07:27:13,105][fairseq.trainer][INFO] - begin training epoch 201
[2024-10-06 07:27:13,111][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 07:27:18,853][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 07:27:18,857][fairseq.trainer][INFO] - begin training epoch 201
[2024-10-06 07:27:18,859][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 07:31:43,240][train_inner][INFO] - {"epoch": 201, "update": 200.188, "loss": "1.02", "ntokens": "262704", "nsentences": "1763.44", "wps": "56933.1", "ups": "0.22", "wpb": "262704", "bsz": "1763.4", "num_updates": "9600", "lr": "0.00015", "gnorm": "0.762", "loss_scale": "1", "train_wall": "64", "gb_free": "39.6", "wall": "462"}
[2024-10-06 07:35:34,472][fairseq_cli.train][INFO] - end of epoch 201 (average epoch stats below)
[2024-10-06 07:35:34,599][train][INFO] - {"epoch": 201, "train_loss": "1.012", "train_ntokens": "260739", "train_nsentences": "1750.04", "train_wps": "45667.2", "train_ups": "0.18", "train_wpb": "260739", "train_bsz": "1750", "train_num_updates": "9639", "train_lr": "0.000150609", "train_gnorm": "0.747", "train_loss_scale": "1", "train_train_wall": "292", "train_gb_free": "39.3", "train_wall": "694"}
[2024-10-06 07:35:35,777][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 07:35:35,799][fairseq.trainer][INFO] - begin training epoch 202
[2024-10-06 07:35:35,803][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 07:36:27,601][fairseq.trainer][WARNING] - OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 182.00 MiB. GPU 
[2024-10-06 07:36:27,602][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 1            |        cudaMalloc retries: 1         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   5359 MiB |   5541 MiB |   7942 MiB |   2583 MiB |
|       from large pool |   5311 MiB |   5493 MiB |   7781 MiB |   2470 MiB |
|       from small pool |     48 MiB |    104 MiB |    161 MiB |    113 MiB |
|---------------------------------------------------------------------------|
| Active memory         |   5359 MiB |   5541 MiB |   7942 MiB |   2583 MiB |
|       from large pool |   5311 MiB |   5493 MiB |   7781 MiB |   2470 MiB |
|       from small pool |     48 MiB |    104 MiB |    161 MiB |    113 MiB |
|---------------------------------------------------------------------------|
| Requested memory      |   5345 MiB |   5527 MiB |   7924 MiB |   2579 MiB |
|       from large pool |   5297 MiB |   5479 MiB |   7763 MiB |   2466 MiB |
|       from small pool |     47 MiB |    102 MiB |    160 MiB |    113 MiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   5678 MiB |   5678 MiB |   5678 MiB |      0 B   |
|       from large pool |   5572 MiB |   5572 MiB |   5572 MiB |      0 B   |
|       from small pool |    106 MiB |    106 MiB |    106 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory | 139582 KiB | 315095 KiB |   2310 MiB |   2174 MiB |
|       from large pool |  80848 KiB | 255832 KiB |   2091 MiB |   2012 MiB |
|       from small pool |  58734 KiB |  70361 KiB |    219 MiB |    161 MiB |
|---------------------------------------------------------------------------|
| Allocations           |    4545    |    6437    |    6722    |    2177    |
|       from large pool |     195    |     196    |     236    |      41    |
|       from small pool |    4350    |    6359    |    6486    |    2136    |
|---------------------------------------------------------------------------|
| Active allocs         |    4545    |    6437    |    6722    |    2177    |
|       from large pool |     195    |     196    |     236    |      41    |
|       from small pool |    4350    |    6359    |    6486    |    2136    |
|---------------------------------------------------------------------------|
| GPU reserved segments |     120    |     120    |     120    |       0    |
|       from large pool |      67    |      67    |      67    |       0    |
|       from small pool |      53    |      53    |      53    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     580    |     582    |     814    |     234    |
|       from large pool |      40    |      42    |      77    |      37    |
|       from small pool |     540    |     541    |     737    |     197    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-06 07:36:27,607][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-06 07:36:27,607][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-06 07:36:27,608][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-06 07:36:27,627][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 4                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-06 07:36:27,627][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 5                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-06 07:36:27,628][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 6                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-06 07:36:27,628][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 7                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-06 07:36:27,635][fairseq.trainer][WARNING] - attempting to recover from OOM in forward/backward pass
[2024-10-06 07:36:29,283][fairseq.trainer][WARNING] - OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 24.00 MiB. GPU 
[2024-10-06 07:36:29,284][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 2            |        cudaMalloc retries: 2         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   1904 MiB |   2018 MiB |   2241 MiB | 344882 KiB |
|       from large pool |   1865 MiB |   1979 MiB |   2129 MiB | 270623 KiB |
|       from small pool |     39 MiB |    104 MiB |    111 MiB |  74258 KiB |
|---------------------------------------------------------------------------|
| Active memory         |   1904 MiB |   2018 MiB |   2241 MiB | 344882 KiB |
|       from large pool |   1865 MiB |   1979 MiB |   2129 MiB | 270623 KiB |
|       from small pool |     39 MiB |    104 MiB |    111 MiB |  74258 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |   1893 MiB |   2007 MiB |   2228 MiB | 342613 KiB |
|       from large pool |   1855 MiB |   1969 MiB |   2117 MiB | 268570 KiB |
|       from small pool |     38 MiB |    102 MiB |    110 MiB |  74043 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   2144 MiB |   2144 MiB |   2144 MiB |      0 B   |
|       from large pool |   2038 MiB |   2038 MiB |   2038 MiB |      0 B   |
|       from small pool |    106 MiB |    106 MiB |    106 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory | 122229 KiB | 128784 KiB | 429741 KiB | 307512 KiB |
|       from large pool |  53799 KiB |  60354 KiB | 246937 KiB | 193137 KiB |
|       from small pool |  68429 KiB |  70361 KiB | 182804 KiB | 114374 KiB |
|---------------------------------------------------------------------------|
| Allocations           |    4412    |    6437    |    6500    |    2088    |
|       from large pool |     100    |     105    |     111    |      11    |
|       from small pool |    4312    |    6359    |    6389    |    2077    |
|---------------------------------------------------------------------------|
| Active allocs         |    4412    |    6437    |    6500    |    2088    |
|       from large pool |     100    |     105    |     111    |      11    |
|       from small pool |    4312    |    6359    |    6389    |    2077    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      89    |      89    |      89    |       0    |
|       from large pool |      36    |      36    |      36    |       0    |
|       from small pool |      53    |      53    |      53    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     558    |     563    |     771    |     213    |
|       from large pool |      18    |      23    |      34    |      16    |
|       from small pool |     540    |     541    |     737    |     197    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-06 07:36:29,289][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-06 07:36:29,290][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-06 07:36:29,290][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-06 07:36:29,290][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 4                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-06 07:36:29,291][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 5                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-06 07:36:29,291][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 6                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-06 07:36:29,291][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 7                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-06 07:36:29,292][fairseq.trainer][WARNING] - attempting to recover from OOM in forward/backward pass
[2024-10-06 07:49:07,781][fairseq_cli.train][INFO] - end of epoch 202 (average epoch stats below)
[2024-10-06 07:49:07,801][train][INFO] - {"epoch": 202, "train_loss": "1.008", "train_ntokens": "260871", "train_nsentences": "1750.04", "train_wps": "15398.2", "train_ups": "0.06", "train_wpb": "260871", "train_bsz": "1750", "train_num_updates": "9687", "train_lr": "0.000151359", "train_gnorm": "0.763", "train_loss_scale": "1", "train_train_wall": "248", "train_gb_free": "41.3", "train_wall": "1507"}
[2024-10-06 07:49:07,937][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 07:49:07,955][fairseq.trainer][INFO] - begin training epoch 203
[2024-10-06 07:49:07,956][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 07:51:17,543][fairseq_cli.train][INFO] - end of epoch 203 (average epoch stats below)
[2024-10-06 07:51:17,552][train][INFO] - {"epoch": 203, "train_loss": "1.008", "train_ntokens": "260480", "train_nsentences": "1750.04", "train_wps": "96363.6", "train_ups": "0.37", "train_wpb": "260480", "train_bsz": "1750", "train_num_updates": "9735", "train_lr": "0.000152109", "train_gnorm": "0.968", "train_loss_scale": "1", "train_train_wall": "46", "train_gb_free": "39.6", "train_wall": "1637"}
[2024-10-06 07:51:17,752][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 07:51:17,761][fairseq.trainer][INFO] - begin training epoch 204
[2024-10-06 07:51:17,761][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 07:53:25,611][fairseq_cli.train][INFO] - end of epoch 204 (average epoch stats below)
[2024-10-06 07:53:25,614][train][INFO] - {"epoch": 204, "train_loss": "1.003", "train_ntokens": "260739", "train_nsentences": "1750.04", "train_wps": "97731.9", "train_ups": "0.37", "train_wpb": "260739", "train_bsz": "1750", "train_num_updates": "9783", "train_lr": "0.000152859", "train_gnorm": "0.915", "train_loss_scale": "1", "train_train_wall": "45", "train_gb_free": "39.4", "train_wall": "1765"}
[2024-10-06 07:53:25,859][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 07:53:25,870][fairseq.trainer][INFO] - begin training epoch 205
[2024-10-06 07:53:25,871][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 07:55:14,083][train_inner][INFO] - {"epoch": 205, "update": 204.354, "loss": "1.006", "ntokens": "260560", "nsentences": "1744.59", "wps": "36936.9", "ups": "0.14", "wpb": "260560", "bsz": "1744.6", "num_updates": "9800", "lr": "0.000153125", "gnorm": "0.87", "loss_scale": "1", "train_wall": "601", "gb_free": "40.5", "wall": "1873"}
[2024-10-06 07:55:43,165][fairseq_cli.train][INFO] - end of epoch 205 (average epoch stats below)
[2024-10-06 07:55:43,169][train][INFO] - {"epoch": 205, "train_loss": "1.003", "train_ntokens": "260927", "train_nsentences": "1750.04", "train_wps": "91053", "train_ups": "0.35", "train_wpb": "260927", "train_bsz": "1750", "train_num_updates": "9831", "train_lr": "0.000153609", "train_gnorm": "0.902", "train_loss_scale": "1", "train_train_wall": "64", "train_gb_free": "40.3", "train_wall": "1902"}
[2024-10-06 07:55:43,238][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 07:55:43,242][fairseq.trainer][INFO] - begin training epoch 206
[2024-10-06 07:55:43,243][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 07:57:50,021][fairseq_cli.train][INFO] - end of epoch 206 (average epoch stats below)
[2024-10-06 07:57:50,024][train][INFO] - {"epoch": 206, "train_loss": "1.001", "train_ntokens": "260621", "train_nsentences": "1750.04", "train_wps": "98615.9", "train_ups": "0.38", "train_wpb": "260621", "train_bsz": "1750", "train_num_updates": "9879", "train_lr": "0.000154359", "train_gnorm": "0.741", "train_loss_scale": "1", "train_train_wall": "56", "train_gb_free": "40.1", "train_wall": "2029"}
[2024-10-06 07:57:50,119][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 07:57:50,131][fairseq.trainer][INFO] - begin training epoch 207
[2024-10-06 07:57:50,131][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 08:00:01,773][fairseq_cli.train][INFO] - end of epoch 207 (average epoch stats below)
[2024-10-06 08:00:01,777][train][INFO] - {"epoch": 207, "train_loss": "0.999", "train_ntokens": "260535", "train_nsentences": "1750.04", "train_wps": "94920", "train_ups": "0.36", "train_wpb": "260535", "train_bsz": "1750", "train_num_updates": "9927", "train_lr": "0.000155109", "train_gnorm": "0.768", "train_loss_scale": "1", "train_train_wall": "60", "train_gb_free": "39.3", "train_wall": "2161"}
[2024-10-06 08:00:01,843][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 08:00:01,847][fairseq.trainer][INFO] - begin training epoch 208
[2024-10-06 08:00:01,847][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 08:02:07,310][fairseq_cli.train][INFO] - end of epoch 208 (average epoch stats below)
[2024-10-06 08:02:07,320][train][INFO] - {"epoch": 208, "train_loss": "1", "train_ntokens": "260683", "train_nsentences": "1750.04", "train_wps": "99671.5", "train_ups": "0.38", "train_wpb": "260683", "train_bsz": "1750", "train_num_updates": "9975", "train_lr": "0.000155859", "train_gnorm": "0.931", "train_loss_scale": "1", "train_train_wall": "47", "train_gb_free": "39.2", "train_wall": "2287"}
[2024-10-06 08:02:07,451][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 08:02:07,613][fairseq.trainer][INFO] - begin training epoch 209
[2024-10-06 08:02:07,614][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 08:03:53,385][train_inner][INFO] - {"epoch": 209, "update": 208.521, "loss": "1", "ntokens": "260744", "nsentences": "1753.93", "wps": "100422", "ups": "0.39", "wpb": "260744", "bsz": "1753.9", "num_updates": "10000", "lr": "0.00015625", "gnorm": "0.812", "loss_scale": "1", "train_wall": "215", "gb_free": "39.6", "wall": "2393"}
[2024-10-06 08:04:19,625][fairseq_cli.train][INFO] - end of epoch 209 (average epoch stats below)
[2024-10-06 08:04:19,628][train][INFO] - {"epoch": 209, "train_loss": "0.995", "train_ntokens": "260332", "train_nsentences": "1750.04", "train_wps": "94447.7", "train_ups": "0.36", "train_wpb": "260332", "train_bsz": "1750", "train_num_updates": "10023", "train_lr": "0.000156609", "train_gnorm": "0.779", "train_loss_scale": "1", "train_train_wall": "50", "train_gb_free": "41.5", "train_wall": "2419"}
[2024-10-06 08:04:19,691][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 08:04:19,695][fairseq.trainer][INFO] - begin training epoch 210
[2024-10-06 08:04:19,696][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 08:06:29,055][fairseq_cli.train][INFO] - end of epoch 210 (average epoch stats below)
[2024-10-06 08:06:29,058][train][INFO] - {"epoch": 210, "train_loss": "0.993", "train_ntokens": "261170", "train_nsentences": "1750.04", "train_wps": "96858", "train_ups": "0.37", "train_wpb": "261170", "train_bsz": "1750", "train_num_updates": "10071", "train_lr": "0.000157359", "train_gnorm": "0.837", "train_loss_scale": "1", "train_train_wall": "52", "train_gb_free": "40", "train_wall": "2548"}
[2024-10-06 08:06:29,121][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 08:06:29,125][fairseq.trainer][INFO] - begin training epoch 211
[2024-10-06 08:06:29,125][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 08:08:38,952][fairseq_cli.train][INFO] - end of epoch 211 (average epoch stats below)
[2024-10-06 08:08:38,955][train][INFO] - {"epoch": 211, "train_loss": "0.992", "train_ntokens": "260436", "train_nsentences": "1750.04", "train_wps": "96238.7", "train_ups": "0.37", "train_wpb": "260436", "train_bsz": "1750", "train_num_updates": "10119", "train_lr": "0.000158109", "train_gnorm": "0.705", "train_loss_scale": "1", "train_train_wall": "41", "train_gb_free": "39.7", "train_wall": "2678"}
[2024-10-06 08:08:39,014][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 08:08:39,018][fairseq.trainer][INFO] - begin training epoch 212
[2024-10-06 08:08:39,019][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 08:10:44,946][fairseq_cli.train][INFO] - end of epoch 212 (average epoch stats below)
[2024-10-06 08:10:44,963][train][INFO] - {"epoch": 212, "train_loss": "0.99", "train_ntokens": "260261", "train_nsentences": "1750.04", "train_wps": "99143.2", "train_ups": "0.38", "train_wpb": "260261", "train_bsz": "1750", "train_num_updates": "10167", "train_lr": "0.000158859", "train_gnorm": "0.855", "train_loss_scale": "1", "train_train_wall": "46", "train_gb_free": "39.6", "train_wall": "2804"}
[2024-10-06 08:10:45,067][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 08:10:45,073][fairseq.trainer][INFO] - begin training epoch 213
[2024-10-06 08:10:45,073][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 08:12:39,717][train_inner][INFO] - {"epoch": 213, "update": 212.688, "loss": "0.992", "ntokens": "260495", "nsentences": "1751.73", "wps": "98985.6", "ups": "0.38", "wpb": "260495", "bsz": "1751.7", "num_updates": "10200", "lr": "0.000159375", "gnorm": "0.82", "loss_scale": "1", "train_wall": "202", "gb_free": "39.6", "wall": "2919"}
[2024-10-06 08:12:53,672][fairseq_cli.train][INFO] - end of epoch 213 (average epoch stats below)
[2024-10-06 08:12:53,675][train][INFO] - {"epoch": 213, "train_loss": "0.992", "train_ntokens": "260722", "train_nsentences": "1750.04", "train_wps": "97232.5", "train_ups": "0.37", "train_wpb": "260722", "train_bsz": "1750", "train_num_updates": "10215", "train_lr": "0.000159609", "train_gnorm": "0.983", "train_loss_scale": "1", "train_train_wall": "50", "train_gb_free": "39.7", "train_wall": "2933"}
[2024-10-06 08:12:53,742][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 08:12:53,748][fairseq.trainer][INFO] - begin training epoch 214
[2024-10-06 08:12:53,748][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 08:15:02,278][fairseq_cli.train][INFO] - end of epoch 214 (average epoch stats below)
[2024-10-06 08:15:02,281][train][INFO] - {"epoch": 214, "train_loss": "0.984", "train_ntokens": "260576", "train_nsentences": "1750.04", "train_wps": "97256.4", "train_ups": "0.37", "train_wpb": "260576", "train_bsz": "1750", "train_num_updates": "10263", "train_lr": "0.000160359", "train_gnorm": "0.696", "train_loss_scale": "1", "train_train_wall": "51", "train_gb_free": "40", "train_wall": "3062"}
[2024-10-06 08:15:02,343][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 08:15:02,347][fairseq.trainer][INFO] - begin training epoch 215
[2024-10-06 08:15:02,347][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 08:17:08,429][fairseq_cli.train][INFO] - end of epoch 215 (average epoch stats below)
[2024-10-06 08:17:08,438][train][INFO] - {"epoch": 215, "train_loss": "0.983", "train_ntokens": "260741", "train_nsentences": "1750.04", "train_wps": "99212", "train_ups": "0.38", "train_wpb": "260741", "train_bsz": "1750", "train_num_updates": "10311", "train_lr": "0.000161109", "train_gnorm": "0.715", "train_loss_scale": "1", "train_train_wall": "49", "train_gb_free": "39.6", "train_wall": "3188"}
[2024-10-06 08:17:08,568][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 08:17:08,585][fairseq.trainer][INFO] - begin training epoch 216
[2024-10-06 08:17:08,586][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 08:19:16,033][fairseq_cli.train][INFO] - end of epoch 216 (average epoch stats below)
[2024-10-06 08:19:16,042][train][INFO] - {"epoch": 216, "train_loss": "0.984", "train_ntokens": "260470", "train_nsentences": "1750.04", "train_wps": "97982.6", "train_ups": "0.38", "train_wpb": "260470", "train_bsz": "1750", "train_num_updates": "10359", "train_lr": "0.000161859", "train_gnorm": "0.857", "train_loss_scale": "1", "train_train_wall": "55", "train_gb_free": "41", "train_wall": "3315"}
[2024-10-06 08:19:16,121][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 08:19:16,130][fairseq.trainer][INFO] - begin training epoch 217
[2024-10-06 08:19:16,131][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 08:21:20,059][train_inner][INFO] - {"epoch": 217, "update": 216.854, "loss": "0.983", "ntokens": "260686", "nsentences": "1748.95", "wps": "100198", "ups": "0.38", "wpb": "260686", "bsz": "1749", "num_updates": "10400", "lr": "0.0001625", "gnorm": "0.771", "loss_scale": "1", "train_wall": "215", "gb_free": "39.7", "wall": "3439"}
[2024-10-06 08:21:22,553][fairseq_cli.train][INFO] - end of epoch 217 (average epoch stats below)
[2024-10-06 08:21:22,555][train][INFO] - {"epoch": 217, "train_loss": "0.981", "train_ntokens": "260839", "train_nsentences": "1750.04", "train_wps": "98966.7", "train_ups": "0.38", "train_wpb": "260839", "train_bsz": "1750", "train_num_updates": "10407", "train_lr": "0.000162609", "train_gnorm": "0.742", "train_loss_scale": "1", "train_train_wall": "50", "train_gb_free": "39.8", "train_wall": "3442"}
[2024-10-06 08:21:22,613][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 08:21:22,616][fairseq.trainer][INFO] - begin training epoch 218
[2024-10-06 08:21:22,616][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 08:23:26,163][fairseq_cli.train][INFO] - end of epoch 218 (average epoch stats below)
[2024-10-06 08:23:26,168][train][INFO] - {"epoch": 218, "train_loss": "0.981", "train_ntokens": "260774", "train_nsentences": "1750.04", "train_wps": "101263", "train_ups": "0.39", "train_wpb": "260774", "train_bsz": "1750", "train_num_updates": "10455", "train_lr": "0.000163359", "train_gnorm": "0.958", "train_loss_scale": "1", "train_train_wall": "48", "train_gb_free": "40.8", "train_wall": "3565"}
[2024-10-06 08:23:26,253][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 08:23:26,262][fairseq.trainer][INFO] - begin training epoch 219
[2024-10-06 08:23:26,262][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 08:25:32,072][fairseq_cli.train][INFO] - end of epoch 219 (average epoch stats below)
[2024-10-06 08:25:32,075][train][INFO] - {"epoch": 219, "train_loss": "0.976", "train_ntokens": "260869", "train_nsentences": "1750.04", "train_wps": "99454.5", "train_ups": "0.38", "train_wpb": "260869", "train_bsz": "1750", "train_num_updates": "10503", "train_lr": "0.000164109", "train_gnorm": "0.737", "train_loss_scale": "1", "train_train_wall": "62", "train_gb_free": "40", "train_wall": "3691"}
[2024-10-06 08:25:32,143][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 08:25:32,147][fairseq.trainer][INFO] - begin training epoch 220
[2024-10-06 08:25:32,147][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 08:27:38,560][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 220 @ 10551 updates
[2024-10-06 08:27:38,566][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-06 08:27:43,150][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-06 08:27:43,152][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 220 @ 10551 updates, score None) (writing took 4.592240842059255 seconds)
[2024-10-06 08:27:43,153][fairseq_cli.train][INFO] - end of epoch 220 (average epoch stats below)
[2024-10-06 08:27:43,154][train][INFO] - {"epoch": 220, "train_loss": "0.979", "train_ntokens": "260739", "train_nsentences": "1750.04", "train_wps": "95482.2", "train_ups": "0.37", "train_wpb": "260739", "train_bsz": "1750", "train_num_updates": "10551", "train_lr": "0.000164859", "train_gnorm": "0.815", "train_loss_scale": "1", "train_train_wall": "43", "train_gb_free": "39.6", "train_wall": "3822"}
[2024-10-06 08:27:43,379][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 08:27:43,384][fairseq.trainer][INFO] - begin training epoch 221
[2024-10-06 08:27:43,384][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 08:29:56,032][fairseq_cli.train][INFO] - end of epoch 221 (average epoch stats below)
[2024-10-06 08:29:56,036][train][INFO] - {"epoch": 221, "train_loss": "0.974", "train_ntokens": "260674", "train_nsentences": "1750.04", "train_wps": "94163.2", "train_ups": "0.36", "train_wpb": "260674", "train_bsz": "1750", "train_num_updates": "10599", "train_lr": "0.000165609", "train_gnorm": "0.933", "train_loss_scale": "1", "train_train_wall": "38", "train_gb_free": "40.1", "train_wall": "3955"}
[2024-10-06 08:29:56,208][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 08:29:56,213][fairseq.trainer][INFO] - begin training epoch 222
[2024-10-06 08:29:56,214][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 08:31:16,744][train_inner][INFO] - {"epoch": 222, "update": 221.021, "loss": "0.978", "ntokens": "260824", "nsentences": "1749.11", "wps": "87426.6", "ups": "0.34", "wpb": "260824", "bsz": "1749.1", "num_updates": "10600", "lr": "0.000165625", "gnorm": "0.859", "loss_scale": "1", "train_wall": "193", "gb_free": "39.3", "wall": "4036"}
[2024-10-06 08:32:03,836][fairseq_cli.train][INFO] - end of epoch 222 (average epoch stats below)
[2024-10-06 08:32:03,838][train][INFO] - {"epoch": 222, "train_loss": "0.975", "train_ntokens": "260585", "train_nsentences": "1750.04", "train_wps": "97873", "train_ups": "0.38", "train_wpb": "260585", "train_bsz": "1750", "train_num_updates": "10647", "train_lr": "0.000166359", "train_gnorm": "0.699", "train_loss_scale": "1", "train_train_wall": "44", "train_gb_free": "40.1", "train_wall": "4083"}
[2024-10-06 08:32:03,900][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 08:32:03,904][fairseq.trainer][INFO] - begin training epoch 223
[2024-10-06 08:32:03,904][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 08:34:14,094][fairseq_cli.train][INFO] - end of epoch 223 (average epoch stats below)
[2024-10-06 08:34:14,098][train][INFO] - {"epoch": 223, "train_loss": "0.974", "train_ntokens": "260564", "train_nsentences": "1750.04", "train_wps": "96018.4", "train_ups": "0.37", "train_wpb": "260564", "train_bsz": "1750", "train_num_updates": "10695", "train_lr": "0.000167109", "train_gnorm": "0.769", "train_loss_scale": "1", "train_train_wall": "57", "train_gb_free": "39.8", "train_wall": "4213"}
[2024-10-06 08:34:14,162][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 08:34:14,165][fairseq.trainer][INFO] - begin training epoch 224
[2024-10-06 08:34:14,166][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 08:36:25,027][fairseq_cli.train][INFO] - end of epoch 224 (average epoch stats below)
[2024-10-06 08:36:25,030][train][INFO] - {"epoch": 224, "train_loss": "0.97", "train_ntokens": "261019", "train_nsentences": "1750.04", "train_wps": "95691.9", "train_ups": "0.37", "train_wpb": "261019", "train_bsz": "1750", "train_num_updates": "10743", "train_lr": "0.000167859", "train_gnorm": "0.841", "train_loss_scale": "1", "train_train_wall": "40", "train_gb_free": "40.1", "train_wall": "4344"}
[2024-10-06 08:36:25,084][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 08:36:25,089][fairseq.trainer][INFO] - begin training epoch 225
[2024-10-06 08:36:25,089][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 08:38:32,058][fairseq_cli.train][INFO] - end of epoch 225 (average epoch stats below)
[2024-10-06 08:38:32,061][train][INFO] - {"epoch": 225, "train_loss": "0.967", "train_ntokens": "260455", "train_nsentences": "1750.04", "train_wps": "98417.5", "train_ups": "0.38", "train_wpb": "260455", "train_bsz": "1750", "train_num_updates": "10791", "train_lr": "0.000168609", "train_gnorm": "0.706", "train_loss_scale": "1", "train_train_wall": "39", "train_gb_free": "40.3", "train_wall": "4471"}
[2024-10-06 08:38:32,132][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 08:38:32,136][fairseq.trainer][INFO] - begin training epoch 226
[2024-10-06 08:38:32,136][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 08:40:07,545][train_inner][INFO] - {"epoch": 226, "update": 225.188, "loss": "0.971", "ntokens": "260643", "nsentences": "1754.51", "wps": "98208.6", "ups": "0.38", "wpb": "260643", "bsz": "1754.5", "num_updates": "10800", "lr": "0.00016875", "gnorm": "0.758", "loss_scale": "1", "train_wall": "201", "gb_free": "39.3", "wall": "4567"}
[2024-10-06 08:40:37,669][fairseq_cli.train][INFO] - end of epoch 226 (average epoch stats below)
[2024-10-06 08:40:37,671][train][INFO] - {"epoch": 226, "train_loss": "0.97", "train_ntokens": "260471", "train_nsentences": "1750.04", "train_wps": "99537.3", "train_ups": "0.38", "train_wpb": "260471", "train_bsz": "1750", "train_num_updates": "10839", "train_lr": "0.000169359", "train_gnorm": "0.863", "train_loss_scale": "1", "train_train_wall": "49", "train_gb_free": "39.6", "train_wall": "4597"}
[2024-10-06 08:40:37,762][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 08:40:37,783][fairseq.trainer][INFO] - begin training epoch 227
[2024-10-06 08:40:37,783][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 08:42:44,949][fairseq_cli.train][INFO] - end of epoch 227 (average epoch stats below)
[2024-10-06 08:42:44,968][train][INFO] - {"epoch": 227, "train_loss": "0.967", "train_ntokens": "260665", "train_nsentences": "1750.04", "train_wps": "98298.5", "train_ups": "0.38", "train_wpb": "260664", "train_bsz": "1750", "train_num_updates": "10887", "train_lr": "0.000170109", "train_gnorm": "0.827", "train_loss_scale": "1", "train_train_wall": "53", "train_gb_free": "39.3", "train_wall": "4724"}
[2024-10-06 08:42:45,131][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 08:42:45,145][fairseq.trainer][INFO] - begin training epoch 228
[2024-10-06 08:42:45,146][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 08:44:53,302][fairseq_cli.train][INFO] - end of epoch 228 (average epoch stats below)
[2024-10-06 08:44:53,317][train][INFO] - {"epoch": 228, "train_loss": "0.961", "train_ntokens": "260669", "train_nsentences": "1750.04", "train_wps": "97488.1", "train_ups": "0.37", "train_wpb": "260669", "train_bsz": "1750", "train_num_updates": "10935", "train_lr": "0.000170859", "train_gnorm": "0.863", "train_loss_scale": "1", "train_train_wall": "44", "train_gb_free": "40.2", "train_wall": "4853"}
[2024-10-06 08:44:53,429][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 08:44:53,433][fairseq.trainer][INFO] - begin training epoch 229
[2024-10-06 08:44:53,433][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 08:46:58,697][fairseq_cli.train][INFO] - end of epoch 229 (average epoch stats below)
[2024-10-06 08:46:58,702][train][INFO] - {"epoch": 229, "train_loss": "0.961", "train_ntokens": "260888", "train_nsentences": "1750.04", "train_wps": "99876.2", "train_ups": "0.38", "train_wpb": "260888", "train_bsz": "1750", "train_num_updates": "10983", "train_lr": "0.000171609", "train_gnorm": "0.773", "train_loss_scale": "1", "train_train_wall": "42", "train_gb_free": "39.6", "train_wall": "4978"}
[2024-10-06 08:46:58,812][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 08:46:58,820][fairseq.trainer][INFO] - begin training epoch 230
[2024-10-06 08:46:58,820][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 08:48:37,709][train_inner][INFO] - {"epoch": 230, "update": 229.354, "loss": "0.963", "ntokens": "260812", "nsentences": "1730.67", "wps": "102248", "ups": "0.39", "wpb": "260812", "bsz": "1730.7", "num_updates": "11000", "lr": "0.000171875", "gnorm": "0.824", "loss_scale": "1", "train_wall": "203", "gb_free": "40.1", "wall": "5077"}
[2024-10-06 08:49:08,599][fairseq_cli.train][INFO] - end of epoch 230 (average epoch stats below)
[2024-10-06 08:49:08,602][train][INFO] - {"epoch": 230, "train_loss": "0.961", "train_ntokens": "260952", "train_nsentences": "1750.04", "train_wps": "96427.4", "train_ups": "0.37", "train_wpb": "260952", "train_bsz": "1750", "train_num_updates": "11031", "train_lr": "0.000172359", "train_gnorm": "0.842", "train_loss_scale": "1", "train_train_wall": "65", "train_gb_free": "39.6", "train_wall": "5108"}
[2024-10-06 08:49:08,774][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 08:49:08,781][fairseq.trainer][INFO] - begin training epoch 231
[2024-10-06 08:49:08,781][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 08:51:16,253][fairseq_cli.train][INFO] - end of epoch 231 (average epoch stats below)
[2024-10-06 08:51:16,256][train][INFO] - {"epoch": 231, "train_loss": "0.961", "train_ntokens": "260641", "train_nsentences": "1750.04", "train_wps": "98007.2", "train_ups": "0.38", "train_wpb": "260641", "train_bsz": "1750", "train_num_updates": "11079", "train_lr": "0.000173109", "train_gnorm": "1.082", "train_loss_scale": "1", "train_train_wall": "57", "train_gb_free": "40.1", "train_wall": "5236"}
[2024-10-06 08:51:16,316][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 08:51:16,332][fairseq.trainer][INFO] - begin training epoch 232
[2024-10-06 08:51:16,332][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 08:53:27,313][fairseq_cli.train][INFO] - end of epoch 232 (average epoch stats below)
[2024-10-06 08:53:27,322][train][INFO] - {"epoch": 232, "train_loss": "0.96", "train_ntokens": "260773", "train_nsentences": "1750.04", "train_wps": "95505", "train_ups": "0.37", "train_wpb": "260773", "train_bsz": "1750", "train_num_updates": "11127", "train_lr": "0.000173859", "train_gnorm": "0.746", "train_loss_scale": "1", "train_train_wall": "45", "train_gb_free": "40.1", "train_wall": "5367"}
[2024-10-06 08:53:27,436][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 08:53:27,446][fairseq.trainer][INFO] - begin training epoch 233
[2024-10-06 08:53:27,447][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 08:55:38,028][fairseq_cli.train][INFO] - end of epoch 233 (average epoch stats below)
[2024-10-06 08:55:38,035][train][INFO] - {"epoch": 233, "train_loss": "0.958", "train_ntokens": "260554", "train_nsentences": "1750.04", "train_wps": "95685.9", "train_ups": "0.37", "train_wpb": "260554", "train_bsz": "1750", "train_num_updates": "11175", "train_lr": "0.000174609", "train_gnorm": "0.703", "train_loss_scale": "1", "train_train_wall": "53", "train_gb_free": "40.3", "train_wall": "5497"}
[2024-10-06 08:55:38,173][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 08:55:38,177][fairseq.trainer][INFO] - begin training epoch 234
[2024-10-06 08:55:38,178][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 08:57:25,062][train_inner][INFO] - {"epoch": 234, "update": 233.521, "loss": "0.959", "ntokens": "260610", "nsentences": "1768.12", "wps": "98838.3", "ups": "0.38", "wpb": "260610", "bsz": "1768.1", "num_updates": "11200", "lr": "0.000175", "gnorm": "0.843", "loss_scale": "1", "train_wall": "204", "gb_free": "39.6", "wall": "5604"}
[2024-10-06 08:57:42,647][fairseq_cli.train][INFO] - end of epoch 234 (average epoch stats below)
[2024-10-06 08:57:42,663][train][INFO] - {"epoch": 234, "train_loss": "0.954", "train_ntokens": "260762", "train_nsentences": "1750.04", "train_wps": "100444", "train_ups": "0.39", "train_wpb": "260762", "train_bsz": "1750", "train_num_updates": "11223", "train_lr": "0.000175359", "train_gnorm": "0.815", "train_loss_scale": "1", "train_train_wall": "37", "train_gb_free": "40.1", "train_wall": "5622"}
[2024-10-06 08:57:42,772][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 08:57:42,784][fairseq.trainer][INFO] - begin training epoch 235
[2024-10-06 08:57:42,785][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 09:00:01,910][fairseq_cli.train][INFO] - end of epoch 235 (average epoch stats below)
[2024-10-06 09:00:01,916][train][INFO] - {"epoch": 235, "train_loss": "0.955", "train_ntokens": "261285", "train_nsentences": "1750.04", "train_wps": "90066.2", "train_ups": "0.34", "train_wpb": "261285", "train_bsz": "1750", "train_num_updates": "11271", "train_lr": "0.000176109", "train_gnorm": "0.755", "train_loss_scale": "1", "train_train_wall": "67", "train_gb_free": "40.8", "train_wall": "5761"}
[2024-10-06 09:00:02,172][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 09:00:02,176][fairseq.trainer][INFO] - begin training epoch 236
[2024-10-06 09:00:02,177][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 09:02:17,151][fairseq_cli.train][INFO] - end of epoch 236 (average epoch stats below)
[2024-10-06 09:02:17,154][train][INFO] - {"epoch": 236, "train_loss": "0.952", "train_ntokens": "260554", "train_nsentences": "1750.04", "train_wps": "92479.8", "train_ups": "0.35", "train_wpb": "260554", "train_bsz": "1750", "train_num_updates": "11319", "train_lr": "0.000176859", "train_gnorm": "0.778", "train_loss_scale": "1", "train_train_wall": "38", "train_gb_free": "39.2", "train_wall": "5896"}
[2024-10-06 09:02:17,217][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 09:02:17,229][fairseq.trainer][INFO] - begin training epoch 237
[2024-10-06 09:02:17,229][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 09:04:22,772][fairseq_cli.train][INFO] - end of epoch 237 (average epoch stats below)
[2024-10-06 09:04:22,775][train][INFO] - {"epoch": 237, "train_loss": "0.952", "train_ntokens": "260511", "train_nsentences": "1750.04", "train_wps": "99544.1", "train_ups": "0.38", "train_wpb": "260511", "train_bsz": "1750", "train_num_updates": "11367", "train_lr": "0.000177609", "train_gnorm": "0.753", "train_loss_scale": "1", "train_train_wall": "54", "train_gb_free": "40.6", "train_wall": "6022"}
[2024-10-06 09:04:24,738][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 09:04:24,743][fairseq.trainer][INFO] - begin training epoch 238
[2024-10-06 09:04:24,743][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 09:06:20,222][train_inner][INFO] - {"epoch": 238, "update": 237.688, "loss": "0.953", "ntokens": "260636", "nsentences": "1750.06", "wps": "97406.1", "ups": "0.37", "wpb": "260636", "bsz": "1750.1", "num_updates": "11400", "lr": "0.000178125", "gnorm": "0.754", "loss_scale": "1", "train_wall": "220", "gb_free": "41.1", "wall": "6140"}
[2024-10-06 09:06:33,377][fairseq_cli.train][INFO] - end of epoch 238 (average epoch stats below)
[2024-10-06 09:06:33,380][train][INFO] - {"epoch": 238, "train_loss": "0.948", "train_ntokens": "260717", "train_nsentences": "1750.04", "train_wps": "95821", "train_ups": "0.37", "train_wpb": "260717", "train_bsz": "1750", "train_num_updates": "11415", "train_lr": "0.000178359", "train_gnorm": "0.666", "train_loss_scale": "1", "train_train_wall": "57", "train_gb_free": "39.7", "train_wall": "6153"}
[2024-10-06 09:06:33,570][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 09:06:33,575][fairseq.trainer][INFO] - begin training epoch 239
[2024-10-06 09:06:33,575][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 09:09:18,902][fairseq_cli.train][INFO] - end of epoch 239 (average epoch stats below)
[2024-10-06 09:09:18,906][train][INFO] - {"epoch": 239, "train_loss": "0.946", "train_ntokens": "260343", "train_nsentences": "1750.04", "train_wps": "75496.6", "train_ups": "0.29", "train_wpb": "260343", "train_bsz": "1750", "train_num_updates": "11463", "train_lr": "0.000179109", "train_gnorm": "0.82", "train_loss_scale": "1", "train_train_wall": "46", "train_gb_free": "39.6", "train_wall": "6318"}
[2024-10-06 09:09:19,051][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 09:09:19,057][fairseq.trainer][INFO] - begin training epoch 240
[2024-10-06 09:09:19,058][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 09:11:26,981][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 240 @ 11511 updates
[2024-10-06 09:11:26,986][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-06 09:11:30,936][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-06 09:11:30,939][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 240 @ 11511 updates, score None) (writing took 3.9574429262429476 seconds)
[2024-10-06 09:11:30,939][fairseq_cli.train][INFO] - end of epoch 240 (average epoch stats below)
[2024-10-06 09:11:30,946][train][INFO] - {"epoch": 240, "train_loss": "0.947", "train_ntokens": "260462", "train_nsentences": "1750.04", "train_wps": "94690.2", "train_ups": "0.36", "train_wpb": "260462", "train_bsz": "1750", "train_num_updates": "11511", "train_lr": "0.000179859", "train_gnorm": "0.73", "train_loss_scale": "1", "train_train_wall": "64", "train_gb_free": "40.1", "train_wall": "6450"}
[2024-10-06 09:11:31,198][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 09:11:31,217][fairseq.trainer][INFO] - begin training epoch 241
[2024-10-06 09:11:31,218][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 09:14:17,494][fairseq_cli.train][INFO] - end of epoch 241 (average epoch stats below)
[2024-10-06 09:14:17,498][train][INFO] - {"epoch": 241, "train_loss": "0.943", "train_ntokens": "260376", "train_nsentences": "1750.04", "train_wps": "75041.4", "train_ups": "0.29", "train_wpb": "260376", "train_bsz": "1750", "train_num_updates": "11559", "train_lr": "0.000180609", "train_gnorm": "0.769", "train_loss_scale": "1", "train_train_wall": "43", "train_gb_free": "40.2", "train_wall": "6617"}
[2024-10-06 09:14:17,754][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 09:14:17,766][fairseq.trainer][INFO] - begin training epoch 242
[2024-10-06 09:14:17,766][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 09:17:03,179][train_inner][INFO] - {"epoch": 242, "update": 241.854, "loss": "0.944", "ntokens": "260539", "nsentences": "1747.84", "wps": "81045.7", "ups": "0.31", "wpb": "260540", "bsz": "1747.8", "num_updates": "11600", "lr": "0.00018125", "gnorm": "0.783", "loss_scale": "1", "train_wall": "229", "gb_free": "39.6", "wall": "6782"}
[2024-10-06 09:17:08,411][fairseq_cli.train][INFO] - end of epoch 242 (average epoch stats below)
[2024-10-06 09:17:08,414][train][INFO] - {"epoch": 242, "train_loss": "0.94", "train_ntokens": "260600", "train_nsentences": "1750.04", "train_wps": "73187.9", "train_ups": "0.28", "train_wpb": "260600", "train_bsz": "1750", "train_num_updates": "11607", "train_lr": "0.000181359", "train_gnorm": "0.846", "train_loss_scale": "1", "train_train_wall": "69", "train_gb_free": "39.3", "train_wall": "6788"}
[2024-10-06 09:17:08,478][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 09:17:08,481][fairseq.trainer][INFO] - begin training epoch 243
[2024-10-06 09:17:08,481][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 09:20:06,573][fairseq_cli.train][INFO] - end of epoch 243 (average epoch stats below)
[2024-10-06 09:20:06,599][train][INFO] - {"epoch": 243, "train_loss": "0.943", "train_ntokens": "260583", "train_nsentences": "1750.04", "train_wps": "70198.8", "train_ups": "0.27", "train_wpb": "260583", "train_bsz": "1750", "train_num_updates": "11655", "train_lr": "0.000182109", "train_gnorm": "0.717", "train_loss_scale": "2", "train_train_wall": "76", "train_gb_free": "39.3", "train_wall": "6966"}
[2024-10-06 09:20:06,775][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 09:20:06,779][fairseq.trainer][INFO] - begin training epoch 244
[2024-10-06 09:20:06,779][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 09:22:43,428][fairseq_cli.train][INFO] - end of epoch 244 (average epoch stats below)
[2024-10-06 09:22:43,447][train][INFO] - {"epoch": 244, "train_loss": "0.942", "train_ntokens": "260921", "train_nsentences": "1750.04", "train_wps": "79851.6", "train_ups": "0.31", "train_wpb": "260921", "train_bsz": "1750", "train_num_updates": "11703", "train_lr": "0.000182859", "train_gnorm": "0.733", "train_loss_scale": "2", "train_train_wall": "72", "train_gb_free": "39.7", "train_wall": "7123"}
[2024-10-06 09:22:43,597][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 09:22:43,608][fairseq.trainer][INFO] - begin training epoch 245
[2024-10-06 09:22:43,608][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 09:25:14,821][fairseq_cli.train][INFO] - end of epoch 245 (average epoch stats below)
[2024-10-06 09:25:14,825][train][INFO] - {"epoch": 245, "train_loss": "0.938", "train_ntokens": "260828", "train_nsentences": "1750.04", "train_wps": "82707", "train_ups": "0.32", "train_wpb": "260828", "train_bsz": "1750", "train_num_updates": "11751", "train_lr": "0.000183609", "train_gnorm": "0.711", "train_loss_scale": "2", "train_train_wall": "72", "train_gb_free": "40.1", "train_wall": "7274"}
[2024-10-06 09:25:14,888][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 09:25:14,903][fairseq.trainer][INFO] - begin training epoch 246
[2024-10-06 09:25:14,903][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 09:28:18,512][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
[2024-10-06 09:28:19,699][fairseq_cli.train][INFO] - end of epoch 246 (average epoch stats below)
[2024-10-06 09:28:19,711][train][INFO] - {"epoch": 246, "train_loss": "0.94", "train_ntokens": "260415", "train_nsentences": "1754.23", "train_wps": "66201.2", "train_ups": "0.25", "train_wpb": "260415", "train_bsz": "1754.2", "train_num_updates": "11798", "train_lr": "0.000184344", "train_gnorm": "0.948", "train_loss_scale": "1", "train_train_wall": "77", "train_gb_free": "39.2", "train_wall": "7459"}
[2024-10-06 09:28:19,910][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 09:28:19,920][fairseq.trainer][INFO] - begin training epoch 247
[2024-10-06 09:28:19,921][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 09:30:22,367][train_inner][INFO] - {"epoch": 247, "update": 246.042, "loss": "0.941", "ntokens": "260763", "nsentences": "1749.46", "wps": "65258.5", "ups": "0.25", "wpb": "260763", "bsz": "1749.5", "num_updates": "11800", "lr": "0.000184375", "gnorm": "0.784", "loss_scale": "1", "train_wall": "327", "gb_free": "39.8", "wall": "7582"}
[2024-10-06 09:31:12,687][fairseq_cli.train][INFO] - end of epoch 247 (average epoch stats below)
[2024-10-06 09:31:12,690][train][INFO] - {"epoch": 247, "train_loss": "0.935", "train_ntokens": "260833", "train_nsentences": "1750.04", "train_wps": "72379.8", "train_ups": "0.28", "train_wpb": "260833", "train_bsz": "1750", "train_num_updates": "11846", "train_lr": "0.000185094", "train_gnorm": "0.782", "train_loss_scale": "1", "train_train_wall": "56", "train_gb_free": "40.6", "train_wall": "7632"}
[2024-10-06 09:31:12,949][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 09:31:12,961][fairseq.trainer][INFO] - begin training epoch 248
[2024-10-06 09:31:12,961][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 09:33:37,548][fairseq_cli.train][INFO] - end of epoch 248 (average epoch stats below)
[2024-10-06 09:33:37,553][train][INFO] - {"epoch": 248, "train_loss": "0.936", "train_ntokens": "260582", "train_nsentences": "1750.04", "train_wps": "86350", "train_ups": "0.33", "train_wpb": "260582", "train_bsz": "1750", "train_num_updates": "11894", "train_lr": "0.000185844", "train_gnorm": "0.739", "train_loss_scale": "1", "train_train_wall": "30", "train_gb_free": "39.8", "train_wall": "7777"}
[2024-10-06 09:33:37,691][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 09:33:37,714][fairseq.trainer][INFO] - begin training epoch 249
[2024-10-06 09:33:37,714][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 09:36:12,797][fairseq_cli.train][INFO] - end of epoch 249 (average epoch stats below)
[2024-10-06 09:36:12,803][train][INFO] - {"epoch": 249, "train_loss": "0.932", "train_ntokens": "260921", "train_nsentences": "1750.04", "train_wps": "80672.9", "train_ups": "0.31", "train_wpb": "260921", "train_bsz": "1750", "train_num_updates": "11942", "train_lr": "0.000186594", "train_gnorm": "0.734", "train_loss_scale": "1", "train_train_wall": "64", "train_gb_free": "40.1", "train_wall": "7932"}
[2024-10-06 09:36:12,970][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 09:36:12,997][fairseq.trainer][INFO] - begin training epoch 250
[2024-10-06 09:36:12,998][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 09:38:40,422][fairseq_cli.train][INFO] - end of epoch 250 (average epoch stats below)
[2024-10-06 09:38:40,426][train][INFO] - {"epoch": 250, "train_loss": "0.932", "train_ntokens": "261176", "train_nsentences": "1750.04", "train_wps": "84923.2", "train_ups": "0.33", "train_wpb": "261176", "train_bsz": "1750", "train_num_updates": "11990", "train_lr": "0.000187344", "train_gnorm": "0.883", "train_loss_scale": "1", "train_train_wall": "60", "train_gb_free": "39.3", "train_wall": "8080"}
[2024-10-06 09:38:40,583][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 09:38:40,622][fairseq.trainer][INFO] - begin training epoch 251
[2024-10-06 09:38:40,622][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 09:40:41,618][train_inner][INFO] - {"epoch": 251, "update": 250.208, "loss": "0.933", "ntokens": "260828", "nsentences": "1746.48", "wps": "84247.8", "ups": "0.32", "wpb": "260828", "bsz": "1746.5", "num_updates": "12000", "lr": "0.0001875", "gnorm": "0.77", "loss_scale": "1", "train_wall": "202", "gb_free": "39.8", "wall": "8201"}
[2024-10-06 09:41:07,991][fairseq_cli.train][INFO] - end of epoch 251 (average epoch stats below)
[2024-10-06 09:41:07,993][train][INFO] - {"epoch": 251, "train_loss": "0.927", "train_ntokens": "260752", "train_nsentences": "1750.04", "train_wps": "84818", "train_ups": "0.33", "train_wpb": "260752", "train_bsz": "1750", "train_num_updates": "12038", "train_lr": "0.000188094", "train_gnorm": "0.583", "train_loss_scale": "1", "train_train_wall": "42", "train_gb_free": "40.1", "train_wall": "8227"}
[2024-10-06 09:41:08,162][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 09:41:08,174][fairseq.trainer][INFO] - begin training epoch 252
[2024-10-06 09:41:08,174][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 09:43:42,017][fairseq_cli.train][INFO] - end of epoch 252 (average epoch stats below)
[2024-10-06 09:43:42,025][train][INFO] - {"epoch": 252, "train_loss": "0.93", "train_ntokens": "260888", "train_nsentences": "1750.04", "train_wps": "81300.7", "train_ups": "0.31", "train_wpb": "260888", "train_bsz": "1750", "train_num_updates": "12086", "train_lr": "0.000188844", "train_gnorm": "0.822", "train_loss_scale": "1", "train_train_wall": "62", "train_gb_free": "40.1", "train_wall": "8381"}
[2024-10-06 09:43:42,246][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 09:43:42,252][fairseq.trainer][INFO] - begin training epoch 253
[2024-10-06 09:43:42,252][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 09:46:17,450][fairseq_cli.train][INFO] - end of epoch 253 (average epoch stats below)
[2024-10-06 09:46:17,454][train][INFO] - {"epoch": 253, "train_loss": "0.927", "train_ntokens": "260796", "train_nsentences": "1750.04", "train_wps": "80541", "train_ups": "0.31", "train_wpb": "260796", "train_bsz": "1750", "train_num_updates": "12134", "train_lr": "0.000189594", "train_gnorm": "0.658", "train_loss_scale": "1", "train_train_wall": "71", "train_gb_free": "39.6", "train_wall": "8537"}
[2024-10-06 09:46:17,588][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 09:46:17,594][fairseq.trainer][INFO] - begin training epoch 254
[2024-10-06 09:46:17,595][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 09:48:27,507][fairseq_cli.train][INFO] - end of epoch 254 (average epoch stats below)
[2024-10-06 09:48:27,523][train][INFO] - {"epoch": 254, "train_loss": "0.926", "train_ntokens": "260715", "train_nsentences": "1750.04", "train_wps": "96217.7", "train_ups": "0.37", "train_wpb": "260715", "train_bsz": "1750", "train_num_updates": "12182", "train_lr": "0.000190344", "train_gnorm": "0.861", "train_loss_scale": "1", "train_train_wall": "60", "train_gb_free": "39.8", "train_wall": "8667"}
[2024-10-06 09:48:27,673][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 09:48:27,690][fairseq.trainer][INFO] - begin training epoch 255
[2024-10-06 09:48:27,690][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 09:50:13,750][train_inner][INFO] - {"epoch": 255, "update": 254.375, "loss": "0.929", "ntokens": "260840", "nsentences": "1760.56", "wps": "91183.8", "ups": "0.35", "wpb": "260840", "bsz": "1760.6", "num_updates": "12200", "lr": "0.000190625", "gnorm": "0.739", "loss_scale": "1", "train_wall": "239", "gb_free": "39.3", "wall": "8773"}
[2024-10-06 09:50:39,801][fairseq_cli.train][INFO] - end of epoch 255 (average epoch stats below)
[2024-10-06 09:50:39,808][train][INFO] - {"epoch": 255, "train_loss": "0.924", "train_ntokens": "260854", "train_nsentences": "1750.04", "train_wps": "94656.5", "train_ups": "0.36", "train_wpb": "260854", "train_bsz": "1750", "train_num_updates": "12230", "train_lr": "0.000191094", "train_gnorm": "0.665", "train_loss_scale": "1", "train_train_wall": "47", "train_gb_free": "40.3", "train_wall": "8799"}
[2024-10-06 09:50:39,930][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 09:50:39,945][fairseq.trainer][INFO] - begin training epoch 256
[2024-10-06 09:50:39,945][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 09:52:45,539][fairseq_cli.train][INFO] - end of epoch 256 (average epoch stats below)
[2024-10-06 09:52:45,544][train][INFO] - {"epoch": 256, "train_loss": "0.925", "train_ntokens": "260610", "train_nsentences": "1750.04", "train_wps": "99491.5", "train_ups": "0.38", "train_wpb": "260610", "train_bsz": "1750", "train_num_updates": "12278", "train_lr": "0.000191844", "train_gnorm": "0.8", "train_loss_scale": "1", "train_train_wall": "57", "train_gb_free": "39.2", "train_wall": "8925"}
[2024-10-06 09:52:45,674][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 09:52:45,681][fairseq.trainer][INFO] - begin training epoch 257
[2024-10-06 09:52:45,682][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 09:54:56,904][fairseq_cli.train][INFO] - end of epoch 257 (average epoch stats below)
[2024-10-06 09:54:56,911][train][INFO] - {"epoch": 257, "train_loss": "0.922", "train_ntokens": "260535", "train_nsentences": "1750.04", "train_wps": "95198.4", "train_ups": "0.37", "train_wpb": "260535", "train_bsz": "1750", "train_num_updates": "12326", "train_lr": "0.000192594", "train_gnorm": "0.725", "train_loss_scale": "1", "train_train_wall": "57", "train_gb_free": "39.8", "train_wall": "9056"}
[2024-10-06 09:54:57,169][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 09:54:57,175][fairseq.trainer][INFO] - begin training epoch 258
[2024-10-06 09:54:57,175][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 09:57:12,300][fairseq_cli.train][INFO] - end of epoch 258 (average epoch stats below)
[2024-10-06 09:57:12,307][train][INFO] - {"epoch": 258, "train_loss": "0.917", "train_ntokens": "260514", "train_nsentences": "1750.04", "train_wps": "92358.6", "train_ups": "0.35", "train_wpb": "260514", "train_bsz": "1750", "train_num_updates": "12374", "train_lr": "0.000193344", "train_gnorm": "0.726", "train_loss_scale": "1", "train_train_wall": "62", "train_gb_free": "40.1", "train_wall": "9192"}
[2024-10-06 09:57:12,471][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 09:57:12,476][fairseq.trainer][INFO] - begin training epoch 259
[2024-10-06 09:57:12,477][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 09:59:06,026][train_inner][INFO] - {"epoch": 259, "update": 258.542, "loss": "0.92", "ntokens": "260548", "nsentences": "1745.21", "wps": "97900.5", "ups": "0.38", "wpb": "260548", "bsz": "1745.2", "num_updates": "12400", "lr": "0.00019375", "gnorm": "0.721", "loss_scale": "1", "train_wall": "240", "gb_free": "39.7", "wall": "9305"}
[2024-10-06 09:59:26,564][fairseq_cli.train][INFO] - end of epoch 259 (average epoch stats below)
[2024-10-06 09:59:26,567][train][INFO] - {"epoch": 259, "train_loss": "0.916", "train_ntokens": "260177", "train_nsentences": "1750.04", "train_wps": "93019.8", "train_ups": "0.36", "train_wpb": "260177", "train_bsz": "1750", "train_num_updates": "12422", "train_lr": "0.000194094", "train_gnorm": "0.622", "train_loss_scale": "1", "train_train_wall": "58", "train_gb_free": "39.8", "train_wall": "9326"}
[2024-10-06 09:59:26,720][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 09:59:26,732][fairseq.trainer][INFO] - begin training epoch 260
[2024-10-06 09:59:26,733][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 10:01:56,283][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 260 @ 12470 updates
[2024-10-06 10:01:56,284][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-06 10:02:00,064][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-06 10:02:00,067][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 260 @ 12470 updates, score None) (writing took 3.783889517188072 seconds)
[2024-10-06 10:02:00,067][fairseq_cli.train][INFO] - end of epoch 260 (average epoch stats below)
[2024-10-06 10:02:00,070][train][INFO] - {"epoch": 260, "train_loss": "0.917", "train_ntokens": "260514", "train_nsentences": "1750.04", "train_wps": "81464", "train_ups": "0.31", "train_wpb": "260514", "train_bsz": "1750", "train_num_updates": "12470", "train_lr": "0.000194844", "train_gnorm": "0.772", "train_loss_scale": "1", "train_train_wall": "47", "train_gb_free": "39.8", "train_wall": "9479"}
[2024-10-06 10:02:00,178][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 10:02:00,185][fairseq.trainer][INFO] - begin training epoch 261
[2024-10-06 10:02:00,185][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 10:04:08,584][fairseq_cli.train][INFO] - end of epoch 261 (average epoch stats below)
[2024-10-06 10:04:08,594][train][INFO] - {"epoch": 261, "train_loss": "0.914", "train_ntokens": "260162", "train_nsentences": "1750.04", "train_wps": "97165.1", "train_ups": "0.37", "train_wpb": "260162", "train_bsz": "1750", "train_num_updates": "12518", "train_lr": "0.000195594", "train_gnorm": "0.706", "train_loss_scale": "1", "train_train_wall": "31", "train_gb_free": "39.3", "train_wall": "9608"}
[2024-10-06 10:04:08,716][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 10:04:08,721][fairseq.trainer][INFO] - begin training epoch 262
[2024-10-06 10:04:08,721][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 10:06:23,295][fairseq_cli.train][INFO] - end of epoch 262 (average epoch stats below)
[2024-10-06 10:06:23,300][train][INFO] - {"epoch": 262, "train_loss": "0.914", "train_ntokens": "260535", "train_nsentences": "1750.04", "train_wps": "92839.1", "train_ups": "0.36", "train_wpb": "260535", "train_bsz": "1750", "train_num_updates": "12566", "train_lr": "0.000196344", "train_gnorm": "0.877", "train_loss_scale": "1", "train_train_wall": "66", "train_gb_free": "40.6", "train_wall": "9743"}
[2024-10-06 10:06:23,447][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 10:06:23,465][fairseq.trainer][INFO] - begin training epoch 263
[2024-10-06 10:06:23,465][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 10:08:35,719][train_inner][INFO] - {"epoch": 263, "update": 262.708, "loss": "0.915", "ntokens": "260360", "nsentences": "1757.22", "wps": "91403.9", "ups": "0.35", "wpb": "260360", "bsz": "1757.2", "num_updates": "12600", "lr": "0.000196875", "gnorm": "0.727", "loss_scale": "1", "train_wall": "218", "gb_free": "39.3", "wall": "9875"}
[2024-10-06 10:08:47,741][fairseq_cli.train][INFO] - end of epoch 263 (average epoch stats below)
[2024-10-06 10:08:47,755][train][INFO] - {"epoch": 263, "train_loss": "0.911", "train_ntokens": "260771", "train_nsentences": "1750.04", "train_wps": "86658.4", "train_ups": "0.33", "train_wpb": "260771", "train_bsz": "1750", "train_num_updates": "12614", "train_lr": "0.000197094", "train_gnorm": "0.598", "train_loss_scale": "1", "train_train_wall": "66", "train_gb_free": "40.3", "train_wall": "9887"}
[2024-10-06 10:08:47,939][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 10:08:47,944][fairseq.trainer][INFO] - begin training epoch 264
[2024-10-06 10:08:47,944][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 10:11:13,810][fairseq_cli.train][INFO] - end of epoch 264 (average epoch stats below)
[2024-10-06 10:11:13,817][train][INFO] - {"epoch": 264, "train_loss": "0.912", "train_ntokens": "260678", "train_nsentences": "1750.04", "train_wps": "85668.5", "train_ups": "0.33", "train_wpb": "260678", "train_bsz": "1750", "train_num_updates": "12662", "train_lr": "0.000197844", "train_gnorm": "0.637", "train_loss_scale": "1", "train_train_wall": "60", "train_gb_free": "39.4", "train_wall": "10033"}
[2024-10-06 10:11:13,977][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 10:11:13,990][fairseq.trainer][INFO] - begin training epoch 265
[2024-10-06 10:11:13,990][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 10:13:36,493][fairseq_cli.train][INFO] - end of epoch 265 (average epoch stats below)
[2024-10-06 10:13:36,503][train][INFO] - {"epoch": 265, "train_loss": "0.91", "train_ntokens": "260550", "train_nsentences": "1750.04", "train_wps": "87655", "train_ups": "0.34", "train_wpb": "260550", "train_bsz": "1750", "train_num_updates": "12710", "train_lr": "0.000198594", "train_gnorm": "0.727", "train_loss_scale": "1", "train_train_wall": "32", "train_gb_free": "39.2", "train_wall": "10176"}
[2024-10-06 10:13:36,700][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 10:13:36,704][fairseq.trainer][INFO] - begin training epoch 266
[2024-10-06 10:13:36,704][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 10:16:00,078][fairseq_cli.train][INFO] - end of epoch 266 (average epoch stats below)
[2024-10-06 10:16:00,086][train][INFO] - {"epoch": 266, "train_loss": "0.909", "train_ntokens": "260681", "train_nsentences": "1750.04", "train_wps": "87147.6", "train_ups": "0.33", "train_wpb": "260681", "train_bsz": "1750", "train_num_updates": "12758", "train_lr": "0.000199344", "train_gnorm": "0.683", "train_loss_scale": "1", "train_train_wall": "50", "train_gb_free": "40.1", "train_wall": "10319"}
[2024-10-06 10:16:00,153][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 10:16:00,171][fairseq.trainer][INFO] - begin training epoch 267
[2024-10-06 10:16:00,172][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 10:18:05,735][train_inner][INFO] - {"epoch": 267, "update": 266.875, "loss": "0.91", "ntokens": "260594", "nsentences": "1750.36", "wps": "91437.7", "ups": "0.35", "wpb": "260594", "bsz": "1750.4", "num_updates": "12800", "lr": "0.0002", "gnorm": "0.701", "loss_scale": "1", "train_wall": "196", "gb_free": "39.7", "wall": "10445"}
[2024-10-06 10:18:07,977][fairseq_cli.train][INFO] - end of epoch 267 (average epoch stats below)
[2024-10-06 10:18:07,980][train][INFO] - {"epoch": 267, "train_loss": "0.906", "train_ntokens": "260703", "train_nsentences": "1750.04", "train_wps": "97847.3", "train_ups": "0.38", "train_wpb": "260703", "train_bsz": "1750", "train_num_updates": "12806", "train_lr": "0.000200094", "train_gnorm": "0.796", "train_loss_scale": "1", "train_train_wall": "45", "train_gb_free": "40.2", "train_wall": "10447"}
[2024-10-06 10:18:08,133][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 10:18:08,147][fairseq.trainer][INFO] - begin training epoch 268
[2024-10-06 10:18:08,148][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 10:20:10,844][fairseq_cli.train][INFO] - end of epoch 268 (average epoch stats below)
[2024-10-06 10:20:10,847][train][INFO] - {"epoch": 268, "train_loss": "0.907", "train_ntokens": "260822", "train_nsentences": "1750.04", "train_wps": "101896", "train_ups": "0.39", "train_wpb": "260822", "train_bsz": "1750", "train_num_updates": "12854", "train_lr": "0.000200844", "train_gnorm": "0.775", "train_loss_scale": "1", "train_train_wall": "47", "train_gb_free": "39.3", "train_wall": "10570"}
[2024-10-06 10:20:10,962][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 10:20:10,968][fairseq.trainer][INFO] - begin training epoch 269
[2024-10-06 10:20:10,968][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 10:22:20,713][fairseq_cli.train][INFO] - end of epoch 269 (average epoch stats below)
[2024-10-06 10:22:20,718][train][INFO] - {"epoch": 269, "train_loss": "0.905", "train_ntokens": "260336", "train_nsentences": "1750.04", "train_wps": "96222.1", "train_ups": "0.37", "train_wpb": "260336", "train_bsz": "1750", "train_num_updates": "12902", "train_lr": "0.000201594", "train_gnorm": "0.838", "train_loss_scale": "1", "train_train_wall": "56", "train_gb_free": "40.3", "train_wall": "10700"}
[2024-10-06 10:22:20,929][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 10:22:20,944][fairseq.trainer][INFO] - begin training epoch 270
[2024-10-06 10:22:20,945][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 10:24:35,077][fairseq_cli.train][INFO] - end of epoch 270 (average epoch stats below)
[2024-10-06 10:24:35,085][train][INFO] - {"epoch": 270, "train_loss": "0.901", "train_ntokens": "260616", "train_nsentences": "1750.04", "train_wps": "93101.8", "train_ups": "0.36", "train_wpb": "260616", "train_bsz": "1750", "train_num_updates": "12950", "train_lr": "0.000202344", "train_gnorm": "0.614", "train_loss_scale": "1", "train_train_wall": "58", "train_gb_free": "39.6", "train_wall": "10834"}
[2024-10-06 10:24:35,198][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 10:24:35,202][fairseq.trainer][INFO] - begin training epoch 271
[2024-10-06 10:24:35,203][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 10:26:57,094][fairseq_cli.train][INFO] - end of epoch 271 (average epoch stats below)
[2024-10-06 10:26:57,097][train][INFO] - {"epoch": 271, "train_loss": "0.9", "train_ntokens": "260718", "train_nsentences": "1750.04", "train_wps": "88124.1", "train_ups": "0.34", "train_wpb": "260718", "train_bsz": "1750", "train_num_updates": "12998", "train_lr": "0.000203094", "train_gnorm": "0.94", "train_loss_scale": "1", "train_train_wall": "48", "train_gb_free": "39.7", "train_wall": "10976"}
[2024-10-06 10:26:57,214][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 10:26:57,229][fairseq.trainer][INFO] - begin training epoch 272
[2024-10-06 10:26:57,229][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 10:28:43,424][train_inner][INFO] - {"epoch": 272, "update": 271.042, "loss": "0.903", "ntokens": "260785", "nsentences": "1738.58", "wps": "81791.4", "ups": "0.31", "wpb": "260785", "bsz": "1738.6", "num_updates": "13000", "lr": "0.000203125", "gnorm": "0.792", "loss_scale": "1", "train_wall": "221", "gb_free": "39.6", "wall": "11083"}
[2024-10-06 10:29:50,193][fairseq_cli.train][INFO] - end of epoch 272 (average epoch stats below)
[2024-10-06 10:29:50,196][train][INFO] - {"epoch": 272, "train_loss": "0.904", "train_ntokens": "260755", "train_nsentences": "1750.04", "train_wps": "72308.2", "train_ups": "0.28", "train_wpb": "260755", "train_bsz": "1750", "train_num_updates": "13046", "train_lr": "0.000203844", "train_gnorm": "0.759", "train_loss_scale": "1", "train_train_wall": "75", "train_gb_free": "39.8", "train_wall": "11150"}
[2024-10-06 10:29:50,346][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 10:29:50,354][fairseq.trainer][INFO] - begin training epoch 273
[2024-10-06 10:29:50,355][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 10:32:33,304][fairseq_cli.train][INFO] - end of epoch 273 (average epoch stats below)
[2024-10-06 10:32:33,324][train][INFO] - {"epoch": 273, "train_loss": "0.901", "train_ntokens": "261081", "train_nsentences": "1750.04", "train_wps": "76824.3", "train_ups": "0.29", "train_wpb": "261082", "train_bsz": "1750", "train_num_updates": "13094", "train_lr": "0.000204594", "train_gnorm": "0.661", "train_loss_scale": "1", "train_train_wall": "53", "train_gb_free": "40.7", "train_wall": "11313"}
[2024-10-06 10:32:33,480][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 10:32:33,483][fairseq.trainer][INFO] - begin training epoch 274
[2024-10-06 10:32:33,484][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 10:34:55,122][fairseq_cli.train][INFO] - end of epoch 274 (average epoch stats below)
[2024-10-06 10:34:55,135][train][INFO] - {"epoch": 274, "train_loss": "0.896", "train_ntokens": "260839", "train_nsentences": "1750.04", "train_wps": "88295.6", "train_ups": "0.34", "train_wpb": "260839", "train_bsz": "1750", "train_num_updates": "13142", "train_lr": "0.000205344", "train_gnorm": "0.718", "train_loss_scale": "1", "train_train_wall": "71", "train_gb_free": "39.7", "train_wall": "11454"}
[2024-10-06 10:34:55,274][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 10:34:55,280][fairseq.trainer][INFO] - begin training epoch 275
[2024-10-06 10:34:55,280][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 10:37:35,777][fairseq_cli.train][INFO] - end of epoch 275 (average epoch stats below)
[2024-10-06 10:37:35,782][train][INFO] - {"epoch": 275, "train_loss": "0.895", "train_ntokens": "260954", "train_nsentences": "1750.04", "train_wps": "77972.6", "train_ups": "0.3", "train_wpb": "260954", "train_bsz": "1750", "train_num_updates": "13190", "train_lr": "0.000206094", "train_gnorm": "0.747", "train_loss_scale": "1", "train_train_wall": "83", "train_gb_free": "39.3", "train_wall": "11615"}
[2024-10-06 10:37:35,922][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 10:37:35,945][fairseq.trainer][INFO] - begin training epoch 276
[2024-10-06 10:37:35,945][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 10:41:02,611][train_inner][INFO] - {"epoch": 276, "update": 275.208, "loss": "0.899", "ntokens": "260766", "nsentences": "1761.49", "wps": "70561.6", "ups": "0.27", "wpb": "260766", "bsz": "1761.5", "num_updates": "13200", "lr": "0.00020625", "gnorm": "0.711", "loss_scale": "1", "train_wall": "365", "gb_free": "40.1", "wall": "11822"}
[2024-10-06 10:41:29,614][fairseq_cli.train][INFO] - end of epoch 276 (average epoch stats below)
[2024-10-06 10:41:29,617][train][INFO] - {"epoch": 276, "train_loss": "0.893", "train_ntokens": "260546", "train_nsentences": "1750.04", "train_wps": "53483.9", "train_ups": "0.21", "train_wpb": "260546", "train_bsz": "1750", "train_num_updates": "13238", "train_lr": "0.000206844", "train_gnorm": "0.562", "train_loss_scale": "1", "train_train_wall": "119", "train_gb_free": "40.1", "train_wall": "11849"}
[2024-10-06 10:41:29,911][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 10:41:29,914][fairseq.trainer][INFO] - begin training epoch 277
[2024-10-06 10:41:29,914][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 10:44:40,649][fairseq_cli.train][INFO] - end of epoch 277 (average epoch stats below)
[2024-10-06 10:44:40,663][train][INFO] - {"epoch": 277, "train_loss": "0.894", "train_ntokens": "260690", "train_nsentences": "1750.04", "train_wps": "65499.4", "train_ups": "0.25", "train_wpb": "260690", "train_bsz": "1750", "train_num_updates": "13286", "train_lr": "0.000207594", "train_gnorm": "0.669", "train_loss_scale": "1", "train_train_wall": "50", "train_gb_free": "39.1", "train_wall": "12040"}
[2024-10-06 10:44:40,836][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 10:44:40,840][fairseq.trainer][INFO] - begin training epoch 278
[2024-10-06 10:44:40,841][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 10:47:29,138][fairseq_cli.train][INFO] - end of epoch 278 (average epoch stats below)
[2024-10-06 10:47:29,142][train][INFO] - {"epoch": 278, "train_loss": "0.891", "train_ntokens": "260570", "train_nsentences": "1750.04", "train_wps": "74238.5", "train_ups": "0.28", "train_wpb": "260570", "train_bsz": "1750", "train_num_updates": "13334", "train_lr": "0.000208344", "train_gnorm": "0.68", "train_loss_scale": "1", "train_train_wall": "78", "train_gb_free": "40.1", "train_wall": "12208"}
[2024-10-06 10:47:29,261][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 10:47:29,281][fairseq.trainer][INFO] - begin training epoch 279
[2024-10-06 10:47:29,281][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 10:50:31,607][fairseq_cli.train][INFO] - end of epoch 279 (average epoch stats below)
[2024-10-06 10:50:31,613][train][INFO] - {"epoch": 279, "train_loss": "0.893", "train_ntokens": "260987", "train_nsentences": "1750.04", "train_wps": "68655.3", "train_ups": "0.26", "train_wpb": "260987", "train_bsz": "1750", "train_num_updates": "13382", "train_lr": "0.000209094", "train_gnorm": "0.942", "train_loss_scale": "1", "train_train_wall": "56", "train_gb_free": "39.3", "train_wall": "12391"}
[2024-10-06 10:50:31,757][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 10:50:31,767][fairseq.trainer][INFO] - begin training epoch 280
[2024-10-06 10:50:31,767][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 10:53:03,849][train_inner][INFO] - {"epoch": 280, "update": 279.375, "loss": "0.892", "ntokens": "260800", "nsentences": "1748.73", "wps": "72328.1", "ups": "0.28", "wpb": "260800", "bsz": "1748.7", "num_updates": "13400", "lr": "0.000209375", "gnorm": "0.702", "loss_scale": "1", "train_wall": "248", "gb_free": "39.3", "wall": "12543"}
[2024-10-06 10:53:20,944][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 280 @ 13430 updates
[2024-10-06 10:53:20,945][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-06 10:53:24,898][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-06 10:53:24,916][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 280 @ 13430 updates, score None) (writing took 3.971807500347495 seconds)
[2024-10-06 10:53:24,917][fairseq_cli.train][INFO] - end of epoch 280 (average epoch stats below)
[2024-10-06 10:53:24,920][train][INFO] - {"epoch": 280, "train_loss": "0.885", "train_ntokens": "260655", "train_nsentences": "1750.04", "train_wps": "72194.8", "train_ups": "0.28", "train_wpb": "260654", "train_bsz": "1750", "train_num_updates": "13430", "train_lr": "0.000209844", "train_gnorm": "0.512", "train_loss_scale": "1", "train_train_wall": "54", "train_gb_free": "39.2", "train_wall": "12564"}
[2024-10-06 10:53:25,053][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 10:53:25,058][fairseq.trainer][INFO] - begin training epoch 281
[2024-10-06 10:53:25,058][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 10:55:37,640][fairseq_cli.train][INFO] - end of epoch 281 (average epoch stats below)
[2024-10-06 10:55:37,644][train][INFO] - {"epoch": 281, "train_loss": "0.893", "train_ntokens": "260836", "train_nsentences": "1750.04", "train_wps": "94334.7", "train_ups": "0.36", "train_wpb": "260836", "train_bsz": "1750", "train_num_updates": "13478", "train_lr": "0.000210594", "train_gnorm": "0.635", "train_loss_scale": "1", "train_train_wall": "59", "train_gb_free": "39.8", "train_wall": "12697"}
[2024-10-06 10:55:37,768][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 10:55:37,776][fairseq.trainer][INFO] - begin training epoch 282
[2024-10-06 10:55:37,776][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 10:58:15,623][fairseq_cli.train][INFO] - end of epoch 282 (average epoch stats below)
[2024-10-06 10:58:15,631][train][INFO] - {"epoch": 282, "train_loss": "0.89", "train_ntokens": "260973", "train_nsentences": "1750.04", "train_wps": "79291.1", "train_ups": "0.3", "train_wpb": "260973", "train_bsz": "1750", "train_num_updates": "13526", "train_lr": "0.000211344", "train_gnorm": "0.807", "train_loss_scale": "1", "train_train_wall": "45", "train_gb_free": "39.2", "train_wall": "12855"}
[2024-10-06 10:58:15,786][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 10:58:15,799][fairseq.trainer][INFO] - begin training epoch 283
[2024-10-06 10:58:15,799][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 11:00:58,561][fairseq_cli.train][INFO] - end of epoch 283 (average epoch stats below)
[2024-10-06 11:00:58,566][train][INFO] - {"epoch": 283, "train_loss": "0.885", "train_ntokens": "260717", "train_nsentences": "1750.04", "train_wps": "76807.7", "train_ups": "0.29", "train_wpb": "260717", "train_bsz": "1750", "train_num_updates": "13574", "train_lr": "0.000212094", "train_gnorm": "0.647", "train_loss_scale": "1", "train_train_wall": "41", "train_gb_free": "40.1", "train_wall": "13018"}
[2024-10-06 11:00:58,729][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 11:00:58,734][fairseq.trainer][INFO] - begin training epoch 284
[2024-10-06 11:00:58,734][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 11:03:32,007][train_inner][INFO] - {"epoch": 284, "update": 283.542, "loss": "0.888", "ntokens": "260854", "nsentences": "1747.35", "wps": "83054.3", "ups": "0.32", "wpb": "260854", "bsz": "1747.3", "num_updates": "13600", "lr": "0.0002125", "gnorm": "0.673", "loss_scale": "1", "train_wall": "230", "gb_free": "39.6", "wall": "13171"}
[2024-10-06 11:03:48,932][fairseq_cli.train][INFO] - end of epoch 284 (average epoch stats below)
[2024-10-06 11:03:48,934][train][INFO] - {"epoch": 284, "train_loss": "0.886", "train_ntokens": "260898", "train_nsentences": "1750.04", "train_wps": "73507.6", "train_ups": "0.28", "train_wpb": "260898", "train_bsz": "1750", "train_num_updates": "13622", "train_lr": "0.000212844", "train_gnorm": "0.717", "train_loss_scale": "1", "train_train_wall": "83", "train_gb_free": "39.7", "train_wall": "13188"}
[2024-10-06 11:03:49,096][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 11:03:49,101][fairseq.trainer][INFO] - begin training epoch 285
[2024-10-06 11:03:49,101][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 11:06:21,109][fairseq_cli.train][INFO] - end of epoch 285 (average epoch stats below)
[2024-10-06 11:06:21,116][train][INFO] - {"epoch": 285, "train_loss": "0.884", "train_ntokens": "260497", "train_nsentences": "1750.04", "train_wps": "82164.9", "train_ups": "0.32", "train_wpb": "260497", "train_bsz": "1750", "train_num_updates": "13670", "train_lr": "0.000213594", "train_gnorm": "0.62", "train_loss_scale": "1", "train_train_wall": "73", "train_gb_free": "39.6", "train_wall": "13340"}
[2024-10-06 11:06:21,258][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 11:06:21,272][fairseq.trainer][INFO] - begin training epoch 286
[2024-10-06 11:06:21,273][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 11:08:58,771][fairseq_cli.train][INFO] - end of epoch 286 (average epoch stats below)
[2024-10-06 11:08:58,775][train][INFO] - {"epoch": 286, "train_loss": "0.884", "train_ntokens": "261415", "train_nsentences": "1750.04", "train_wps": "79591", "train_ups": "0.3", "train_wpb": "261415", "train_bsz": "1750", "train_num_updates": "13718", "train_lr": "0.000214344", "train_gnorm": "0.702", "train_loss_scale": "1", "train_train_wall": "62", "train_gb_free": "40.1", "train_wall": "13498"}
[2024-10-06 11:08:58,904][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 11:08:58,925][fairseq.trainer][INFO] - begin training epoch 287
[2024-10-06 11:08:58,926][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 11:11:35,242][fairseq_cli.train][INFO] - end of epoch 287 (average epoch stats below)
[2024-10-06 11:11:35,260][train][INFO] - {"epoch": 287, "train_loss": "0.883", "train_ntokens": "260399", "train_nsentences": "1750.04", "train_wps": "79877.9", "train_ups": "0.31", "train_wpb": "260399", "train_bsz": "1750", "train_num_updates": "13766", "train_lr": "0.000215094", "train_gnorm": "0.587", "train_loss_scale": "1", "train_train_wall": "71", "train_gb_free": "40.1", "train_wall": "13655"}
[2024-10-06 11:11:35,429][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 11:11:35,435][fairseq.trainer][INFO] - begin training epoch 288
[2024-10-06 11:11:35,436][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 11:13:43,227][train_inner][INFO] - {"epoch": 288, "update": 287.708, "loss": "0.884", "ntokens": "260605", "nsentences": "1749.34", "wps": "85274.6", "ups": "0.33", "wpb": "260605", "bsz": "1749.3", "num_updates": "13800", "lr": "0.000215625", "gnorm": "0.647", "loss_scale": "1", "train_wall": "268", "gb_free": "39.3", "wall": "13783"}
[2024-10-06 11:13:56,483][fairseq_cli.train][INFO] - end of epoch 288 (average epoch stats below)
[2024-10-06 11:13:56,485][train][INFO] - {"epoch": 288, "train_loss": "0.88", "train_ntokens": "260812", "train_nsentences": "1750.04", "train_wps": "88647.9", "train_ups": "0.34", "train_wpb": "260812", "train_bsz": "1750", "train_num_updates": "13814", "train_lr": "0.000215844", "train_gnorm": "0.698", "train_loss_scale": "1", "train_train_wall": "59", "train_gb_free": "39.5", "train_wall": "13796"}
[2024-10-06 11:13:56,677][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 11:13:56,691][fairseq.trainer][INFO] - begin training epoch 289
[2024-10-06 11:13:56,691][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 11:16:36,178][fairseq_cli.train][INFO] - end of epoch 289 (average epoch stats below)
[2024-10-06 11:16:36,188][train][INFO] - {"epoch": 289, "train_loss": "0.88", "train_ntokens": "260676", "train_nsentences": "1750.04", "train_wps": "78349.6", "train_ups": "0.3", "train_wpb": "260676", "train_bsz": "1750", "train_num_updates": "13862", "train_lr": "0.000216594", "train_gnorm": "0.897", "train_loss_scale": "2", "train_train_wall": "75", "train_gb_free": "39.8", "train_wall": "13955"}
[2024-10-06 11:16:36,381][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 11:16:36,385][fairseq.trainer][INFO] - begin training epoch 290
[2024-10-06 11:16:36,386][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 11:19:17,758][fairseq_cli.train][INFO] - end of epoch 290 (average epoch stats below)
[2024-10-06 11:19:17,766][train][INFO] - {"epoch": 290, "train_loss": "0.873", "train_ntokens": "260630", "train_nsentences": "1750.04", "train_wps": "77427.8", "train_ups": "0.3", "train_wpb": "260630", "train_bsz": "1750", "train_num_updates": "13910", "train_lr": "0.000217344", "train_gnorm": "0.582", "train_loss_scale": "2", "train_train_wall": "55", "train_gb_free": "39.9", "train_wall": "14117"}
[2024-10-06 11:19:17,917][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 11:19:17,927][fairseq.trainer][INFO] - begin training epoch 291
[2024-10-06 11:19:17,928][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 11:21:57,915][fairseq_cli.train][INFO] - end of epoch 291 (average epoch stats below)
[2024-10-06 11:21:57,918][train][INFO] - {"epoch": 291, "train_loss": "0.882", "train_ntokens": "260422", "train_nsentences": "1750.04", "train_wps": "78053.5", "train_ups": "0.3", "train_wpb": "260422", "train_bsz": "1750", "train_num_updates": "13958", "train_lr": "0.000218094", "train_gnorm": "0.725", "train_loss_scale": "2", "train_train_wall": "81", "train_gb_free": "39.2", "train_wall": "14277"}
[2024-10-06 11:21:58,067][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 11:21:58,086][fairseq.trainer][INFO] - begin training epoch 292
[2024-10-06 11:21:58,086][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 11:24:27,252][train_inner][INFO] - {"epoch": 292, "update": 291.875, "loss": "0.877", "ntokens": "260802", "nsentences": "1738.46", "wps": "80991.7", "ups": "0.31", "wpb": "260802", "bsz": "1738.5", "num_updates": "14000", "lr": "0.00021875", "gnorm": "0.724", "loss_scale": "2", "train_wall": "273", "gb_free": "40.6", "wall": "14427"}
[2024-10-06 11:24:30,405][fairseq_cli.train][INFO] - end of epoch 292 (average epoch stats below)
[2024-10-06 11:24:30,407][train][INFO] - {"epoch": 292, "train_loss": "0.873", "train_ntokens": "260561", "train_nsentences": "1750.04", "train_wps": "82020.1", "train_ups": "0.31", "train_wpb": "260561", "train_bsz": "1750", "train_num_updates": "14006", "train_lr": "0.000218844", "train_gnorm": "0.639", "train_loss_scale": "2", "train_train_wall": "52", "train_gb_free": "40.3", "train_wall": "14430"}
[2024-10-06 11:24:30,551][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 11:24:30,566][fairseq.trainer][INFO] - begin training epoch 293
[2024-10-06 11:24:30,566][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 11:27:11,807][fairseq_cli.train][INFO] - end of epoch 293 (average epoch stats below)
[2024-10-06 11:27:11,813][train][INFO] - {"epoch": 293, "train_loss": "0.875", "train_ntokens": "260783", "train_nsentences": "1750.04", "train_wps": "77555.3", "train_ups": "0.3", "train_wpb": "260783", "train_bsz": "1750", "train_num_updates": "14054", "train_lr": "0.000219594", "train_gnorm": "0.814", "train_loss_scale": "2", "train_train_wall": "49", "train_gb_free": "39.8", "train_wall": "14591"}
[2024-10-06 11:27:11,962][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 11:27:11,966][fairseq.trainer][INFO] - begin training epoch 294
[2024-10-06 11:27:11,966][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 11:29:57,034][fairseq_cli.train][INFO] - end of epoch 294 (average epoch stats below)
[2024-10-06 11:29:57,044][train][INFO] - {"epoch": 294, "train_loss": "0.875", "train_ntokens": "260882", "train_nsentences": "1750.04", "train_wps": "75788.2", "train_ups": "0.29", "train_wpb": "260882", "train_bsz": "1750", "train_num_updates": "14102", "train_lr": "0.000220344", "train_gnorm": "0.7", "train_loss_scale": "2", "train_train_wall": "81", "train_gb_free": "39.8", "train_wall": "14756"}
[2024-10-06 11:29:57,166][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 11:29:57,185][fairseq.trainer][INFO] - begin training epoch 295
[2024-10-06 11:29:57,185][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 11:32:45,936][fairseq_cli.train][INFO] - end of epoch 295 (average epoch stats below)
[2024-10-06 11:32:45,941][train][INFO] - {"epoch": 295, "train_loss": "0.871", "train_ntokens": "260467", "train_nsentences": "1750.04", "train_wps": "74025.5", "train_ups": "0.28", "train_wpb": "260467", "train_bsz": "1750", "train_num_updates": "14150", "train_lr": "0.000221094", "train_gnorm": "0.622", "train_loss_scale": "2", "train_train_wall": "88", "train_gb_free": "40.5", "train_wall": "14925"}
[2024-10-06 11:32:46,058][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 11:32:46,067][fairseq.trainer][INFO] - begin training epoch 296
[2024-10-06 11:32:46,067][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 11:34:58,155][fairseq_cli.train][INFO] - end of epoch 296 (average epoch stats below)
[2024-10-06 11:34:58,168][train][INFO] - {"epoch": 296, "train_loss": "0.872", "train_ntokens": "261064", "train_nsentences": "1750.04", "train_wps": "94776.7", "train_ups": "0.36", "train_wpb": "261064", "train_bsz": "1750", "train_num_updates": "14198", "train_lr": "0.000221844", "train_gnorm": "0.748", "train_loss_scale": "2", "train_train_wall": "39", "train_gb_free": "40", "train_wall": "15057"}
[2024-10-06 11:34:58,291][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 11:34:58,295][fairseq.trainer][INFO] - begin training epoch 297
[2024-10-06 11:34:58,295][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 11:36:30,392][train_inner][INFO] - {"epoch": 297, "update": 296.042, "loss": "0.873", "ntokens": "260728", "nsentences": "1758.51", "wps": "72111.7", "ups": "0.28", "wpb": "260728", "bsz": "1758.5", "num_updates": "14200", "lr": "0.000221875", "gnorm": "0.715", "loss_scale": "2", "train_wall": "271", "gb_free": "39.6", "wall": "15150"}
[2024-10-06 11:37:04,039][fairseq_cli.train][INFO] - end of epoch 297 (average epoch stats below)
[2024-10-06 11:37:04,050][train][INFO] - {"epoch": 297, "train_loss": "0.871", "train_ntokens": "260828", "train_nsentences": "1750.04", "train_wps": "99466", "train_ups": "0.38", "train_wpb": "260828", "train_bsz": "1750", "train_num_updates": "14246", "train_lr": "0.000222594", "train_gnorm": "0.57", "train_loss_scale": "2", "train_train_wall": "43", "train_gb_free": "39.7", "train_wall": "15183"}
[2024-10-06 11:37:04,154][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 11:37:04,169][fairseq.trainer][INFO] - begin training epoch 298
[2024-10-06 11:37:04,169][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 11:39:12,568][fairseq_cli.train][INFO] - end of epoch 298 (average epoch stats below)
[2024-10-06 11:39:12,575][train][INFO] - {"epoch": 298, "train_loss": "0.866", "train_ntokens": "260583", "train_nsentences": "1750.04", "train_wps": "97322.4", "train_ups": "0.37", "train_wpb": "260583", "train_bsz": "1750", "train_num_updates": "14294", "train_lr": "0.000223344", "train_gnorm": "0.693", "train_loss_scale": "2", "train_train_wall": "58", "train_gb_free": "39.6", "train_wall": "15312"}
[2024-10-06 11:39:12,796][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 11:39:12,800][fairseq.trainer][INFO] - begin training epoch 299
[2024-10-06 11:39:12,801][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 11:41:37,556][fairseq_cli.train][INFO] - end of epoch 299 (average epoch stats below)
[2024-10-06 11:41:37,569][train][INFO] - {"epoch": 299, "train_loss": "0.867", "train_ntokens": "260467", "train_nsentences": "1750.04", "train_wps": "86229.1", "train_ups": "0.33", "train_wpb": "260467", "train_bsz": "1750", "train_num_updates": "14342", "train_lr": "0.000224094", "train_gnorm": "0.725", "train_loss_scale": "2", "train_train_wall": "50", "train_gb_free": "39.2", "train_wall": "15457"}
[2024-10-06 11:41:37,779][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 11:41:37,783][fairseq.trainer][INFO] - begin training epoch 300
[2024-10-06 11:41:37,784][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 11:44:43,094][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 300 @ 14390 updates
[2024-10-06 11:44:43,100][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-06 11:44:47,609][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-06 11:44:47,613][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 300 @ 14390 updates, score None) (writing took 4.518068806268275 seconds)
[2024-10-06 11:44:47,614][fairseq_cli.train][INFO] - end of epoch 300 (average epoch stats below)
[2024-10-06 11:44:47,618][train][INFO] - {"epoch": 300, "train_loss": "0.861", "train_ntokens": "260681", "train_nsentences": "1750.04", "train_wps": "65841.1", "train_ups": "0.25", "train_wpb": "260681", "train_bsz": "1750", "train_num_updates": "14390", "train_lr": "0.000224844", "train_gnorm": "0.598", "train_loss_scale": "2", "train_train_wall": "91", "train_gb_free": "40", "train_wall": "15647"}
[2024-10-06 11:44:47,801][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 11:44:47,828][fairseq.trainer][INFO] - begin training epoch 301
[2024-10-06 11:44:47,829][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 11:47:03,655][train_inner][INFO] - {"epoch": 301, "update": 300.208, "loss": "0.866", "ntokens": "260665", "nsentences": "1737.92", "wps": "82325.1", "ups": "0.32", "wpb": "260665", "bsz": "1737.9", "num_updates": "14400", "lr": "0.000225", "gnorm": "0.653", "loss_scale": "2", "train_wall": "280", "gb_free": "45", "wall": "15783"}
[2024-10-06 11:47:40,013][fairseq_cli.train][INFO] - end of epoch 301 (average epoch stats below)
[2024-10-06 11:47:40,018][train][INFO] - {"epoch": 301, "train_loss": "0.864", "train_ntokens": "260810", "train_nsentences": "1750.04", "train_wps": "72618.1", "train_ups": "0.28", "train_wpb": "260810", "train_bsz": "1750", "train_num_updates": "14438", "train_lr": "0.000225594", "train_gnorm": "0.67", "train_loss_scale": "2", "train_train_wall": "86", "train_gb_free": "40", "train_wall": "15819"}
[2024-10-06 11:47:40,237][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 11:47:40,243][fairseq.trainer][INFO] - begin training epoch 302
[2024-10-06 11:47:40,246][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 11:50:41,278][fairseq_cli.train][INFO] - end of epoch 302 (average epoch stats below)
[2024-10-06 11:50:41,283][train][INFO] - {"epoch": 302, "train_loss": "0.857", "train_ntokens": "260485", "train_nsentences": "1750.04", "train_wps": "68979.6", "train_ups": "0.26", "train_wpb": "260485", "train_bsz": "1750", "train_num_updates": "14486", "train_lr": "0.000226344", "train_gnorm": "0.558", "train_loss_scale": "2", "train_train_wall": "66", "train_gb_free": "40.2", "train_wall": "16001"}
[2024-10-06 11:50:41,495][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 11:50:41,513][fairseq.trainer][INFO] - begin training epoch 303
[2024-10-06 11:50:41,513][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 11:53:34,037][fairseq_cli.train][INFO] - end of epoch 303 (average epoch stats below)
[2024-10-06 11:53:34,053][train][INFO] - {"epoch": 303, "train_loss": "0.862", "train_ntokens": "260739", "train_nsentences": "1750.04", "train_wps": "72442.9", "train_ups": "0.28", "train_wpb": "260739", "train_bsz": "1750", "train_num_updates": "14534", "train_lr": "0.000227094", "train_gnorm": "0.726", "train_loss_scale": "2", "train_train_wall": "69", "train_gb_free": "39.8", "train_wall": "16173"}
[2024-10-06 11:53:34,319][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 11:53:34,324][fairseq.trainer][INFO] - begin training epoch 304
[2024-10-06 11:53:34,324][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 11:56:33,235][fairseq_cli.train][INFO] - end of epoch 304 (average epoch stats below)
[2024-10-06 11:56:33,240][train][INFO] - {"epoch": 304, "train_loss": "0.857", "train_ntokens": "261024", "train_nsentences": "1750.04", "train_wps": "69923.4", "train_ups": "0.27", "train_wpb": "261024", "train_bsz": "1750", "train_num_updates": "14582", "train_lr": "0.000227844", "train_gnorm": "0.713", "train_loss_scale": "2", "train_train_wall": "86", "train_gb_free": "40.1", "train_wall": "16353"}
[2024-10-06 11:56:33,422][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 11:56:33,430][fairseq.trainer][INFO] - begin training epoch 305
[2024-10-06 11:56:33,431][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 11:59:08,677][train_inner][INFO] - {"epoch": 305, "update": 304.375, "loss": "0.86", "ntokens": "260780", "nsentences": "1764.54", "wps": "71941", "ups": "0.28", "wpb": "260780", "bsz": "1764.5", "num_updates": "14600", "lr": "0.000228125", "gnorm": "0.67", "loss_scale": "2", "train_wall": "320", "gb_free": "39.3", "wall": "16508"}
[2024-10-06 11:59:29,032][fairseq_cli.train][INFO] - end of epoch 305 (average epoch stats below)
[2024-10-06 11:59:29,034][train][INFO] - {"epoch": 305, "train_loss": "0.859", "train_ntokens": "260940", "train_nsentences": "1750.04", "train_wps": "71250", "train_ups": "0.27", "train_wpb": "260940", "train_bsz": "1750", "train_num_updates": "14630", "train_lr": "0.000228594", "train_gnorm": "0.715", "train_loss_scale": "2", "train_train_wall": "83", "train_gb_free": "39.2", "train_wall": "16528"}
[2024-10-06 11:59:29,244][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 11:59:29,254][fairseq.trainer][INFO] - begin training epoch 306
[2024-10-06 11:59:29,254][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 12:02:39,420][fairseq_cli.train][INFO] - end of epoch 306 (average epoch stats below)
[2024-10-06 12:02:39,427][train][INFO] - {"epoch": 306, "train_loss": "0.858", "train_ntokens": "260666", "train_nsentences": "1750.04", "train_wps": "65717.9", "train_ups": "0.25", "train_wpb": "260666", "train_bsz": "1750", "train_num_updates": "14678", "train_lr": "0.000229344", "train_gnorm": "0.569", "train_loss_scale": "2", "train_train_wall": "103", "train_gb_free": "39.8", "train_wall": "16719"}
[2024-10-06 12:02:39,642][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 12:02:39,654][fairseq.trainer][INFO] - begin training epoch 307
[2024-10-06 12:02:39,654][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 12:05:54,762][fairseq_cli.train][INFO] - end of epoch 307 (average epoch stats below)
[2024-10-06 12:05:54,768][train][INFO] - {"epoch": 307, "train_loss": "0.855", "train_ntokens": "260611", "train_nsentences": "1750.04", "train_wps": "64039.6", "train_ups": "0.25", "train_wpb": "260611", "train_bsz": "1750", "train_num_updates": "14726", "train_lr": "0.000230094", "train_gnorm": "0.656", "train_loss_scale": "2", "train_train_wall": "92", "train_gb_free": "39.3", "train_wall": "16914"}
[2024-10-06 12:05:54,981][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 12:05:55,004][fairseq.trainer][INFO] - begin training epoch 308
[2024-10-06 12:05:55,004][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 12:08:59,599][fairseq_cli.train][INFO] - end of epoch 308 (average epoch stats below)
[2024-10-06 12:08:59,606][train][INFO] - {"epoch": 308, "train_loss": "0.854", "train_ntokens": "260714", "train_nsentences": "1750.04", "train_wps": "67705.7", "train_ups": "0.26", "train_wpb": "260714", "train_bsz": "1750", "train_num_updates": "14774", "train_lr": "0.000230844", "train_gnorm": "0.65", "train_loss_scale": "2", "train_train_wall": "70", "train_gb_free": "40.6", "train_wall": "17099"}
[2024-10-06 12:08:59,814][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 12:08:59,819][fairseq.trainer][INFO] - begin training epoch 309
[2024-10-06 12:08:59,819][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 12:11:43,253][train_inner][INFO] - {"epoch": 309, "update": 308.542, "loss": "0.856", "ntokens": "260838", "nsentences": "1740.5", "wps": "69136.7", "ups": "0.27", "wpb": "260838", "bsz": "1740.5", "num_updates": "14800", "lr": "0.00023125", "gnorm": "0.66", "loss_scale": "2", "train_wall": "325", "gb_free": "39.2", "wall": "17263"}
[2024-10-06 12:11:43,497][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
[2024-10-06 12:11:57,559][fairseq_cli.train][INFO] - end of epoch 309 (average epoch stats below)
[2024-10-06 12:11:57,563][train][INFO] - {"epoch": 309, "train_loss": "0.852", "train_ntokens": "260584", "train_nsentences": "1735.23", "train_wps": "68824.5", "train_ups": "0.26", "train_wpb": "260584", "train_bsz": "1735.2", "train_num_updates": "14821", "train_lr": "0.000231578", "train_gnorm": "0.788", "train_loss_scale": "1", "train_train_wall": "55", "train_gb_free": "39.7", "train_wall": "17277"}
[2024-10-06 12:11:57,762][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 12:11:57,766][fairseq.trainer][INFO] - begin training epoch 310
[2024-10-06 12:11:57,766][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 12:14:48,230][fairseq_cli.train][INFO] - end of epoch 310 (average epoch stats below)
[2024-10-06 12:14:48,235][train][INFO] - {"epoch": 310, "train_loss": "0.852", "train_ntokens": "260713", "train_nsentences": "1750.04", "train_wps": "73324.4", "train_ups": "0.28", "train_wpb": "260712", "train_bsz": "1750", "train_num_updates": "14869", "train_lr": "0.000232328", "train_gnorm": "0.609", "train_loss_scale": "1", "train_train_wall": "83", "train_gb_free": "40.7", "train_wall": "17448"}
[2024-10-06 12:14:48,414][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 12:14:48,423][fairseq.trainer][INFO] - begin training epoch 311
[2024-10-06 12:14:48,423][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 12:17:51,059][fairseq_cli.train][INFO] - end of epoch 311 (average epoch stats below)
[2024-10-06 12:17:51,064][train][INFO] - {"epoch": 311, "train_loss": "0.854", "train_ntokens": "260791", "train_nsentences": "1750.04", "train_wps": "68469.5", "train_ups": "0.26", "train_wpb": "260791", "train_bsz": "1750", "train_num_updates": "14917", "train_lr": "0.000233078", "train_gnorm": "0.595", "train_loss_scale": "1", "train_train_wall": "54", "train_gb_free": "39.6", "train_wall": "17630"}
[2024-10-06 12:17:51,220][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 12:17:51,226][fairseq.trainer][INFO] - begin training epoch 312
[2024-10-06 12:17:51,226][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 12:20:18,856][fairseq_cli.train][INFO] - end of epoch 312 (average epoch stats below)
[2024-10-06 12:20:18,865][train][INFO] - {"epoch": 312, "train_loss": "0.848", "train_ntokens": "260941", "train_nsentences": "1750.04", "train_wps": "84745", "train_ups": "0.32", "train_wpb": "260940", "train_bsz": "1750", "train_num_updates": "14965", "train_lr": "0.000233828", "train_gnorm": "0.736", "train_loss_scale": "1", "train_train_wall": "35", "train_gb_free": "39.8", "train_wall": "17778"}
[2024-10-06 12:20:19,022][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 12:20:19,037][fairseq.trainer][INFO] - begin training epoch 313
[2024-10-06 12:20:19,037][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 12:22:20,906][train_inner][INFO] - {"epoch": 313, "update": 312.729, "loss": "0.851", "ntokens": "260571", "nsentences": "1749.87", "wps": "81728.8", "ups": "0.31", "wpb": "260571", "bsz": "1749.9", "num_updates": "15000", "lr": "0.000234375", "gnorm": "0.645", "loss_scale": "1", "train_wall": "214", "gb_free": "39.3", "wall": "17900"}
[2024-10-06 12:22:32,299][fairseq_cli.train][INFO] - end of epoch 313 (average epoch stats below)
[2024-10-06 12:22:32,305][train][INFO] - {"epoch": 313, "train_loss": "0.849", "train_ntokens": "260848", "train_nsentences": "1750.04", "train_wps": "93835.2", "train_ups": "0.36", "train_wpb": "260848", "train_bsz": "1750", "train_num_updates": "15013", "train_lr": "0.000234578", "train_gnorm": "0.612", "train_loss_scale": "1", "train_train_wall": "39", "train_gb_free": "39.8", "train_wall": "17912"}
[2024-10-06 12:22:32,455][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 12:22:32,466][fairseq.trainer][INFO] - begin training epoch 314
[2024-10-06 12:22:32,467][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 12:24:45,126][fairseq_cli.train][INFO] - end of epoch 314 (average epoch stats below)
[2024-10-06 12:24:45,129][train][INFO] - {"epoch": 314, "train_loss": "0.849", "train_ntokens": "260676", "train_nsentences": "1750.04", "train_wps": "94205.4", "train_ups": "0.36", "train_wpb": "260676", "train_bsz": "1750", "train_num_updates": "15061", "train_lr": "0.000235328", "train_gnorm": "0.635", "train_loss_scale": "1", "train_train_wall": "56", "train_gb_free": "39.2", "train_wall": "18044"}
[2024-10-06 12:24:45,263][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 12:24:45,270][fairseq.trainer][INFO] - begin training epoch 315
[2024-10-06 12:24:45,270][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 12:27:17,643][fairseq_cli.train][INFO] - end of epoch 315 (average epoch stats below)
[2024-10-06 12:27:17,654][train][INFO] - {"epoch": 315, "train_loss": "0.846", "train_ntokens": "260813", "train_nsentences": "1750.04", "train_wps": "82081", "train_ups": "0.31", "train_wpb": "260813", "train_bsz": "1750", "train_num_updates": "15109", "train_lr": "0.000236078", "train_gnorm": "0.565", "train_loss_scale": "1", "train_train_wall": "72", "train_gb_free": "39.4", "train_wall": "18197"}
[2024-10-06 12:27:17,801][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 12:27:17,806][fairseq.trainer][INFO] - begin training epoch 316
[2024-10-06 12:27:17,806][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 12:29:48,010][fairseq_cli.train][INFO] - end of epoch 316 (average epoch stats below)
[2024-10-06 12:29:48,017][train][INFO] - {"epoch": 316, "train_loss": "0.846", "train_ntokens": "260553", "train_nsentences": "1750.04", "train_wps": "83177.8", "train_ups": "0.32", "train_wpb": "260554", "train_bsz": "1750", "train_num_updates": "15157", "train_lr": "0.000236828", "train_gnorm": "0.667", "train_loss_scale": "1", "train_train_wall": "63", "train_gb_free": "39.8", "train_wall": "18347"}
[2024-10-06 12:29:48,190][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 12:29:48,197][fairseq.trainer][INFO] - begin training epoch 317
[2024-10-06 12:29:48,198][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 12:32:20,367][train_inner][INFO] - {"epoch": 317, "update": 316.896, "loss": "0.846", "ntokens": "260636", "nsentences": "1750.38", "wps": "86959.4", "ups": "0.33", "wpb": "260636", "bsz": "1750.4", "num_updates": "15200", "lr": "0.0002375", "gnorm": "0.635", "loss_scale": "1", "train_wall": "271", "gb_free": "40.1", "wall": "18500"}
[2024-10-06 12:32:22,164][fairseq_cli.train][INFO] - end of epoch 317 (average epoch stats below)
[2024-10-06 12:32:22,166][train][INFO] - {"epoch": 317, "train_loss": "0.843", "train_ntokens": "260058", "train_nsentences": "1750.04", "train_wps": "80980.4", "train_ups": "0.31", "train_wpb": "260058", "train_bsz": "1750", "train_num_updates": "15205", "train_lr": "0.000237578", "train_gnorm": "0.647", "train_loss_scale": "1", "train_train_wall": "71", "train_gb_free": "40.6", "train_wall": "18501"}
[2024-10-06 12:32:22,314][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 12:32:22,328][fairseq.trainer][INFO] - begin training epoch 318
[2024-10-06 12:32:22,331][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 12:34:58,782][fairseq_cli.train][INFO] - end of epoch 318 (average epoch stats below)
[2024-10-06 12:34:58,791][train][INFO] - {"epoch": 318, "train_loss": "0.845", "train_ntokens": "260855", "train_nsentences": "1750.04", "train_wps": "79946.1", "train_ups": "0.31", "train_wpb": "260855", "train_bsz": "1750", "train_num_updates": "15253", "train_lr": "0.000238328", "train_gnorm": "0.641", "train_loss_scale": "1", "train_train_wall": "33", "train_gb_free": "39.8", "train_wall": "18658"}
[2024-10-06 12:34:58,970][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 12:34:58,974][fairseq.trainer][INFO] - begin training epoch 319
[2024-10-06 12:34:58,974][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 12:37:41,005][fairseq_cli.train][INFO] - end of epoch 319 (average epoch stats below)
[2024-10-06 12:37:41,012][train][INFO] - {"epoch": 319, "train_loss": "0.839", "train_ntokens": "260548", "train_nsentences": "1750.04", "train_wps": "77096.3", "train_ups": "0.3", "train_wpb": "260548", "train_bsz": "1750", "train_num_updates": "15301", "train_lr": "0.000239078", "train_gnorm": "0.639", "train_loss_scale": "1", "train_train_wall": "62", "train_gb_free": "39.8", "train_wall": "18820"}
[2024-10-06 12:37:41,167][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 12:37:41,174][fairseq.trainer][INFO] - begin training epoch 320
[2024-10-06 12:37:41,174][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 12:40:20,594][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 320 @ 15349 updates
[2024-10-06 12:40:20,599][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-06 12:40:25,529][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-06 12:40:25,532][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 320 @ 15349 updates, score None) (writing took 4.937492100521922 seconds)
[2024-10-06 12:40:25,532][fairseq_cli.train][INFO] - end of epoch 320 (average epoch stats below)
[2024-10-06 12:40:25,534][train][INFO] - {"epoch": 320, "train_loss": "0.835", "train_ntokens": "260710", "train_nsentences": "1750.04", "train_wps": "76065.3", "train_ups": "0.29", "train_wpb": "260710", "train_bsz": "1750", "train_num_updates": "15349", "train_lr": "0.000239828", "train_gnorm": "0.568", "train_loss_scale": "1", "train_train_wall": "81", "train_gb_free": "39.6", "train_wall": "18985"}
[2024-10-06 12:40:25,657][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 12:40:25,677][fairseq.trainer][INFO] - begin training epoch 321
[2024-10-06 12:40:25,677][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 12:42:39,974][fairseq_cli.train][INFO] - end of epoch 321 (average epoch stats below)
[2024-10-06 12:42:39,979][train][INFO] - {"epoch": 321, "train_loss": "0.838", "train_ntokens": "260842", "train_nsentences": "1750.04", "train_wps": "93128.5", "train_ups": "0.36", "train_wpb": "260842", "train_bsz": "1750", "train_num_updates": "15397", "train_lr": "0.000240578", "train_gnorm": "0.679", "train_loss_scale": "1", "train_train_wall": "45", "train_gb_free": "40.6", "train_wall": "19119"}
[2024-10-06 12:42:40,141][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 12:42:40,147][fairseq.trainer][INFO] - begin training epoch 322
[2024-10-06 12:42:40,148][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 12:44:13,986][train_inner][INFO] - {"epoch": 322, "update": 321.062, "loss": "0.839", "ntokens": "260745", "nsentences": "1748.96", "wps": "73077.4", "ups": "0.28", "wpb": "260745", "bsz": "1749", "num_updates": "15400", "lr": "0.000240625", "gnorm": "0.637", "loss_scale": "1", "train_wall": "239", "gb_free": "40.1", "wall": "19213"}
[2024-10-06 12:44:54,483][fairseq_cli.train][INFO] - end of epoch 322 (average epoch stats below)
[2024-10-06 12:44:54,495][train][INFO] - {"epoch": 322, "train_loss": "0.838", "train_ntokens": "260894", "train_nsentences": "1750.04", "train_wps": "93105", "train_ups": "0.36", "train_wpb": "260894", "train_bsz": "1750", "train_num_updates": "15445", "train_lr": "0.000241328", "train_gnorm": "0.704", "train_loss_scale": "1", "train_train_wall": "52", "train_gb_free": "39.3", "train_wall": "19254"}
[2024-10-06 12:44:54,665][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 12:44:54,686][fairseq.trainer][INFO] - begin training epoch 323
[2024-10-06 12:44:54,686][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 12:47:23,884][fairseq_cli.train][INFO] - end of epoch 323 (average epoch stats below)
[2024-10-06 12:47:23,894][train][INFO] - {"epoch": 323, "train_loss": "0.833", "train_ntokens": "260754", "train_nsentences": "1750.04", "train_wps": "83778.9", "train_ups": "0.32", "train_wpb": "260754", "train_bsz": "1750", "train_num_updates": "15493", "train_lr": "0.000242078", "train_gnorm": "0.629", "train_loss_scale": "1", "train_train_wall": "68", "train_gb_free": "39.7", "train_wall": "19403"}
[2024-10-06 12:47:24,111][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 12:47:24,116][fairseq.trainer][INFO] - begin training epoch 324
[2024-10-06 12:47:24,117][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 12:51:29,135][fairseq_cli.train][INFO] - end of epoch 324 (average epoch stats below)
[2024-10-06 12:51:29,150][train][INFO] - {"epoch": 324, "train_loss": "0.838", "train_ntokens": "260667", "train_nsentences": "1750.04", "train_wps": "51017", "train_ups": "0.2", "train_wpb": "260667", "train_bsz": "1750", "train_num_updates": "15541", "train_lr": "0.000242828", "train_gnorm": "0.77", "train_loss_scale": "1", "train_train_wall": "131", "train_gb_free": "39.8", "train_wall": "19648"}
[2024-10-06 12:51:29,668][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 12:51:29,689][fairseq.trainer][INFO] - begin training epoch 325
[2024-10-06 12:51:29,689][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 12:57:50,226][fairseq_cli.train][INFO] - end of epoch 325 (average epoch stats below)
[2024-10-06 12:57:50,240][train][INFO] - {"epoch": 325, "train_loss": "0.828", "train_ntokens": "260612", "train_nsentences": "1750.04", "train_wps": "32825.7", "train_ups": "0.13", "train_wpb": "260612", "train_bsz": "1750", "train_num_updates": "15589", "train_lr": "0.000243578", "train_gnorm": "0.553", "train_loss_scale": "1", "train_train_wall": "160", "train_gb_free": "40.1", "train_wall": "20030"}
[2024-10-06 12:57:50,561][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 12:57:50,589][fairseq.trainer][INFO] - begin training epoch 326
[2024-10-06 12:57:50,589][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 13:03:07,521][train_inner][INFO] - {"epoch": 326, "update": 325.229, "loss": "0.834", "ntokens": "260644", "nsentences": "1756.49", "wps": "45988.5", "ups": "0.18", "wpb": "260644", "bsz": "1756.5", "num_updates": "15600", "lr": "0.00024375", "gnorm": "0.65", "loss_scale": "1", "train_wall": "498", "gb_free": "39.8", "wall": "20347"}
[2024-10-06 13:03:29,921][fairseq_cli.train][INFO] - end of epoch 326 (average epoch stats below)
[2024-10-06 13:03:29,925][train][INFO] - {"epoch": 326, "train_loss": "0.832", "train_ntokens": "260818", "train_nsentences": "1750.04", "train_wps": "36856.1", "train_ups": "0.14", "train_wpb": "260818", "train_bsz": "1750", "train_num_updates": "15637", "train_lr": "0.000244328", "train_gnorm": "0.7", "train_loss_scale": "1", "train_train_wall": "125", "train_gb_free": "39.9", "train_wall": "20369"}
[2024-10-06 13:03:30,172][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 13:03:30,177][fairseq.trainer][INFO] - begin training epoch 327
[2024-10-06 13:03:30,177][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 13:06:27,642][fairseq_cli.train][INFO] - end of epoch 327 (average epoch stats below)
[2024-10-06 13:06:27,646][train][INFO] - {"epoch": 327, "train_loss": "0.829", "train_ntokens": "260415", "train_nsentences": "1750.04", "train_wps": "70335.7", "train_ups": "0.27", "train_wpb": "260414", "train_bsz": "1750", "train_num_updates": "15685", "train_lr": "0.000245078", "train_gnorm": "0.535", "train_loss_scale": "1", "train_train_wall": "54", "train_gb_free": "40.2", "train_wall": "20547"}
[2024-10-06 13:06:27,716][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 13:06:27,724][fairseq.trainer][INFO] - begin training epoch 328
[2024-10-06 13:06:27,725][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 13:08:46,531][fairseq_cli.train][INFO] - end of epoch 328 (average epoch stats below)
[2024-10-06 13:08:46,534][train][INFO] - {"epoch": 328, "train_loss": "0.832", "train_ntokens": "260803", "train_nsentences": "1750.04", "train_wps": "90137.2", "train_ups": "0.35", "train_wpb": "260803", "train_bsz": "1750", "train_num_updates": "15733", "train_lr": "0.000245828", "train_gnorm": "0.574", "train_loss_scale": "1", "train_train_wall": "50", "train_gb_free": "39.7", "train_wall": "20686"}
[2024-10-06 13:08:46,669][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 13:08:46,680][fairseq.trainer][INFO] - begin training epoch 329
[2024-10-06 13:08:46,680][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 13:10:57,921][fairseq_cli.train][INFO] - end of epoch 329 (average epoch stats below)
[2024-10-06 13:10:57,924][train][INFO] - {"epoch": 329, "train_loss": "0.83", "train_ntokens": "260758", "train_nsentences": "1750.04", "train_wps": "95264.1", "train_ups": "0.37", "train_wpb": "260758", "train_bsz": "1750", "train_num_updates": "15781", "train_lr": "0.000246578", "train_gnorm": "0.707", "train_loss_scale": "1", "train_train_wall": "32", "train_gb_free": "39.7", "train_wall": "20817"}
[2024-10-06 13:10:58,050][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 13:10:58,059][fairseq.trainer][INFO] - begin training epoch 330
[2024-10-06 13:10:58,059][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 13:13:19,461][train_inner][INFO] - {"epoch": 330, "update": 329.396, "loss": "0.831", "ntokens": "260501", "nsentences": "1762.22", "wps": "85146.6", "ups": "0.33", "wpb": "260501", "bsz": "1762.2", "num_updates": "15800", "lr": "0.000246875", "gnorm": "0.639", "loss_scale": "1", "train_wall": "219", "gb_free": "40.1", "wall": "20959"}
[2024-10-06 13:13:35,104][fairseq_cli.train][INFO] - end of epoch 330 (average epoch stats below)
[2024-10-06 13:13:35,107][train][INFO] - {"epoch": 330, "train_loss": "0.828", "train_ntokens": "260705", "train_nsentences": "1750.04", "train_wps": "79615.3", "train_ups": "0.31", "train_wpb": "260705", "train_bsz": "1750", "train_num_updates": "15829", "train_lr": "0.000247328", "train_gnorm": "0.611", "train_loss_scale": "1", "train_train_wall": "77", "train_gb_free": "39.6", "train_wall": "20974"}
[2024-10-06 13:13:35,262][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 13:13:35,267][fairseq.trainer][INFO] - begin training epoch 331
[2024-10-06 13:13:35,268][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 13:16:01,955][fairseq_cli.train][INFO] - end of epoch 331 (average epoch stats below)
[2024-10-06 13:16:01,958][train][INFO] - {"epoch": 331, "train_loss": "0.829", "train_ntokens": "260840", "train_nsentences": "1750.04", "train_wps": "85260.8", "train_ups": "0.33", "train_wpb": "260840", "train_bsz": "1750", "train_num_updates": "15877", "train_lr": "0.000248078", "train_gnorm": "0.774", "train_loss_scale": "1", "train_train_wall": "63", "train_gb_free": "39.6", "train_wall": "21121"}
[2024-10-06 13:16:02,049][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 13:16:02,055][fairseq.trainer][INFO] - begin training epoch 332
[2024-10-06 13:16:02,056][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 13:18:17,326][fairseq_cli.train][INFO] - end of epoch 332 (average epoch stats below)
[2024-10-06 13:18:17,343][train][INFO] - {"epoch": 332, "train_loss": "0.827", "train_ntokens": "261011", "train_nsentences": "1750.04", "train_wps": "92542.6", "train_ups": "0.35", "train_wpb": "261011", "train_bsz": "1750", "train_num_updates": "15925", "train_lr": "0.000248828", "train_gnorm": "0.649", "train_loss_scale": "1", "train_train_wall": "52", "train_gb_free": "40.2", "train_wall": "21257"}
[2024-10-06 13:18:17,471][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 13:18:17,478][fairseq.trainer][INFO] - begin training epoch 333
[2024-10-06 13:18:17,478][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 13:20:52,467][fairseq_cli.train][INFO] - end of epoch 333 (average epoch stats below)
[2024-10-06 13:20:52,487][train][INFO] - {"epoch": 333, "train_loss": "0.827", "train_ntokens": "260762", "train_nsentences": "1750.04", "train_wps": "80684.8", "train_ups": "0.31", "train_wpb": "260762", "train_bsz": "1750", "train_num_updates": "15973", "train_lr": "0.000249578", "train_gnorm": "0.544", "train_loss_scale": "1", "train_train_wall": "64", "train_gb_free": "41.1", "train_wall": "21412"}
[2024-10-06 13:20:52,668][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 13:20:52,672][fairseq.trainer][INFO] - begin training epoch 334
[2024-10-06 13:20:52,672][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 13:23:22,151][train_inner][INFO] - {"epoch": 334, "update": 333.562, "loss": "0.827", "ntokens": "261060", "nsentences": "1729.54", "wps": "86632.6", "ups": "0.33", "wpb": "261060", "bsz": "1729.5", "num_updates": "16000", "lr": "0.00025", "gnorm": "0.636", "loss_scale": "1", "train_wall": "245", "gb_free": "40.1", "wall": "21561"}
[2024-10-06 13:23:35,301][fairseq_cli.train][INFO] - end of epoch 334 (average epoch stats below)
[2024-10-06 13:23:35,315][train][INFO] - {"epoch": 334, "train_loss": "0.825", "train_ntokens": "260778", "train_nsentences": "1750.04", "train_wps": "76882.1", "train_ups": "0.29", "train_wpb": "260778", "train_bsz": "1750", "train_num_updates": "16021", "train_lr": "0.000250328", "train_gnorm": "0.59", "train_loss_scale": "1", "train_train_wall": "64", "train_gb_free": "40", "train_wall": "21575"}
[2024-10-06 13:23:35,504][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 13:23:35,508][fairseq.trainer][INFO] - begin training epoch 335
[2024-10-06 13:23:35,509][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 13:26:20,316][fairseq_cli.train][INFO] - end of epoch 335 (average epoch stats below)
[2024-10-06 13:26:20,333][train][INFO] - {"epoch": 335, "train_loss": "0.82", "train_ntokens": "260767", "train_nsentences": "1750.04", "train_wps": "75853.9", "train_ups": "0.29", "train_wpb": "260767", "train_bsz": "1750", "train_num_updates": "16069", "train_lr": "0.000251078", "train_gnorm": "0.687", "train_loss_scale": "1", "train_train_wall": "45", "train_gb_free": "39.3", "train_wall": "21740"}
[2024-10-06 13:26:20,510][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 13:26:20,533][fairseq.trainer][INFO] - begin training epoch 336
[2024-10-06 13:26:20,534][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 13:29:03,911][fairseq_cli.train][INFO] - end of epoch 336 (average epoch stats below)
[2024-10-06 13:29:03,920][train][INFO] - {"epoch": 336, "train_loss": "0.822", "train_ntokens": "261061", "train_nsentences": "1750.04", "train_wps": "76602.6", "train_ups": "0.29", "train_wpb": "261061", "train_bsz": "1750", "train_num_updates": "16117", "train_lr": "0.000251828", "train_gnorm": "0.737", "train_loss_scale": "1", "train_train_wall": "71", "train_gb_free": "40.2", "train_wall": "21903"}
[2024-10-06 13:29:04,093][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 13:29:04,117][fairseq.trainer][INFO] - begin training epoch 337
[2024-10-06 13:29:04,120][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 13:31:57,178][fairseq_cli.train][INFO] - end of epoch 337 (average epoch stats below)
[2024-10-06 13:31:57,190][train][INFO] - {"epoch": 337, "train_loss": "0.821", "train_ntokens": "260863", "train_nsentences": "1750.04", "train_wps": "72268.5", "train_ups": "0.28", "train_wpb": "260863", "train_bsz": "1750", "train_num_updates": "16165", "train_lr": "0.000252578", "train_gnorm": "0.561", "train_loss_scale": "1", "train_train_wall": "63", "train_gb_free": "40.1", "train_wall": "22076"}
[2024-10-06 13:31:57,351][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 13:31:57,355][fairseq.trainer][INFO] - begin training epoch 338
[2024-10-06 13:31:57,355][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 13:34:38,074][train_inner][INFO] - {"epoch": 338, "update": 337.729, "loss": "0.822", "ntokens": "260855", "nsentences": "1751.86", "wps": "77185.9", "ups": "0.3", "wpb": "260855", "bsz": "1751.9", "num_updates": "16200", "lr": "0.000253125", "gnorm": "0.658", "loss_scale": "1", "train_wall": "254", "gb_free": "39.2", "wall": "22237"}
[2024-10-06 13:34:50,368][fairseq_cli.train][INFO] - end of epoch 338 (average epoch stats below)
[2024-10-06 13:34:50,372][train][INFO] - {"epoch": 338, "train_loss": "0.821", "train_ntokens": "260589", "train_nsentences": "1750.04", "train_wps": "72228", "train_ups": "0.28", "train_wpb": "260589", "train_bsz": "1750", "train_num_updates": "16213", "train_lr": "0.000253328", "train_gnorm": "0.635", "train_loss_scale": "1", "train_train_wall": "75", "train_gb_free": "39.6", "train_wall": "22250"}
[2024-10-06 13:34:50,541][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 13:34:50,545][fairseq.trainer][INFO] - begin training epoch 339
[2024-10-06 13:34:50,545][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 13:37:46,926][fairseq_cli.train][INFO] - end of epoch 339 (average epoch stats below)
[2024-10-06 13:37:46,934][train][INFO] - {"epoch": 339, "train_loss": "0.821", "train_ntokens": "260657", "train_nsentences": "1750.04", "train_wps": "70863.1", "train_ups": "0.27", "train_wpb": "260657", "train_bsz": "1750", "train_num_updates": "16261", "train_lr": "0.000254078", "train_gnorm": "0.637", "train_loss_scale": "1", "train_train_wall": "65", "train_gb_free": "40.7", "train_wall": "22426"}
[2024-10-06 13:37:47,112][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 13:37:47,117][fairseq.trainer][INFO] - begin training epoch 340
[2024-10-06 13:37:47,117][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 13:40:40,570][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 340 @ 16309 updates
[2024-10-06 13:40:40,572][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-06 13:40:44,967][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-06 13:40:44,970][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 340 @ 16309 updates, score None) (writing took 4.400029024109244 seconds)
[2024-10-06 13:40:44,971][fairseq_cli.train][INFO] - end of epoch 340 (average epoch stats below)
[2024-10-06 13:40:44,973][train][INFO] - {"epoch": 340, "train_loss": "0.814", "train_ntokens": "260724", "train_nsentences": "1750.04", "train_wps": "70294.3", "train_ups": "0.27", "train_wpb": "260724", "train_bsz": "1750", "train_num_updates": "16309", "train_lr": "0.000254828", "train_gnorm": "0.545", "train_loss_scale": "1", "train_train_wall": "74", "train_gb_free": "40.1", "train_wall": "22604"}
[2024-10-06 13:40:45,183][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 13:40:45,219][fairseq.trainer][INFO] - begin training epoch 341
[2024-10-06 13:40:45,220][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 13:43:30,209][fairseq_cli.train][INFO] - end of epoch 341 (average epoch stats below)
[2024-10-06 13:43:30,227][train][INFO] - {"epoch": 341, "train_loss": "0.814", "train_ntokens": "261036", "train_nsentences": "1750.04", "train_wps": "75822.3", "train_ups": "0.29", "train_wpb": "261036", "train_bsz": "1750", "train_num_updates": "16357", "train_lr": "0.000255578", "train_gnorm": "0.602", "train_loss_scale": "1", "train_train_wall": "26", "train_gb_free": "39.3", "train_wall": "22770"}
[2024-10-06 13:43:30,424][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 13:43:30,428][fairseq.trainer][INFO] - begin training epoch 342
[2024-10-06 13:43:30,428][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 13:45:54,727][train_inner][INFO] - {"epoch": 342, "update": 341.896, "loss": "0.817", "ntokens": "260641", "nsentences": "1753.37", "wps": "77041.4", "ups": "0.3", "wpb": "260641", "bsz": "1753.4", "num_updates": "16400", "lr": "0.00025625", "gnorm": "0.571", "loss_scale": "1", "train_wall": "234", "gb_free": "41.5", "wall": "22914"}
[2024-10-06 13:45:56,299][fairseq_cli.train][INFO] - end of epoch 342 (average epoch stats below)
[2024-10-06 13:45:56,313][train][INFO] - {"epoch": 342, "train_loss": "0.815", "train_ntokens": "260337", "train_nsentences": "1750.04", "train_wps": "85548.4", "train_ups": "0.33", "train_wpb": "260337", "train_bsz": "1750", "train_num_updates": "16405", "train_lr": "0.000256328", "train_gnorm": "0.509", "train_loss_scale": "1", "train_train_wall": "60", "train_gb_free": "39.7", "train_wall": "22916"}
[2024-10-06 13:45:56,467][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 13:45:56,471][fairseq.trainer][INFO] - begin training epoch 343
[2024-10-06 13:45:56,471][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 13:48:12,952][fairseq_cli.train][INFO] - end of epoch 343 (average epoch stats below)
[2024-10-06 13:48:12,968][train][INFO] - {"epoch": 343, "train_loss": "0.809", "train_ntokens": "260623", "train_nsentences": "1750.04", "train_wps": "91548.6", "train_ups": "0.35", "train_wpb": "260623", "train_bsz": "1750", "train_num_updates": "16453", "train_lr": "0.000257078", "train_gnorm": "0.593", "train_loss_scale": "1", "train_train_wall": "54", "train_gb_free": "39.3", "train_wall": "23052"}
[2024-10-06 13:48:13,166][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 13:48:13,173][fairseq.trainer][INFO] - begin training epoch 344
[2024-10-06 13:48:13,173][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 13:50:26,665][fairseq_cli.train][INFO] - end of epoch 344 (average epoch stats below)
[2024-10-06 13:50:26,678][train][INFO] - {"epoch": 344, "train_loss": "0.812", "train_ntokens": "260653", "train_nsentences": "1750.04", "train_wps": "93580.6", "train_ups": "0.36", "train_wpb": "260653", "train_bsz": "1750", "train_num_updates": "16501", "train_lr": "0.000257828", "train_gnorm": "0.607", "train_loss_scale": "1", "train_train_wall": "54", "train_gb_free": "40.5", "train_wall": "23186"}
[2024-10-06 13:50:26,801][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 13:50:26,817][fairseq.trainer][INFO] - begin training epoch 345
[2024-10-06 13:50:26,817][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 13:52:38,530][fairseq_cli.train][INFO] - end of epoch 345 (average epoch stats below)
[2024-10-06 13:52:38,534][train][INFO] - {"epoch": 345, "train_loss": "0.81", "train_ntokens": "260611", "train_nsentences": "1750.04", "train_wps": "94873.7", "train_ups": "0.36", "train_wpb": "260611", "train_bsz": "1750", "train_num_updates": "16549", "train_lr": "0.000258578", "train_gnorm": "0.619", "train_loss_scale": "1", "train_train_wall": "55", "train_gb_free": "39.9", "train_wall": "23318"}
[2024-10-06 13:52:38,633][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 13:52:38,638][fairseq.trainer][INFO] - begin training epoch 346
[2024-10-06 13:52:38,639][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 13:54:53,147][fairseq_cli.train][INFO] - end of epoch 346 (average epoch stats below)
[2024-10-06 13:54:53,155][train][INFO] - {"epoch": 346, "train_loss": "0.808", "train_ntokens": "260838", "train_nsentences": "1750.04", "train_wps": "93006.9", "train_ups": "0.36", "train_wpb": "260838", "train_bsz": "1750", "train_num_updates": "16597", "train_lr": "0.000259328", "train_gnorm": "0.517", "train_loss_scale": "1", "train_train_wall": "55", "train_gb_free": "39.4", "train_wall": "23452"}
[2024-10-06 13:54:53,369][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 13:54:53,387][fairseq.trainer][INFO] - begin training epoch 347
[2024-10-06 13:54:53,387][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 13:56:29,059][train_inner][INFO] - {"epoch": 347, "update": 346.062, "loss": "0.81", "ntokens": "260695", "nsentences": "1752.94", "wps": "82196.2", "ups": "0.32", "wpb": "260695", "bsz": "1752.9", "num_updates": "16600", "lr": "0.000259375", "gnorm": "0.579", "loss_scale": "1", "train_wall": "221", "gb_free": "41.2", "wall": "23548"}
[2024-10-06 13:57:03,889][fairseq_cli.train][INFO] - end of epoch 347 (average epoch stats below)
[2024-10-06 13:57:03,891][train][INFO] - {"epoch": 347, "train_loss": "0.807", "train_ntokens": "260701", "train_nsentences": "1750.04", "train_wps": "95720.2", "train_ups": "0.37", "train_wpb": "260701", "train_bsz": "1750", "train_num_updates": "16645", "train_lr": "0.000260078", "train_gnorm": "0.621", "train_loss_scale": "1", "train_train_wall": "34", "train_gb_free": "39.9", "train_wall": "23583"}
[2024-10-06 13:57:04,082][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 13:57:04,086][fairseq.trainer][INFO] - begin training epoch 348
[2024-10-06 13:57:04,087][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 13:59:41,356][fairseq_cli.train][INFO] - end of epoch 348 (average epoch stats below)
[2024-10-06 13:59:41,372][train][INFO] - {"epoch": 348, "train_loss": "0.809", "train_ntokens": "260924", "train_nsentences": "1750.04", "train_wps": "79531.5", "train_ups": "0.3", "train_wpb": "260924", "train_bsz": "1750", "train_num_updates": "16693", "train_lr": "0.000260828", "train_gnorm": "0.566", "train_loss_scale": "1", "train_train_wall": "74", "train_gb_free": "40.1", "train_wall": "23741"}
[2024-10-06 13:59:41,505][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 13:59:41,508][fairseq.trainer][INFO] - begin training epoch 349
[2024-10-06 13:59:41,509][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 14:02:25,802][fairseq_cli.train][INFO] - end of epoch 349 (average epoch stats below)
[2024-10-06 14:02:25,806][train][INFO] - {"epoch": 349, "train_loss": "0.806", "train_ntokens": "260501", "train_nsentences": "1750.04", "train_wps": "76044.8", "train_ups": "0.29", "train_wpb": "260500", "train_bsz": "1750", "train_num_updates": "16741", "train_lr": "0.000261578", "train_gnorm": "0.532", "train_loss_scale": "1", "train_train_wall": "88", "train_gb_free": "39.3", "train_wall": "23905"}
[2024-10-06 14:02:25,938][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 14:02:25,979][fairseq.trainer][INFO] - begin training epoch 350
[2024-10-06 14:02:25,979][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 14:05:11,794][fairseq_cli.train][INFO] - end of epoch 350 (average epoch stats below)
[2024-10-06 14:05:11,798][train][INFO] - {"epoch": 350, "train_loss": "0.812", "train_ntokens": "260367", "train_nsentences": "1750.04", "train_wps": "75291.8", "train_ups": "0.29", "train_wpb": "260367", "train_bsz": "1750", "train_num_updates": "16789", "train_lr": "0.000262328", "train_gnorm": "0.649", "train_loss_scale": "1", "train_train_wall": "64", "train_gb_free": "39.9", "train_wall": "24071"}
[2024-10-06 14:05:11,921][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 14:05:11,944][fairseq.trainer][INFO] - begin training epoch 351
[2024-10-06 14:05:11,944][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 14:07:37,344][train_inner][INFO] - {"epoch": 351, "update": 350.229, "loss": "0.808", "ntokens": "260679", "nsentences": "1742.93", "wps": "78014.7", "ups": "0.3", "wpb": "260678", "bsz": "1742.9", "num_updates": "16800", "lr": "0.0002625", "gnorm": "0.592", "loss_scale": "1", "train_wall": "313", "gb_free": "40.1", "wall": "24217"}
[2024-10-06 14:08:00,003][fairseq_cli.train][INFO] - end of epoch 351 (average epoch stats below)
[2024-10-06 14:08:00,004][train][INFO] - {"epoch": 351, "train_loss": "0.809", "train_ntokens": "260234", "train_nsentences": "1750.04", "train_wps": "74262.8", "train_ups": "0.29", "train_wpb": "260234", "train_bsz": "1750", "train_num_updates": "16837", "train_lr": "0.000263078", "train_gnorm": "0.534", "train_loss_scale": "1", "train_train_wall": "76", "train_gb_free": "39.7", "train_wall": "24239"}
[2024-10-06 14:08:00,157][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 14:08:00,161][fairseq.trainer][INFO] - begin training epoch 352
[2024-10-06 14:08:00,161][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 14:10:48,194][fairseq_cli.train][INFO] - end of epoch 352 (average epoch stats below)
[2024-10-06 14:10:48,210][train][INFO] - {"epoch": 352, "train_loss": "0.807", "train_ntokens": "260525", "train_nsentences": "1750.04", "train_wps": "74346.8", "train_ups": "0.29", "train_wpb": "260525", "train_bsz": "1750", "train_num_updates": "16885", "train_lr": "0.000263828", "train_gnorm": "0.595", "train_loss_scale": "2", "train_train_wall": "77", "train_gb_free": "39.3", "train_wall": "24408"}
[2024-10-06 14:10:48,403][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 14:10:48,407][fairseq.trainer][INFO] - begin training epoch 353
[2024-10-06 14:10:48,407][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 14:13:43,277][fairseq_cli.train][INFO] - end of epoch 353 (average epoch stats below)
[2024-10-06 14:13:43,282][train][INFO] - {"epoch": 353, "train_loss": "0.805", "train_ntokens": "260733", "train_nsentences": "1750.04", "train_wps": "71488.4", "train_ups": "0.27", "train_wpb": "260733", "train_bsz": "1750", "train_num_updates": "16933", "train_lr": "0.000264578", "train_gnorm": "0.666", "train_loss_scale": "2", "train_train_wall": "72", "train_gb_free": "39.3", "train_wall": "24583"}
[2024-10-06 14:13:43,443][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 14:13:43,448][fairseq.trainer][INFO] - begin training epoch 354
[2024-10-06 14:13:43,448][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 14:16:37,690][fairseq_cli.train][INFO] - end of epoch 354 (average epoch stats below)
[2024-10-06 14:16:37,695][train][INFO] - {"epoch": 354, "train_loss": "0.801", "train_ntokens": "260489", "train_nsentences": "1750.04", "train_wps": "71690.3", "train_ups": "0.28", "train_wpb": "260489", "train_bsz": "1750", "train_num_updates": "16981", "train_lr": "0.000265328", "train_gnorm": "0.597", "train_loss_scale": "2", "train_train_wall": "75", "train_gb_free": "39.9", "train_wall": "24757"}
[2024-10-06 14:16:37,854][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 14:16:37,892][fairseq.trainer][INFO] - begin training epoch 355
[2024-10-06 14:16:37,892][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 14:18:31,977][train_inner][INFO] - {"epoch": 355, "update": 354.396, "loss": "0.805", "ntokens": "260418", "nsentences": "1751.52", "wps": "79562.4", "ups": "0.31", "wpb": "260418", "bsz": "1751.5", "num_updates": "17000", "lr": "0.000265625", "gnorm": "0.62", "loss_scale": "2", "train_wall": "259", "gb_free": "39.4", "wall": "24871"}
[2024-10-06 14:18:52,672][fairseq_cli.train][INFO] - end of epoch 355 (average epoch stats below)
[2024-10-06 14:18:52,677][train][INFO] - {"epoch": 355, "train_loss": "0.802", "train_ntokens": "260931", "train_nsentences": "1750.04", "train_wps": "92792.7", "train_ups": "0.36", "train_wpb": "260931", "train_bsz": "1750", "train_num_updates": "17029", "train_lr": "0.000266078", "train_gnorm": "0.61", "train_loss_scale": "2", "train_train_wall": "32", "train_gb_free": "39.3", "train_wall": "24892"}
[2024-10-06 14:18:52,844][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 14:18:52,848][fairseq.trainer][INFO] - begin training epoch 356
[2024-10-06 14:18:52,849][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 14:21:04,961][fairseq_cli.train][INFO] - end of epoch 356 (average epoch stats below)
[2024-10-06 14:21:04,978][train][INFO] - {"epoch": 356, "train_loss": "0.801", "train_ntokens": "260332", "train_nsentences": "1750.04", "train_wps": "94453.7", "train_ups": "0.36", "train_wpb": "260332", "train_bsz": "1750", "train_num_updates": "17077", "train_lr": "0.000266828", "train_gnorm": "0.588", "train_loss_scale": "2", "train_train_wall": "60", "train_gb_free": "39.3", "train_wall": "25024"}
[2024-10-06 14:21:05,111][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 14:21:05,124][fairseq.trainer][INFO] - begin training epoch 357
[2024-10-06 14:21:05,124][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 14:23:19,416][fairseq_cli.train][INFO] - end of epoch 357 (average epoch stats below)
[2024-10-06 14:23:19,428][train][INFO] - {"epoch": 357, "train_loss": "0.791", "train_ntokens": "260294", "train_nsentences": "1750.04", "train_wps": "92930.8", "train_ups": "0.36", "train_wpb": "260294", "train_bsz": "1750", "train_num_updates": "17125", "train_lr": "0.000267578", "train_gnorm": "0.617", "train_loss_scale": "2", "train_train_wall": "63", "train_gb_free": "40.2", "train_wall": "25159"}
[2024-10-06 14:23:19,555][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 14:23:19,572][fairseq.trainer][INFO] - begin training epoch 358
[2024-10-06 14:23:19,573][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 14:25:51,435][fairseq_cli.train][INFO] - end of epoch 358 (average epoch stats below)
[2024-10-06 14:25:51,447][train][INFO] - {"epoch": 358, "train_loss": "0.797", "train_ntokens": "260962", "train_nsentences": "1750.04", "train_wps": "82401.1", "train_ups": "0.32", "train_wpb": "260962", "train_bsz": "1750", "train_num_updates": "17173", "train_lr": "0.000268328", "train_gnorm": "0.585", "train_loss_scale": "2", "train_train_wall": "54", "train_gb_free": "40.3", "train_wall": "25311"}
[2024-10-06 14:25:51,573][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 14:25:51,584][fairseq.trainer][INFO] - begin training epoch 359
[2024-10-06 14:25:51,584][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 14:28:09,126][train_inner][INFO] - {"epoch": 359, "update": 358.562, "loss": "0.798", "ntokens": "260703", "nsentences": "1747.46", "wps": "90342.8", "ups": "0.35", "wpb": "260703", "bsz": "1747.5", "num_updates": "17200", "lr": "0.00026875", "gnorm": "0.583", "loss_scale": "2", "train_wall": "234", "gb_free": "39.6", "wall": "25448"}
[2024-10-06 14:28:27,203][fairseq_cli.train][INFO] - end of epoch 359 (average epoch stats below)
[2024-10-06 14:28:27,204][train][INFO] - {"epoch": 359, "train_loss": "0.797", "train_ntokens": "260904", "train_nsentences": "1750.04", "train_wps": "80404.9", "train_ups": "0.31", "train_wpb": "260904", "train_bsz": "1750", "train_num_updates": "17221", "train_lr": "0.000269078", "train_gnorm": "0.578", "train_loss_scale": "2", "train_train_wall": "57", "train_gb_free": "39.6", "train_wall": "25467"}
[2024-10-06 14:28:27,330][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 14:28:27,335][fairseq.trainer][INFO] - begin training epoch 360
[2024-10-06 14:28:27,335][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 14:31:04,548][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 360 @ 17269 updates
[2024-10-06 14:31:04,551][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-06 14:31:09,219][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-06 14:31:09,236][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 360 @ 17269 updates, score None) (writing took 4.688364896923304 seconds)
[2024-10-06 14:31:09,237][fairseq_cli.train][INFO] - end of epoch 360 (average epoch stats below)
[2024-10-06 14:31:09,240][train][INFO] - {"epoch": 360, "train_loss": "0.795", "train_ntokens": "260596", "train_nsentences": "1750.04", "train_wps": "77199.1", "train_ups": "0.3", "train_wpb": "260596", "train_bsz": "1750", "train_num_updates": "17269", "train_lr": "0.000269828", "train_gnorm": "0.556", "train_loss_scale": "2", "train_train_wall": "72", "train_gb_free": "40.1", "train_wall": "25629"}
[2024-10-06 14:31:09,386][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 14:31:09,409][fairseq.trainer][INFO] - begin training epoch 361
[2024-10-06 14:31:09,409][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 14:33:50,899][fairseq_cli.train][INFO] - end of epoch 361 (average epoch stats below)
[2024-10-06 14:33:50,902][train][INFO] - {"epoch": 361, "train_loss": "0.795", "train_ntokens": "261232", "train_nsentences": "1750.04", "train_wps": "77565.8", "train_ups": "0.3", "train_wpb": "261232", "train_bsz": "1750", "train_num_updates": "17317", "train_lr": "0.000270578", "train_gnorm": "0.64", "train_loss_scale": "2", "train_train_wall": "75", "train_gb_free": "39.7", "train_wall": "25790"}
[2024-10-06 14:33:51,019][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 14:33:51,030][fairseq.trainer][INFO] - begin training epoch 362
[2024-10-06 14:33:51,030][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 14:36:33,851][fairseq_cli.train][INFO] - end of epoch 362 (average epoch stats below)
[2024-10-06 14:36:33,856][train][INFO] - {"epoch": 362, "train_loss": "0.792", "train_ntokens": "260935", "train_nsentences": "1750.04", "train_wps": "76863.2", "train_ups": "0.29", "train_wpb": "260935", "train_bsz": "1750", "train_num_updates": "17365", "train_lr": "0.000271328", "train_gnorm": "0.512", "train_loss_scale": "2", "train_train_wall": "88", "train_gb_free": "39.8", "train_wall": "25953"}
[2024-10-06 14:36:33,978][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 14:36:34,002][fairseq.trainer][INFO] - begin training epoch 363
[2024-10-06 14:36:34,002][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 14:39:06,996][train_inner][INFO] - {"epoch": 363, "update": 362.729, "loss": "0.795", "ntokens": "260877", "nsentences": "1757.04", "wps": "79310.5", "ups": "0.3", "wpb": "260877", "bsz": "1757", "num_updates": "17400", "lr": "0.000271875", "gnorm": "0.556", "loss_scale": "2", "train_wall": "300", "gb_free": "39.3", "wall": "26106"}
[2024-10-06 14:39:19,047][fairseq_cli.train][INFO] - end of epoch 363 (average epoch stats below)
[2024-10-06 14:39:19,063][train][INFO] - {"epoch": 363, "train_loss": "0.794", "train_ntokens": "260791", "train_nsentences": "1750.04", "train_wps": "75780", "train_ups": "0.29", "train_wpb": "260791", "train_bsz": "1750", "train_num_updates": "17413", "train_lr": "0.000272078", "train_gnorm": "0.551", "train_loss_scale": "2", "train_train_wall": "59", "train_gb_free": "39.6", "train_wall": "26118"}
[2024-10-06 14:39:19,272][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 14:39:19,286][fairseq.trainer][INFO] - begin training epoch 364
[2024-10-06 14:39:19,287][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 14:42:11,868][fairseq_cli.train][INFO] - end of epoch 364 (average epoch stats below)
[2024-10-06 14:42:11,872][train][INFO] - {"epoch": 364, "train_loss": "0.789", "train_ntokens": "260610", "train_nsentences": "1750.04", "train_wps": "72389.5", "train_ups": "0.28", "train_wpb": "260610", "train_bsz": "1750", "train_num_updates": "17461", "train_lr": "0.000272828", "train_gnorm": "0.604", "train_loss_scale": "2", "train_train_wall": "84", "train_gb_free": "40", "train_wall": "26291"}
[2024-10-06 14:42:12,039][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 14:42:12,042][fairseq.trainer][INFO] - begin training epoch 365
[2024-10-06 14:42:12,042][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 14:44:51,875][fairseq_cli.train][INFO] - end of epoch 365 (average epoch stats below)
[2024-10-06 14:44:51,887][train][INFO] - {"epoch": 365, "train_loss": "0.792", "train_ntokens": "260846", "train_nsentences": "1750.04", "train_wps": "78252.4", "train_ups": "0.3", "train_wpb": "260846", "train_bsz": "1750", "train_num_updates": "17509", "train_lr": "0.000273578", "train_gnorm": "0.563", "train_loss_scale": "2", "train_train_wall": "66", "train_gb_free": "39.3", "train_wall": "26451"}
[2024-10-06 14:44:52,014][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 14:44:52,017][fairseq.trainer][INFO] - begin training epoch 366
[2024-10-06 14:44:52,018][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 14:48:04,360][fairseq_cli.train][INFO] - end of epoch 366 (average epoch stats below)
[2024-10-06 14:48:04,367][train][INFO] - {"epoch": 366, "train_loss": "0.789", "train_ntokens": "260750", "train_nsentences": "1750.04", "train_wps": "65026.7", "train_ups": "0.25", "train_wpb": "260750", "train_bsz": "1750", "train_num_updates": "17557", "train_lr": "0.000274328", "train_gnorm": "0.48", "train_loss_scale": "2", "train_train_wall": "61", "train_gb_free": "39.4", "train_wall": "26644"}
[2024-10-06 14:48:04,485][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 14:48:04,500][fairseq.trainer][INFO] - begin training epoch 367
[2024-10-06 14:48:04,500][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 14:51:21,253][train_inner][INFO] - {"epoch": 367, "update": 366.896, "loss": "0.79", "ntokens": "260680", "nsentences": "1752.09", "wps": "71005.8", "ups": "0.27", "wpb": "260680", "bsz": "1752.1", "num_updates": "17600", "lr": "0.000275", "gnorm": "0.575", "loss_scale": "2", "train_wall": "252", "gb_free": "39.3", "wall": "26841"}
[2024-10-06 14:51:22,630][fairseq_cli.train][INFO] - end of epoch 367 (average epoch stats below)
[2024-10-06 14:51:22,636][train][INFO] - {"epoch": 367, "train_loss": "0.79", "train_ntokens": "260537", "train_nsentences": "1750.04", "train_wps": "63077.1", "train_ups": "0.24", "train_wpb": "260537", "train_bsz": "1750", "train_num_updates": "17605", "train_lr": "0.000275078", "train_gnorm": "0.639", "train_loss_scale": "2", "train_train_wall": "31", "train_gb_free": "40.6", "train_wall": "26842"}
[2024-10-06 14:51:22,829][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 14:51:22,838][fairseq.trainer][INFO] - begin training epoch 368
[2024-10-06 14:51:22,838][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 14:55:16,019][fairseq_cli.train][INFO] - end of epoch 368 (average epoch stats below)
[2024-10-06 14:55:16,025][train][INFO] - {"epoch": 368, "train_loss": "0.791", "train_ntokens": "260857", "train_nsentences": "1750.04", "train_wps": "53650.3", "train_ups": "0.21", "train_wpb": "260856", "train_bsz": "1750", "train_num_updates": "17653", "train_lr": "0.000275828", "train_gnorm": "0.535", "train_loss_scale": "2", "train_train_wall": "47", "train_gb_free": "39.4", "train_wall": "27075"}
[2024-10-06 14:55:16,172][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 14:55:16,335][fairseq.trainer][INFO] - begin training epoch 369
[2024-10-06 14:55:16,335][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 14:57:55,850][fairseq_cli.train][INFO] - end of epoch 369 (average epoch stats below)
[2024-10-06 14:57:55,853][train][INFO] - {"epoch": 369, "train_loss": "0.785", "train_ntokens": "260458", "train_nsentences": "1750.04", "train_wps": "78223.6", "train_ups": "0.3", "train_wpb": "260458", "train_bsz": "1750", "train_num_updates": "17701", "train_lr": "0.000276578", "train_gnorm": "0.544", "train_loss_scale": "2", "train_train_wall": "27", "train_gb_free": "39.8", "train_wall": "27235"}
[2024-10-06 14:57:56,168][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 14:57:56,173][fairseq.trainer][INFO] - begin training epoch 370
[2024-10-06 14:57:56,174][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 15:01:16,905][fairseq_cli.train][INFO] - end of epoch 370 (average epoch stats below)
[2024-10-06 15:01:16,923][train][INFO] - {"epoch": 370, "train_loss": "0.787", "train_ntokens": "260840", "train_nsentences": "1750.04", "train_wps": "62269.6", "train_ups": "0.24", "train_wpb": "260840", "train_bsz": "1750", "train_num_updates": "17749", "train_lr": "0.000277328", "train_gnorm": "0.581", "train_loss_scale": "2", "train_train_wall": "76", "train_gb_free": "40", "train_wall": "27436"}
[2024-10-06 15:01:17,071][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 15:01:17,078][fairseq.trainer][INFO] - begin training epoch 371
[2024-10-06 15:01:17,078][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 15:04:44,566][fairseq_cli.train][INFO] - end of epoch 371 (average epoch stats below)
[2024-10-06 15:04:44,570][train][INFO] - {"epoch": 371, "train_loss": "0.787", "train_ntokens": "260748", "train_nsentences": "1750.04", "train_wps": "60276.4", "train_ups": "0.23", "train_wpb": "260748", "train_bsz": "1750", "train_num_updates": "17797", "train_lr": "0.000278078", "train_gnorm": "0.55", "train_loss_scale": "2", "train_train_wall": "60", "train_gb_free": "39.9", "train_wall": "27644"}
[2024-10-06 15:04:44,718][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 15:04:44,747][fairseq.trainer][INFO] - begin training epoch 372
[2024-10-06 15:04:44,748][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 15:07:14,531][train_inner][INFO] - {"epoch": 372, "update": 371.062, "loss": "0.788", "ntokens": "260794", "nsentences": "1747.68", "wps": "54716.3", "ups": "0.21", "wpb": "260794", "bsz": "1747.7", "num_updates": "17800", "lr": "0.000278125", "gnorm": "0.551", "loss_scale": "2", "train_wall": "229", "gb_free": "39.7", "wall": "27794"}
[2024-10-06 15:07:56,868][fairseq_cli.train][INFO] - end of epoch 372 (average epoch stats below)
[2024-10-06 15:07:56,871][train][INFO] - {"epoch": 372, "train_loss": "0.789", "train_ntokens": "260904", "train_nsentences": "1750.04", "train_wps": "65125.3", "train_ups": "0.25", "train_wpb": "260904", "train_bsz": "1750", "train_num_updates": "17845", "train_lr": "0.000278828", "train_gnorm": "0.615", "train_loss_scale": "2", "train_train_wall": "59", "train_gb_free": "40.1", "train_wall": "27836"}
[2024-10-06 15:07:57,059][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 15:07:57,064][fairseq.trainer][INFO] - begin training epoch 373
[2024-10-06 15:07:57,065][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 15:11:58,398][fairseq_cli.train][INFO] - end of epoch 373 (average epoch stats below)
[2024-10-06 15:11:58,436][train][INFO] - {"epoch": 373, "train_loss": "0.785", "train_ntokens": "260581", "train_nsentences": "1750.04", "train_wps": "51780.1", "train_ups": "0.2", "train_wpb": "260581", "train_bsz": "1750", "train_num_updates": "17893", "train_lr": "0.000279578", "train_gnorm": "0.473", "train_loss_scale": "2", "train_train_wall": "66", "train_gb_free": "39.8", "train_wall": "28078"}
[2024-10-06 15:11:58,652][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 15:11:58,661][fairseq.trainer][INFO] - begin training epoch 374
[2024-10-06 15:11:58,662][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 15:15:45,567][fairseq_cli.train][INFO] - end of epoch 374 (average epoch stats below)
[2024-10-06 15:15:45,571][train][INFO] - {"epoch": 374, "train_loss": "0.784", "train_ntokens": "260153", "train_nsentences": "1750.04", "train_wps": "54978.3", "train_ups": "0.21", "train_wpb": "260153", "train_bsz": "1750", "train_num_updates": "17941", "train_lr": "0.000280328", "train_gnorm": "0.573", "train_loss_scale": "2", "train_train_wall": "56", "train_gb_free": "40.1", "train_wall": "28305"}
[2024-10-06 15:15:45,728][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 15:15:45,762][fairseq.trainer][INFO] - begin training epoch 375
[2024-10-06 15:15:45,763][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 15:18:50,220][fairseq_cli.train][INFO] - end of epoch 375 (average epoch stats below)
[2024-10-06 15:18:50,242][train][INFO] - {"epoch": 375, "train_loss": "0.78", "train_ntokens": "260410", "train_nsentences": "1750.04", "train_wps": "67693.9", "train_ups": "0.26", "train_wpb": "260410", "train_bsz": "1750", "train_num_updates": "17989", "train_lr": "0.000281078", "train_gnorm": "0.587", "train_loss_scale": "2", "train_train_wall": "68", "train_gb_free": "39.7", "train_wall": "28490"}
[2024-10-06 15:18:50,402][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 15:18:50,408][fairseq.trainer][INFO] - begin training epoch 376
[2024-10-06 15:18:50,409][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 15:21:42,378][train_inner][INFO] - {"epoch": 376, "update": 375.229, "loss": "0.784", "ntokens": "260546", "nsentences": "1746.1", "wps": "60046.7", "ups": "0.23", "wpb": "260546", "bsz": "1746.1", "num_updates": "18000", "lr": "0.00028125", "gnorm": "0.562", "loss_scale": "2", "train_wall": "300", "gb_free": "39.2", "wall": "28662"}
[2024-10-06 15:22:11,916][fairseq_cli.train][INFO] - end of epoch 376 (average epoch stats below)
[2024-10-06 15:22:11,918][train][INFO] - {"epoch": 376, "train_loss": "0.779", "train_ntokens": "260466", "train_nsentences": "1750.04", "train_wps": "61993.6", "train_ups": "0.24", "train_wpb": "260466", "train_bsz": "1750", "train_num_updates": "18037", "train_lr": "0.000281828", "train_gnorm": "0.548", "train_loss_scale": "2", "train_train_wall": "97", "train_gb_free": "39.4", "train_wall": "28691"}
[2024-10-06 15:22:12,439][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 15:22:12,445][fairseq.trainer][INFO] - begin training epoch 377
[2024-10-06 15:22:12,445][fairseq_cli.train][INFO] - Start iterating over samples
