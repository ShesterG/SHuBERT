[2024-10-06 18:05:22,491][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:13063', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'gloss'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_800k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-06 18:05:23,074][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:13418', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'gloss'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_800k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-06 18:05:23,237][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:13040', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'gloss'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_800k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-06 18:05:23,286][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:15695', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'gloss'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_800k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-06 18:05:23,288][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:14536', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'gloss'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_800k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-06 18:05:23,884][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:17067', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'gloss'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_800k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-06 18:05:23,917][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:15956', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'gloss'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_800k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-06 18:05:24,071][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:14730', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'gloss'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_800k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-06 18:05:25,800][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-06 18:05:25,807][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-06 18:05:25,807][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-06 18:05:25,807][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-06 18:05:25,808][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-06 18:05:25,808][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-06 18:05:26,446][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-06 18:05:26,459][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-06 18:05:26,466][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-06 18:05:26,466][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-06 18:05:26,467][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-06 18:05:26,468][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-06 18:05:26,712][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-06 18:05:26,719][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-06 18:05:26,719][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-06 18:05:26,719][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-06 18:05:26,720][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-06 18:05:26,720][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-06 18:05:27,772][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-06 18:05:27,774][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-06 18:05:27,774][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-06 18:05:27,774][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-06 18:05:27,775][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-06 18:05:27,775][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-06 18:05:28,712][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-06 18:05:28,714][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-06 18:05:28,714][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-06 18:05:28,714][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-06 18:05:28,715][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-06 18:05:28,715][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-06 18:05:29,054][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-06 18:05:29,072][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-06 18:05:29,072][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-06 18:05:29,072][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-06 18:05:29,072][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-06 18:05:29,073][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-06 18:05:30,218][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-06 18:05:31,640][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-06 18:05:31,986][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-06 18:05:32,649][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-06 18:05:33,416][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-06 18:05:34,183][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-06 18:05:35,245][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-06 18:05:35,403][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-06 18:05:35,403][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-06 18:05:35,404][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-06 18:05:35,404][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-06 18:05:35,405][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-06 18:05:49,149][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-06 18:05:49,278][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-06 18:05:49,278][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-06 18:05:49,278][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-06 18:05:49,279][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-06 18:05:49,280][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-06 18:05:59,462][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-06 18:06:30,123][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-06 18:09:46,928][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 18:09:46,948][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 18:09:46,948][fairseq.utils][INFO] - rank   1: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 18:09:46,949][fairseq.utils][INFO] - rank   2: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 18:09:46,949][fairseq.utils][INFO] - rank   3: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 18:09:46,949][fairseq.utils][INFO] - rank   4: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 18:09:46,949][fairseq.utils][INFO] - rank   5: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 18:09:46,949][fairseq.utils][INFO] - rank   6: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 18:09:46,949][fairseq.utils][INFO] - rank   7: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 18:09:46,949][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 18:09:46,950][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-06 18:09:46,955][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-06 18:09:46,956][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-06 18:10:40,747][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 123 @ 58411 updates)
[2024-10-06 18:10:40,753][fairseq.trainer][INFO] - loading train data for epoch 123
[2024-10-06 18:11:00,535][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 18:11:00,535][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 18:11:00,535][fairseq.utils][INFO] - rank   1: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 18:11:00,535][fairseq.utils][INFO] - rank   2: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 18:11:00,536][fairseq.utils][INFO] - rank   3: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 18:11:00,536][fairseq.utils][INFO] - rank   4: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 18:11:00,536][fairseq.utils][INFO] - rank   5: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 18:11:00,536][fairseq.utils][INFO] - rank   6: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 18:11:00,536][fairseq.utils][INFO] - rank   7: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 18:11:00,536][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 18:11:00,536][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-06 18:11:00,536][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-06 18:11:00,545][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-06 18:11:16,741][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-06 18:11:20,980][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 18:11:20,981][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 18:11:20,981][fairseq.utils][INFO] - rank   1: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 18:11:20,981][fairseq.utils][INFO] - rank   2: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 18:11:20,981][fairseq.utils][INFO] - rank   3: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 18:11:20,981][fairseq.utils][INFO] - rank   4: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 18:11:20,981][fairseq.utils][INFO] - rank   5: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 18:11:20,981][fairseq.utils][INFO] - rank   6: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 18:11:20,981][fairseq.utils][INFO] - rank   7: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 18:11:20,981][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 18:11:20,981][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-06 18:11:20,981][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-06 18:11:20,982][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-06 18:11:26,547][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 18:11:26,547][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 18:11:26,547][fairseq.utils][INFO] - rank   1: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 18:11:26,547][fairseq.utils][INFO] - rank   2: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 18:11:26,547][fairseq.utils][INFO] - rank   3: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 18:11:26,548][fairseq.utils][INFO] - rank   4: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 18:11:26,548][fairseq.utils][INFO] - rank   5: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 18:11:26,548][fairseq.utils][INFO] - rank   6: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 18:11:26,548][fairseq.utils][INFO] - rank   7: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 18:11:26,548][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 18:11:26,548][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-06 18:11:26,549][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-06 18:11:26,549][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-06 18:11:38,549][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 123 @ 58411 updates)
[2024-10-06 18:11:38,557][fairseq.trainer][INFO] - loading train data for epoch 123
[2024-10-06 18:11:43,341][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-06 18:11:50,260][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 123 @ 58411 updates)
[2024-10-06 18:11:50,312][fairseq.trainer][INFO] - loading train data for epoch 123
[2024-10-06 18:11:58,835][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-06 18:12:07,248][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 123 @ 58411 updates)
[2024-10-06 18:12:07,249][fairseq.trainer][INFO] - loading train data for epoch 123
[2024-10-06 18:12:57,706][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-06 18:13:14,076][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 18:13:14,084][fairseq.trainer][INFO] - begin training epoch 123
[2024-10-06 18:13:14,084][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 18:13:54,533][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 18:13:54,536][fairseq.trainer][INFO] - begin training epoch 123
[2024-10-06 18:13:54,536][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 18:14:55,326][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 18:14:55,332][fairseq.trainer][INFO] - begin training epoch 123
[2024-10-06 18:14:55,332][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 18:17:28,478][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 18:17:28,480][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 18:17:28,480][fairseq.utils][INFO] - rank   1: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 18:17:28,480][fairseq.utils][INFO] - rank   2: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 18:17:28,480][fairseq.utils][INFO] - rank   3: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 18:17:28,480][fairseq.utils][INFO] - rank   4: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 18:17:28,481][fairseq.utils][INFO] - rank   5: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 18:17:28,481][fairseq.utils][INFO] - rank   6: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 18:17:28,481][fairseq.utils][INFO] - rank   7: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 18:17:28,481][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 18:17:28,482][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-06 18:17:28,482][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-06 18:17:28,483][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-06 18:18:00,335][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 123 @ 58411 updates)
[2024-10-06 18:18:00,432][fairseq.trainer][INFO] - loading train data for epoch 123
[2024-10-06 18:18:01,202][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 18:18:01,219][fairseq.trainer][INFO] - begin training epoch 123
[2024-10-06 18:18:01,219][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 18:18:02,543][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-06 18:18:26,972][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 18:18:26,972][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 18:18:26,972][fairseq.utils][INFO] - rank   1: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 18:18:26,972][fairseq.utils][INFO] - rank   2: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 18:18:26,972][fairseq.utils][INFO] - rank   3: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 18:18:26,972][fairseq.utils][INFO] - rank   4: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 18:18:26,972][fairseq.utils][INFO] - rank   5: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 18:18:26,973][fairseq.utils][INFO] - rank   6: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 18:18:26,973][fairseq.utils][INFO] - rank   7: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 18:18:26,973][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 18:18:26,973][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-06 18:18:26,973][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-06 18:18:26,974][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-06 18:18:29,659][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 18:18:29,660][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 18:18:29,660][fairseq.utils][INFO] - rank   1: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 18:18:29,660][fairseq.utils][INFO] - rank   2: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 18:18:29,660][fairseq.utils][INFO] - rank   3: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 18:18:29,660][fairseq.utils][INFO] - rank   4: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 18:18:29,660][fairseq.utils][INFO] - rank   5: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 18:18:29,660][fairseq.utils][INFO] - rank   6: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 18:18:29,660][fairseq.utils][INFO] - rank   7: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 18:18:29,660][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 18:18:29,660][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-06 18:18:29,660][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-06 18:18:29,661][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-06 18:18:59,696][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 123 @ 58411 updates)
[2024-10-06 18:18:59,805][fairseq.trainer][INFO] - loading train data for epoch 123
[2024-10-06 18:19:03,602][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-06 18:19:06,287][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 123 @ 58411 updates)
[2024-10-06 18:19:06,338][fairseq.trainer][INFO] - loading train data for epoch 123
[2024-10-06 18:19:08,025][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 18:19:08,031][fairseq.trainer][INFO] - begin training epoch 123
[2024-10-06 18:19:08,032][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 18:19:10,142][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-06 18:19:18,588][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 18:19:18,588][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 18:19:18,588][fairseq.utils][INFO] - rank   1: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 18:19:18,588][fairseq.utils][INFO] - rank   2: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 18:19:18,588][fairseq.utils][INFO] - rank   3: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 18:19:18,588][fairseq.utils][INFO] - rank   4: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 18:19:18,588][fairseq.utils][INFO] - rank   5: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 18:19:18,588][fairseq.utils][INFO] - rank   6: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 18:19:18,588][fairseq.utils][INFO] - rank   7: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 18:19:18,588][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 18:19:18,589][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-06 18:19:18,594][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-06 18:19:18,595][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-06 18:19:49,849][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 123 @ 58411 updates)
[2024-10-06 18:19:51,601][fairseq.trainer][INFO] - loading train data for epoch 123
[2024-10-06 18:20:02,072][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-06 18:20:13,059][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 18:20:13,068][fairseq.trainer][INFO] - begin training epoch 123
[2024-10-06 18:20:13,069][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 18:20:14,089][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 18:20:14,094][fairseq.trainer][INFO] - begin training epoch 123
[2024-10-06 18:20:14,096][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 18:20:55,676][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 18:20:55,679][fairseq.trainer][INFO] - begin training epoch 123
[2024-10-06 18:20:55,679][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 19:15:06,011][train_inner][INFO] - {"epoch": 123, "update": 122.395, "loss": "1.041", "ntokens": "239351", "nsentences": "1770.69", "wps": "31300.3", "ups": "0.13", "wpb": "239351", "bsz": "1770.7", "num_updates": "58600", "lr": "0.000463859", "gnorm": "0.293", "loss_scale": "2", "train_wall": "1539", "gb_free": "40.1", "wall": "3347"}
[2024-10-06 19:15:06,012][train_inner][INFO] - {"epoch": 123, "update": 122.395, "loss": "1.041", "ntokens": "239351", "nsentences": "1770.69", "wps": "29210.7", "ups": "0.12", "wpb": "239351", "bsz": "1770.7", "num_updates": "58600", "lr": "0.000463859", "gnorm": "0.295", "loss_scale": "2", "train_wall": "1746", "gb_free": "40.1", "wall": "3399"}
[2024-10-06 19:47:47,054][train_inner][INFO] - {"epoch": 123, "update": 122.812, "loss": "1.045", "ntokens": "239960", "nsentences": "1770.79", "wps": "24473.2", "ups": "0.1", "wpb": "239960", "bsz": "1770.8", "num_updates": "58800", "lr": "0.000463587", "gnorm": "0.277", "loss_scale": "2", "train_wall": "1958", "gb_free": "40.1", "wall": "5308"}
[2024-10-06 19:47:47,053][train_inner][INFO] - {"epoch": 123, "update": 122.812, "loss": "1.045", "ntokens": "239960", "nsentences": "1770.79", "wps": "24473.3", "ups": "0.1", "wpb": "239960", "bsz": "1770.8", "num_updates": "58800", "lr": "0.000463587", "gnorm": "0.273", "loss_scale": "2", "train_wall": "1958", "gb_free": "40.1", "wall": "5360"}
[2024-10-06 19:56:20,776][fairseq_cli.train][INFO] - end of epoch 123 (average epoch stats below)
[2024-10-06 19:56:20,938][train][INFO] - {"epoch": 123, "train_loss": "1.044", "train_ntokens": "239402", "train_nsentences": "1753.71", "train_wps": "28498.7", "train_ups": "0.12", "train_wpb": "239402", "train_bsz": "1753.7", "train_num_updates": "58890", "train_lr": "0.000463465", "train_gnorm": "0.281", "train_loss_scale": "2", "train_train_wall": "4214", "train_gb_free": "39.4", "train_wall": "5874"}
[2024-10-06 19:56:21,273][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 19:56:21,307][fairseq.trainer][INFO] - begin training epoch 124
[2024-10-06 19:56:21,308][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 19:57:11,357][fairseq_cli.train][INFO] - end of epoch 123 (average epoch stats below)
[2024-10-06 19:57:11,779][train][INFO] - {"epoch": 123, "train_loss": "1.044", "train_ntokens": "239402", "train_nsentences": "1753.71", "train_wps": "28874.4", "train_ups": "0.12", "train_wpb": "239402", "train_bsz": "1753.7", "train_num_updates": "58890", "train_lr": "0.000463465", "train_gnorm": "0.282", "train_loss_scale": "2", "train_train_wall": "4050", "train_gb_free": "39.4", "train_wall": "5873"}
[2024-10-06 19:57:12,907][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 19:57:13,006][fairseq.trainer][INFO] - begin training epoch 124
[2024-10-06 19:57:13,051][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 20:11:11,197][train_inner][INFO] - {"epoch": 124, "update": 123.23, "loss": "1.043", "ntokens": "239429", "nsentences": "1714.28", "wps": "34104", "ups": "0.14", "wpb": "239429", "bsz": "1714.3", "num_updates": "59000", "lr": "0.000463315", "gnorm": "0.276", "loss_scale": "2", "train_wall": "828", "gb_free": "39.6", "wall": "6764"}
[2024-10-06 20:11:11,226][train_inner][INFO] - {"epoch": 124, "update": 123.23, "loss": "1.042", "ntokens": "239429", "nsentences": "1714.28", "wps": "34103.5", "ups": "0.14", "wpb": "239429", "bsz": "1714.3", "num_updates": "59000", "lr": "0.000463315", "gnorm": "0.269", "loss_scale": "2", "train_wall": "831", "gb_free": "39.6", "wall": "6713"}
[2024-10-06 20:17:26,426][train_inner][INFO] - {"epoch": 124, "update": 123.647, "loss": "1.043", "ntokens": "238955", "nsentences": "1801.06", "wps": "127374", "ups": "0.53", "wpb": "238955", "bsz": "1801.1", "num_updates": "59200", "lr": "0.000463043", "gnorm": "0.27", "loss_scale": "2", "train_wall": "345", "gb_free": "40.5", "wall": "7139"}
[2024-10-06 20:17:33,703][train_inner][INFO] - {"epoch": 124, "update": 123.647, "loss": "1.043", "ntokens": "238955", "nsentences": "1801.06", "wps": "124975", "ups": "0.52", "wpb": "238955", "bsz": "1801.1", "num_updates": "59200", "lr": "0.000463043", "gnorm": "0.267", "loss_scale": "2", "train_wall": "320", "gb_free": "40.5", "wall": "7095"}
[2024-10-06 20:32:15,460][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 124 @ 59369 updates
[2024-10-06 20:32:15,471][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-06 20:32:30,976][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-06 20:32:31,286][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 124 @ 59369 updates, score None) (writing took 15.826471284031868 seconds)
[2024-10-06 20:32:31,287][fairseq_cli.train][INFO] - end of epoch 124 (average epoch stats below)
[2024-10-06 20:32:31,315][train][INFO] - {"epoch": 124, "train_loss": "1.042", "train_ntokens": "239201", "train_nsentences": "1753.71", "train_wps": "52792.1", "train_ups": "0.22", "train_wpb": "239201", "train_bsz": "1753.7", "train_num_updates": "59369", "train_lr": "0.000462814", "train_gnorm": "0.282", "train_loss_scale": "2", "train_train_wall": "1472", "train_gb_free": "39.2", "train_wall": "8044"}
[2024-10-06 20:32:31,633][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 20:32:31,664][fairseq.trainer][INFO] - begin training epoch 125
[2024-10-06 20:32:31,664][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 20:33:17,930][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 124 @ 59369 updates
[2024-10-06 20:33:17,931][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-06 20:33:22,497][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-06 20:33:22,504][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 124 @ 59369 updates, score None) (writing took 4.574156848713756 seconds)
[2024-10-06 20:33:22,505][fairseq_cli.train][INFO] - end of epoch 124 (average epoch stats below)
[2024-10-06 20:33:22,514][train][INFO] - {"epoch": 124, "train_loss": "1.042", "train_ntokens": "239201", "train_nsentences": "1753.71", "train_wps": "52782.9", "train_ups": "0.22", "train_wpb": "239201", "train_bsz": "1753.7", "train_num_updates": "59369", "train_lr": "0.000462814", "train_gnorm": "0.278", "train_loss_scale": "2", "train_train_wall": "1437", "train_gb_free": "39.2", "train_wall": "8044"}
[2024-10-06 20:33:22,987][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 20:33:22,998][fairseq.trainer][INFO] - begin training epoch 125
[2024-10-06 20:33:22,999][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 20:41:10,447][train_inner][INFO] - {"epoch": 125, "update": 124.065, "loss": "1.043", "ntokens": "238935", "nsentences": "1716.5", "wps": "33558.6", "ups": "0.14", "wpb": "238935", "bsz": "1716.5", "num_updates": "59400", "lr": "0.000462772", "gnorm": "0.296", "loss_scale": "2", "train_wall": "962", "gb_free": "39.3", "wall": "8563"}
[2024-10-06 20:41:14,445][train_inner][INFO] - {"epoch": 125, "update": 124.065, "loss": "1.042", "ntokens": "238935", "nsentences": "1716.5", "wps": "33635.7", "ups": "0.14", "wpb": "238935", "bsz": "1716.5", "num_updates": "59400", "lr": "0.000462772", "gnorm": "0.296", "loss_scale": "2", "train_wall": "898", "gb_free": "39.3", "wall": "8516"}
[2024-10-06 20:53:08,489][train_inner][INFO] - {"epoch": 125, "update": 124.482, "loss": "1.04", "ntokens": "239399", "nsentences": "1785.13", "wps": "66695.7", "ups": "0.28", "wpb": "239399", "bsz": "1785.1", "num_updates": "59600", "lr": "0.0004625", "gnorm": "0.279", "loss_scale": "2", "train_wall": "709", "gb_free": "39.1", "wall": "9282"}
[2024-10-06 20:53:09,099][train_inner][INFO] - {"epoch": 125, "update": 124.482, "loss": "1.04", "ntokens": "239399", "nsentences": "1785.13", "wps": "66997.8", "ups": "0.28", "wpb": "239399", "bsz": "1785.1", "num_updates": "59600", "lr": "0.0004625", "gnorm": "0.275", "loss_scale": "2", "train_wall": "709", "gb_free": "39.1", "wall": "9231"}
[2024-10-06 21:03:34,812][train_inner][INFO] - {"epoch": 125, "update": 124.9, "loss": "1.044", "ntokens": "239603", "nsentences": "1733.34", "wps": "76519", "ups": "0.32", "wpb": "239603", "bsz": "1733.3", "num_updates": "59800", "lr": "0.000462228", "gnorm": "0.286", "loss_scale": "2", "train_wall": "600", "gb_free": "39.6", "wall": "9908"}
[2024-10-06 21:03:35,557][train_inner][INFO] - {"epoch": 125, "update": 124.9, "loss": "1.043", "ntokens": "239603", "nsentences": "1733.34", "wps": "76496.2", "ups": "0.32", "wpb": "239603", "bsz": "1733.3", "num_updates": "59800", "lr": "0.000462228", "gnorm": "0.28", "loss_scale": "2", "train_wall": "569", "gb_free": "39.6", "wall": "9857"}
[2024-10-06 21:05:16,126][fairseq_cli.train][INFO] - end of epoch 125 (average epoch stats below)
[2024-10-06 21:05:16,150][train][INFO] - {"epoch": 125, "train_loss": "1.042", "train_ntokens": "239204", "train_nsentences": "1753.71", "train_wps": "58315.2", "train_ups": "0.24", "train_wpb": "239204", "train_bsz": "1753.7", "train_num_updates": "59848", "train_lr": "0.000462163", "train_gnorm": "0.281", "train_loss_scale": "2", "train_train_wall": "1562", "train_gb_free": "40.2", "train_wall": "10009"}
[2024-10-06 21:05:16,406][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 21:05:16,441][fairseq.trainer][INFO] - begin training epoch 126
[2024-10-06 21:05:16,442][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 21:05:19,658][fairseq_cli.train][INFO] - end of epoch 125 (average epoch stats below)
[2024-10-06 21:05:19,690][train][INFO] - {"epoch": 125, "train_loss": "1.041", "train_ntokens": "239204", "train_nsentences": "1753.71", "train_wps": "59765.1", "train_ups": "0.25", "train_wpb": "239204", "train_bsz": "1753.7", "train_num_updates": "59848", "train_lr": "0.000462163", "train_gnorm": "0.278", "train_loss_scale": "2", "train_train_wall": "1435", "train_gb_free": "40.2", "train_wall": "9961"}
[2024-10-06 21:05:20,358][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 21:05:20,390][fairseq.trainer][INFO] - begin training epoch 126
[2024-10-06 21:05:20,391][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 21:22:29,839][train_inner][INFO] - {"epoch": 126, "update": 125.317, "loss": "1.039", "ntokens": "238692", "nsentences": "1741.92", "wps": "42060.3", "ups": "0.18", "wpb": "238692", "bsz": "1741.9", "num_updates": "60000", "lr": "0.000461957", "gnorm": "0.275", "loss_scale": "2", "train_wall": "636", "gb_free": "40.1", "wall": "11043"}
[2024-10-06 21:22:31,276][train_inner][INFO] - {"epoch": 126, "update": 125.317, "loss": "1.039", "ntokens": "238692", "nsentences": "1741.92", "wps": "42034.2", "ups": "0.18", "wpb": "238692", "bsz": "1741.9", "num_updates": "60000", "lr": "0.000461957", "gnorm": "0.272", "loss_scale": "2", "train_wall": "610", "gb_free": "40.1", "wall": "10993"}
[2024-10-06 21:33:59,192][train_inner][INFO] - {"epoch": 126, "update": 125.735, "loss": "1.041", "ntokens": "239754", "nsentences": "1727.04", "wps": "69705.4", "ups": "0.29", "wpb": "239754", "bsz": "1727", "num_updates": "60200", "lr": "0.000461685", "gnorm": "0.283", "loss_scale": "2", "train_wall": "558", "gb_free": "39.7", "wall": "11681"}
[2024-10-06 21:33:59,861][train_inner][INFO] - {"epoch": 126, "update": 125.735, "loss": "1.041", "ntokens": "239754", "nsentences": "1727.04", "wps": "69494.7", "ups": "0.29", "wpb": "239754", "bsz": "1727", "num_updates": "60200", "lr": "0.000461685", "gnorm": "0.283", "loss_scale": "2", "train_wall": "571", "gb_free": "39.7", "wall": "11733"}
[2024-10-06 21:43:35,111][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 126 @ 60327 updates
[2024-10-06 21:43:35,112][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-06 21:43:41,713][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-06 21:43:41,715][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 126 @ 60327 updates, score None) (writing took 6.6041780626401305 seconds)
[2024-10-06 21:43:41,715][fairseq_cli.train][INFO] - end of epoch 126 (average epoch stats below)
[2024-10-06 21:43:41,723][train][INFO] - {"epoch": 126, "train_loss": "1.04", "train_ntokens": "239069", "train_nsentences": "1753.71", "train_wps": "49668.5", "train_ups": "0.21", "train_wpb": "239069", "train_bsz": "1753.7", "train_num_updates": "60327", "train_lr": "0.000461512", "train_gnorm": "0.282", "train_loss_scale": "2", "train_train_wall": "1641", "train_gb_free": "39.9", "train_wall": "12315"}
[2024-10-06 21:43:41,914][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 21:43:41,927][fairseq.trainer][INFO] - begin training epoch 127
[2024-10-06 21:43:41,927][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 21:43:48,251][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 126 @ 60327 updates
[2024-10-06 21:43:48,259][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-06 21:43:54,480][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-06 21:43:54,494][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 126 @ 60327 updates, score None) (writing took 6.2388808354735374 seconds)
[2024-10-06 21:43:54,495][fairseq_cli.train][INFO] - end of epoch 126 (average epoch stats below)
[2024-10-06 21:43:54,500][train][INFO] - {"epoch": 126, "train_loss": "1.04", "train_ntokens": "239069", "train_nsentences": "1753.71", "train_wps": "49470.3", "train_ups": "0.21", "train_wpb": "239069", "train_bsz": "1753.7", "train_num_updates": "60327", "train_lr": "0.000461512", "train_gnorm": "0.277", "train_loss_scale": "2", "train_train_wall": "1612", "train_gb_free": "39.9", "train_wall": "12276"}
[2024-10-06 21:43:54,589][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 21:43:54,593][fairseq.trainer][INFO] - begin training epoch 127
[2024-10-06 21:43:54,594][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 21:54:03,722][train_inner][INFO] - {"epoch": 127, "update": 126.152, "loss": "1.04", "ntokens": "238384", "nsentences": "1760.68", "wps": "39585.1", "ups": "0.17", "wpb": "238384", "bsz": "1760.7", "num_updates": "60400", "lr": "0.000461413", "gnorm": "0.288", "loss_scale": "2", "train_wall": "700", "gb_free": "39.2", "wall": "12885"}
[2024-10-06 21:54:26,243][train_inner][INFO] - {"epoch": 127, "update": 126.152, "loss": "1.04", "ntokens": "238384", "nsentences": "1760.68", "wps": "38876.2", "ups": "0.16", "wpb": "238384", "bsz": "1760.7", "num_updates": "60400", "lr": "0.000461413", "gnorm": "0.294", "loss_scale": "2", "train_wall": "693", "gb_free": "39.2", "wall": "12959"}
[2024-10-06 22:02:12,721][train_inner][INFO] - {"epoch": 127, "update": 126.57, "loss": "1.036", "ntokens": "239956", "nsentences": "1725.65", "wps": "102881", "ups": "0.43", "wpb": "239956", "bsz": "1725.7", "num_updates": "60600", "lr": "0.000461141", "gnorm": "0.273", "loss_scale": "4", "train_wall": "453", "gb_free": "39.6", "wall": "13426"}
[2024-10-06 22:02:19,556][train_inner][INFO] - {"epoch": 127, "update": 126.57, "loss": "1.036", "ntokens": "239956", "nsentences": "1725.65", "wps": "96792.1", "ups": "0.4", "wpb": "239956", "bsz": "1725.7", "num_updates": "60600", "lr": "0.000461141", "gnorm": "0.281", "loss_scale": "4", "train_wall": "370", "gb_free": "39.6", "wall": "13381"}
[2024-10-06 22:09:51,976][train_inner][INFO] - {"epoch": 127, "update": 126.987, "loss": "1.043", "ntokens": "238852", "nsentences": "1805.54", "wps": "104027", "ups": "0.44", "wpb": "238852", "bsz": "1805.5", "num_updates": "60800", "lr": "0.00046087", "gnorm": "0.28", "loss_scale": "4", "train_wall": "360", "gb_free": "39.3", "wall": "13885"}
[2024-10-06 22:10:20,060][fairseq_cli.train][INFO] - end of epoch 127 (average epoch stats below)
[2024-10-06 22:10:20,143][train][INFO] - {"epoch": 127, "train_loss": "1.039", "train_ntokens": "239027", "train_nsentences": "1753.71", "train_wps": "71633.1", "train_ups": "0.3", "train_wpb": "239027", "train_bsz": "1753.7", "train_num_updates": "60806", "train_lr": "0.000460861", "train_gnorm": "0.282", "train_loss_scale": "4", "train_train_wall": "1000", "train_gb_free": "41.1", "train_wall": "13913"}
[2024-10-06 22:10:20,409][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 22:10:20,416][fairseq.trainer][INFO] - begin training epoch 128
[2024-10-06 22:10:20,423][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 22:10:21,911][train_inner][INFO] - {"epoch": 127, "update": 126.987, "loss": "1.043", "ntokens": "238852", "nsentences": "1805.54", "wps": "99044.1", "ups": "0.41", "wpb": "238852", "bsz": "1805.5", "num_updates": "60800", "lr": "0.00046087", "gnorm": "0.279", "loss_scale": "4", "train_wall": "387", "gb_free": "39.3", "wall": "13863"}
[2024-10-06 22:10:28,589][fairseq_cli.train][INFO] - end of epoch 127 (average epoch stats below)
[2024-10-06 22:10:28,622][train][INFO] - {"epoch": 127, "train_loss": "1.039", "train_ntokens": "239027", "train_nsentences": "1753.71", "train_wps": "71823.9", "train_ups": "0.3", "train_wpb": "239027", "train_bsz": "1753.7", "train_num_updates": "60806", "train_lr": "0.000460861", "train_gnorm": "0.284", "train_loss_scale": "4", "train_train_wall": "922", "train_gb_free": "41.1", "train_wall": "13870"}
[2024-10-06 22:10:29,095][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 22:10:29,108][fairseq.trainer][INFO] - begin training epoch 128
[2024-10-06 22:10:29,108][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 22:20:41,822][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2024-10-06 22:20:42,010][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2024-10-06 22:23:29,301][train_inner][INFO] - {"epoch": 128, "update": 127.407, "loss": "1.037", "ntokens": "239016", "nsentences": "1731.21", "wps": "58489.7", "ups": "0.24", "wpb": "239016", "bsz": "1731.2", "num_updates": "61000", "lr": "0.000460598", "gnorm": "0.277", "loss_scale": "2", "train_wall": "420", "gb_free": "39.2", "wall": "14702"}
[2024-10-06 22:23:32,074][train_inner][INFO] - {"epoch": 128, "update": 127.407, "loss": "1.037", "ntokens": "239016", "nsentences": "1731.21", "wps": "60500.1", "ups": "0.25", "wpb": "239016", "bsz": "1731.2", "num_updates": "61000", "lr": "0.000460598", "gnorm": "0.296", "loss_scale": "2", "train_wall": "405", "gb_free": "39.2", "wall": "14653"}
[2024-10-06 22:30:42,859][train_inner][INFO] - {"epoch": 128, "update": 127.825, "loss": "1.041", "ntokens": "239457", "nsentences": "1758.61", "wps": "111174", "ups": "0.46", "wpb": "239457", "bsz": "1758.6", "num_updates": "61200", "lr": "0.000460326", "gnorm": "0.29", "loss_scale": "2", "train_wall": "417", "gb_free": "39.8", "wall": "15084"}
[2024-10-06 22:30:42,859][train_inner][INFO] - {"epoch": 128, "update": 127.825, "loss": "1.041", "ntokens": "239457", "nsentences": "1758.61", "wps": "110469", "ups": "0.46", "wpb": "239457", "bsz": "1758.6", "num_updates": "61200", "lr": "0.000460326", "gnorm": "0.284", "loss_scale": "2", "train_wall": "428", "gb_free": "39.8", "wall": "15136"}
[2024-10-06 22:34:29,428][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 128 @ 61284 updates
[2024-10-06 22:34:29,435][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-06 22:34:34,457][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-06 22:34:34,466][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 128 @ 61284 updates, score None) (writing took 5.03861116245389 seconds)
[2024-10-06 22:34:34,467][fairseq_cli.train][INFO] - end of epoch 128 (average epoch stats below)
[2024-10-06 22:34:34,491][train][INFO] - {"epoch": 128, "train_loss": "1.039", "train_ntokens": "239305", "train_nsentences": "1750.15", "train_wps": "78654.7", "train_ups": "0.33", "train_wpb": "239305", "train_bsz": "1750.2", "train_num_updates": "61284", "train_lr": "0.000460212", "train_gnorm": "0.281", "train_loss_scale": "2", "train_train_wall": "1046", "train_gb_free": "39.6", "train_wall": "15367"}
[2024-10-06 22:34:34,586][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 22:34:34,601][fairseq.trainer][INFO] - begin training epoch 129
[2024-10-06 22:34:34,602][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 22:35:11,737][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 128 @ 61284 updates
[2024-10-06 22:35:11,739][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-06 22:35:16,339][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-06 22:35:16,343][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 128 @ 61284 updates, score None) (writing took 4.606071866117418 seconds)
[2024-10-06 22:35:16,344][fairseq_cli.train][INFO] - end of epoch 128 (average epoch stats below)
[2024-10-06 22:35:16,359][train][INFO] - {"epoch": 128, "train_loss": "1.039", "train_ntokens": "239305", "train_nsentences": "1750.15", "train_wps": "76887.9", "train_ups": "0.32", "train_wpb": "239305", "train_bsz": "1750.2", "train_num_updates": "61284", "train_lr": "0.000460212", "train_gnorm": "0.291", "train_loss_scale": "2", "train_train_wall": "1078", "train_gb_free": "39.6", "train_wall": "15358"}
[2024-10-06 22:35:16,594][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 22:35:16,607][fairseq.trainer][INFO] - begin training epoch 129
[2024-10-06 22:35:16,608][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 22:46:31,129][train_inner][INFO] - {"epoch": 129, "update": 128.242, "loss": "1.037", "ntokens": "238596", "nsentences": "1750.52", "wps": "50323.3", "ups": "0.21", "wpb": "238596", "bsz": "1750.5", "num_updates": "61400", "lr": "0.000460054", "gnorm": "0.273", "loss_scale": "2", "train_wall": "461", "gb_free": "39.7", "wall": "16033"}
[2024-10-06 22:46:31,879][train_inner][INFO] - {"epoch": 129, "update": 128.242, "loss": "1.037", "ntokens": "238596", "nsentences": "1750.52", "wps": "50284.9", "ups": "0.21", "wpb": "238596", "bsz": "1750.5", "num_updates": "61400", "lr": "0.000460054", "gnorm": "0.274", "loss_scale": "2", "train_wall": "419", "gb_free": "39.7", "wall": "16085"}
[2024-10-06 22:52:41,838][train_inner][INFO] - {"epoch": 129, "update": 128.66, "loss": "1.04", "ntokens": "240454", "nsentences": "1722.58", "wps": "130001", "ups": "0.54", "wpb": "240454", "bsz": "1722.6", "num_updates": "61600", "lr": "0.000459783", "gnorm": "0.284", "loss_scale": "2", "train_wall": "365", "gb_free": "39.8", "wall": "16455"}
[2024-10-06 22:53:04,103][train_inner][INFO] - {"epoch": 129, "update": 128.66, "loss": "1.04", "ntokens": "240454", "nsentences": "1722.58", "wps": "122400", "ups": "0.51", "wpb": "240454", "bsz": "1722.6", "num_updates": "61600", "lr": "0.000459783", "gnorm": "0.281", "loss_scale": "2", "train_wall": "373", "gb_free": "39.8", "wall": "16426"}
[2024-10-06 22:57:50,537][fairseq_cli.train][INFO] - end of epoch 129 (average epoch stats below)
[2024-10-06 22:57:50,590][train][INFO] - {"epoch": 129, "train_loss": "1.04", "train_ntokens": "239457", "train_nsentences": "1753.71", "train_wps": "82158", "train_ups": "0.34", "train_wpb": "239457", "train_bsz": "1753.7", "train_num_updates": "61763", "train_lr": "0.000459561", "train_gnorm": "0.278", "train_loss_scale": "2", "train_train_wall": "854", "train_gb_free": "39.9", "train_wall": "16764"}
[2024-10-06 22:57:51,033][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 22:57:51,075][fairseq.trainer][INFO] - begin training epoch 130
[2024-10-06 22:57:51,084][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 22:58:51,572][fairseq_cli.train][INFO] - end of epoch 129 (average epoch stats below)
[2024-10-06 22:58:51,597][train][INFO] - {"epoch": 129, "train_loss": "1.04", "train_ntokens": "239457", "train_nsentences": "1753.71", "train_wps": "81047.3", "train_ups": "0.34", "train_wpb": "239457", "train_bsz": "1753.7", "train_num_updates": "61763", "train_lr": "0.000459561", "train_gnorm": "0.278", "train_loss_scale": "2", "train_train_wall": "850", "train_gb_free": "39.9", "train_wall": "16773"}
[2024-10-06 22:58:51,763][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 22:58:51,770][fairseq.trainer][INFO] - begin training epoch 130
[2024-10-06 22:58:51,771][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 23:07:25,647][train_inner][INFO] - {"epoch": 130, "update": 129.077, "loss": "1.042", "ntokens": "238700", "nsentences": "1775.35", "wps": "54017.6", "ups": "0.23", "wpb": "238700", "bsz": "1775.3", "num_updates": "61800", "lr": "0.000459511", "gnorm": "0.29", "loss_scale": "2", "train_wall": "346", "gb_free": "40.6", "wall": "17339"}
[2024-10-06 23:07:27,771][train_inner][INFO] - {"epoch": 130, "update": 129.077, "loss": "1.042", "ntokens": "238700", "nsentences": "1775.35", "wps": "55277.6", "ups": "0.23", "wpb": "238700", "bsz": "1775.3", "num_updates": "61800", "lr": "0.000459511", "gnorm": "0.292", "loss_scale": "2", "train_wall": "345", "gb_free": "40.6", "wall": "17289"}
[2024-10-06 23:09:50,502][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
[2024-10-06 23:13:11,618][train_inner][INFO] - {"epoch": 130, "update": 129.495, "loss": "1.037", "ntokens": "239664", "nsentences": "1775.52", "wps": "138559", "ups": "0.58", "wpb": "239664", "bsz": "1775.5", "num_updates": "62000", "lr": "0.000459239", "gnorm": "0.262", "loss_scale": "2", "train_wall": "291", "gb_free": "39.6", "wall": "17685"}
[2024-10-06 23:13:21,291][train_inner][INFO] - {"epoch": 130, "update": 129.497, "loss": "1.037", "ntokens": "239628", "nsentences": "1764.27", "wps": "135572", "ups": "0.57", "wpb": "239628", "bsz": "1764.3", "num_updates": "62000", "lr": "0.000459239", "gnorm": "0.266", "loss_scale": "1", "train_wall": "311", "gb_free": "40.1", "wall": "17643"}
[2024-10-06 23:19:33,666][train_inner][INFO] - {"epoch": 130, "update": 129.914, "loss": "1.041", "ntokens": "239500", "nsentences": "1761.32", "wps": "128638", "ups": "0.54", "wpb": "239500", "bsz": "1761.3", "num_updates": "62200", "lr": "0.000458967", "gnorm": "0.285", "loss_scale": "1", "train_wall": "284", "gb_free": "40.6", "wall": "18015"}
[2024-10-06 23:19:40,855][train_inner][INFO] - {"epoch": 130, "update": 129.912, "loss": "1.041", "ntokens": "239497", "nsentences": "1762.16", "wps": "123072", "ups": "0.51", "wpb": "239497", "bsz": "1762.2", "num_updates": "62200", "lr": "0.000458967", "gnorm": "0.274", "loss_scale": "2", "train_wall": "331", "gb_free": "40.1", "wall": "18074"}
[2024-10-06 23:20:57,352][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 130 @ 62241 updates
[2024-10-06 23:20:57,363][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-06 23:20:57,840][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 130 @ 62242 updates
[2024-10-06 23:20:57,841][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-06 23:21:12,584][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-06 23:21:12,971][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 130 @ 62242 updates, score None) (writing took 15.131406079046428 seconds)
[2024-10-06 23:21:12,972][fairseq_cli.train][INFO] - end of epoch 130 (average epoch stats below)
[2024-10-06 23:21:12,979][train][INFO] - {"epoch": 130, "train_loss": "1.039", "train_ntokens": "239407", "train_nsentences": "1753.71", "train_wps": "81772.3", "train_ups": "0.34", "train_wpb": "239407", "train_bsz": "1753.7", "train_num_updates": "62242", "train_lr": "0.00045891", "train_gnorm": "0.274", "train_loss_scale": "2", "train_train_wall": "749", "train_gb_free": "39.3", "train_wall": "18166"}
[2024-10-06 23:21:13,183][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 23:21:13,204][fairseq.trainer][INFO] - begin training epoch 131
[2024-10-06 23:21:13,205][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 23:29:40,433][train_inner][INFO] - {"epoch": 131, "update": 130.33, "loss": "1.038", "ntokens": "239416", "nsentences": "1721.19", "wps": "79862.9", "ups": "0.33", "wpb": "239416", "bsz": "1721.2", "num_updates": "62400", "lr": "0.000458696", "gnorm": "0.289", "loss_scale": "2", "train_wall": "265", "gb_free": "39.6", "wall": "18673"}
[2024-10-06 23:32:57,434][train_inner][INFO] - {"epoch": 131, "update": 130.747, "loss": "1.037", "ntokens": "239104", "nsentences": "1785.36", "wps": "242754", "ups": "1.02", "wpb": "239104", "bsz": "1785.4", "num_updates": "62600", "lr": "0.000458424", "gnorm": "0.277", "loss_scale": "2", "train_wall": "185", "gb_free": "40.6", "wall": "18870"}
[2024-10-06 23:35:09,633][fairseq_cli.train][INFO] - end of epoch 131 (average epoch stats below)
[2024-10-06 23:35:09,667][train][INFO] - {"epoch": 131, "train_loss": "1.037", "train_ntokens": "239168", "train_nsentences": "1753.71", "train_wps": "136923", "train_ups": "0.57", "train_wpb": "239168", "train_bsz": "1753.7", "train_num_updates": "62721", "train_lr": "0.00045826", "train_gnorm": "0.286", "train_loss_scale": "2", "train_train_wall": "504", "train_gb_free": "40.6", "train_wall": "19003"}
[2024-10-06 23:35:09,994][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 23:35:10,463][fairseq.trainer][INFO] - begin training epoch 132
[2024-10-06 23:35:10,463][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 23:41:03,674][train_inner][INFO] - {"epoch": 132, "update": 131.165, "loss": "1.036", "ntokens": "238328", "nsentences": "1729.04", "wps": "98030.4", "ups": "0.41", "wpb": "238328", "bsz": "1729", "num_updates": "62800", "lr": "0.000458152", "gnorm": "0.288", "loss_scale": "2", "train_wall": "217", "gb_free": "40.1", "wall": "19357"}
[2024-10-06 23:43:49,739][train_inner][INFO] - {"epoch": 132, "update": 131.582, "loss": "1.036", "ntokens": "239778", "nsentences": "1746.76", "wps": "288781", "ups": "1.2", "wpb": "239778", "bsz": "1746.8", "num_updates": "63000", "lr": "0.00045788", "gnorm": "0.274", "loss_scale": "4", "train_wall": "162", "gb_free": "40.2", "wall": "19523"}
[2024-10-06 23:47:12,375][train_inner][INFO] - {"epoch": 132, "update": 132.0, "loss": "1.038", "ntokens": "238775", "nsentences": "1770.65", "wps": "235712", "ups": "0.99", "wpb": "238775", "bsz": "1770.7", "num_updates": "63200", "lr": "0.000457609", "gnorm": "0.29", "loss_scale": "4", "train_wall": "198", "gb_free": "40.5", "wall": "19725"}
[2024-10-06 23:47:12,384][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 132 @ 63200 updates
[2024-10-06 23:47:12,384][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-06 23:47:22,630][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-06 23:47:22,638][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 132 @ 63200 updates, score None) (writing took 10.254262478090823 seconds)
[2024-10-06 23:47:22,639][fairseq_cli.train][INFO] - end of epoch 132 (average epoch stats below)
[2024-10-06 23:47:22,647][train][INFO] - {"epoch": 132, "train_loss": "1.036", "train_ntokens": "239282", "train_nsentences": "1753.71", "train_wps": "156371", "train_ups": "0.65", "train_wpb": "239282", "train_bsz": "1753.7", "train_num_updates": "63200", "train_lr": "0.000457609", "train_gnorm": "0.281", "train_loss_scale": "4", "train_train_wall": "447", "train_gb_free": "40.5", "train_wall": "19736"}
[2024-10-06 23:47:22,723][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 23:47:22,727][fairseq.trainer][INFO] - begin training epoch 133
[2024-10-06 23:47:22,728][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 23:52:25,322][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2024-10-06 23:54:53,598][train_inner][INFO] - {"epoch": 133, "update": 132.42, "loss": "1.034", "ntokens": "239864", "nsentences": "1789.35", "wps": "104014", "ups": "0.43", "wpb": "239864", "bsz": "1789.4", "num_updates": "63400", "lr": "0.000457337", "gnorm": "0.274", "loss_scale": "2", "train_wall": "187", "gb_free": "39.2", "wall": "20187"}
[2024-10-06 23:57:58,403][train_inner][INFO] - {"epoch": 133, "update": 132.837, "loss": "1.038", "ntokens": "240162", "nsentences": "1739.95", "wps": "259920", "ups": "1.08", "wpb": "240162", "bsz": "1740", "num_updates": "63600", "lr": "0.000457065", "gnorm": "0.284", "loss_scale": "2", "train_wall": "181", "gb_free": "40.6", "wall": "20371"}
[2024-10-06 23:59:22,192][fairseq_cli.train][INFO] - end of epoch 133 (average epoch stats below)
[2024-10-06 23:59:22,209][train][INFO] - {"epoch": 133, "train_loss": "1.037", "train_ntokens": "239539", "train_nsentences": "1754", "train_wps": "159125", "train_ups": "0.66", "train_wpb": "239539", "train_bsz": "1754", "train_num_updates": "63678", "train_lr": "0.000456959", "train_gnorm": "0.277", "train_loss_scale": "2", "train_train_wall": "450", "train_gb_free": "39.6", "train_wall": "20455"}
[2024-10-06 23:59:22,334][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 23:59:22,349][fairseq.trainer][INFO] - begin training epoch 134
[2024-10-06 23:59:22,349][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 00:05:49,250][train_inner][INFO] - {"epoch": 134, "update": 133.255, "loss": "1.033", "ntokens": "239044", "nsentences": "1726.8", "wps": "101539", "ups": "0.42", "wpb": "239044", "bsz": "1726.8", "num_updates": "63800", "lr": "0.000456793", "gnorm": "0.266", "loss_scale": "2", "train_wall": "207", "gb_free": "39.7", "wall": "20842"}
[2024-10-07 00:09:00,069][train_inner][INFO] - {"epoch": 134, "update": 133.672, "loss": "1.036", "ntokens": "239398", "nsentences": "1775.14", "wps": "250925", "ups": "1.05", "wpb": "239398", "bsz": "1775.1", "num_updates": "64000", "lr": "0.000456522", "gnorm": "0.288", "loss_scale": "2", "train_wall": "187", "gb_free": "39.3", "wall": "21033"}
[2024-10-07 00:11:24,268][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 134 @ 64157 updates
[2024-10-07 00:11:24,275][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-07 00:11:32,323][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-07 00:11:32,330][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 134 @ 64157 updates, score None) (writing took 8.062360912561417 seconds)
[2024-10-07 00:11:32,331][fairseq_cli.train][INFO] - end of epoch 134 (average epoch stats below)
[2024-10-07 00:11:32,339][train][INFO] - {"epoch": 134, "train_loss": "1.036", "train_ntokens": "239247", "train_nsentences": "1753.71", "train_wps": "156959", "train_ups": "0.66", "train_wpb": "239247", "train_bsz": "1753.7", "train_num_updates": "64157", "train_lr": "0.000456308", "train_gnorm": "0.284", "train_loss_scale": "2", "train_train_wall": "452", "train_gb_free": "39.6", "train_wall": "21185"}
[2024-10-07 00:11:32,422][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-07 00:11:32,429][fairseq.trainer][INFO] - begin training epoch 135
[2024-10-07 00:11:32,430][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 00:16:49,353][train_inner][INFO] - {"epoch": 135, "update": 134.09, "loss": "1.038", "ntokens": "238892", "nsentences": "1722.35", "wps": "101812", "ups": "0.43", "wpb": "238892", "bsz": "1722.3", "num_updates": "64200", "lr": "0.00045625", "gnorm": "0.296", "loss_scale": "2", "train_wall": "172", "gb_free": "39.6", "wall": "21502"}
[2024-10-07 00:19:52,111][train_inner][INFO] - {"epoch": 135, "update": 134.507, "loss": "1.034", "ntokens": "239547", "nsentences": "1744.87", "wps": "262153", "ups": "1.09", "wpb": "239547", "bsz": "1744.9", "num_updates": "64400", "lr": "0.000455978", "gnorm": "0.274", "loss_scale": "2", "train_wall": "177", "gb_free": "39.6", "wall": "21685"}
[2024-10-07 00:23:03,994][train_inner][INFO] - {"epoch": 135, "update": 134.925, "loss": "1.035", "ntokens": "239938", "nsentences": "1758.28", "wps": "250093", "ups": "1.04", "wpb": "239938", "bsz": "1758.3", "num_updates": "64600", "lr": "0.000455707", "gnorm": "0.275", "loss_scale": "2", "train_wall": "186", "gb_free": "39.6", "wall": "21877"}
[2024-10-07 00:23:29,477][fairseq_cli.train][INFO] - end of epoch 135 (average epoch stats below)
[2024-10-07 00:23:29,488][train][INFO] - {"epoch": 135, "train_loss": "1.035", "train_ntokens": "239180", "train_nsentences": "1753.71", "train_wps": "159754", "train_ups": "0.67", "train_wpb": "239180", "train_bsz": "1753.7", "train_num_updates": "64636", "train_lr": "0.000455658", "train_gnorm": "0.278", "train_loss_scale": "2", "train_train_wall": "419", "train_gb_free": "39.8", "train_wall": "21903"}
[2024-10-07 00:23:29,661][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-07 00:23:29,679][fairseq.trainer][INFO] - begin training epoch 136
[2024-10-07 00:23:29,680][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 00:30:46,480][train_inner][INFO] - {"epoch": 136, "update": 135.342, "loss": "1.032", "ntokens": "238196", "nsentences": "1788.12", "wps": "103008", "ups": "0.43", "wpb": "238196", "bsz": "1788.1", "num_updates": "64800", "lr": "0.000455435", "gnorm": "0.277", "loss_scale": "2", "train_wall": "166", "gb_free": "39.8", "wall": "22340"}
[2024-10-07 00:33:58,155][train_inner][INFO] - {"epoch": 136, "update": 135.76, "loss": "1.035", "ntokens": "239203", "nsentences": "1766.93", "wps": "249600", "ups": "1.04", "wpb": "239203", "bsz": "1766.9", "num_updates": "65000", "lr": "0.000455163", "gnorm": "0.282", "loss_scale": "2", "train_wall": "188", "gb_free": "39.7", "wall": "22531"}
[2024-10-07 00:35:49,512][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 136 @ 65115 updates
[2024-10-07 00:35:49,513][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-07 00:35:55,385][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-07 00:35:55,389][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 136 @ 65115 updates, score None) (writing took 5.877005129121244 seconds)
[2024-10-07 00:35:55,397][fairseq_cli.train][INFO] - end of epoch 136 (average epoch stats below)
[2024-10-07 00:35:55,399][train][INFO] - {"epoch": 136, "train_loss": "1.034", "train_ntokens": "239288", "train_nsentences": "1753.71", "train_wps": "153663", "train_ups": "0.64", "train_wpb": "239288", "train_bsz": "1753.7", "train_num_updates": "65115", "train_lr": "0.000455007", "train_gnorm": "0.28", "train_loss_scale": "2", "train_train_wall": "438", "train_gb_free": "40.6", "train_wall": "22648"}
[2024-10-07 00:35:55,541][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-07 00:35:55,556][fairseq.trainer][INFO] - begin training epoch 137
[2024-10-07 00:35:55,557][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 00:41:57,930][train_inner][INFO] - {"epoch": 137, "update": 136.177, "loss": "1.035", "ntokens": "238703", "nsentences": "1743.31", "wps": "99509.2", "ups": "0.42", "wpb": "238703", "bsz": "1743.3", "num_updates": "65200", "lr": "0.000454891", "gnorm": "0.284", "loss_scale": "2", "train_wall": "196", "gb_free": "40.4", "wall": "23011"}
[2024-10-07 00:43:26,896][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2024-10-07 00:44:58,570][train_inner][INFO] - {"epoch": 137, "update": 136.597, "loss": "1.032", "ntokens": "240084", "nsentences": "1753", "wps": "265822", "ups": "1.11", "wpb": "240084", "bsz": "1753", "num_updates": "65400", "lr": "0.00045462", "gnorm": "0.272", "loss_scale": "2", "train_wall": "176", "gb_free": "39.8", "wall": "23192"}
[2024-10-07 00:48:14,587][fairseq_cli.train][INFO] - end of epoch 137 (average epoch stats below)
[2024-10-07 00:48:14,670][train][INFO] - {"epoch": 137, "train_loss": "1.034", "train_ntokens": "239314", "train_nsentences": "1753.25", "train_wps": "154745", "train_ups": "0.65", "train_wpb": "239314", "train_bsz": "1753.3", "train_num_updates": "65593", "train_lr": "0.000454357", "train_gnorm": "0.28", "train_loss_scale": "2", "train_train_wall": "454", "train_gb_free": "39.3", "train_wall": "23388"}
[2024-10-07 00:48:14,845][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-07 00:48:14,864][fairseq.trainer][INFO] - begin training epoch 138
[2024-10-07 00:48:14,865][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 00:53:03,250][train_inner][INFO] - {"epoch": 138, "update": 137.015, "loss": "1.037", "ntokens": "238755", "nsentences": "1729.48", "wps": "98521.9", "ups": "0.41", "wpb": "238755", "bsz": "1729.5", "num_updates": "65600", "lr": "0.000454348", "gnorm": "0.285", "loss_scale": "2", "train_wall": "207", "gb_free": "40.6", "wall": "23676"}
[2024-10-07 00:55:51,356][train_inner][INFO] - {"epoch": 138, "update": 137.432, "loss": "1.033", "ntokens": "239961", "nsentences": "1732.4", "wps": "285497", "ups": "1.19", "wpb": "239961", "bsz": "1732.4", "num_updates": "65800", "lr": "0.000454076", "gnorm": "0.279", "loss_scale": "2", "train_wall": "149", "gb_free": "39.6", "wall": "23844"}
[2024-10-07 00:59:27,601][train_inner][INFO] - {"epoch": 138, "update": 137.85, "loss": "1.034", "ntokens": "239203", "nsentences": "1787.48", "wps": "221240", "ups": "0.92", "wpb": "239203", "bsz": "1787.5", "num_updates": "66000", "lr": "0.000453804", "gnorm": "0.275", "loss_scale": "2", "train_wall": "212", "gb_free": "40.1", "wall": "24061"}
[2024-10-07 01:00:56,336][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 138 @ 66072 updates
[2024-10-07 01:00:56,338][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-07 01:01:00,172][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-07 01:01:00,177][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 138 @ 66072 updates, score None) (writing took 3.8409525975584984 seconds)
[2024-10-07 01:01:00,178][fairseq_cli.train][INFO] - end of epoch 138 (average epoch stats below)
[2024-10-07 01:01:00,180][train][INFO] - {"epoch": 138, "train_loss": "1.034", "train_ntokens": "239204", "train_nsentences": "1753.71", "train_wps": "149677", "train_ups": "0.63", "train_wpb": "239204", "train_bsz": "1753.7", "train_num_updates": "66072", "train_lr": "0.000453707", "train_gnorm": "0.276", "train_loss_scale": "2", "train_train_wall": "463", "train_gb_free": "39.8", "train_wall": "24153"}
[2024-10-07 01:01:00,249][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-07 01:01:00,267][fairseq.trainer][INFO] - begin training epoch 139
[2024-10-07 01:01:00,267][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 01:07:19,139][train_inner][INFO] - {"epoch": 139, "update": 138.267, "loss": "1.03", "ntokens": "239213", "nsentences": "1711.57", "wps": "101462", "ups": "0.42", "wpb": "239212", "bsz": "1711.6", "num_updates": "66200", "lr": "0.000453533", "gnorm": "0.276", "loss_scale": "2", "train_wall": "201", "gb_free": "39.7", "wall": "24532"}
[2024-10-07 01:10:24,249][train_inner][INFO] - {"epoch": 139, "update": 138.685, "loss": "1.032", "ntokens": "239160", "nsentences": "1762.92", "wps": "258405", "ups": "1.08", "wpb": "239160", "bsz": "1762.9", "num_updates": "66400", "lr": "0.000453261", "gnorm": "0.273", "loss_scale": "2", "train_wall": "174", "gb_free": "39.8", "wall": "24717"}
[2024-10-07 01:12:50,564][fairseq_cli.train][INFO] - end of epoch 139 (average epoch stats below)
[2024-10-07 01:12:50,584][train][INFO] - {"epoch": 139, "train_loss": "1.032", "train_ntokens": "239192", "train_nsentences": "1753.71", "train_wps": "161280", "train_ups": "0.67", "train_wpb": "239192", "train_bsz": "1753.7", "train_num_updates": "66551", "train_lr": "0.000453056", "train_gnorm": "0.276", "train_loss_scale": "2", "train_train_wall": "430", "train_gb_free": "40.1", "train_wall": "24864"}
[2024-10-07 01:12:50,692][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-07 01:12:50,708][fairseq.trainer][INFO] - begin training epoch 140
[2024-10-07 01:12:50,709][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 01:18:34,248][train_inner][INFO] - {"epoch": 140, "update": 139.102, "loss": "1.034", "ntokens": "238136", "nsentences": "1799.87", "wps": "97200.1", "ups": "0.41", "wpb": "238136", "bsz": "1799.9", "num_updates": "66600", "lr": "0.000452989", "gnorm": "0.282", "loss_scale": "2", "train_wall": "191", "gb_free": "39.3", "wall": "25207"}
[2024-10-07 01:21:08,084][train_inner][INFO] - {"epoch": 140, "update": 139.52, "loss": "1.029", "ntokens": "239956", "nsentences": "1716.97", "wps": "311969", "ups": "1.3", "wpb": "239956", "bsz": "1717", "num_updates": "66800", "lr": "0.000452717", "gnorm": "0.274", "loss_scale": "2", "train_wall": "150", "gb_free": "40.1", "wall": "25361"}
[2024-10-07 01:24:12,848][train_inner][INFO] - {"epoch": 140, "update": 139.937, "loss": "1.034", "ntokens": "239647", "nsentences": "1785.08", "wps": "259414", "ups": "1.08", "wpb": "239647", "bsz": "1785.1", "num_updates": "67000", "lr": "0.000452446", "gnorm": "0.283", "loss_scale": "2", "train_wall": "181", "gb_free": "40.1", "wall": "25546"}
[2024-10-07 01:25:03,356][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 140 @ 67030 updates
[2024-10-07 01:25:03,357][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-07 01:25:11,930][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-07 01:25:11,932][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 140 @ 67030 updates, score None) (writing took 8.5760413762182 seconds)
[2024-10-07 01:25:11,933][fairseq_cli.train][INFO] - end of epoch 140 (average epoch stats below)
[2024-10-07 01:25:11,935][train][INFO] - {"epoch": 140, "train_loss": "1.031", "train_ntokens": "239226", "train_nsentences": "1753.71", "train_wps": "154569", "train_ups": "0.65", "train_wpb": "239226", "train_bsz": "1753.7", "train_num_updates": "67030", "train_lr": "0.000452405", "train_gnorm": "0.282", "train_loss_scale": "2", "train_train_wall": "428", "train_gb_free": "40.6", "train_wall": "25605"}
[2024-10-07 01:25:11,986][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-07 01:25:12,000][fairseq.trainer][INFO] - begin training epoch 141
[2024-10-07 01:25:12,001][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 01:32:07,971][train_inner][INFO] - {"epoch": 141, "update": 140.355, "loss": "1.03", "ntokens": "239322", "nsentences": "1732.64", "wps": "100742", "ups": "0.42", "wpb": "239322", "bsz": "1732.6", "num_updates": "67200", "lr": "0.000452174", "gnorm": "0.276", "loss_scale": "2", "train_wall": "202", "gb_free": "40.1", "wall": "26021"}
[2024-10-07 01:35:16,768][train_inner][INFO] - {"epoch": 141, "update": 140.772, "loss": "1.032", "ntokens": "239291", "nsentences": "1758.07", "wps": "253495", "ups": "1.06", "wpb": "239291", "bsz": "1758.1", "num_updates": "67400", "lr": "0.000451902", "gnorm": "0.276", "loss_scale": "4", "train_wall": "185", "gb_free": "40.7", "wall": "26210"}
[2024-10-07 01:37:14,617][fairseq_cli.train][INFO] - end of epoch 141 (average epoch stats below)
[2024-10-07 01:37:14,644][train][INFO] - {"epoch": 141, "train_loss": "1.031", "train_ntokens": "239369", "train_nsentences": "1753.71", "train_wps": "158651", "train_ups": "0.66", "train_wpb": "239369", "train_bsz": "1753.7", "train_num_updates": "67509", "train_lr": "0.000451754", "train_gnorm": "0.274", "train_loss_scale": "4", "train_train_wall": "452", "train_gb_free": "40.1", "train_wall": "26328"}
[2024-10-07 01:37:14,913][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-07 01:37:14,940][fairseq.trainer][INFO] - begin training epoch 142
[2024-10-07 01:37:14,940][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 01:41:53,386][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2024-10-07 01:43:18,081][train_inner][INFO] - {"epoch": 142, "update": 141.192, "loss": "1.03", "ntokens": "239044", "nsentences": "1739.88", "wps": "99330.7", "ups": "0.42", "wpb": "239044", "bsz": "1739.9", "num_updates": "67600", "lr": "0.00045163", "gnorm": "0.3", "loss_scale": "2", "train_wall": "200", "gb_free": "40.4", "wall": "26691"}
[2024-10-07 01:46:15,401][train_inner][INFO] - {"epoch": 142, "update": 141.61, "loss": "1.032", "ntokens": "239388", "nsentences": "1799.18", "wps": "270016", "ups": "1.13", "wpb": "239388", "bsz": "1799.2", "num_updates": "67800", "lr": "0.000451359", "gnorm": "0.274", "loss_scale": "2", "train_wall": "173", "gb_free": "39.7", "wall": "26868"}
[2024-10-07 01:52:05,067][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 142 @ 67987 updates
[2024-10-07 01:52:05,075][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-07 01:52:13,210][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-07 01:52:13,215][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 142 @ 67987 updates, score None) (writing took 8.147344721481204 seconds)
[2024-10-07 01:52:13,215][fairseq_cli.train][INFO] - end of epoch 142 (average epoch stats below)
[2024-10-07 01:52:13,235][train][INFO] - {"epoch": 142, "train_loss": "1.031", "train_ntokens": "239348", "train_nsentences": "1755.25", "train_wps": "127323", "train_ups": "0.53", "train_wpb": "239348", "train_bsz": "1755.3", "train_num_updates": "67987", "train_lr": "0.000451105", "train_gnorm": "0.286", "train_loss_scale": "2", "train_train_wall": "582", "train_gb_free": "39.7", "train_wall": "27226"}
[2024-10-07 01:52:13,342][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-07 01:52:13,387][fairseq.trainer][INFO] - begin training epoch 143
[2024-10-07 01:52:13,388][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 01:57:21,686][train_inner][INFO] - {"epoch": 143, "update": 142.027, "loss": "1.033", "ntokens": "239013", "nsentences": "1715.45", "wps": "71745.4", "ups": "0.3", "wpb": "239013", "bsz": "1715.5", "num_updates": "68000", "lr": "0.000451087", "gnorm": "0.277", "loss_scale": "2", "train_wall": "347", "gb_free": "39.6", "wall": "27535"}
[2024-10-07 02:01:14,817][train_inner][INFO] - {"epoch": 143, "update": 142.445, "loss": "1.027", "ntokens": "238274", "nsentences": "1811.54", "wps": "204436", "ups": "0.86", "wpb": "238274", "bsz": "1811.5", "num_updates": "68200", "lr": "0.000450815", "gnorm": "0.288", "loss_scale": "2", "train_wall": "131", "gb_free": "40.1", "wall": "27768"}
