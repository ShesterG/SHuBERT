[2024-10-06 16:20:34,783][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:17559', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-06 16:20:35,942][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:15521', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-06 16:20:38,469][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-06 16:20:38,472][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-06 16:20:38,472][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-06 16:20:38,472][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-06 16:20:38,473][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-06 16:20:38,473][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-06 16:20:38,685][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-06 16:20:39,950][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-06 16:20:39,953][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-06 16:20:39,953][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-06 16:20:39,953][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-06 16:20:39,954][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-06 16:20:40,315][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-06 16:20:47,599][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:18720', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-06 16:20:48,038][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:19972', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-06 16:20:48,060][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:18807', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-06 16:20:49,955][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:15763', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-06 16:20:50,025][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:13742', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'random'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_80k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase80/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-06 16:20:51,158][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-06 16:20:51,184][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-06 16:20:51,184][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-06 16:20:51,184][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-06 16:20:51,185][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-06 16:20:51,186][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-06 16:20:51,871][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-06 16:20:53,050][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-06 16:20:53,065][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-06 16:20:53,065][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-06 16:20:53,065][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-06 16:20:53,066][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-06 16:20:53,071][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-06 16:20:53,531][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-06 16:20:54,906][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-06 16:20:54,927][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-06 16:20:54,928][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-06 16:20:54,928][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-06 16:20:54,929][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-06 16:20:54,930][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-06 16:20:55,495][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-06 16:20:56,107][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-06 16:20:57,067][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-06 16:20:57,079][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-06 16:20:57,079][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-06 16:20:57,079][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-06 16:20:57,080][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-06 16:20:57,081][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-06 16:20:57,559][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-06 16:20:58,370][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-06 16:20:58,415][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-06 16:20:58,423][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-06 16:20:58,423][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-06 16:20:58,428][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-06 16:20:58,435][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-06 16:20:59,380][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-06 16:22:02,041][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 16:22:02,047][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 16:22:02,047][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 16:22:02,047][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 16:22:02,047][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 16:22:02,047][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 16:22:02,047][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 16:22:02,047][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 16:22:02,047][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 16:22:02,047][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 16:22:02,047][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-06 16:22:02,047][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-06 16:22:02,063][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-06 16:22:05,184][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 16:22:05,184][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 16:22:05,184][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 16:22:05,184][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 16:22:05,184][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 16:22:05,184][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 16:22:05,184][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 16:22:05,184][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 16:22:05,184][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 16:22:05,184][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 16:22:05,185][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-06 16:22:05,185][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-06 16:22:05,187][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-06 16:22:30,245][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 361 @ 17269 updates)
[2024-10-06 16:22:30,248][fairseq.trainer][INFO] - loading train data for epoch 361
[2024-10-06 16:22:30,889][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-06 16:22:34,004][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 16:22:34,005][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 16:22:34,005][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 16:22:34,005][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 16:22:34,005][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 16:22:34,005][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 16:22:34,005][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 16:22:34,005][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 16:22:34,005][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 16:22:34,005][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 16:22:34,005][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-06 16:22:34,005][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-06 16:22:34,006][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-06 16:22:48,806][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 16:22:48,810][fairseq.trainer][INFO] - begin training epoch 361
[2024-10-06 16:22:48,850][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 16:22:52,742][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 361 @ 17269 updates)
[2024-10-06 16:22:52,787][fairseq.trainer][INFO] - loading train data for epoch 361
[2024-10-06 16:22:53,159][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-06 16:22:59,809][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 16:22:59,832][fairseq.trainer][INFO] - begin training epoch 361
[2024-10-06 16:22:59,838][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 16:23:18,130][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 361 @ 17269 updates)
[2024-10-06 16:23:18,148][fairseq.trainer][INFO] - loading train data for epoch 361
[2024-10-06 16:23:18,406][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-06 16:23:22,905][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 16:23:22,908][fairseq.trainer][INFO] - begin training epoch 361
[2024-10-06 16:23:22,909][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 16:24:10,224][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 16:24:10,229][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 16:24:10,229][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 16:24:10,229][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 16:24:10,229][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 16:24:10,230][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 16:24:10,230][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 16:24:10,230][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 16:24:10,230][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 16:24:10,230][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 16:24:10,230][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-06 16:24:10,230][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-06 16:24:10,237][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-06 16:24:29,577][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 16:24:29,587][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 16:24:29,587][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 16:24:29,587][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 16:24:29,588][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 16:24:29,588][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 16:24:29,588][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 16:24:29,588][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 16:24:29,588][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 16:24:29,588][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 16:24:29,588][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-06 16:24:29,589][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-06 16:24:29,590][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-06 16:24:39,172][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 361 @ 17269 updates)
[2024-10-06 16:24:39,174][fairseq.trainer][INFO] - loading train data for epoch 361
[2024-10-06 16:24:40,060][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 16:24:40,060][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 16:24:40,060][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 16:24:40,060][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 16:24:40,060][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 16:24:40,060][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 16:24:40,068][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 16:24:40,068][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 16:24:40,068][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 16:24:40,068][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 16:24:40,068][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-06 16:24:40,069][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-06 16:24:40,071][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-06 16:24:40,926][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-06 16:24:58,422][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 16:24:58,436][fairseq.trainer][INFO] - begin training epoch 361
[2024-10-06 16:24:58,437][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 16:25:13,971][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 361 @ 17269 updates)
[2024-10-06 16:25:13,972][fairseq.trainer][INFO] - loading train data for epoch 361
[2024-10-06 16:25:14,274][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-06 16:25:22,784][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 361 @ 17269 updates)
[2024-10-06 16:25:22,792][fairseq.trainer][INFO] - loading train data for epoch 361
[2024-10-06 16:25:22,832][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 16:25:22,833][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 16:25:22,833][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 16:25:22,833][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 16:25:22,833][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 16:25:22,833][fairseq.utils][INFO] - rank   4: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 16:25:22,833][fairseq.utils][INFO] - rank   5: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 16:25:22,833][fairseq.utils][INFO] - rank   6: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 16:25:22,833][fairseq.utils][INFO] - rank   7: capabilities =  8.9  ; total memory = 47.500 GB ; name = NVIDIA RTX 6000 Ada Generation          
[2024-10-06 16:25:22,833][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 16:25:22,834][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-06 16:25:22,834][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-06 16:25:22,835][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-06 16:25:24,179][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-06 16:25:33,646][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 16:25:33,662][fairseq.trainer][INFO] - begin training epoch 361
[2024-10-06 16:25:33,667][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 16:25:36,694][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 16:25:36,704][fairseq.trainer][INFO] - begin training epoch 361
[2024-10-06 16:25:36,705][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 16:25:46,557][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 361 @ 17269 updates)
[2024-10-06 16:25:46,558][fairseq.trainer][INFO] - loading train data for epoch 361
[2024-10-06 16:25:46,850][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 84002, skipped 0 samples
[2024-10-06 16:25:54,691][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 16:25:54,701][fairseq.trainer][INFO] - begin training epoch 361
[2024-10-06 16:25:54,701][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 16:31:48,656][fairseq.trainer][WARNING] - OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 174.00 MiB. GPU 
[2024-10-06 16:31:48,659][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 1            |        cudaMalloc retries: 1         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   2190 MiB |   2191 MiB |   2492 MiB | 309626 KiB |
|       from large pool |   2151 MiB |   2151 MiB |   2380 MiB | 234490 KiB |
|       from small pool |     38 MiB |    101 MiB |    111 MiB |  75136 KiB |
|---------------------------------------------------------------------------|
| Active memory         |   2190 MiB |   2191 MiB |   2492 MiB | 309626 KiB |
|       from large pool |   2151 MiB |   2151 MiB |   2380 MiB | 234490 KiB |
|       from small pool |     38 MiB |    101 MiB |    111 MiB |  75136 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |   2179 MiB |   2180 MiB |   2480 MiB | 308368 KiB |
|       from large pool |   2141 MiB |   2141 MiB |   2368 MiB | 233271 KiB |
|       from small pool |     38 MiB |    101 MiB |    111 MiB |  75097 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   2294 MiB |   2332 MiB |   2332 MiB |  38912 KiB |
|       from large pool |   2206 MiB |   2228 MiB |   2228 MiB |  22528 KiB |
|       from small pool |     88 MiB |    104 MiB |    104 MiB |  16384 KiB |
|---------------------------------------------------------------------------|
| Non-releasable memory | 106342 KiB | 137833 KiB | 469905 KiB | 363563 KiB |
|       from large pool |  55617 KiB |  86183 KiB | 298744 KiB | 243127 KiB |
|       from small pool |  50725 KiB |  53385 KiB | 171161 KiB | 120436 KiB |
|---------------------------------------------------------------------------|
| Allocations           |     999    |    1301    |    1375    |     376    |
|       from large pool |     111    |     111    |     119    |       8    |
|       from small pool |     888    |    1223    |    1256    |     368    |
|---------------------------------------------------------------------------|
| Active allocs         |     999    |    1301    |    1375    |     376    |
|       from large pool |     111    |     111    |     119    |       8    |
|       from small pool |     888    |    1223    |    1256    |     368    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      84    |      93    |      93    |       9    |
|       from large pool |      40    |      41    |      41    |       1    |
|       from small pool |      44    |      52    |      52    |       8    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     144    |     144    |     264    |     120    |
|       from large pool |      21    |      21    |      34    |      13    |
|       from small pool |     123    |     125    |     230    |     107    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-06 16:31:48,659][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-06 16:31:48,660][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-06 16:31:48,660][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-06 16:31:48,660][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 4                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-06 16:31:48,660][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 5                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-06 16:31:48,661][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 6                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-06 16:31:48,661][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 7                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-06 16:31:48,661][fairseq.trainer][WARNING] - attempting to recover from OOM in forward/backward pass
[2024-10-06 16:32:57,090][fairseq_cli.train][INFO] - end of epoch 361 (average epoch stats below)
[2024-10-06 16:32:57,252][train][INFO] - {"epoch": 361, "train_loss": "0.795", "train_ntokens": "260767", "train_nsentences": "1750.04", "train_wps": "62117.4", "train_ups": "0.24", "train_wpb": "260767", "train_bsz": "1750", "train_num_updates": "17317", "train_lr": "0.000270578", "train_gnorm": "0.484", "train_loss_scale": "2", "train_train_wall": "337", "train_gb_free": "39.6", "train_wall": "655"}
[2024-10-06 16:32:57,648][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 16:32:57,658][fairseq.trainer][INFO] - begin training epoch 362
[2024-10-06 16:32:57,658][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 16:33:35,766][fairseq.trainer][WARNING] - OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 174.00 MiB. GPU 
[2024-10-06 16:33:35,768][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 1            |        cudaMalloc retries: 1         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   4562 MiB |   4563 MiB |   6546 MiB |   1983 MiB |
|       from large pool |   4519 MiB |   4519 MiB |   6399 MiB |   1880 MiB |
|       from small pool |     42 MiB |    101 MiB |    146 MiB |    103 MiB |
|---------------------------------------------------------------------------|
| Active memory         |   4562 MiB |   4563 MiB |   6546 MiB |   1983 MiB |
|       from large pool |   4519 MiB |   4519 MiB |   6399 MiB |   1880 MiB |
|       from small pool |     42 MiB |    101 MiB |    146 MiB |    103 MiB |
|---------------------------------------------------------------------------|
| Requested memory      |   4542 MiB |   4543 MiB |   6518 MiB |   1975 MiB |
|       from large pool |   4500 MiB |   4500 MiB |   6372 MiB |   1872 MiB |
|       from small pool |     42 MiB |    101 MiB |    146 MiB |    103 MiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   4674 MiB |   4688 MiB |   4688 MiB |  14336 KiB |
|       from large pool |   4584 MiB |   4584 MiB |   4584 MiB |      0 KiB |
|       from small pool |     90 MiB |    104 MiB |    104 MiB |  14336 KiB |
|---------------------------------------------------------------------------|
| Non-releasable memory | 114041 KiB | 282829 KiB |   1970 MiB |   1859 MiB |
|       from large pool |  65893 KiB | 233876 KiB |   1772 MiB |   1708 MiB |
|       from small pool |  48148 KiB |  56466 KiB |    197 MiB |    150 MiB |
|---------------------------------------------------------------------------|
| Allocations           |    1088    |    1301    |    1525    |     437    |
|       from large pool |     175    |     175    |     209    |      34    |
|       from small pool |     913    |    1223    |    1316    |     403    |
|---------------------------------------------------------------------------|
| Active allocs         |    1088    |    1301    |    1525    |     437    |
|       from large pool |     175    |     175    |     209    |      34    |
|       from small pool |     913    |    1223    |    1316    |     403    |
|---------------------------------------------------------------------------|
| GPU reserved segments |     103    |     110    |     110    |       7    |
|       from large pool |      58    |      58    |      58    |       0    |
|       from small pool |      45    |      52    |      52    |       7    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     153    |     155    |     294    |     141    |
|       from large pool |      28    |      30    |      61    |      33    |
|       from small pool |     125    |     127    |     233    |     108    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-06 16:33:35,779][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-06 16:33:35,779][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-06 16:33:35,780][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-06 16:33:35,780][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 4                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-06 16:33:35,785][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 5                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-06 16:33:35,785][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 6                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-06 16:33:35,785][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 7                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-06 16:33:35,791][fairseq.trainer][WARNING] - attempting to recover from OOM in forward/backward pass
[2024-10-06 16:33:37,231][fairseq_cli.train][INFO] - end of epoch 361 (average epoch stats below)
[2024-10-06 16:33:37,318][train][INFO] - {"epoch": 361, "train_loss": "0.795", "train_ntokens": "260767", "train_nsentences": "1750.04", "train_wps": "61740.4", "train_ups": "0.24", "train_wpb": "260767", "train_bsz": "1750", "train_num_updates": "17317", "train_lr": "0.000270578", "train_gnorm": "0.483", "train_loss_scale": "2", "train_train_wall": "359", "train_gb_free": "39.6", "train_wall": "692"}
[2024-10-06 16:33:38,292][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 16:33:38,297][fairseq.trainer][INFO] - begin training epoch 362
[2024-10-06 16:33:38,306][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 16:33:44,563][fairseq.trainer][WARNING] - OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 
[2024-10-06 16:33:44,564][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 1            |        cudaMalloc retries: 2         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   1597 MiB |   1618 MiB |   1662 MiB |  67329 KiB |
|       from large pool |   1560 MiB |   1560 MiB |   1560 MiB |      0 KiB |
|       from small pool |     36 MiB |    101 MiB |    102 MiB |  67329 KiB |
|---------------------------------------------------------------------------|
| Active memory         |   1597 MiB |   1618 MiB |   1662 MiB |  67329 KiB |
|       from large pool |   1560 MiB |   1560 MiB |   1560 MiB |      0 KiB |
|       from small pool |     36 MiB |    101 MiB |    102 MiB |  67329 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |   1590 MiB |   1611 MiB |   1656 MiB |  67290 KiB |
|       from large pool |   1553 MiB |   1553 MiB |   1553 MiB |      0 KiB |
|       from small pool |     36 MiB |    101 MiB |    102 MiB |  67290 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   1672 MiB |   1672 MiB |   1684 MiB |  12288 KiB |
|       from large pool |   1580 MiB |   1580 MiB |   1580 MiB |      0 KiB |
|       from small pool |     92 MiB |    104 MiB |    104 MiB |  12288 KiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  76767 KiB |  76883 KiB | 336056 KiB | 259289 KiB |
|       from large pool |  20268 KiB |  20268 KiB | 172204 KiB | 151936 KiB |
|       from small pool |  56499 KiB |  57481 KiB | 163852 KiB | 107353 KiB |
|---------------------------------------------------------------------------|
| Allocations           |     956    |    1301    |    1313    |     357    |
|       from large pool |      82    |      82    |      82    |       0    |
|       from small pool |     874    |    1223    |    1231    |     357    |
|---------------------------------------------------------------------------|
| Active allocs         |     956    |    1301    |    1313    |     357    |
|       from large pool |      82    |      82    |      82    |       0    |
|       from small pool |     874    |    1223    |    1231    |     357    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      63    |      68    |      69    |       6    |
|       from large pool |      17    |      17    |      17    |       0    |
|       from small pool |      46    |      52    |      52    |       6    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     138    |     138    |     246    |     108    |
|       from large pool |       9    |       9    |      17    |       8    |
|       from small pool |     129    |     129    |     229    |     100    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-06 16:33:44,572][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-06 16:33:44,573][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-06 16:33:44,573][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-06 16:33:44,573][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 4                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-06 16:33:44,574][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 5                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-06 16:33:44,574][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 6                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-06 16:33:44,574][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 7                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[2024-10-06 16:33:44,574][fairseq.trainer][WARNING] - attempting to recover from OOM in forward/backward pass
[2024-10-06 16:42:52,408][fairseq_cli.train][INFO] - end of epoch 362 (average epoch stats below)
[2024-10-06 16:42:52,584][train][INFO] - {"epoch": 362, "train_loss": "0.796", "train_ntokens": "260459", "train_nsentences": "1750.04", "train_wps": "21004.2", "train_ups": "0.08", "train_wpb": "260459", "train_bsz": "1750", "train_num_updates": "17365", "train_lr": "0.000271328", "train_gnorm": "0.636", "train_loss_scale": "2", "train_train_wall": "218", "train_gb_free": "39.8", "train_wall": "1250"}
[2024-10-06 16:42:52,894][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 16:42:52,900][fairseq.trainer][INFO] - begin training epoch 363
[2024-10-06 16:42:52,901][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 16:43:16,978][fairseq_cli.train][INFO] - end of epoch 362 (average epoch stats below)
[2024-10-06 16:43:16,994][train][INFO] - {"epoch": 362, "train_loss": "0.796", "train_ntokens": "260459", "train_nsentences": "1750.04", "train_wps": "21567.8", "train_ups": "0.08", "train_wpb": "260459", "train_bsz": "1750", "train_num_updates": "17365", "train_lr": "0.000271328", "train_gnorm": "0.638", "train_loss_scale": "2", "train_train_wall": "288", "train_gb_free": "39.8", "train_wall": "1272"}
[2024-10-06 16:43:17,072][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 16:43:17,077][fairseq.trainer][INFO] - begin training epoch 363
[2024-10-06 16:43:17,079][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 16:46:11,040][train_inner][INFO] - {"epoch": 363, "update": 362.729, "loss": "0.793", "ntokens": "260570", "nsentences": "1750.45", "wps": "34172.9", "ups": "0.13", "wpb": "260570", "bsz": "1750.5", "num_updates": "17400", "lr": "0.000271875", "gnorm": "0.545", "loss_scale": "2", "train_wall": "633", "gb_free": "39.3", "wall": "1449"}
[2024-10-06 16:46:33,360][fairseq_cli.train][INFO] - end of epoch 363 (average epoch stats below)
[2024-10-06 16:46:33,373][train][INFO] - {"epoch": 363, "train_loss": "0.787", "train_ntokens": "260592", "train_nsentences": "1750.04", "train_wps": "56656.6", "train_ups": "0.22", "train_wpb": "260592", "train_bsz": "1750", "train_num_updates": "17413", "train_lr": "0.000272078", "train_gnorm": "0.564", "train_loss_scale": "2", "train_train_wall": "99", "train_gb_free": "40", "train_wall": "1471"}
[2024-10-06 16:46:33,484][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 16:46:33,492][fairseq.trainer][INFO] - begin training epoch 364
[2024-10-06 16:46:33,492][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 16:46:38,660][train_inner][INFO] - {"epoch": 363, "update": 362.729, "loss": "0.793", "ntokens": "260570", "nsentences": "1750.45", "wps": "34567.1", "ups": "0.13", "wpb": "260570", "bsz": "1750.5", "num_updates": "17400", "lr": "0.000271875", "gnorm": "0.544", "loss_scale": "2", "train_wall": "722", "gb_free": "39.3", "wall": "1473"}
[2024-10-06 16:46:56,329][fairseq_cli.train][INFO] - end of epoch 363 (average epoch stats below)
[2024-10-06 16:46:56,335][train][INFO] - {"epoch": 363, "train_loss": "0.787", "train_ntokens": "260592", "train_nsentences": "1750.04", "train_wps": "57028.8", "train_ups": "0.22", "train_wpb": "260592", "train_bsz": "1750", "train_num_updates": "17413", "train_lr": "0.000272078", "train_gnorm": "0.565", "train_loss_scale": "2", "train_train_wall": "84", "train_gb_free": "40", "train_wall": "1491"}
[2024-10-06 16:46:56,493][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 16:46:56,497][fairseq.trainer][INFO] - begin training epoch 364
[2024-10-06 16:46:56,498][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 16:49:54,111][fairseq_cli.train][INFO] - end of epoch 364 (average epoch stats below)
[2024-10-06 16:49:54,135][train][INFO] - {"epoch": 364, "train_loss": "0.789", "train_ntokens": "260569", "train_nsentences": "1750.04", "train_wps": "62306.1", "train_ups": "0.24", "train_wpb": "260569", "train_bsz": "1750", "train_num_updates": "17461", "train_lr": "0.000272828", "train_gnorm": "0.618", "train_loss_scale": "2", "train_train_wall": "104", "train_gb_free": "39.6", "train_wall": "1672"}
[2024-10-06 16:49:54,241][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 16:49:54,256][fairseq.trainer][INFO] - begin training epoch 365
[2024-10-06 16:49:54,257][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 16:50:22,124][fairseq_cli.train][INFO] - end of epoch 364 (average epoch stats below)
[2024-10-06 16:50:22,139][train][INFO] - {"epoch": 364, "train_loss": "0.789", "train_ntokens": "260569", "train_nsentences": "1750.04", "train_wps": "60777", "train_ups": "0.23", "train_wpb": "260569", "train_bsz": "1750", "train_num_updates": "17461", "train_lr": "0.000272828", "train_gnorm": "0.621", "train_loss_scale": "2", "train_train_wall": "88", "train_gb_free": "39.6", "train_wall": "1697"}
[2024-10-06 16:50:22,221][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 16:50:22,226][fairseq.trainer][INFO] - begin training epoch 365
[2024-10-06 16:50:22,226][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 16:53:06,297][fairseq_cli.train][INFO] - end of epoch 365 (average epoch stats below)
[2024-10-06 16:53:06,304][train][INFO] - {"epoch": 365, "train_loss": "0.786", "train_ntokens": "260611", "train_nsentences": "1750.04", "train_wps": "65096.6", "train_ups": "0.25", "train_wpb": "260612", "train_bsz": "1750", "train_num_updates": "17509", "train_lr": "0.000273578", "train_gnorm": "0.595", "train_loss_scale": "2", "train_train_wall": "93", "train_gb_free": "39.8", "train_wall": "1864"}
[2024-10-06 16:53:06,438][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 16:53:06,453][fairseq.trainer][INFO] - begin training epoch 366
[2024-10-06 16:53:06,454][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 16:53:37,871][fairseq_cli.train][INFO] - end of epoch 365 (average epoch stats below)
[2024-10-06 16:53:37,875][train][INFO] - {"epoch": 365, "train_loss": "0.787", "train_ntokens": "260611", "train_nsentences": "1750.04", "train_wps": "63910.3", "train_ups": "0.25", "train_wpb": "260612", "train_bsz": "1750", "train_num_updates": "17509", "train_lr": "0.000273578", "train_gnorm": "0.612", "train_loss_scale": "2", "train_train_wall": "97", "train_gb_free": "39.8", "train_wall": "1893"}
[2024-10-06 16:53:37,986][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 16:53:37,990][fairseq.trainer][INFO] - begin training epoch 366
[2024-10-06 16:53:37,993][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 16:56:23,811][fairseq_cli.train][INFO] - end of epoch 366 (average epoch stats below)
[2024-10-06 16:56:23,815][train][INFO] - {"epoch": 366, "train_loss": "0.788", "train_ntokens": "260447", "train_nsentences": "1750.04", "train_wps": "63295.7", "train_ups": "0.24", "train_wpb": "260447", "train_bsz": "1750", "train_num_updates": "17557", "train_lr": "0.000274328", "train_gnorm": "0.602", "train_loss_scale": "2", "train_train_wall": "106", "train_gb_free": "39.4", "train_wall": "2062"}
[2024-10-06 16:56:23,877][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 16:56:23,882][fairseq.trainer][INFO] - begin training epoch 367
[2024-10-06 16:56:23,882][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 16:56:53,704][fairseq_cli.train][INFO] - end of epoch 366 (average epoch stats below)
[2024-10-06 16:56:53,707][train][INFO] - {"epoch": 366, "train_loss": "0.788", "train_ntokens": "260447", "train_nsentences": "1750.04", "train_wps": "63839.4", "train_ups": "0.25", "train_wpb": "260447", "train_bsz": "1750", "train_num_updates": "17557", "train_lr": "0.000274328", "train_gnorm": "0.589", "train_loss_scale": "2", "train_train_wall": "80", "train_gb_free": "39.4", "train_wall": "2089"}
[2024-10-06 16:56:53,839][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 16:56:53,850][fairseq.trainer][INFO] - begin training epoch 367
[2024-10-06 16:56:53,851][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 16:59:39,875][train_inner][INFO] - {"epoch": 367, "update": 366.896, "loss": "0.788", "ntokens": "260581", "nsentences": "1752.09", "wps": "64436.3", "ups": "0.25", "wpb": "260581", "bsz": "1752.1", "num_updates": "17600", "lr": "0.000275", "gnorm": "0.598", "loss_scale": "2", "train_wall": "428", "gb_free": "39.8", "wall": "2258"}
[2024-10-06 16:59:42,556][fairseq_cli.train][INFO] - end of epoch 367 (average epoch stats below)
[2024-10-06 16:59:42,559][train][INFO] - {"epoch": 367, "train_loss": "0.786", "train_ntokens": "260793", "train_nsentences": "1750.04", "train_wps": "62986.7", "train_ups": "0.24", "train_wpb": "260793", "train_bsz": "1750", "train_num_updates": "17605", "train_lr": "0.000275078", "train_gnorm": "0.566", "train_loss_scale": "2", "train_train_wall": "106", "train_gb_free": "39.6", "train_wall": "2261"}
[2024-10-06 16:59:42,676][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 16:59:42,683][fairseq.trainer][INFO] - begin training epoch 368
[2024-10-06 16:59:42,684][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 17:00:09,495][train_inner][INFO] - {"epoch": 367, "update": 366.896, "loss": "0.788", "ntokens": "260581", "nsentences": "1752.09", "wps": "64276.2", "ups": "0.25", "wpb": "260581", "bsz": "1752.1", "num_updates": "17600", "lr": "0.000275", "gnorm": "0.604", "loss_scale": "2", "train_wall": "353", "gb_free": "39.8", "wall": "2284"}
[2024-10-06 17:00:12,141][fairseq_cli.train][INFO] - end of epoch 367 (average epoch stats below)
[2024-10-06 17:00:12,143][train][INFO] - {"epoch": 367, "train_loss": "0.787", "train_ntokens": "260793", "train_nsentences": "1750.04", "train_wps": "63084.5", "train_ups": "0.24", "train_wpb": "260793", "train_bsz": "1750", "train_num_updates": "17605", "train_lr": "0.000275078", "train_gnorm": "0.586", "train_loss_scale": "2", "train_train_wall": "82", "train_gb_free": "39.6", "train_wall": "2287"}
[2024-10-06 17:00:12,226][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 17:00:12,229][fairseq.trainer][INFO] - begin training epoch 368
[2024-10-06 17:00:12,229][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 17:02:52,509][fairseq_cli.train][INFO] - end of epoch 368 (average epoch stats below)
[2024-10-06 17:02:52,535][train][INFO] - {"epoch": 368, "train_loss": "0.792", "train_ntokens": "261101", "train_nsentences": "1750.04", "train_wps": "65977.8", "train_ups": "0.25", "train_wpb": "261101", "train_bsz": "1750", "train_num_updates": "17653", "train_lr": "0.000275828", "train_gnorm": "0.514", "train_loss_scale": "2", "train_train_wall": "84", "train_gb_free": "39.3", "train_wall": "2450"}
[2024-10-06 17:02:52,637][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 17:02:52,645][fairseq.trainer][INFO] - begin training epoch 369
[2024-10-06 17:02:52,645][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 17:03:30,526][fairseq_cli.train][INFO] - end of epoch 368 (average epoch stats below)
[2024-10-06 17:03:30,529][train][INFO] - {"epoch": 368, "train_loss": "0.791", "train_ntokens": "261101", "train_nsentences": "1750.04", "train_wps": "63175.2", "train_ups": "0.24", "train_wpb": "261101", "train_bsz": "1750", "train_num_updates": "17653", "train_lr": "0.000275828", "train_gnorm": "0.488", "train_loss_scale": "2", "train_train_wall": "74", "train_gb_free": "39.3", "train_wall": "2485"}
[2024-10-06 17:03:30,677][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 17:03:30,684][fairseq.trainer][INFO] - begin training epoch 369
[2024-10-06 17:03:30,685][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 17:06:02,186][fairseq_cli.train][INFO] - end of epoch 369 (average epoch stats below)
[2024-10-06 17:06:02,190][train][INFO] - {"epoch": 369, "train_loss": "0.788", "train_ntokens": "260500", "train_nsentences": "1750.04", "train_wps": "65931.5", "train_ups": "0.25", "train_wpb": "260500", "train_bsz": "1750", "train_num_updates": "17701", "train_lr": "0.000276578", "train_gnorm": "0.65", "train_loss_scale": "2", "train_train_wall": "53", "train_gb_free": "39.8", "train_wall": "2640"}
[2024-10-06 17:06:02,549][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 17:06:02,554][fairseq.trainer][INFO] - begin training epoch 370
[2024-10-06 17:06:02,555][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 17:06:42,985][fairseq_cli.train][INFO] - end of epoch 369 (average epoch stats below)
[2024-10-06 17:06:42,988][train][INFO] - {"epoch": 369, "train_loss": "0.787", "train_ntokens": "260500", "train_nsentences": "1750.04", "train_wps": "64970.7", "train_ups": "0.25", "train_wpb": "260500", "train_bsz": "1750", "train_num_updates": "17701", "train_lr": "0.000276578", "train_gnorm": "0.653", "train_loss_scale": "2", "train_train_wall": "62", "train_gb_free": "39.8", "train_wall": "2678"}
[2024-10-06 17:06:43,043][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 17:06:43,046][fairseq.trainer][INFO] - begin training epoch 370
[2024-10-06 17:06:43,047][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 17:09:14,850][fairseq_cli.train][INFO] - end of epoch 370 (average epoch stats below)
[2024-10-06 17:09:14,876][train][INFO] - {"epoch": 370, "train_loss": "0.782", "train_ntokens": "260976", "train_nsentences": "1750.04", "train_wps": "65018.8", "train_ups": "0.25", "train_wpb": "260976", "train_bsz": "1750", "train_num_updates": "17749", "train_lr": "0.000277328", "train_gnorm": "0.499", "train_loss_scale": "2", "train_train_wall": "92", "train_gb_free": "39.6", "train_wall": "2833"}
[2024-10-06 17:09:15,093][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 17:09:15,100][fairseq.trainer][INFO] - begin training epoch 371
[2024-10-06 17:09:15,101][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 17:10:09,649][fairseq_cli.train][INFO] - end of epoch 370 (average epoch stats below)
[2024-10-06 17:10:09,655][train][INFO] - {"epoch": 370, "train_loss": "0.782", "train_ntokens": "260976", "train_nsentences": "1750.04", "train_wps": "60615.2", "train_ups": "0.23", "train_wpb": "260976", "train_bsz": "1750", "train_num_updates": "17749", "train_lr": "0.000277328", "train_gnorm": "0.501", "train_loss_scale": "2", "train_train_wall": "96", "train_gb_free": "39.6", "train_wall": "2884"}
[2024-10-06 17:10:09,748][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 17:10:09,752][fairseq.trainer][INFO] - begin training epoch 371
[2024-10-06 17:10:09,753][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 17:12:22,382][fairseq_cli.train][INFO] - end of epoch 371 (average epoch stats below)
[2024-10-06 17:12:22,406][train][INFO] - {"epoch": 371, "train_loss": "0.786", "train_ntokens": "260790", "train_nsentences": "1750.04", "train_wps": "66759.3", "train_ups": "0.26", "train_wpb": "260790", "train_bsz": "1750", "train_num_updates": "17797", "train_lr": "0.000278078", "train_gnorm": "0.51", "train_loss_scale": "2", "train_train_wall": "88", "train_gb_free": "39.9", "train_wall": "3020"}
[2024-10-06 17:12:22,501][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 17:12:22,512][fairseq.trainer][INFO] - begin training epoch 372
[2024-10-06 17:12:22,513][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 17:13:25,508][fairseq_cli.train][INFO] - end of epoch 371 (average epoch stats below)
[2024-10-06 17:13:25,516][train][INFO] - {"epoch": 371, "train_loss": "0.786", "train_ntokens": "260790", "train_nsentences": "1750.04", "train_wps": "63913.4", "train_ups": "0.25", "train_wpb": "260790", "train_bsz": "1750", "train_num_updates": "17797", "train_lr": "0.000278078", "train_gnorm": "0.52", "train_loss_scale": "2", "train_train_wall": "92", "train_gb_free": "39.9", "train_wall": "3080"}
[2024-10-06 17:13:25,633][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 17:13:25,642][fairseq.trainer][INFO] - begin training epoch 372
[2024-10-06 17:13:25,642][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 17:14:33,646][train_inner][INFO] - {"epoch": 372, "update": 371.062, "loss": "0.786", "ntokens": "260871", "nsentences": "1747.68", "wps": "58376.3", "ups": "0.22", "wpb": "260872", "bsz": "1747.7", "num_updates": "17800", "lr": "0.000278125", "gnorm": "0.549", "loss_scale": "2", "train_wall": "342", "gb_free": "39.7", "wall": "3152"}
[2024-10-06 17:15:35,093][fairseq_cli.train][INFO] - end of epoch 372 (average epoch stats below)
[2024-10-06 17:15:35,108][train][INFO] - {"epoch": 372, "train_loss": "0.78", "train_ntokens": "260803", "train_nsentences": "1750.04", "train_wps": "64968.4", "train_ups": "0.25", "train_wpb": "260803", "train_bsz": "1750", "train_num_updates": "17845", "train_lr": "0.000278828", "train_gnorm": "0.634", "train_loss_scale": "2", "train_train_wall": "83", "train_gb_free": "39.6", "train_wall": "3213"}
[2024-10-06 17:15:35,211][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 17:15:35,215][fairseq.trainer][INFO] - begin training epoch 373
[2024-10-06 17:15:35,217][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 17:15:45,418][train_inner][INFO] - {"epoch": 372, "update": 371.062, "loss": "0.786", "ntokens": "260871", "nsentences": "1747.68", "wps": "55746.5", "ups": "0.21", "wpb": "260872", "bsz": "1747.7", "num_updates": "17800", "lr": "0.000278125", "gnorm": "0.549", "loss_scale": "2", "train_wall": "347", "gb_free": "39.7", "wall": "3220"}
[2024-10-06 17:16:39,127][fairseq_cli.train][INFO] - end of epoch 372 (average epoch stats below)
[2024-10-06 17:16:39,129][train][INFO] - {"epoch": 372, "train_loss": "0.779", "train_ntokens": "260803", "train_nsentences": "1750.04", "train_wps": "64658.3", "train_ups": "0.25", "train_wpb": "260803", "train_bsz": "1750", "train_num_updates": "17845", "train_lr": "0.000278828", "train_gnorm": "0.626", "train_loss_scale": "2", "train_train_wall": "71", "train_gb_free": "39.6", "train_wall": "3274"}
[2024-10-06 17:16:39,248][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 17:16:39,251][fairseq.trainer][INFO] - begin training epoch 373
[2024-10-06 17:16:39,252][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 17:18:48,375][fairseq_cli.train][INFO] - end of epoch 373 (average epoch stats below)
[2024-10-06 17:18:48,388][train][INFO] - {"epoch": 373, "train_loss": "0.78", "train_ntokens": "260862", "train_nsentences": "1750.04", "train_wps": "64787.2", "train_ups": "0.25", "train_wpb": "260862", "train_bsz": "1750", "train_num_updates": "17893", "train_lr": "0.000279578", "train_gnorm": "0.555", "train_loss_scale": "2", "train_train_wall": "87", "train_gb_free": "39.8", "train_wall": "3406"}
[2024-10-06 17:18:48,509][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 17:18:48,516][fairseq.trainer][INFO] - begin training epoch 374
[2024-10-06 17:18:48,517][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 17:19:53,157][fairseq_cli.train][INFO] - end of epoch 373 (average epoch stats below)
[2024-10-06 17:19:53,167][train][INFO] - {"epoch": 373, "train_loss": "0.781", "train_ntokens": "260862", "train_nsentences": "1750.04", "train_wps": "64533.4", "train_ups": "0.25", "train_wpb": "260862", "train_bsz": "1750", "train_num_updates": "17893", "train_lr": "0.000279578", "train_gnorm": "0.611", "train_loss_scale": "2", "train_train_wall": "76", "train_gb_free": "39.8", "train_wall": "3468"}
[2024-10-06 17:19:53,296][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 17:19:53,320][fairseq.trainer][INFO] - begin training epoch 374
[2024-10-06 17:19:53,321][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 17:22:06,631][fairseq_cli.train][INFO] - end of epoch 374 (average epoch stats below)
[2024-10-06 17:22:06,653][train][INFO] - {"epoch": 374, "train_loss": "0.783", "train_ntokens": "260595", "train_nsentences": "1750.04", "train_wps": "63094.7", "train_ups": "0.24", "train_wpb": "260595", "train_bsz": "1750", "train_num_updates": "17941", "train_lr": "0.000280328", "train_gnorm": "0.627", "train_loss_scale": "2", "train_train_wall": "86", "train_gb_free": "40.1", "train_wall": "3605"}
[2024-10-06 17:22:06,772][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 17:22:06,776][fairseq.trainer][INFO] - begin training epoch 375
[2024-10-06 17:22:06,777][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 17:23:09,000][fairseq_cli.train][INFO] - end of epoch 374 (average epoch stats below)
[2024-10-06 17:23:09,020][train][INFO] - {"epoch": 374, "train_loss": "0.782", "train_ntokens": "260595", "train_nsentences": "1750.04", "train_wps": "63872.3", "train_ups": "0.25", "train_wpb": "260595", "train_bsz": "1750", "train_num_updates": "17941", "train_lr": "0.000280328", "train_gnorm": "0.521", "train_loss_scale": "2", "train_train_wall": "68", "train_gb_free": "40.1", "train_wall": "3664"}
[2024-10-06 17:23:09,168][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 17:23:09,173][fairseq.trainer][INFO] - begin training epoch 375
[2024-10-06 17:23:09,173][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 17:25:25,227][fairseq_cli.train][INFO] - end of epoch 375 (average epoch stats below)
[2024-10-06 17:25:25,247][train][INFO] - {"epoch": 375, "train_loss": "0.786", "train_ntokens": "260469", "train_nsentences": "1750.04", "train_wps": "62959.4", "train_ups": "0.24", "train_wpb": "260469", "train_bsz": "1750", "train_num_updates": "17989", "train_lr": "0.000281078", "train_gnorm": "0.659", "train_loss_scale": "2", "train_train_wall": "88", "train_gb_free": "39.7", "train_wall": "3803"}
[2024-10-06 17:25:25,424][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 17:25:25,428][fairseq.trainer][INFO] - begin training epoch 376
[2024-10-06 17:25:25,429][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 17:26:24,083][fairseq_cli.train][INFO] - end of epoch 375 (average epoch stats below)
[2024-10-06 17:26:24,085][train][INFO] - {"epoch": 375, "train_loss": "0.787", "train_ntokens": "260469", "train_nsentences": "1750.04", "train_wps": "64094.9", "train_ups": "0.25", "train_wpb": "260469", "train_bsz": "1750", "train_num_updates": "17989", "train_lr": "0.000281078", "train_gnorm": "0.676", "train_loss_scale": "2", "train_train_wall": "74", "train_gb_free": "39.7", "train_wall": "3859"}
[2024-10-06 17:26:24,150][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 17:26:24,153][fairseq.trainer][INFO] - begin training epoch 376
[2024-10-06 17:26:24,153][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 17:27:34,676][train_inner][INFO] - {"epoch": 376, "update": 375.229, "loss": "0.782", "ntokens": "260778", "nsentences": "1746.1", "wps": "66778.6", "ups": "0.26", "wpb": "260778", "bsz": "1746.1", "num_updates": "18000", "lr": "0.00028125", "gnorm": "0.616", "loss_scale": "2", "train_wall": "355", "gb_free": "39.2", "wall": "3933"}
[2024-10-06 17:28:36,488][fairseq_cli.train][INFO] - end of epoch 376 (average epoch stats below)
[2024-10-06 17:28:36,499][train][INFO] - {"epoch": 376, "train_loss": "0.779", "train_ntokens": "260720", "train_nsentences": "1750.04", "train_wps": "65438.6", "train_ups": "0.25", "train_wpb": "260720", "train_bsz": "1750", "train_num_updates": "18037", "train_lr": "0.000281828", "train_gnorm": "0.535", "train_loss_scale": "2", "train_train_wall": "94", "train_gb_free": "39.4", "train_wall": "3994"}
[2024-10-06 17:28:36,629][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 17:28:36,636][fairseq.trainer][INFO] - begin training epoch 377
[2024-10-06 17:28:36,637][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 17:28:49,806][train_inner][INFO] - {"epoch": 376, "update": 375.229, "loss": "0.782", "ntokens": "260778", "nsentences": "1746.1", "wps": "66492.8", "ups": "0.25", "wpb": "260778", "bsz": "1746.1", "num_updates": "18000", "lr": "0.00028125", "gnorm": "0.607", "loss_scale": "2", "train_wall": "296", "gb_free": "39.2", "wall": "4005"}
[2024-10-06 17:29:37,049][fairseq_cli.train][INFO] - end of epoch 376 (average epoch stats below)
[2024-10-06 17:29:37,051][train][INFO] - {"epoch": 376, "train_loss": "0.778", "train_ntokens": "260720", "train_nsentences": "1750.04", "train_wps": "64854.7", "train_ups": "0.25", "train_wpb": "260720", "train_bsz": "1750", "train_num_updates": "18037", "train_lr": "0.000281828", "train_gnorm": "0.511", "train_loss_scale": "2", "train_train_wall": "58", "train_gb_free": "39.4", "train_wall": "4052"}
[2024-10-06 17:29:37,157][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 17:29:37,168][fairseq.trainer][INFO] - begin training epoch 377
[2024-10-06 17:29:37,169][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 17:31:54,183][fairseq_cli.train][INFO] - end of epoch 377 (average epoch stats below)
[2024-10-06 17:31:54,207][train][INFO] - {"epoch": 377, "train_loss": "0.776", "train_ntokens": "260652", "train_nsentences": "1750.04", "train_wps": "63284.5", "train_ups": "0.24", "train_wpb": "260652", "train_bsz": "1750", "train_num_updates": "18085", "train_lr": "0.000282578", "train_gnorm": "0.586", "train_loss_scale": "2", "train_train_wall": "84", "train_gb_free": "40.3", "train_wall": "4192"}
[2024-10-06 17:31:54,440][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 17:31:54,445][fairseq.trainer][INFO] - begin training epoch 378
[2024-10-06 17:31:54,446][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 17:32:58,244][fairseq_cli.train][INFO] - end of epoch 377 (average epoch stats below)
[2024-10-06 17:32:58,247][train][INFO] - {"epoch": 377, "train_loss": "0.777", "train_ntokens": "260652", "train_nsentences": "1750.04", "train_wps": "62185.2", "train_ups": "0.24", "train_wpb": "260652", "train_bsz": "1750", "train_num_updates": "18085", "train_lr": "0.000282578", "train_gnorm": "0.589", "train_loss_scale": "2", "train_train_wall": "78", "train_gb_free": "40.3", "train_wall": "4253"}
[2024-10-06 17:32:58,318][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 17:32:58,322][fairseq.trainer][INFO] - begin training epoch 378
[2024-10-06 17:32:58,323][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 17:35:03,033][fairseq_cli.train][INFO] - end of epoch 378 (average epoch stats below)
[2024-10-06 17:35:03,048][train][INFO] - {"epoch": 378, "train_loss": "0.776", "train_ntokens": "260604", "train_nsentences": "1750.04", "train_wps": "66245.5", "train_ups": "0.25", "train_wpb": "260604", "train_bsz": "1750", "train_num_updates": "18133", "train_lr": "0.000283328", "train_gnorm": "0.472", "train_loss_scale": "2", "train_train_wall": "81", "train_gb_free": "40.6", "train_wall": "4381"}
[2024-10-06 17:35:03,252][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 17:35:03,255][fairseq.trainer][INFO] - begin training epoch 379
[2024-10-06 17:35:03,256][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 17:36:11,601][fairseq_cli.train][INFO] - end of epoch 378 (average epoch stats below)
[2024-10-06 17:36:11,604][train][INFO] - {"epoch": 378, "train_loss": "0.775", "train_ntokens": "260604", "train_nsentences": "1750.04", "train_wps": "64694.3", "train_ups": "0.25", "train_wpb": "260604", "train_bsz": "1750", "train_num_updates": "18133", "train_lr": "0.000283328", "train_gnorm": "0.47", "train_loss_scale": "2", "train_train_wall": "70", "train_gb_free": "40.6", "train_wall": "4446"}
[2024-10-06 17:36:11,699][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 17:36:11,720][fairseq.trainer][INFO] - begin training epoch 379
[2024-10-06 17:36:11,721][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 17:38:19,769][fairseq_cli.train][INFO] - end of epoch 379 (average epoch stats below)
[2024-10-06 17:38:19,783][train][INFO] - {"epoch": 379, "train_loss": "0.774", "train_ntokens": "260785", "train_nsentences": "1750.04", "train_wps": "63631.5", "train_ups": "0.24", "train_wpb": "260785", "train_bsz": "1750", "train_num_updates": "18181", "train_lr": "0.000284078", "train_gnorm": "0.581", "train_loss_scale": "2", "train_train_wall": "89", "train_gb_free": "39.3", "train_wall": "4578"}
[2024-10-06 17:38:19,908][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 17:38:19,921][fairseq.trainer][INFO] - begin training epoch 380
[2024-10-06 17:38:19,922][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 17:39:22,235][fairseq_cli.train][INFO] - end of epoch 379 (average epoch stats below)
[2024-10-06 17:39:22,243][train][INFO] - {"epoch": 379, "train_loss": "0.774", "train_ntokens": "260785", "train_nsentences": "1750.04", "train_wps": "65662.8", "train_ups": "0.25", "train_wpb": "260785", "train_bsz": "1750", "train_num_updates": "18181", "train_lr": "0.000284078", "train_gnorm": "0.595", "train_loss_scale": "2", "train_train_wall": "84", "train_gb_free": "39.3", "train_wall": "4637"}
[2024-10-06 17:39:22,350][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 17:39:22,362][fairseq.trainer][INFO] - begin training epoch 380
[2024-10-06 17:39:22,363][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 17:40:50,890][train_inner][INFO] - {"epoch": 380, "update": 379.396, "loss": "0.776", "ntokens": "260549", "nsentences": "1752.71", "wps": "65447.7", "ups": "0.25", "wpb": "260550", "bsz": "1752.7", "num_updates": "18200", "lr": "0.000284375", "gnorm": "0.557", "loss_scale": "2", "train_wall": "339", "gb_free": "39.3", "wall": "4729"}
[2024-10-06 17:41:40,609][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 380 @ 18229 updates
[2024-10-06 17:41:40,609][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-06 17:41:44,183][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-06 17:41:44,198][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 380 @ 18229 updates, score None) (writing took 3.5898404214531183 seconds)
[2024-10-06 17:41:44,199][fairseq_cli.train][INFO] - end of epoch 380 (average epoch stats below)
[2024-10-06 17:41:44,212][train][INFO] - {"epoch": 380, "train_loss": "0.774", "train_ntokens": "260794", "train_nsentences": "1750.04", "train_wps": "61239.2", "train_ups": "0.23", "train_wpb": "260794", "train_bsz": "1750", "train_num_updates": "18229", "train_lr": "0.000284828", "train_gnorm": "0.527", "train_loss_scale": "2", "train_train_wall": "74", "train_gb_free": "39.8", "train_wall": "4782"}
[2024-10-06 17:41:44,266][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 17:41:44,291][fairseq.trainer][INFO] - begin training epoch 381
[2024-10-06 17:41:44,292][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 17:42:03,205][train_inner][INFO] - {"epoch": 380, "update": 379.396, "loss": "0.776", "ntokens": "260549", "nsentences": "1752.71", "wps": "65680.7", "ups": "0.25", "wpb": "260550", "bsz": "1752.7", "num_updates": "18200", "lr": "0.000284375", "gnorm": "0.546", "loss_scale": "2", "train_wall": "295", "gb_free": "39.3", "wall": "4798"}
[2024-10-06 17:42:35,003][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 380 @ 18229 updates
[2024-10-06 17:42:35,015][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-06 17:42:38,755][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-06 17:42:38,758][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 380 @ 18229 updates, score None) (writing took 3.754953498020768 seconds)
[2024-10-06 17:42:38,758][fairseq_cli.train][INFO] - end of epoch 380 (average epoch stats below)
[2024-10-06 17:42:38,760][train][INFO] - {"epoch": 380, "train_loss": "0.774", "train_ntokens": "260794", "train_nsentences": "1750.04", "train_wps": "63700.8", "train_ups": "0.24", "train_wpb": "260794", "train_bsz": "1750", "train_num_updates": "18229", "train_lr": "0.000284828", "train_gnorm": "0.532", "train_loss_scale": "2", "train_train_wall": "64", "train_gb_free": "39.8", "train_wall": "4834"}
[2024-10-06 17:42:38,818][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 17:42:38,822][fairseq.trainer][INFO] - begin training epoch 381
[2024-10-06 17:42:38,822][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 17:44:39,387][fairseq_cli.train][INFO] - end of epoch 381 (average epoch stats below)
[2024-10-06 17:44:39,520][train][INFO] - {"epoch": 381, "train_loss": "0.774", "train_ntokens": "260688", "train_nsentences": "1750.04", "train_wps": "71380", "train_ups": "0.27", "train_wpb": "260688", "train_bsz": "1750", "train_num_updates": "18277", "train_lr": "0.000285578", "train_gnorm": "0.547", "train_loss_scale": "2", "train_train_wall": "41", "train_gb_free": "39.3", "train_wall": "4957"}
[2024-10-06 17:44:42,925][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 17:44:43,488][fairseq.trainer][INFO] - begin training epoch 382
[2024-10-06 17:44:43,489][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 17:46:23,288][fairseq_cli.train][INFO] - end of epoch 381 (average epoch stats below)
[2024-10-06 17:46:23,296][train][INFO] - {"epoch": 381, "train_loss": "0.774", "train_ntokens": "260688", "train_nsentences": "1750.04", "train_wps": "55729.1", "train_ups": "0.21", "train_wpb": "260688", "train_bsz": "1750", "train_num_updates": "18277", "train_lr": "0.000285578", "train_gnorm": "0.534", "train_loss_scale": "2", "train_train_wall": "45", "train_gb_free": "39.3", "train_wall": "5058"}
[2024-10-06 17:46:23,479][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 17:46:23,483][fairseq.trainer][INFO] - begin training epoch 382
[2024-10-06 17:46:23,485][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 17:48:39,769][fairseq_cli.train][INFO] - end of epoch 382 (average epoch stats below)
[2024-10-06 17:48:39,786][train][INFO] - {"epoch": 382, "train_loss": "0.772", "train_ntokens": "260545", "train_nsentences": "1750.04", "train_wps": "52052.3", "train_ups": "0.2", "train_wpb": "260545", "train_bsz": "1750", "train_num_updates": "18325", "train_lr": "0.000286328", "train_gnorm": "0.544", "train_loss_scale": "2", "train_train_wall": "59", "train_gb_free": "39.7", "train_wall": "5198"}
[2024-10-06 17:48:40,223][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 17:48:40,229][fairseq.trainer][INFO] - begin training epoch 383
[2024-10-06 17:48:40,229][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 17:49:58,055][fairseq_cli.train][INFO] - end of epoch 382 (average epoch stats below)
[2024-10-06 17:49:58,061][train][INFO] - {"epoch": 382, "train_loss": "0.772", "train_ntokens": "260545", "train_nsentences": "1750.04", "train_wps": "58232.9", "train_ups": "0.22", "train_wpb": "260545", "train_bsz": "1750", "train_num_updates": "18325", "train_lr": "0.000286328", "train_gnorm": "0.542", "train_loss_scale": "2", "train_train_wall": "56", "train_gb_free": "39.7", "train_wall": "5273"}
[2024-10-06 17:49:58,226][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 17:49:58,231][fairseq.trainer][INFO] - begin training epoch 383
[2024-10-06 17:49:58,232][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 17:52:02,741][fairseq_cli.train][INFO] - end of epoch 383 (average epoch stats below)
[2024-10-06 17:52:02,819][train][INFO] - {"epoch": 383, "train_loss": "0.772", "train_ntokens": "260770", "train_nsentences": "1750.04", "train_wps": "61651.3", "train_ups": "0.24", "train_wpb": "260770", "train_bsz": "1750", "train_num_updates": "18373", "train_lr": "0.000287078", "train_gnorm": "0.586", "train_loss_scale": "2", "train_train_wall": "50", "train_gb_free": "39.3", "train_wall": "5401"}
[2024-10-06 17:52:05,607][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 17:52:06,068][fairseq.trainer][INFO] - begin training epoch 384
[2024-10-06 17:52:06,069][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 17:53:49,481][fairseq_cli.train][INFO] - end of epoch 383 (average epoch stats below)
[2024-10-06 17:53:49,494][train][INFO] - {"epoch": 383, "train_loss": "0.773", "train_ntokens": "260770", "train_nsentences": "1750.04", "train_wps": "54086.5", "train_ups": "0.21", "train_wpb": "260770", "train_bsz": "1750", "train_num_updates": "18373", "train_lr": "0.000287078", "train_gnorm": "0.638", "train_loss_scale": "2", "train_train_wall": "55", "train_gb_free": "39.3", "train_wall": "5504"}
[2024-10-06 17:53:49,730][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 17:53:49,740][fairseq.trainer][INFO] - begin training epoch 384
[2024-10-06 17:53:49,741][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 17:55:58,132][train_inner][INFO] - {"epoch": 384, "update": 383.562, "loss": "0.773", "ntokens": "260495", "nsentences": "1763.44", "wps": "57426.1", "ups": "0.22", "wpb": "260495", "bsz": "1763.4", "num_updates": "18400", "lr": "0.0002875", "gnorm": "0.555", "loss_scale": "2", "train_wall": "237", "gb_free": "39.7", "wall": "5636"}
[2024-10-06 17:56:20,876][fairseq_cli.train][INFO] - end of epoch 384 (average epoch stats below)
[2024-10-06 17:56:20,880][train][INFO] - {"epoch": 384, "train_loss": "0.77", "train_ntokens": "260491", "train_nsentences": "1750.04", "train_wps": "48452.6", "train_ups": "0.19", "train_wpb": "260491", "train_bsz": "1750", "train_num_updates": "18421", "train_lr": "0.000287828", "train_gnorm": "0.563", "train_loss_scale": "2", "train_train_wall": "61", "train_gb_free": "39.6", "train_wall": "5659"}
[2024-10-06 17:56:22,483][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 17:56:22,672][fairseq.trainer][INFO] - begin training epoch 385
[2024-10-06 17:56:22,673][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 17:57:19,255][train_inner][INFO] - {"epoch": 384, "update": 383.562, "loss": "0.773", "ntokens": "260495", "nsentences": "1763.44", "wps": "56874.1", "ups": "0.22", "wpb": "260495", "bsz": "1763.4", "num_updates": "18400", "lr": "0.0002875", "gnorm": "0.562", "loss_scale": "2", "train_wall": "221", "gb_free": "39.7", "wall": "5714"}
[2024-10-06 17:57:39,627][fairseq_cli.train][INFO] - end of epoch 384 (average epoch stats below)
[2024-10-06 17:57:39,630][train][INFO] - {"epoch": 384, "train_loss": "0.77", "train_ntokens": "260491", "train_nsentences": "1750.04", "train_wps": "54331.6", "train_ups": "0.21", "train_wpb": "260491", "train_bsz": "1750", "train_num_updates": "18421", "train_lr": "0.000287828", "train_gnorm": "0.542", "train_loss_scale": "2", "train_train_wall": "53", "train_gb_free": "39.6", "train_wall": "5734"}
[2024-10-06 17:57:39,852][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 17:57:39,861][fairseq.trainer][INFO] - begin training epoch 385
[2024-10-06 17:57:39,862][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 17:59:50,355][fairseq_cli.train][INFO] - end of epoch 385 (average epoch stats below)
[2024-10-06 17:59:50,382][train][INFO] - {"epoch": 385, "train_loss": "0.774", "train_ntokens": "260741", "train_nsentences": "1750.04", "train_wps": "59741.5", "train_ups": "0.23", "train_wpb": "260741", "train_bsz": "1750", "train_num_updates": "18469", "train_lr": "0.000288578", "train_gnorm": "0.472", "train_loss_scale": "2", "train_train_wall": "66", "train_gb_free": "40.1", "train_wall": "5868"}
[2024-10-06 17:59:50,787][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 17:59:50,795][fairseq.trainer][INFO] - begin training epoch 386
[2024-10-06 17:59:50,796][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 18:01:24,983][fairseq_cli.train][INFO] - end of epoch 385 (average epoch stats below)
[2024-10-06 18:01:24,992][train][INFO] - {"epoch": 385, "train_loss": "0.774", "train_ntokens": "260741", "train_nsentences": "1750.04", "train_wps": "55536.8", "train_ups": "0.21", "train_wpb": "260741", "train_bsz": "1750", "train_num_updates": "18469", "train_lr": "0.000288578", "train_gnorm": "0.507", "train_loss_scale": "2", "train_train_wall": "56", "train_gb_free": "40.1", "train_wall": "5960"}
[2024-10-06 18:01:25,128][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 18:01:25,131][fairseq.trainer][INFO] - begin training epoch 386
[2024-10-06 18:01:25,132][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 18:04:01,207][fairseq_cli.train][INFO] - end of epoch 386 (average epoch stats below)
[2024-10-06 18:04:01,220][train][INFO] - {"epoch": 386, "train_loss": "0.769", "train_ntokens": "260385", "train_nsentences": "1750.04", "train_wps": "49827.4", "train_ups": "0.19", "train_wpb": "260385", "train_bsz": "1750", "train_num_updates": "18517", "train_lr": "0.000289328", "train_gnorm": "0.637", "train_loss_scale": "2", "train_train_wall": "65", "train_gb_free": "39.2", "train_wall": "6119"}
[2024-10-06 18:04:01,932][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 18:04:02,008][fairseq.trainer][INFO] - begin training epoch 387
[2024-10-06 18:04:02,009][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 18:05:19,778][fairseq_cli.train][INFO] - end of epoch 386 (average epoch stats below)
[2024-10-06 18:05:19,781][train][INFO] - {"epoch": 386, "train_loss": "0.768", "train_ntokens": "260385", "train_nsentences": "1750.04", "train_wps": "53233.3", "train_ups": "0.2", "train_wpb": "260385", "train_bsz": "1750", "train_num_updates": "18517", "train_lr": "0.000289328", "train_gnorm": "0.487", "train_loss_scale": "2", "train_train_wall": "58", "train_gb_free": "39.2", "train_wall": "6195"}
[2024-10-06 18:05:19,839][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 18:05:19,843][fairseq.trainer][INFO] - begin training epoch 387
[2024-10-06 18:05:19,843][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 18:07:34,808][fairseq_cli.train][INFO] - end of epoch 387 (average epoch stats below)
[2024-10-06 18:07:34,821][train][INFO] - {"epoch": 387, "train_loss": "0.768", "train_ntokens": "260786", "train_nsentences": "1750.04", "train_wps": "58605.7", "train_ups": "0.22", "train_wpb": "260786", "train_bsz": "1750", "train_num_updates": "18565", "train_lr": "0.000290078", "train_gnorm": "0.54", "train_loss_scale": "2", "train_train_wall": "60", "train_gb_free": "40.3", "train_wall": "6333"}
[2024-10-06 18:07:35,037][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 18:07:35,043][fairseq.trainer][INFO] - begin training epoch 388
[2024-10-06 18:07:35,044][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 18:09:10,090][fairseq_cli.train][INFO] - end of epoch 387 (average epoch stats below)
[2024-10-06 18:09:10,093][train][INFO] - {"epoch": 387, "train_loss": "0.769", "train_ntokens": "260786", "train_nsentences": "1750.04", "train_wps": "54352.1", "train_ups": "0.21", "train_wpb": "260786", "train_bsz": "1750", "train_num_updates": "18565", "train_lr": "0.000290078", "train_gnorm": "0.615", "train_loss_scale": "2", "train_train_wall": "57", "train_gb_free": "40.3", "train_wall": "6425"}
[2024-10-06 18:09:10,227][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 18:09:10,240][fairseq.trainer][INFO] - begin training epoch 388
[2024-10-06 18:09:10,240][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 18:10:50,063][train_inner][INFO] - {"epoch": 388, "update": 387.729, "loss": "0.77", "ntokens": "260795", "nsentences": "1734.21", "wps": "58479.6", "ups": "0.22", "wpb": "260795", "bsz": "1734.2", "num_updates": "18600", "lr": "0.000290625", "gnorm": "0.533", "loss_scale": "2", "train_wall": "245", "gb_free": "39.8", "wall": "6528"}
[2024-10-06 18:11:10,062][fairseq_cli.train][INFO] - end of epoch 388 (average epoch stats below)
[2024-10-06 18:11:10,074][train][INFO] - {"epoch": 388, "train_loss": "0.769", "train_ntokens": "260405", "train_nsentences": "1750.04", "train_wps": "58072.1", "train_ups": "0.22", "train_wpb": "260405", "train_bsz": "1750", "train_num_updates": "18613", "train_lr": "0.000290828", "train_gnorm": "0.517", "train_loss_scale": "2", "train_train_wall": "51", "train_gb_free": "39.6", "train_wall": "6548"}
[2024-10-06 18:11:10,371][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 18:11:10,377][fairseq.trainer][INFO] - begin training epoch 389
[2024-10-06 18:11:10,378][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 18:12:08,535][train_inner][INFO] - {"epoch": 388, "update": 387.729, "loss": "0.77", "ntokens": "260795", "nsentences": "1734.21", "wps": "58653.8", "ups": "0.22", "wpb": "260795", "bsz": "1734.2", "num_updates": "18600", "lr": "0.000290625", "gnorm": "0.54", "loss_scale": "2", "train_wall": "226", "gb_free": "39.8", "wall": "6603"}
[2024-10-06 18:12:23,472][fairseq_cli.train][INFO] - end of epoch 388 (average epoch stats below)
[2024-10-06 18:12:23,478][train][INFO] - {"epoch": 388, "train_loss": "0.769", "train_ntokens": "260405", "train_nsentences": "1750.04", "train_wps": "64637.6", "train_ups": "0.25", "train_wpb": "260405", "train_bsz": "1750", "train_num_updates": "18613", "train_lr": "0.000290828", "train_gnorm": "0.561", "train_loss_scale": "2", "train_train_wall": "50", "train_gb_free": "39.6", "train_wall": "6618"}
[2024-10-06 18:12:23,634][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 18:12:23,645][fairseq.trainer][INFO] - begin training epoch 389
[2024-10-06 18:12:23,645][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 18:14:24,528][fairseq_cli.train][INFO] - end of epoch 389 (average epoch stats below)
[2024-10-06 18:14:24,604][train][INFO] - {"epoch": 389, "train_loss": "0.77", "train_ntokens": "260817", "train_nsentences": "1750.04", "train_wps": "64362.4", "train_ups": "0.25", "train_wpb": "260817", "train_bsz": "1750", "train_num_updates": "18661", "train_lr": "0.000291578", "train_gnorm": "0.587", "train_loss_scale": "2", "train_train_wall": "79", "train_gb_free": "39.7", "train_wall": "6743"}
[2024-10-06 18:14:25,339][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 18:14:25,349][fairseq.trainer][INFO] - begin training epoch 390
[2024-10-06 18:14:25,349][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 18:15:42,835][fairseq_cli.train][INFO] - end of epoch 389 (average epoch stats below)
[2024-10-06 18:15:42,851][train][INFO] - {"epoch": 389, "train_loss": "0.769", "train_ntokens": "260817", "train_nsentences": "1750.04", "train_wps": "62794.4", "train_ups": "0.24", "train_wpb": "260817", "train_bsz": "1750", "train_num_updates": "18661", "train_lr": "0.000291578", "train_gnorm": "0.451", "train_loss_scale": "2", "train_train_wall": "83", "train_gb_free": "39.7", "train_wall": "6818"}
[2024-10-06 18:15:42,939][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 18:15:42,942][fairseq.trainer][INFO] - begin training epoch 390
[2024-10-06 18:15:42,942][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 18:17:53,534][fairseq_cli.train][INFO] - end of epoch 390 (average epoch stats below)
[2024-10-06 18:17:53,547][train][INFO] - {"epoch": 390, "train_loss": "0.764", "train_ntokens": "260823", "train_nsentences": "1750.04", "train_wps": "59921.8", "train_ups": "0.23", "train_wpb": "260823", "train_bsz": "1750", "train_num_updates": "18709", "train_lr": "0.000292328", "train_gnorm": "0.601", "train_loss_scale": "2", "train_train_wall": "91", "train_gb_free": "39.6", "train_wall": "6951"}
[2024-10-06 18:17:53,695][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 18:17:53,698][fairseq.trainer][INFO] - begin training epoch 391
[2024-10-06 18:17:53,699][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 18:19:00,319][fairseq_cli.train][INFO] - end of epoch 390 (average epoch stats below)
[2024-10-06 18:19:00,322][train][INFO] - {"epoch": 390, "train_loss": "0.764", "train_ntokens": "260823", "train_nsentences": "1750.04", "train_wps": "63399.9", "train_ups": "0.24", "train_wpb": "260823", "train_bsz": "1750", "train_num_updates": "18709", "train_lr": "0.000292328", "train_gnorm": "0.602", "train_loss_scale": "2", "train_train_wall": "82", "train_gb_free": "39.6", "train_wall": "7015"}
[2024-10-06 18:19:00,611][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 18:19:00,636][fairseq.trainer][INFO] - begin training epoch 391
[2024-10-06 18:19:00,636][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 18:21:21,421][fairseq_cli.train][INFO] - end of epoch 391 (average epoch stats below)
[2024-10-06 18:21:21,449][train][INFO] - {"epoch": 391, "train_loss": "0.769", "train_ntokens": "260852", "train_nsentences": "1750.04", "train_wps": "60225.9", "train_ups": "0.23", "train_wpb": "260852", "train_bsz": "1750", "train_num_updates": "18757", "train_lr": "0.000293078", "train_gnorm": "0.505", "train_loss_scale": "2", "train_train_wall": "65", "train_gb_free": "39.6", "train_wall": "7159"}
[2024-10-06 18:21:21,965][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 18:21:22,031][fairseq.trainer][INFO] - begin training epoch 392
[2024-10-06 18:21:22,032][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 18:22:41,429][fairseq_cli.train][INFO] - end of epoch 391 (average epoch stats below)
[2024-10-06 18:22:41,434][train][INFO] - {"epoch": 391, "train_loss": "0.769", "train_ntokens": "260852", "train_nsentences": "1750.04", "train_wps": "56627.7", "train_ups": "0.22", "train_wpb": "260852", "train_bsz": "1750", "train_num_updates": "18757", "train_lr": "0.000293078", "train_gnorm": "0.513", "train_loss_scale": "2", "train_train_wall": "48", "train_gb_free": "39.6", "train_wall": "7236"}
[2024-10-06 18:22:41,626][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 18:22:41,631][fairseq.trainer][INFO] - begin training epoch 392
[2024-10-06 18:22:41,632][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 18:24:40,207][train_inner][INFO] - {"epoch": 392, "update": 391.896, "loss": "0.768", "ntokens": "260765", "nsentences": "1752.44", "wps": "62825.8", "ups": "0.24", "wpb": "260765", "bsz": "1752.4", "num_updates": "18800", "lr": "0.00029375", "gnorm": "0.551", "loss_scale": "2", "train_wall": "331", "gb_free": "39.3", "wall": "7358"}
[2024-10-06 18:24:41,801][fairseq_cli.train][INFO] - end of epoch 392 (average epoch stats below)
[2024-10-06 18:24:41,805][train][INFO] - {"epoch": 392, "train_loss": "0.767", "train_ntokens": "260659", "train_nsentences": "1750.04", "train_wps": "62448.4", "train_ups": "0.24", "train_wpb": "260659", "train_bsz": "1750", "train_num_updates": "18805", "train_lr": "0.000293828", "train_gnorm": "0.521", "train_loss_scale": "2", "train_train_wall": "79", "train_gb_free": "40.1", "train_wall": "7360"}
[2024-10-06 18:24:42,081][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 18:24:42,105][fairseq.trainer][INFO] - begin training epoch 393
[2024-10-06 18:24:42,106][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 18:26:35,315][train_inner][INFO] - {"epoch": 392, "update": 391.896, "loss": "0.768", "ntokens": "260765", "nsentences": "1752.44", "wps": "60169.2", "ups": "0.23", "wpb": "260765", "bsz": "1752.4", "num_updates": "18800", "lr": "0.00029375", "gnorm": "0.52", "loss_scale": "2", "train_wall": "319", "gb_free": "39.3", "wall": "7470"}
[2024-10-06 18:26:37,976][fairseq_cli.train][INFO] - end of epoch 392 (average epoch stats below)
[2024-10-06 18:26:37,984][train][INFO] - {"epoch": 392, "train_loss": "0.768", "train_ntokens": "260659", "train_nsentences": "1750.04", "train_wps": "52894.2", "train_ups": "0.2", "train_wpb": "260659", "train_bsz": "1750", "train_num_updates": "18805", "train_lr": "0.000293828", "train_gnorm": "0.525", "train_loss_scale": "2", "train_train_wall": "93", "train_gb_free": "40.1", "train_wall": "7473"}
[2024-10-06 18:26:38,322][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 18:26:38,334][fairseq.trainer][INFO] - begin training epoch 393
[2024-10-06 18:26:38,335][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 18:29:30,648][fairseq_cli.train][INFO] - end of epoch 393 (average epoch stats below)
[2024-10-06 18:29:30,672][train][INFO] - {"epoch": 393, "train_loss": "0.76", "train_ntokens": "260350", "train_nsentences": "1750.04", "train_wps": "43262.1", "train_ups": "0.17", "train_wpb": "260350", "train_bsz": "1750", "train_num_updates": "18853", "train_lr": "0.000294578", "train_gnorm": "0.489", "train_loss_scale": "2", "train_train_wall": "48", "train_gb_free": "39.6", "train_wall": "7649"}
[2024-10-06 18:29:31,182][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 18:29:31,197][fairseq.trainer][INFO] - begin training epoch 394
[2024-10-06 18:29:31,197][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 18:30:32,923][fairseq_cli.train][INFO] - end of epoch 393 (average epoch stats below)
[2024-10-06 18:30:32,925][train][INFO] - {"epoch": 393, "train_loss": "0.76", "train_ntokens": "260350", "train_nsentences": "1750.04", "train_wps": "53191.8", "train_ups": "0.2", "train_wpb": "260350", "train_bsz": "1750", "train_num_updates": "18853", "train_lr": "0.000294578", "train_gnorm": "0.509", "train_loss_scale": "2", "train_train_wall": "33", "train_gb_free": "39.6", "train_wall": "7708"}
[2024-10-06 18:30:33,125][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 18:30:33,135][fairseq.trainer][INFO] - begin training epoch 394
[2024-10-06 18:30:33,136][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 18:33:15,265][fairseq_cli.train][INFO] - end of epoch 394 (average epoch stats below)
[2024-10-06 18:33:15,484][train][INFO] - {"epoch": 394, "train_loss": "0.764", "train_ntokens": "260878", "train_nsentences": "1750.04", "train_wps": "55703.2", "train_ups": "0.21", "train_wpb": "260878", "train_bsz": "1750", "train_num_updates": "18901", "train_lr": "0.000295328", "train_gnorm": "0.517", "train_loss_scale": "2", "train_train_wall": "62", "train_gb_free": "39.6", "train_wall": "7873"}
[2024-10-06 18:33:19,436][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 18:33:19,839][fairseq.trainer][INFO] - begin training epoch 395
[2024-10-06 18:33:19,840][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 18:35:04,102][fairseq_cli.train][INFO] - end of epoch 394 (average epoch stats below)
[2024-10-06 18:35:04,105][train][INFO] - {"epoch": 394, "train_loss": "0.764", "train_ntokens": "260878", "train_nsentences": "1750.04", "train_wps": "46177", "train_ups": "0.18", "train_wpb": "260878", "train_bsz": "1750", "train_num_updates": "18901", "train_lr": "0.000295328", "train_gnorm": "0.543", "train_loss_scale": "2", "train_train_wall": "109", "train_gb_free": "39.6", "train_wall": "7979"}
[2024-10-06 18:35:04,370][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 18:35:04,385][fairseq.trainer][INFO] - begin training epoch 395
[2024-10-06 18:35:04,385][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 18:39:33,343][fairseq_cli.train][INFO] - end of epoch 395 (average epoch stats below)
[2024-10-06 18:39:33,349][train][INFO] - {"epoch": 395, "train_loss": "0.769", "train_ntokens": "260627", "train_nsentences": "1750.04", "train_wps": "33107.6", "train_ups": "0.13", "train_wpb": "260628", "train_bsz": "1750", "train_num_updates": "18949", "train_lr": "0.000296078", "train_gnorm": "0.515", "train_loss_scale": "2", "train_train_wall": "70", "train_gb_free": "40.1", "train_wall": "8251"}
[2024-10-06 18:39:34,127][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 18:39:34,131][fairseq.trainer][INFO] - begin training epoch 396
[2024-10-06 18:39:34,131][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 18:43:23,497][fairseq_cli.train][INFO] - end of epoch 395 (average epoch stats below)
[2024-10-06 18:43:23,508][train][INFO] - {"epoch": 395, "train_loss": "0.768", "train_ntokens": "260627", "train_nsentences": "1750.04", "train_wps": "25050.3", "train_ups": "0.1", "train_wpb": "260628", "train_bsz": "1750", "train_num_updates": "18949", "train_lr": "0.000296078", "train_gnorm": "0.536", "train_loss_scale": "2", "train_train_wall": "59", "train_gb_free": "40.1", "train_wall": "8478"}
[2024-10-06 18:43:23,958][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 18:43:23,965][fairseq.trainer][INFO] - begin training epoch 396
[2024-10-06 18:43:23,965][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 18:46:27,731][fairseq_cli.train][INFO] - end of epoch 396 (average epoch stats below)
[2024-10-06 18:46:27,751][train][INFO] - {"epoch": 396, "train_loss": "0.766", "train_ntokens": "260896", "train_nsentences": "1750.04", "train_wps": "30220.2", "train_ups": "0.12", "train_wpb": "260896", "train_bsz": "1750", "train_num_updates": "18997", "train_lr": "0.000296828", "train_gnorm": "0.552", "train_loss_scale": "2", "train_train_wall": "74", "train_gb_free": "39.8", "train_wall": "8666"}
[2024-10-06 18:46:27,975][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 18:46:27,979][fairseq.trainer][INFO] - begin training epoch 397
[2024-10-06 18:46:27,980][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 18:48:45,642][fairseq_cli.train][INFO] - end of epoch 396 (average epoch stats below)
[2024-10-06 18:48:45,660][train][INFO] - {"epoch": 396, "train_loss": "0.766", "train_ntokens": "260896", "train_nsentences": "1750.04", "train_wps": "38873.5", "train_ups": "0.15", "train_wpb": "260896", "train_bsz": "1750", "train_num_updates": "18997", "train_lr": "0.000296828", "train_gnorm": "0.558", "train_loss_scale": "2", "train_train_wall": "49", "train_gb_free": "39.8", "train_wall": "8800"}
[2024-10-06 18:48:45,910][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 18:48:45,920][fairseq.trainer][INFO] - begin training epoch 397
[2024-10-06 18:48:45,920][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 18:49:52,613][train_inner][INFO] - {"epoch": 397, "update": 396.062, "loss": "0.765", "ntokens": "260703", "nsentences": "1752.13", "wps": "34475.6", "ups": "0.13", "wpb": "260703", "bsz": "1752.1", "num_updates": "19000", "lr": "0.000296875", "gnorm": "0.519", "loss_scale": "2", "train_wall": "282", "gb_free": "40.2", "wall": "8871"}
[2024-10-06 18:50:32,544][fairseq_cli.train][INFO] - end of epoch 397 (average epoch stats below)
[2024-10-06 18:50:32,546][train][INFO] - {"epoch": 397, "train_loss": "0.762", "train_ntokens": "260687", "train_nsentences": "1750.04", "train_wps": "51116.6", "train_ups": "0.2", "train_wpb": "260686", "train_bsz": "1750", "train_num_updates": "19045", "train_lr": "0.000297578", "train_gnorm": "0.513", "train_loss_scale": "2", "train_train_wall": "66", "train_gb_free": "39.7", "train_wall": "8910"}
[2024-10-06 18:50:33,064][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 18:50:33,070][fairseq.trainer][INFO] - begin training epoch 398
[2024-10-06 18:50:33,074][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 18:53:30,759][train_inner][INFO] - {"epoch": 397, "update": 396.062, "loss": "0.765", "ntokens": "260703", "nsentences": "1752.13", "wps": "32276.7", "ups": "0.12", "wpb": "260703", "bsz": "1752.1", "num_updates": "19000", "lr": "0.000296875", "gnorm": "0.538", "loss_scale": "2", "train_wall": "313", "gb_free": "40.2", "wall": "9086"}
[2024-10-06 18:54:25,656][fairseq_cli.train][INFO] - end of epoch 397 (average epoch stats below)
[2024-10-06 18:54:25,659][train][INFO] - {"epoch": 397, "train_loss": "0.762", "train_ntokens": "260687", "train_nsentences": "1750.04", "train_wps": "36803.2", "train_ups": "0.14", "train_wpb": "260686", "train_bsz": "1750", "train_num_updates": "19045", "train_lr": "0.000297578", "train_gnorm": "0.498", "train_loss_scale": "2", "train_train_wall": "115", "train_gb_free": "39.7", "train_wall": "9140"}
[2024-10-06 18:54:28,001][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 18:54:28,176][fairseq.trainer][INFO] - begin training epoch 398
[2024-10-06 18:54:28,176][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 18:55:56,917][fairseq_cli.train][INFO] - end of epoch 398 (average epoch stats below)
[2024-10-06 18:55:56,924][train][INFO] - {"epoch": 398, "train_loss": "0.759", "train_ntokens": "260696", "train_nsentences": "1750.04", "train_wps": "38577", "train_ups": "0.15", "train_wpb": "260696", "train_bsz": "1750", "train_num_updates": "19093", "train_lr": "0.000298328", "train_gnorm": "0.54", "train_loss_scale": "2", "train_train_wall": "88", "train_gb_free": "39.6", "train_wall": "9235"}
[2024-10-06 18:55:57,270][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 18:55:57,274][fairseq.trainer][INFO] - begin training epoch 399
[2024-10-06 18:55:57,275][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 18:59:30,474][fairseq_cli.train][INFO] - end of epoch 398 (average epoch stats below)
[2024-10-06 18:59:30,621][train][INFO] - {"epoch": 398, "train_loss": "0.759", "train_ntokens": "260696", "train_nsentences": "1750.04", "train_wps": "41033", "train_ups": "0.16", "train_wpb": "260696", "train_bsz": "1750", "train_num_updates": "19093", "train_lr": "0.000298328", "train_gnorm": "0.518", "train_loss_scale": "2", "train_train_wall": "101", "train_gb_free": "39.6", "train_wall": "9445"}
[2024-10-06 18:59:34,401][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 18:59:34,998][fairseq.trainer][INFO] - begin training epoch 399
[2024-10-06 18:59:34,999][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 19:01:43,682][fairseq_cli.train][INFO] - end of epoch 399 (average epoch stats below)
[2024-10-06 19:01:43,687][train][INFO] - {"epoch": 399, "train_loss": "0.761", "train_ntokens": "260749", "train_nsentences": "1750.04", "train_wps": "36094", "train_ups": "0.14", "train_wpb": "260749", "train_bsz": "1750", "train_num_updates": "19141", "train_lr": "0.000299078", "train_gnorm": "0.474", "train_loss_scale": "2", "train_train_wall": "119", "train_gb_free": "39.2", "train_wall": "9582"}
[2024-10-06 19:01:43,972][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 19:01:43,987][fairseq.trainer][INFO] - begin training epoch 400
[2024-10-06 19:01:43,988][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 19:04:51,699][fairseq_cli.train][INFO] - end of epoch 399 (average epoch stats below)
[2024-10-06 19:04:51,702][train][INFO] - {"epoch": 399, "train_loss": "0.761", "train_ntokens": "260749", "train_nsentences": "1750.04", "train_wps": "38980.9", "train_ups": "0.15", "train_wpb": "260749", "train_bsz": "1750", "train_num_updates": "19141", "train_lr": "0.000299078", "train_gnorm": "0.542", "train_loss_scale": "2", "train_train_wall": "95", "train_gb_free": "39.2", "train_wall": "9767"}
[2024-10-06 19:04:51,975][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 19:04:52,009][fairseq.trainer][INFO] - begin training epoch 400
[2024-10-06 19:04:52,009][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 19:06:54,510][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 400 @ 19189 updates
[2024-10-06 19:06:54,513][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-06 19:06:59,200][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-06 19:06:59,203][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 400 @ 19189 updates, score None) (writing took 4.6937315473333 seconds)
[2024-10-06 19:06:59,204][fairseq_cli.train][INFO] - end of epoch 400 (average epoch stats below)
[2024-10-06 19:06:59,207][train][INFO] - {"epoch": 400, "train_loss": "0.759", "train_ntokens": "260483", "train_nsentences": "1750.04", "train_wps": "39627.8", "train_ups": "0.15", "train_wpb": "260483", "train_bsz": "1750", "train_num_updates": "19189", "train_lr": "0.000299828", "train_gnorm": "0.704", "train_loss_scale": "2", "train_train_wall": "74", "train_gb_free": "40.5", "train_wall": "9897"}
[2024-10-06 19:06:59,434][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 19:06:59,460][fairseq.trainer][INFO] - begin training epoch 401
[2024-10-06 19:06:59,461][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 19:10:00,988][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 400 @ 19189 updates
[2024-10-06 19:10:00,990][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-06 19:10:07,665][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-06 19:10:07,667][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 400 @ 19189 updates, score None) (writing took 6.679025925695896 seconds)
[2024-10-06 19:10:07,667][fairseq_cli.train][INFO] - end of epoch 400 (average epoch stats below)
[2024-10-06 19:10:07,669][train][INFO] - {"epoch": 400, "train_loss": "0.757", "train_ntokens": "260483", "train_nsentences": "1750.04", "train_wps": "39571.4", "train_ups": "0.15", "train_wpb": "260483", "train_bsz": "1750", "train_num_updates": "19189", "train_lr": "0.000299828", "train_gnorm": "0.583", "train_loss_scale": "2", "train_train_wall": "71", "train_gb_free": "40.5", "train_wall": "10082"}
[2024-10-06 19:10:09,230][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 19:10:09,285][fairseq.trainer][INFO] - begin training epoch 401
[2024-10-06 19:10:09,286][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 19:11:14,606][train_inner][INFO] - {"epoch": 401, "update": 400.229, "loss": "0.759", "ntokens": "260738", "nsentences": "1747.27", "wps": "40677.8", "ups": "0.16", "wpb": "260738", "bsz": "1747.3", "num_updates": "19200", "lr": "0.0003", "gnorm": "0.55", "loss_scale": "2", "train_wall": "345", "gb_free": "39.1", "wall": "10153"}
[2024-10-06 19:11:44,290][fairseq_cli.train][INFO] - end of epoch 401 (average epoch stats below)
[2024-10-06 19:11:44,293][train][INFO] - {"epoch": 401, "train_loss": "0.755", "train_ntokens": "260979", "train_nsentences": "1750.04", "train_wps": "43941.7", "train_ups": "0.17", "train_wpb": "260979", "train_bsz": "1750", "train_num_updates": "19237", "train_lr": "0.000300578", "train_gnorm": "0.497", "train_loss_scale": "2", "train_train_wall": "51", "train_gb_free": "39.8", "train_wall": "10182"}
[2024-10-06 19:11:44,492][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 19:11:44,513][fairseq.trainer][INFO] - begin training epoch 402
[2024-10-06 19:11:44,515][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 19:13:34,360][train_inner][INFO] - {"epoch": 401, "update": 400.229, "loss": "0.759", "ntokens": "260738", "nsentences": "1747.27", "wps": "43326.4", "ups": "0.17", "wpb": "260738", "bsz": "1747.3", "num_updates": "19200", "lr": "0.0003", "gnorm": "0.528", "loss_scale": "2", "train_wall": "342", "gb_free": "39.1", "wall": "10289"}
[2024-10-06 19:14:01,208][fairseq_cli.train][INFO] - end of epoch 401 (average epoch stats below)
[2024-10-06 19:14:01,211][train][INFO] - {"epoch": 401, "train_loss": "0.755", "train_ntokens": "260979", "train_nsentences": "1750.04", "train_wps": "53639.8", "train_ups": "0.21", "train_wpb": "260979", "train_bsz": "1750", "train_num_updates": "19237", "train_lr": "0.000300578", "train_gnorm": "0.508", "train_loss_scale": "2", "train_train_wall": "48", "train_gb_free": "39.8", "train_wall": "10316"}
[2024-10-06 19:14:01,683][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 19:14:01,779][fairseq.trainer][INFO] - begin training epoch 402
[2024-10-06 19:14:01,779][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 19:15:52,425][fairseq_cli.train][INFO] - end of epoch 402 (average epoch stats below)
[2024-10-06 19:15:52,431][train][INFO] - {"epoch": 402, "train_loss": "0.759", "train_ntokens": "260859", "train_nsentences": "1750.04", "train_wps": "50461.6", "train_ups": "0.19", "train_wpb": "260859", "train_bsz": "1750", "train_num_updates": "19285", "train_lr": "0.000301328", "train_gnorm": "0.507", "train_loss_scale": "2", "train_train_wall": "45", "train_gb_free": "39.8", "train_wall": "10430"}
[2024-10-06 19:15:52,606][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 19:15:52,610][fairseq.trainer][INFO] - begin training epoch 403
[2024-10-06 19:15:52,611][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 19:18:51,391][fairseq_cli.train][INFO] - end of epoch 402 (average epoch stats below)
[2024-10-06 19:18:51,421][train][INFO] - {"epoch": 402, "train_loss": "0.759", "train_ntokens": "260859", "train_nsentences": "1750.04", "train_wps": "43145.8", "train_ups": "0.17", "train_wpb": "260859", "train_bsz": "1750", "train_num_updates": "19285", "train_lr": "0.000301328", "train_gnorm": "0.504", "train_loss_scale": "2", "train_train_wall": "50", "train_gb_free": "39.8", "train_wall": "10606"}
[2024-10-06 19:18:52,274][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 19:18:52,290][fairseq.trainer][INFO] - begin training epoch 403
[2024-10-06 19:18:52,290][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 19:20:18,106][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2024-10-06 19:20:20,520][fairseq_cli.train][INFO] - end of epoch 403 (average epoch stats below)
[2024-10-06 19:20:20,525][train][INFO] - {"epoch": 403, "train_loss": "0.759", "train_ntokens": "260895", "train_nsentences": "1745.36", "train_wps": "45738.9", "train_ups": "0.18", "train_wpb": "260895", "train_bsz": "1745.4", "train_num_updates": "19332", "train_lr": "0.000302063", "train_gnorm": "0.577", "train_loss_scale": "2", "train_train_wall": "73", "train_gb_free": "39.6", "train_wall": "10698"}
[2024-10-06 19:20:20,743][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 19:20:20,750][fairseq.trainer][INFO] - begin training epoch 404
[2024-10-06 19:20:20,751][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 19:23:14,075][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2024-10-06 19:23:18,127][fairseq_cli.train][INFO] - end of epoch 403 (average epoch stats below)
[2024-10-06 19:23:18,140][train][INFO] - {"epoch": 403, "train_loss": "0.76", "train_ntokens": "260932", "train_nsentences": "1741.28", "train_wps": "45980.8", "train_ups": "0.18", "train_wpb": "260932", "train_bsz": "1741.3", "train_num_updates": "19332", "train_lr": "0.000302063", "train_gnorm": "0.669", "train_loss_scale": "2", "train_train_wall": "95", "train_gb_free": "39.6", "train_wall": "10873"}
[2024-10-06 19:23:19,820][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 19:23:19,953][fairseq.trainer][INFO] - begin training epoch 404
[2024-10-06 19:23:19,953][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 19:24:58,145][fairseq_cli.train][INFO] - end of epoch 404 (average epoch stats below)
[2024-10-06 19:24:58,152][train][INFO] - {"epoch": 404, "train_loss": "0.757", "train_ntokens": "260614", "train_nsentences": "1750.04", "train_wps": "45059.3", "train_ups": "0.17", "train_wpb": "260614", "train_bsz": "1750", "train_num_updates": "19380", "train_lr": "0.000302812", "train_gnorm": "0.528", "train_loss_scale": "2", "train_train_wall": "69", "train_gb_free": "40.3", "train_wall": "10976"}
[2024-10-06 19:24:58,337][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 19:24:58,345][fairseq.trainer][INFO] - begin training epoch 405
[2024-10-06 19:24:58,345][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 19:28:07,046][fairseq_cli.train][INFO] - end of epoch 404 (average epoch stats below)
[2024-10-06 19:28:07,146][train][INFO] - {"epoch": 404, "train_loss": "0.756", "train_ntokens": "260614", "train_nsentences": "1750.04", "train_wps": "43284.9", "train_ups": "0.17", "train_wpb": "260614", "train_bsz": "1750", "train_num_updates": "19380", "train_lr": "0.000302812", "train_gnorm": "0.531", "train_loss_scale": "2", "train_train_wall": "93", "train_gb_free": "40.3", "train_wall": "11162"}
[2024-10-06 19:28:10,052][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 19:28:10,191][fairseq.trainer][INFO] - begin training epoch 405
[2024-10-06 19:28:10,191][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 19:29:51,134][train_inner][INFO] - {"epoch": 405, "update": 404.417, "loss": "0.758", "ntokens": "260789", "nsentences": "1752.78", "wps": "46714.9", "ups": "0.18", "wpb": "260788", "bsz": "1752.8", "num_updates": "19400", "lr": "0.000303125", "gnorm": "0.529", "loss_scale": "2", "train_wall": "263", "gb_free": "39.3", "wall": "11269"}
[2024-10-06 19:30:12,021][fairseq_cli.train][INFO] - end of epoch 405 (average epoch stats below)
[2024-10-06 19:30:12,035][train][INFO] - {"epoch": 405, "train_loss": "0.754", "train_ntokens": "260720", "train_nsentences": "1750.04", "train_wps": "39871.9", "train_ups": "0.15", "train_wpb": "260720", "train_bsz": "1750", "train_num_updates": "19428", "train_lr": "0.000303563", "train_gnorm": "0.48", "train_loss_scale": "2", "train_train_wall": "69", "train_gb_free": "39.6", "train_wall": "11290"}
[2024-10-06 19:30:12,303][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 19:30:12,339][fairseq.trainer][INFO] - begin training epoch 406
[2024-10-06 19:30:12,339][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 19:32:08,543][train_inner][INFO] - {"epoch": 405, "update": 404.417, "loss": "0.758", "ntokens": "260797", "nsentences": "1751.82", "wps": "46814.8", "ups": "0.18", "wpb": "260797", "bsz": "1751.8", "num_updates": "19400", "lr": "0.000303125", "gnorm": "0.554", "loss_scale": "2", "train_wall": "301", "gb_free": "39.3", "wall": "11403"}
[2024-10-06 19:32:30,784][fairseq_cli.train][INFO] - end of epoch 405 (average epoch stats below)
[2024-10-06 19:32:30,788][train][INFO] - {"epoch": 405, "train_loss": "0.754", "train_ntokens": "260720", "train_nsentences": "1750.04", "train_wps": "47468.8", "train_ups": "0.18", "train_wpb": "260720", "train_bsz": "1750", "train_num_updates": "19428", "train_lr": "0.000303563", "train_gnorm": "0.486", "train_loss_scale": "2", "train_train_wall": "58", "train_gb_free": "39.6", "train_wall": "11426"}
[2024-10-06 19:32:31,067][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 19:32:31,072][fairseq.trainer][INFO] - begin training epoch 406
[2024-10-06 19:32:31,073][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 19:35:36,160][fairseq_cli.train][INFO] - end of epoch 406 (average epoch stats below)
[2024-10-06 19:35:36,180][train][INFO] - {"epoch": 406, "train_loss": "0.756", "train_ntokens": "260713", "train_nsentences": "1750.04", "train_wps": "38607.2", "train_ups": "0.15", "train_wpb": "260713", "train_bsz": "1750", "train_num_updates": "19476", "train_lr": "0.000304313", "train_gnorm": "0.582", "train_loss_scale": "2", "train_train_wall": "157", "train_gb_free": "40", "train_wall": "11614"}
[2024-10-06 19:35:36,660][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 19:35:36,712][fairseq.trainer][INFO] - begin training epoch 407
[2024-10-06 19:35:36,712][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 19:39:35,631][fairseq_cli.train][INFO] - end of epoch 406 (average epoch stats below)
[2024-10-06 19:39:35,641][train][INFO] - {"epoch": 406, "train_loss": "0.756", "train_ntokens": "260713", "train_nsentences": "1750.04", "train_wps": "29455.6", "train_ups": "0.11", "train_wpb": "260713", "train_bsz": "1750", "train_num_updates": "19476", "train_lr": "0.000304313", "train_gnorm": "0.543", "train_loss_scale": "2", "train_train_wall": "158", "train_gb_free": "40", "train_wall": "11850"}
[2024-10-06 19:39:36,231][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 19:39:36,277][fairseq.trainer][INFO] - begin training epoch 407
[2024-10-06 19:39:36,277][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 19:43:33,209][fairseq_cli.train][INFO] - end of epoch 407 (average epoch stats below)
[2024-10-06 19:43:33,213][train][INFO] - {"epoch": 407, "train_loss": "0.758", "train_ntokens": "260596", "train_nsentences": "1750.04", "train_wps": "26221.9", "train_ups": "0.1", "train_wpb": "260596", "train_bsz": "1750", "train_num_updates": "19524", "train_lr": "0.000305062", "train_gnorm": "0.486", "train_loss_scale": "2", "train_train_wall": "191", "train_gb_free": "40.5", "train_wall": "12091"}
[2024-10-06 19:43:33,481][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 19:43:33,512][fairseq.trainer][INFO] - begin training epoch 408
[2024-10-06 19:43:33,513][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 19:47:12,621][fairseq_cli.train][INFO] - end of epoch 407 (average epoch stats below)
[2024-10-06 19:47:12,630][train][INFO] - {"epoch": 407, "train_loss": "0.758", "train_ntokens": "260596", "train_nsentences": "1750.04", "train_wps": "27371.9", "train_ups": "0.11", "train_wpb": "260596", "train_bsz": "1750", "train_num_updates": "19524", "train_lr": "0.000305062", "train_gnorm": "0.487", "train_loss_scale": "2", "train_train_wall": "155", "train_gb_free": "40.5", "train_wall": "12307"}
[2024-10-06 19:47:13,248][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 19:47:13,254][fairseq.trainer][INFO] - begin training epoch 408
[2024-10-06 19:47:13,254][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 19:49:57,675][fairseq_cli.train][INFO] - end of epoch 408 (average epoch stats below)
[2024-10-06 19:49:57,688][train][INFO] - {"epoch": 408, "train_loss": "0.756", "train_ntokens": "260670", "train_nsentences": "1750.04", "train_wps": "32543.9", "train_ups": "0.12", "train_wpb": "260670", "train_bsz": "1750", "train_num_updates": "19572", "train_lr": "0.000305812", "train_gnorm": "0.59", "train_loss_scale": "2", "train_train_wall": "112", "train_gb_free": "39.8", "train_wall": "12476"}
[2024-10-06 19:49:58,003][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 19:49:58,006][fairseq.trainer][INFO] - begin training epoch 409
[2024-10-06 19:49:58,007][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 19:52:46,725][fairseq_cli.train][INFO] - end of epoch 408 (average epoch stats below)
[2024-10-06 19:52:46,737][train][INFO] - {"epoch": 408, "train_loss": "0.756", "train_ntokens": "260670", "train_nsentences": "1750.04", "train_wps": "37450", "train_ups": "0.14", "train_wpb": "260670", "train_bsz": "1750", "train_num_updates": "19572", "train_lr": "0.000305812", "train_gnorm": "0.572", "train_loss_scale": "2", "train_train_wall": "97", "train_gb_free": "39.8", "train_wall": "12642"}
[2024-10-06 19:52:47,099][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 19:52:47,120][fairseq.trainer][INFO] - begin training epoch 409
[2024-10-06 19:52:47,121][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 19:55:08,586][train_inner][INFO] - {"epoch": 409, "update": 408.583, "loss": "0.754", "ntokens": "260508", "nsentences": "1746.46", "wps": "34335.2", "ups": "0.13", "wpb": "260508", "bsz": "1746.5", "num_updates": "19600", "lr": "0.00030625", "gnorm": "0.536", "loss_scale": "2", "train_wall": "601", "gb_free": "40.3", "wall": "12787"}
[2024-10-06 19:55:32,461][fairseq_cli.train][INFO] - end of epoch 409 (average epoch stats below)
[2024-10-06 19:55:32,465][train][INFO] - {"epoch": 409, "train_loss": "0.747", "train_ntokens": "260344", "train_nsentences": "1750.04", "train_wps": "37328.5", "train_ups": "0.14", "train_wpb": "260344", "train_bsz": "1750", "train_num_updates": "19620", "train_lr": "0.000306563", "train_gnorm": "0.514", "train_loss_scale": "2", "train_train_wall": "145", "train_gb_free": "40.3", "train_wall": "12810"}
[2024-10-06 19:55:32,659][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 19:55:32,664][fairseq.trainer][INFO] - begin training epoch 410
[2024-10-06 19:55:32,665][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 19:57:28,655][train_inner][INFO] - {"epoch": 409, "update": 408.583, "loss": "0.754", "ntokens": "260508", "nsentences": "1746.46", "wps": "34275.1", "ups": "0.13", "wpb": "260508", "bsz": "1746.5", "num_updates": "19600", "lr": "0.00030625", "gnorm": "0.523", "loss_scale": "2", "train_wall": "514", "gb_free": "40.3", "wall": "12923"}
[2024-10-06 19:57:47,960][fairseq_cli.train][INFO] - end of epoch 409 (average epoch stats below)
[2024-10-06 19:57:47,962][train][INFO] - {"epoch": 409, "train_loss": "0.747", "train_ntokens": "260344", "train_nsentences": "1750.04", "train_wps": "41485.9", "train_ups": "0.16", "train_wpb": "260344", "train_bsz": "1750", "train_num_updates": "19620", "train_lr": "0.000306563", "train_gnorm": "0.502", "train_loss_scale": "2", "train_train_wall": "100", "train_gb_free": "40.3", "train_wall": "12943"}
[2024-10-06 19:57:48,287][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 19:57:48,325][fairseq.trainer][INFO] - begin training epoch 410
[2024-10-06 19:57:48,326][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 20:00:12,718][fairseq_cli.train][INFO] - end of epoch 410 (average epoch stats below)
[2024-10-06 20:00:12,730][train][INFO] - {"epoch": 410, "train_loss": "0.75", "train_ntokens": "260731", "train_nsentences": "1750.04", "train_wps": "44655.4", "train_ups": "0.17", "train_wpb": "260731", "train_bsz": "1750", "train_num_updates": "19668", "train_lr": "0.000307312", "train_gnorm": "0.602", "train_loss_scale": "2", "train_train_wall": "70", "train_gb_free": "39.8", "train_wall": "13091"}
[2024-10-06 20:00:12,899][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 20:00:12,906][fairseq.trainer][INFO] - begin training epoch 411
[2024-10-06 20:00:12,906][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 20:02:16,562][fairseq_cli.train][INFO] - end of epoch 410 (average epoch stats below)
[2024-10-06 20:02:16,574][train][INFO] - {"epoch": 410, "train_loss": "0.749", "train_ntokens": "260731", "train_nsentences": "1750.04", "train_wps": "46592.2", "train_ups": "0.18", "train_wpb": "260731", "train_bsz": "1750", "train_num_updates": "19668", "train_lr": "0.000307312", "train_gnorm": "0.537", "train_loss_scale": "2", "train_train_wall": "60", "train_gb_free": "39.8", "train_wall": "13211"}
[2024-10-06 20:02:16,724][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 20:02:16,728][fairseq.trainer][INFO] - begin training epoch 411
[2024-10-06 20:02:16,729][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 20:04:20,259][fairseq_cli.train][INFO] - end of epoch 411 (average epoch stats below)
[2024-10-06 20:04:20,272][train][INFO] - {"epoch": 411, "train_loss": "0.747", "train_ntokens": "261164", "train_nsentences": "1750.04", "train_wps": "50642.2", "train_ups": "0.19", "train_wpb": "261164", "train_bsz": "1750", "train_num_updates": "19716", "train_lr": "0.000308063", "train_gnorm": "0.459", "train_loss_scale": "2", "train_train_wall": "87", "train_gb_free": "39.6", "train_wall": "13338"}
[2024-10-06 20:04:20,479][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 20:04:20,492][fairseq.trainer][INFO] - begin training epoch 412
[2024-10-06 20:04:20,493][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 20:06:21,586][fairseq_cli.train][INFO] - end of epoch 411 (average epoch stats below)
[2024-10-06 20:06:21,592][train][INFO] - {"epoch": 411, "train_loss": "0.748", "train_ntokens": "261164", "train_nsentences": "1750.04", "train_wps": "51164", "train_ups": "0.2", "train_wpb": "261164", "train_bsz": "1750", "train_num_updates": "19716", "train_lr": "0.000308063", "train_gnorm": "0.49", "train_loss_scale": "2", "train_train_wall": "68", "train_gb_free": "39.6", "train_wall": "13456"}
[2024-10-06 20:06:21,776][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 20:06:21,782][fairseq.trainer][INFO] - begin training epoch 412
[2024-10-06 20:06:21,782][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 20:07:58,038][fairseq_cli.train][INFO] - end of epoch 412 (average epoch stats below)
[2024-10-06 20:07:58,049][train][INFO] - {"epoch": 412, "train_loss": "0.754", "train_ntokens": "260750", "train_nsentences": "1750.04", "train_wps": "57472.7", "train_ups": "0.22", "train_wpb": "260750", "train_bsz": "1750", "train_num_updates": "19764", "train_lr": "0.000308813", "train_gnorm": "0.496", "train_loss_scale": "2", "train_train_wall": "58", "train_gb_free": "39.7", "train_wall": "13556"}
[2024-10-06 20:07:58,245][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 20:07:58,252][fairseq.trainer][INFO] - begin training epoch 413
[2024-10-06 20:07:58,253][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 20:09:56,582][fairseq_cli.train][INFO] - end of epoch 412 (average epoch stats below)
[2024-10-06 20:09:56,590][train][INFO] - {"epoch": 412, "train_loss": "0.754", "train_ntokens": "260750", "train_nsentences": "1750.04", "train_wps": "58215.6", "train_ups": "0.22", "train_wpb": "260750", "train_bsz": "1750", "train_num_updates": "19764", "train_lr": "0.000308813", "train_gnorm": "0.516", "train_loss_scale": "2", "train_train_wall": "43", "train_gb_free": "39.7", "train_wall": "13671"}
[2024-10-06 20:09:56,763][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 20:09:56,766][fairseq.trainer][INFO] - begin training epoch 413
[2024-10-06 20:09:56,767][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 20:11:23,269][train_inner][INFO] - {"epoch": 413, "update": 412.75, "loss": "0.75", "ntokens": "260841", "nsentences": "1761.55", "wps": "53524.9", "ups": "0.21", "wpb": "260841", "bsz": "1761.5", "num_updates": "19800", "lr": "0.000309375", "gnorm": "0.515", "loss_scale": "2", "train_wall": "302", "gb_free": "39.3", "wall": "13761"}
[2024-10-06 20:11:36,489][fairseq_cli.train][INFO] - end of epoch 413 (average epoch stats below)
[2024-10-06 20:11:36,492][train][INFO] - {"epoch": 413, "train_loss": "0.741", "train_ntokens": "261201", "train_nsentences": "1750.04", "train_wps": "57396.8", "train_ups": "0.22", "train_wpb": "261201", "train_bsz": "1750", "train_num_updates": "19812", "train_lr": "0.000309563", "train_gnorm": "0.492", "train_loss_scale": "2", "train_train_wall": "77", "train_gb_free": "39.2", "train_wall": "13774"}
[2024-10-06 20:11:36,677][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 20:11:36,681][fairseq.trainer][INFO] - begin training epoch 414
[2024-10-06 20:11:36,682][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 20:12:58,658][train_inner][INFO] - {"epoch": 413, "update": 412.75, "loss": "0.75", "ntokens": "260841", "nsentences": "1761.55", "wps": "56095.2", "ups": "0.22", "wpb": "260841", "bsz": "1761.5", "num_updates": "19800", "lr": "0.000309375", "gnorm": "0.507", "loss_scale": "2", "train_wall": "249", "gb_free": "39.3", "wall": "13853"}
[2024-10-06 20:13:12,955][fairseq_cli.train][INFO] - end of epoch 413 (average epoch stats below)
[2024-10-06 20:13:12,957][train][INFO] - {"epoch": 413, "train_loss": "0.741", "train_ntokens": "261201", "train_nsentences": "1750.04", "train_wps": "63849", "train_ups": "0.24", "train_wpb": "261201", "train_bsz": "1750", "train_num_updates": "19812", "train_lr": "0.000309563", "train_gnorm": "0.511", "train_loss_scale": "2", "train_train_wall": "74", "train_gb_free": "39.2", "train_wall": "13868"}
[2024-10-06 20:13:13,036][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 20:13:13,041][fairseq.trainer][INFO] - begin training epoch 414
[2024-10-06 20:13:13,041][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 20:14:49,980][fairseq_cli.train][INFO] - end of epoch 414 (average epoch stats below)
[2024-10-06 20:14:49,984][train][INFO] - {"epoch": 414, "train_loss": "0.75", "train_ntokens": "261044", "train_nsentences": "1750.04", "train_wps": "64758.8", "train_ups": "0.25", "train_wpb": "261044", "train_bsz": "1750", "train_num_updates": "19860", "train_lr": "0.000310312", "train_gnorm": "0.504", "train_loss_scale": "2", "train_train_wall": "73", "train_gb_free": "40.1", "train_wall": "13968"}
[2024-10-06 20:14:50,112][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 20:14:50,116][fairseq.trainer][INFO] - begin training epoch 415
[2024-10-06 20:14:50,116][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 20:16:27,442][fairseq_cli.train][INFO] - end of epoch 414 (average epoch stats below)
[2024-10-06 20:16:27,445][train][INFO] - {"epoch": 414, "train_loss": "0.75", "train_ntokens": "261044", "train_nsentences": "1750.04", "train_wps": "64427.3", "train_ups": "0.25", "train_wpb": "261044", "train_bsz": "1750", "train_num_updates": "19860", "train_lr": "0.000310312", "train_gnorm": "0.518", "train_loss_scale": "2", "train_train_wall": "80", "train_gb_free": "40.1", "train_wall": "14062"}
[2024-10-06 20:16:27,590][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 20:16:27,604][fairseq.trainer][INFO] - begin training epoch 415
[2024-10-06 20:16:27,605][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 20:18:04,205][fairseq_cli.train][INFO] - end of epoch 415 (average epoch stats below)
[2024-10-06 20:18:04,213][train][INFO] - {"epoch": 415, "train_loss": "0.748", "train_ntokens": "260646", "train_nsentences": "1750.04", "train_wps": "64414.8", "train_ups": "0.25", "train_wpb": "260646", "train_bsz": "1750", "train_num_updates": "19908", "train_lr": "0.000311063", "train_gnorm": "0.542", "train_loss_scale": "2", "train_train_wall": "48", "train_gb_free": "39.3", "train_wall": "14162"}
[2024-10-06 20:18:04,352][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 20:18:04,360][fairseq.trainer][INFO] - begin training epoch 416
[2024-10-06 20:18:04,361][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 20:19:56,243][fairseq_cli.train][INFO] - end of epoch 415 (average epoch stats below)
[2024-10-06 20:19:56,246][train][INFO] - {"epoch": 415, "train_loss": "0.748", "train_ntokens": "260646", "train_nsentences": "1750.04", "train_wps": "59918.9", "train_ups": "0.23", "train_wpb": "260646", "train_bsz": "1750", "train_num_updates": "19908", "train_lr": "0.000311063", "train_gnorm": "0.518", "train_loss_scale": "2", "train_train_wall": "47", "train_gb_free": "39.3", "train_wall": "14271"}
[2024-10-06 20:19:56,527][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 20:19:56,548][fairseq.trainer][INFO] - begin training epoch 416
[2024-10-06 20:19:56,548][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 20:22:47,781][fairseq_cli.train][INFO] - end of epoch 416 (average epoch stats below)
[2024-10-06 20:22:47,797][train][INFO] - {"epoch": 416, "train_loss": "0.747", "train_ntokens": "260838", "train_nsentences": "1750.04", "train_wps": "44151.7", "train_ups": "0.17", "train_wpb": "260838", "train_bsz": "1750", "train_num_updates": "19956", "train_lr": "0.000311813", "train_gnorm": "0.474", "train_loss_scale": "2", "train_train_wall": "121", "train_gb_free": "39.7", "train_wall": "14446"}
[2024-10-06 20:22:48,006][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 20:22:48,014][fairseq.trainer][INFO] - begin training epoch 417
[2024-10-06 20:22:48,014][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 20:25:31,397][fairseq_cli.train][INFO] - end of epoch 416 (average epoch stats below)
[2024-10-06 20:25:31,402][train][INFO] - {"epoch": 416, "train_loss": "0.748", "train_ntokens": "260838", "train_nsentences": "1750.04", "train_wps": "37356.8", "train_ups": "0.14", "train_wpb": "260838", "train_bsz": "1750", "train_num_updates": "19956", "train_lr": "0.000311813", "train_gnorm": "0.505", "train_loss_scale": "2", "train_train_wall": "126", "train_gb_free": "39.7", "train_wall": "14606"}
[2024-10-06 20:25:31,583][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 20:25:31,592][fairseq.trainer][INFO] - begin training epoch 417
[2024-10-06 20:25:31,593][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 20:28:07,577][train_inner][INFO] - {"epoch": 417, "update": 416.917, "loss": "0.746", "ntokens": "261048", "nsentences": "1737.9", "wps": "51986", "ups": "0.2", "wpb": "261048", "bsz": "1737.9", "num_updates": "20000", "lr": "0.0003125", "gnorm": "0.506", "loss_scale": "2", "train_wall": "332", "gb_free": "40.3", "wall": "14766"}
[2024-10-06 20:28:10,002][fairseq_cli.train][INFO] - end of epoch 417 (average epoch stats below)
[2024-10-06 20:28:10,009][train][INFO] - {"epoch": 417, "train_loss": "0.742", "train_ntokens": "260779", "train_nsentences": "1750.04", "train_wps": "38848.6", "train_ups": "0.15", "train_wpb": "260779", "train_bsz": "1750", "train_num_updates": "20004", "train_lr": "0.000312563", "train_gnorm": "0.525", "train_loss_scale": "2", "train_train_wall": "77", "train_gb_free": "42", "train_wall": "14768"}
[2024-10-06 20:28:10,180][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 20:28:10,197][fairseq.trainer][INFO] - begin training epoch 418
[2024-10-06 20:28:10,198][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 20:30:38,274][train_inner][INFO] - {"epoch": 417, "update": 416.917, "loss": "0.746", "ntokens": "261048", "nsentences": "1737.9", "wps": "49272.5", "ups": "0.19", "wpb": "261048", "bsz": "1737.9", "num_updates": "20000", "lr": "0.0003125", "gnorm": "0.504", "loss_scale": "2", "train_wall": "351", "gb_free": "40.3", "wall": "14913"}
[2024-10-06 20:30:39,854][fairseq_cli.train][INFO] - end of epoch 417 (average epoch stats below)
[2024-10-06 20:30:39,858][train][INFO] - {"epoch": 417, "train_loss": "0.742", "train_ntokens": "260779", "train_nsentences": "1750.04", "train_wps": "40581.3", "train_ups": "0.16", "train_wpb": "260779", "train_bsz": "1750", "train_num_updates": "20004", "train_lr": "0.000312563", "train_gnorm": "0.459", "train_loss_scale": "2", "train_train_wall": "86", "train_gb_free": "42", "train_wall": "14915"}
[2024-10-06 20:30:40,083][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 20:30:40,087][fairseq.trainer][INFO] - begin training epoch 418
[2024-10-06 20:30:40,087][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 20:33:09,865][fairseq_cli.train][INFO] - end of epoch 418 (average epoch stats below)
[2024-10-06 20:33:09,872][train][INFO] - {"epoch": 418, "train_loss": "0.745", "train_ntokens": "260709", "train_nsentences": "1750.04", "train_wps": "41733.2", "train_ups": "0.16", "train_wpb": "260709", "train_bsz": "1750", "train_num_updates": "20052", "train_lr": "0.000313313", "train_gnorm": "0.554", "train_loss_scale": "2", "train_train_wall": "94", "train_gb_free": "39.6", "train_wall": "15068"}
[2024-10-06 20:33:10,070][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 20:33:10,081][fairseq.trainer][INFO] - begin training epoch 419
[2024-10-06 20:33:10,082][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 20:35:08,929][fairseq_cli.train][INFO] - end of epoch 418 (average epoch stats below)
[2024-10-06 20:35:08,932][train][INFO] - {"epoch": 418, "train_loss": "0.745", "train_ntokens": "260709", "train_nsentences": "1750.04", "train_wps": "46508.3", "train_ups": "0.18", "train_wpb": "260709", "train_bsz": "1750", "train_num_updates": "20052", "train_lr": "0.000313313", "train_gnorm": "0.551", "train_loss_scale": "2", "train_train_wall": "96", "train_gb_free": "39.6", "train_wall": "15184"}
[2024-10-06 20:35:09,155][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 20:35:09,167][fairseq.trainer][INFO] - begin training epoch 419
[2024-10-06 20:35:09,168][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 20:37:17,433][fairseq_cli.train][INFO] - end of epoch 419 (average epoch stats below)
[2024-10-06 20:37:17,446][train][INFO] - {"epoch": 419, "train_loss": "0.74", "train_ntokens": "260841", "train_nsentences": "1750.04", "train_wps": "50572.9", "train_ups": "0.19", "train_wpb": "260841", "train_bsz": "1750", "train_num_updates": "20100", "train_lr": "0.000314063", "train_gnorm": "0.51", "train_loss_scale": "2", "train_train_wall": "50", "train_gb_free": "39.7", "train_wall": "15315"}
[2024-10-06 20:37:17,787][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 20:37:17,792][fairseq.trainer][INFO] - begin training epoch 420
[2024-10-06 20:37:17,792][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 20:39:24,379][fairseq_cli.train][INFO] - end of epoch 419 (average epoch stats below)
[2024-10-06 20:39:24,399][train][INFO] - {"epoch": 419, "train_loss": "0.74", "train_ntokens": "260841", "train_nsentences": "1750.04", "train_wps": "49012.7", "train_ups": "0.19", "train_wpb": "260841", "train_bsz": "1750", "train_num_updates": "20100", "train_lr": "0.000314063", "train_gnorm": "0.501", "train_loss_scale": "2", "train_train_wall": "51", "train_gb_free": "39.7", "train_wall": "15439"}
[2024-10-06 20:39:24,616][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 20:39:24,620][fairseq.trainer][INFO] - begin training epoch 420
[2024-10-06 20:39:24,621][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 20:41:43,039][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 420 @ 20148 updates
[2024-10-06 20:41:43,043][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-06 20:41:46,831][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-06 20:41:46,834][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 420 @ 20148 updates, score None) (writing took 3.7943808026611805 seconds)
[2024-10-06 20:41:46,834][fairseq_cli.train][INFO] - end of epoch 420 (average epoch stats below)
[2024-10-06 20:41:46,838][train][INFO] - {"epoch": 420, "train_loss": "0.741", "train_ntokens": "260376", "train_nsentences": "1750.04", "train_wps": "46394.3", "train_ups": "0.18", "train_wpb": "260376", "train_bsz": "1750", "train_num_updates": "20148", "train_lr": "0.000314812", "train_gnorm": "0.416", "train_loss_scale": "2", "train_train_wall": "90", "train_gb_free": "39.7", "train_wall": "15585"}
[2024-10-06 20:41:47,013][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 20:41:47,028][fairseq.trainer][INFO] - begin training epoch 421
[2024-10-06 20:41:47,029][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 20:43:48,962][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 420 @ 20148 updates
[2024-10-06 20:43:48,973][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-06 20:43:53,510][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-06 20:43:53,512][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 420 @ 20148 updates, score None) (writing took 4.549843999557197 seconds)
[2024-10-06 20:43:53,513][fairseq_cli.train][INFO] - end of epoch 420 (average epoch stats below)
[2024-10-06 20:43:53,515][train][INFO] - {"epoch": 420, "train_loss": "0.742", "train_ntokens": "260376", "train_nsentences": "1750.04", "train_wps": "46441.7", "train_ups": "0.18", "train_wpb": "260376", "train_bsz": "1750", "train_num_updates": "20148", "train_lr": "0.000314812", "train_gnorm": "0.433", "train_loss_scale": "2", "train_train_wall": "83", "train_gb_free": "39.7", "train_wall": "15708"}
[2024-10-06 20:43:53,674][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 20:43:53,684][fairseq.trainer][INFO] - begin training epoch 421
[2024-10-06 20:43:53,685][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 20:46:10,518][fairseq_cli.train][INFO] - end of epoch 421 (average epoch stats below)
[2024-10-06 20:46:10,540][train][INFO] - {"epoch": 421, "train_loss": "0.747", "train_ntokens": "260355", "train_nsentences": "1750.04", "train_wps": "47392.1", "train_ups": "0.18", "train_wpb": "260355", "train_bsz": "1750", "train_num_updates": "20196", "train_lr": "0.000315563", "train_gnorm": "0.59", "train_loss_scale": "2", "train_train_wall": "22", "train_gb_free": "40.8", "train_wall": "15848"}
[2024-10-06 20:46:10,699][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 20:46:10,706][fairseq.trainer][INFO] - begin training epoch 422
[2024-10-06 20:46:10,707][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 20:48:18,830][fairseq_cli.train][INFO] - end of epoch 421 (average epoch stats below)
[2024-10-06 20:48:18,846][train][INFO] - {"epoch": 421, "train_loss": "0.747", "train_ntokens": "260355", "train_nsentences": "1750.04", "train_wps": "47102.6", "train_ups": "0.18", "train_wpb": "260355", "train_bsz": "1750", "train_num_updates": "20196", "train_lr": "0.000315563", "train_gnorm": "0.583", "train_loss_scale": "2", "train_train_wall": "61", "train_gb_free": "40.8", "train_wall": "15974"}
[2024-10-06 20:48:19,094][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 20:48:19,098][fairseq.trainer][INFO] - begin training epoch 422
[2024-10-06 20:48:19,098][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 20:49:49,742][train_inner][INFO] - {"epoch": 422, "update": 421.083, "loss": "0.743", "ntokens": "260550", "nsentences": "1751.93", "wps": "40018.2", "ups": "0.15", "wpb": "260550", "bsz": "1751.9", "num_updates": "20200", "lr": "0.000315625", "gnorm": "0.519", "loss_scale": "2", "train_wall": "311", "gb_free": "40.1", "wall": "16068"}
[2024-10-06 20:50:21,672][fairseq_cli.train][INFO] - end of epoch 422 (average epoch stats below)
[2024-10-06 20:50:21,675][train][INFO] - {"epoch": 422, "train_loss": "0.741", "train_ntokens": "260398", "train_nsentences": "1750.04", "train_wps": "49771", "train_ups": "0.19", "train_wpb": "260398", "train_bsz": "1750", "train_num_updates": "20244", "train_lr": "0.000316313", "train_gnorm": "0.455", "train_loss_scale": "2", "train_train_wall": "85", "train_gb_free": "39.2", "train_wall": "16100"}
[2024-10-06 20:50:21,885][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 20:50:21,895][fairseq.trainer][INFO] - begin training epoch 423
[2024-10-06 20:50:21,895][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 20:51:41,080][train_inner][INFO] - {"epoch": 422, "update": 421.083, "loss": "0.743", "ntokens": "260550", "nsentences": "1751.93", "wps": "41265.8", "ups": "0.16", "wpb": "260550", "bsz": "1751.9", "num_updates": "20200", "lr": "0.000315625", "gnorm": "0.516", "loss_scale": "2", "train_wall": "343", "gb_free": "40.1", "wall": "16176"}
[2024-10-06 20:52:17,643][fairseq_cli.train][INFO] - end of epoch 422 (average epoch stats below)
[2024-10-06 20:52:17,653][train][INFO] - {"epoch": 422, "train_loss": "0.74", "train_ntokens": "260398", "train_nsentences": "1750.04", "train_wps": "52340.8", "train_ups": "0.2", "train_wpb": "260398", "train_bsz": "1750", "train_num_updates": "20244", "train_lr": "0.000316313", "train_gnorm": "0.422", "train_loss_scale": "2", "train_train_wall": "86", "train_gb_free": "39.2", "train_wall": "16212"}
[2024-10-06 20:52:17,794][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 20:52:17,799][fairseq.trainer][INFO] - begin training epoch 423
[2024-10-06 20:52:17,799][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 20:54:25,994][fairseq_cli.train][INFO] - end of epoch 423 (average epoch stats below)
[2024-10-06 20:54:26,007][train][INFO] - {"epoch": 423, "train_loss": "0.739", "train_ntokens": "260712", "train_nsentences": "1750.04", "train_wps": "51218.8", "train_ups": "0.2", "train_wpb": "260712", "train_bsz": "1750", "train_num_updates": "20292", "train_lr": "0.000317063", "train_gnorm": "0.512", "train_loss_scale": "2", "train_train_wall": "79", "train_gb_free": "40", "train_wall": "16344"}
[2024-10-06 20:54:26,284][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 20:54:26,290][fairseq.trainer][INFO] - begin training epoch 424
[2024-10-06 20:54:26,291][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 20:56:34,481][fairseq_cli.train][INFO] - end of epoch 423 (average epoch stats below)
[2024-10-06 20:56:34,484][train][INFO] - {"epoch": 423, "train_loss": "0.739", "train_ntokens": "260712", "train_nsentences": "1750.04", "train_wps": "48725.7", "train_ups": "0.19", "train_wpb": "260712", "train_bsz": "1750", "train_num_updates": "20292", "train_lr": "0.000317063", "train_gnorm": "0.537", "train_loss_scale": "2", "train_train_wall": "98", "train_gb_free": "40", "train_wall": "16469"}
[2024-10-06 20:56:34,748][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 20:56:34,759][fairseq.trainer][INFO] - begin training epoch 424
[2024-10-06 20:56:34,759][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 20:58:48,311][fairseq_cli.train][INFO] - end of epoch 424 (average epoch stats below)
[2024-10-06 20:58:48,319][train][INFO] - {"epoch": 424, "train_loss": "0.734", "train_ntokens": "260685", "train_nsentences": "1750.04", "train_wps": "47703.2", "train_ups": "0.18", "train_wpb": "260685", "train_bsz": "1750", "train_num_updates": "20340", "train_lr": "0.000317813", "train_gnorm": "0.514", "train_loss_scale": "2", "train_train_wall": "66", "train_gb_free": "40.1", "train_wall": "16606"}
[2024-10-06 20:58:48,538][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 20:58:48,549][fairseq.trainer][INFO] - begin training epoch 425
[2024-10-06 20:58:48,550][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 21:00:42,982][fairseq_cli.train][INFO] - end of epoch 424 (average epoch stats below)
[2024-10-06 21:00:42,999][train][INFO] - {"epoch": 424, "train_loss": "0.734", "train_ntokens": "260685", "train_nsentences": "1750.04", "train_wps": "50354", "train_ups": "0.19", "train_wpb": "260685", "train_bsz": "1750", "train_num_updates": "20340", "train_lr": "0.000317813", "train_gnorm": "0.54", "train_loss_scale": "2", "train_train_wall": "60", "train_gb_free": "40.1", "train_wall": "16718"}
[2024-10-06 21:00:43,104][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 21:00:43,116][fairseq.trainer][INFO] - begin training epoch 425
[2024-10-06 21:00:43,117][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 21:02:37,437][fairseq_cli.train][INFO] - end of epoch 425 (average epoch stats below)
[2024-10-06 21:02:37,451][train][INFO] - {"epoch": 425, "train_loss": "0.735", "train_ntokens": "261258", "train_nsentences": "1750.04", "train_wps": "54732.5", "train_ups": "0.21", "train_wpb": "261258", "train_bsz": "1750", "train_num_updates": "20388", "train_lr": "0.000318563", "train_gnorm": "0.575", "train_loss_scale": "2", "train_train_wall": "66", "train_gb_free": "39.8", "train_wall": "16835"}
[2024-10-06 21:02:37,694][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 21:02:37,712][fairseq.trainer][INFO] - begin training epoch 426
[2024-10-06 21:02:37,712][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 21:04:21,070][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
[2024-10-06 21:04:22,932][fairseq_cli.train][INFO] - end of epoch 425 (average epoch stats below)
[2024-10-06 21:04:22,941][train][INFO] - {"epoch": 425, "train_loss": "0.735", "train_ntokens": "261129", "train_nsentences": "1762.96", "train_wps": "55802.3", "train_ups": "0.21", "train_wpb": "261129", "train_bsz": "1763", "train_num_updates": "20387", "train_lr": "0.000318547", "train_gnorm": "0.566", "train_loss_scale": "1", "train_train_wall": "63", "train_gb_free": "39.8", "train_wall": "16938"}
[2024-10-06 21:04:23,176][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 21:04:23,179][fairseq.trainer][INFO] - begin training epoch 426
[2024-10-06 21:04:23,179][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 21:05:48,261][train_inner][INFO] - {"epoch": 426, "update": 425.25, "loss": "0.737", "ntokens": "260636", "nsentences": "1761.47", "wps": "54384.8", "ups": "0.21", "wpb": "260636", "bsz": "1761.5", "num_updates": "20400", "lr": "0.00031875", "gnorm": "0.52", "loss_scale": "2", "train_wall": "262", "gb_free": "39.2", "wall": "17026"}
[2024-10-06 21:06:23,162][fairseq_cli.train][INFO] - end of epoch 426 (average epoch stats below)
[2024-10-06 21:06:23,166][train][INFO] - {"epoch": 426, "train_loss": "0.734", "train_ntokens": "260497", "train_nsentences": "1750.04", "train_wps": "55397.6", "train_ups": "0.21", "train_wpb": "260497", "train_bsz": "1750", "train_num_updates": "20436", "train_lr": "0.000319312", "train_gnorm": "0.508", "train_loss_scale": "2", "train_train_wall": "54", "train_gb_free": "40", "train_wall": "17061"}
[2024-10-06 21:06:23,254][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 21:06:23,258][fairseq.trainer][INFO] - begin training epoch 427
[2024-10-06 21:06:23,258][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 21:07:14,743][train_inner][INFO] - {"epoch": 426, "update": 425.271, "loss": "0.737", "ntokens": "260605", "nsentences": "1762.66", "wps": "55843.2", "ups": "0.21", "wpb": "260605", "bsz": "1762.7", "num_updates": "20400", "lr": "0.00031875", "gnorm": "0.524", "loss_scale": "1", "train_wall": "278", "gb_free": "40.3", "wall": "17110"}
[2024-10-06 21:07:50,761][fairseq_cli.train][INFO] - end of epoch 426 (average epoch stats below)
[2024-10-06 21:07:50,765][train][INFO] - {"epoch": 426, "train_loss": "0.733", "train_ntokens": "260497", "train_nsentences": "1750.04", "train_wps": "60166.6", "train_ups": "0.23", "train_wpb": "260497", "train_bsz": "1750", "train_num_updates": "20435", "train_lr": "0.000319297", "train_gnorm": "0.477", "train_loss_scale": "1", "train_train_wall": "57", "train_gb_free": "40", "train_wall": "17146"}
[2024-10-06 21:07:50,965][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 21:07:50,968][fairseq.trainer][INFO] - begin training epoch 427
[2024-10-06 21:07:50,972][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 21:10:25,141][fairseq_cli.train][INFO] - end of epoch 427 (average epoch stats below)
[2024-10-06 21:10:25,146][train][INFO] - {"epoch": 427, "train_loss": "0.739", "train_ntokens": "260268", "train_nsentences": "1750.04", "train_wps": "51628.3", "train_ups": "0.2", "train_wpb": "260268", "train_bsz": "1750", "train_num_updates": "20484", "train_lr": "0.000320063", "train_gnorm": "0.486", "train_loss_scale": "2", "train_train_wall": "108", "train_gb_free": "39.2", "train_wall": "17303"}
[2024-10-06 21:10:25,385][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 21:10:25,388][fairseq.trainer][INFO] - begin training epoch 428
[2024-10-06 21:10:25,389][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 21:13:35,959][fairseq_cli.train][INFO] - end of epoch 427 (average epoch stats below)
[2024-10-06 21:13:35,975][train][INFO] - {"epoch": 427, "train_loss": "0.74", "train_ntokens": "260268", "train_nsentences": "1750.04", "train_wps": "36191", "train_ups": "0.14", "train_wpb": "260268", "train_bsz": "1750", "train_num_updates": "20483", "train_lr": "0.000320047", "train_gnorm": "0.444", "train_loss_scale": "1", "train_train_wall": "62", "train_gb_free": "39.2", "train_wall": "17491"}
[2024-10-06 21:13:36,182][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 21:13:36,186][fairseq.trainer][INFO] - begin training epoch 428
[2024-10-06 21:13:36,187][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 21:15:18,318][fairseq_cli.train][INFO] - end of epoch 428 (average epoch stats below)
[2024-10-06 21:15:18,333][train][INFO] - {"epoch": 428, "train_loss": "0.734", "train_ntokens": "260705", "train_nsentences": "1750.04", "train_wps": "42682.7", "train_ups": "0.16", "train_wpb": "260705", "train_bsz": "1750", "train_num_updates": "20532", "train_lr": "0.000320813", "train_gnorm": "0.487", "train_loss_scale": "2", "train_train_wall": "43", "train_gb_free": "41.3", "train_wall": "17596"}
[2024-10-06 21:15:18,460][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 21:15:18,472][fairseq.trainer][INFO] - begin training epoch 429
[2024-10-06 21:15:18,473][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 21:18:02,773][fairseq_cli.train][INFO] - end of epoch 428 (average epoch stats below)
[2024-10-06 21:18:02,775][train][INFO] - {"epoch": 428, "train_loss": "0.735", "train_ntokens": "260705", "train_nsentences": "1750.04", "train_wps": "46903.8", "train_ups": "0.18", "train_wpb": "260705", "train_bsz": "1750", "train_num_updates": "20531", "train_lr": "0.000320797", "train_gnorm": "0.495", "train_loss_scale": "1", "train_train_wall": "56", "train_gb_free": "41.3", "train_wall": "17758"}
[2024-10-06 21:18:03,210][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 21:18:03,214][fairseq.trainer][INFO] - begin training epoch 429
[2024-10-06 21:18:03,214][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 21:20:22,455][fairseq_cli.train][INFO] - end of epoch 429 (average epoch stats below)
[2024-10-06 21:20:22,459][train][INFO] - {"epoch": 429, "train_loss": "0.735", "train_ntokens": "260809", "train_nsentences": "1750.04", "train_wps": "41163.7", "train_ups": "0.16", "train_wpb": "260809", "train_bsz": "1750", "train_num_updates": "20580", "train_lr": "0.000321562", "train_gnorm": "0.438", "train_loss_scale": "2", "train_train_wall": "68", "train_gb_free": "39.7", "train_wall": "17900"}
[2024-10-06 21:20:22,597][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 21:20:22,600][fairseq.trainer][INFO] - begin training epoch 430
[2024-10-06 21:20:22,600][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 21:22:54,856][fairseq_cli.train][INFO] - end of epoch 429 (average epoch stats below)
[2024-10-06 21:22:54,888][train][INFO] - {"epoch": 429, "train_loss": "0.735", "train_ntokens": "260809", "train_nsentences": "1750.04", "train_wps": "42857.4", "train_ups": "0.16", "train_wpb": "260809", "train_bsz": "1750", "train_num_updates": "20579", "train_lr": "0.000321547", "train_gnorm": "0.469", "train_loss_scale": "1", "train_train_wall": "83", "train_gb_free": "39.7", "train_wall": "18050"}
[2024-10-06 21:22:55,145][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 21:22:55,148][fairseq.trainer][INFO] - begin training epoch 430
[2024-10-06 21:22:55,149][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 21:24:46,406][train_inner][INFO] - {"epoch": 430, "update": 429.417, "loss": "0.735", "ntokens": "260718", "nsentences": "1732.23", "wps": "45815.1", "ups": "0.18", "wpb": "260718", "bsz": "1732.2", "num_updates": "20600", "lr": "0.000321875", "gnorm": "0.477", "loss_scale": "2", "train_wall": "285", "gb_free": "39.6", "wall": "18164"}
[2024-10-06 21:25:12,184][fairseq_cli.train][INFO] - end of epoch 430 (average epoch stats below)
[2024-10-06 21:25:12,187][train][INFO] - {"epoch": 430, "train_loss": "0.732", "train_ntokens": "260579", "train_nsentences": "1750.04", "train_wps": "43171.4", "train_ups": "0.17", "train_wpb": "260580", "train_bsz": "1750", "train_num_updates": "20628", "train_lr": "0.000322313", "train_gnorm": "0.508", "train_loss_scale": "2", "train_train_wall": "57", "train_gb_free": "40.1", "train_wall": "18190"}
[2024-10-06 21:25:12,411][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 21:25:12,415][fairseq.trainer][INFO] - begin training epoch 431
[2024-10-06 21:25:12,416][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 21:27:54,694][train_inner][INFO] - {"epoch": 430, "update": 429.438, "loss": "0.735", "ntokens": "260603", "nsentences": "1740.31", "wps": "42034.5", "ups": "0.16", "wpb": "260603", "bsz": "1740.3", "num_updates": "20600", "lr": "0.000321875", "gnorm": "0.469", "loss_scale": "1", "train_wall": "260", "gb_free": "39.6", "wall": "18350"}
[2024-10-06 21:28:18,151][fairseq_cli.train][INFO] - end of epoch 430 (average epoch stats below)
[2024-10-06 21:28:18,153][train][INFO] - {"epoch": 430, "train_loss": "0.733", "train_ntokens": "260579", "train_nsentences": "1750.04", "train_wps": "38692.6", "train_ups": "0.15", "train_wpb": "260580", "train_bsz": "1750", "train_num_updates": "20627", "train_lr": "0.000322297", "train_gnorm": "0.543", "train_loss_scale": "1", "train_train_wall": "47", "train_gb_free": "40.1", "train_wall": "18373"}
[2024-10-06 21:28:18,534][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 21:28:18,547][fairseq.trainer][INFO] - begin training epoch 431
[2024-10-06 21:28:18,548][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 21:30:37,645][fairseq_cli.train][INFO] - end of epoch 431 (average epoch stats below)
[2024-10-06 21:30:37,652][train][INFO] - {"epoch": 431, "train_loss": "0.732", "train_ntokens": "260789", "train_nsentences": "1750.04", "train_wps": "38461.9", "train_ups": "0.15", "train_wpb": "260789", "train_bsz": "1750", "train_num_updates": "20676", "train_lr": "0.000323062", "train_gnorm": "0.484", "train_loss_scale": "2", "train_train_wall": "71", "train_gb_free": "40.2", "train_wall": "18516"}
[2024-10-06 21:30:37,935][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 21:30:37,938][fairseq.trainer][INFO] - begin training epoch 432
[2024-10-06 21:30:37,939][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 21:34:25,244][fairseq_cli.train][INFO] - end of epoch 431 (average epoch stats below)
[2024-10-06 21:34:25,251][train][INFO] - {"epoch": 431, "train_loss": "0.732", "train_ntokens": "260789", "train_nsentences": "1750.04", "train_wps": "34100.1", "train_ups": "0.13", "train_wpb": "260789", "train_bsz": "1750", "train_num_updates": "20675", "train_lr": "0.000323047", "train_gnorm": "0.515", "train_loss_scale": "1", "train_train_wall": "71", "train_gb_free": "40.2", "train_wall": "18740"}
[2024-10-06 21:34:25,463][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 21:34:25,469][fairseq.trainer][INFO] - begin training epoch 432
[2024-10-06 21:34:25,470][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 21:35:53,529][fairseq_cli.train][INFO] - end of epoch 432 (average epoch stats below)
[2024-10-06 21:35:53,545][train][INFO] - {"epoch": 432, "train_loss": "0.73", "train_ntokens": "260842", "train_nsentences": "1750.04", "train_wps": "39636.1", "train_ups": "0.15", "train_wpb": "260842", "train_bsz": "1750", "train_num_updates": "20724", "train_lr": "0.000323812", "train_gnorm": "0.432", "train_loss_scale": "2", "train_train_wall": "65", "train_gb_free": "39.6", "train_wall": "18831"}
[2024-10-06 21:35:53,956][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 21:35:53,960][fairseq.trainer][INFO] - begin training epoch 433
[2024-10-06 21:35:53,961][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 21:39:29,304][fairseq_cli.train][INFO] - end of epoch 432 (average epoch stats below)
[2024-10-06 21:39:29,316][train][INFO] - {"epoch": 432, "train_loss": "0.73", "train_ntokens": "260842", "train_nsentences": "1750.04", "train_wps": "41177.5", "train_ups": "0.16", "train_wpb": "260842", "train_bsz": "1750", "train_num_updates": "20723", "train_lr": "0.000323797", "train_gnorm": "0.504", "train_loss_scale": "1", "train_train_wall": "67", "train_gb_free": "39.6", "train_wall": "19044"}
[2024-10-06 21:39:32,698][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 21:39:32,705][fairseq.trainer][INFO] - begin training epoch 433
[2024-10-06 21:39:32,705][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 21:41:57,602][fairseq_cli.train][INFO] - end of epoch 433 (average epoch stats below)
[2024-10-06 21:41:57,605][train][INFO] - {"epoch": 433, "train_loss": "0.733", "train_ntokens": "260727", "train_nsentences": "1750.04", "train_wps": "34376.2", "train_ups": "0.13", "train_wpb": "260727", "train_bsz": "1750", "train_num_updates": "20772", "train_lr": "0.000324562", "train_gnorm": "0.513", "train_loss_scale": "2", "train_train_wall": "68", "train_gb_free": "39.2", "train_wall": "19196"}
[2024-10-06 21:41:57,938][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 21:41:57,943][fairseq.trainer][INFO] - begin training epoch 434
[2024-10-06 21:41:57,943][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 21:45:07,673][fairseq_cli.train][INFO] - end of epoch 433 (average epoch stats below)
[2024-10-06 21:45:07,692][train][INFO] - {"epoch": 433, "train_loss": "0.732", "train_ntokens": "260727", "train_nsentences": "1750.04", "train_wps": "36986.7", "train_ups": "0.14", "train_wpb": "260727", "train_bsz": "1750", "train_num_updates": "20771", "train_lr": "0.000324547", "train_gnorm": "0.52", "train_loss_scale": "1", "train_train_wall": "43", "train_gb_free": "39.2", "train_wall": "19382"}
[2024-10-06 21:45:07,951][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 21:45:07,961][fairseq.trainer][INFO] - begin training epoch 434
[2024-10-06 21:45:07,961][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 21:46:51,530][train_inner][INFO] - {"epoch": 434, "update": 433.583, "loss": "0.73", "ntokens": "260705", "nsentences": "1739.88", "wps": "39348.5", "ups": "0.15", "wpb": "260705", "bsz": "1739.9", "num_updates": "20800", "lr": "0.000325", "gnorm": "0.476", "loss_scale": "2", "train_wall": "282", "gb_free": "40.1", "wall": "19489"}
[2024-10-06 21:47:08,306][fairseq_cli.train][INFO] - end of epoch 434 (average epoch stats below)
[2024-10-06 21:47:08,309][train][INFO] - {"epoch": 434, "train_loss": "0.727", "train_ntokens": "260480", "train_nsentences": "1750.04", "train_wps": "40241.7", "train_ups": "0.15", "train_wpb": "260480", "train_bsz": "1750", "train_num_updates": "20820", "train_lr": "0.000325313", "train_gnorm": "0.445", "train_loss_scale": "2", "train_train_wall": "67", "train_gb_free": "39.3", "train_wall": "19506"}
[2024-10-06 21:47:08,448][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 21:47:08,451][fairseq.trainer][INFO] - begin training epoch 435
[2024-10-06 21:47:08,451][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 21:49:34,322][train_inner][INFO] - {"epoch": 434, "update": 433.604, "loss": "0.73", "ntokens": "260833", "nsentences": "1732.42", "wps": "40140", "ups": "0.15", "wpb": "260833", "bsz": "1732.4", "num_updates": "20800", "lr": "0.000325", "gnorm": "0.514", "loss_scale": "1", "train_wall": "256", "gb_free": "39.9", "wall": "19649"}
[2024-10-06 21:50:02,422][fairseq_cli.train][INFO] - end of epoch 434 (average epoch stats below)
[2024-10-06 21:50:02,431][train][INFO] - {"epoch": 434, "train_loss": "0.727", "train_ntokens": "260480", "train_nsentences": "1750.04", "train_wps": "42422.1", "train_ups": "0.16", "train_wpb": "260480", "train_bsz": "1750", "train_num_updates": "20819", "train_lr": "0.000325297", "train_gnorm": "0.481", "train_loss_scale": "1", "train_train_wall": "80", "train_gb_free": "39.3", "train_wall": "19677"}
[2024-10-06 21:50:02,765][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 21:50:02,772][fairseq.trainer][INFO] - begin training epoch 435
[2024-10-06 21:50:02,773][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 21:51:10,230][fairseq_cli.train][INFO] - end of epoch 435 (average epoch stats below)
[2024-10-06 21:51:10,243][train][INFO] - {"epoch": 435, "train_loss": "0.726", "train_ntokens": "260592", "train_nsentences": "1750.04", "train_wps": "51703.4", "train_ups": "0.2", "train_wpb": "260592", "train_bsz": "1750", "train_num_updates": "20868", "train_lr": "0.000326062", "train_gnorm": "0.446", "train_loss_scale": "2", "train_train_wall": "46", "train_gb_free": "42", "train_wall": "19748"}
[2024-10-06 21:51:10,542][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 21:51:10,546][fairseq.trainer][INFO] - begin training epoch 436
[2024-10-06 21:51:10,546][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 21:54:34,621][fairseq_cli.train][INFO] - end of epoch 435 (average epoch stats below)
[2024-10-06 21:54:34,623][train][INFO] - {"epoch": 435, "train_loss": "0.727", "train_ntokens": "260592", "train_nsentences": "1750.04", "train_wps": "45954.7", "train_ups": "0.18", "train_wpb": "260592", "train_bsz": "1750", "train_num_updates": "20867", "train_lr": "0.000326047", "train_gnorm": "0.489", "train_loss_scale": "1", "train_train_wall": "39", "train_gb_free": "42", "train_wall": "19949"}
[2024-10-06 21:54:34,941][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 21:54:34,944][fairseq.trainer][INFO] - begin training epoch 436
[2024-10-06 21:54:34,944][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 21:57:28,526][fairseq_cli.train][INFO] - end of epoch 436 (average epoch stats below)
[2024-10-06 21:57:28,531][train][INFO] - {"epoch": 436, "train_loss": "0.73", "train_ntokens": "260775", "train_nsentences": "1750.04", "train_wps": "33089.5", "train_ups": "0.13", "train_wpb": "260775", "train_bsz": "1750", "train_num_updates": "20916", "train_lr": "0.000326813", "train_gnorm": "0.574", "train_loss_scale": "2", "train_train_wall": "70", "train_gb_free": "39.8", "train_wall": "20126"}
[2024-10-06 21:57:30,265][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 21:57:30,268][fairseq.trainer][INFO] - begin training epoch 437
[2024-10-06 21:57:30,268][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 22:01:34,686][fairseq_cli.train][INFO] - end of epoch 436 (average epoch stats below)
[2024-10-06 22:01:34,695][train][INFO] - {"epoch": 436, "train_loss": "0.729", "train_ntokens": "260775", "train_nsentences": "1750.04", "train_wps": "29798", "train_ups": "0.11", "train_wpb": "260775", "train_bsz": "1750", "train_num_updates": "20915", "train_lr": "0.000326797", "train_gnorm": "0.53", "train_loss_scale": "1", "train_train_wall": "65", "train_gb_free": "39.8", "train_wall": "20370"}
[2024-10-06 22:01:34,861][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 22:01:34,865][fairseq.trainer][INFO] - begin training epoch 437
[2024-10-06 22:01:34,865][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 22:03:32,054][fairseq_cli.train][INFO] - end of epoch 437 (average epoch stats below)
[2024-10-06 22:03:32,083][train][INFO] - {"epoch": 437, "train_loss": "0.722", "train_ntokens": "260398", "train_nsentences": "1750.04", "train_wps": "34381.5", "train_ups": "0.13", "train_wpb": "260398", "train_bsz": "1750", "train_num_updates": "20964", "train_lr": "0.000327562", "train_gnorm": "0.564", "train_loss_scale": "2", "train_train_wall": "77", "train_gb_free": "39.3", "train_wall": "20490"}
[2024-10-06 22:03:32,321][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 22:03:32,326][fairseq.trainer][INFO] - begin training epoch 438
[2024-10-06 22:03:32,326][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 22:05:37,825][fairseq_cli.train][INFO] - end of epoch 437 (average epoch stats below)
[2024-10-06 22:05:37,833][train][INFO] - {"epoch": 437, "train_loss": "0.721", "train_ntokens": "260398", "train_nsentences": "1750.04", "train_wps": "51409.4", "train_ups": "0.2", "train_wpb": "260398", "train_bsz": "1750", "train_num_updates": "20963", "train_lr": "0.000327547", "train_gnorm": "0.506", "train_loss_scale": "1", "train_train_wall": "73", "train_gb_free": "39.3", "train_wall": "20613"}
[2024-10-06 22:05:37,997][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 22:05:37,999][fairseq.trainer][INFO] - begin training epoch 438
[2024-10-06 22:05:38,000][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 22:06:47,692][train_inner][INFO] - {"epoch": 438, "update": 437.75, "loss": "0.727", "ntokens": "260567", "nsentences": "1765.56", "wps": "43567.2", "ups": "0.17", "wpb": "260567", "bsz": "1765.6", "num_updates": "21000", "lr": "0.000328125", "gnorm": "0.515", "loss_scale": "2", "train_wall": "239", "gb_free": "39.7", "wall": "20686"}
[2024-10-06 22:07:00,975][fairseq_cli.train][INFO] - end of epoch 438 (average epoch stats below)
[2024-10-06 22:07:00,980][train][INFO] - {"epoch": 438, "train_loss": "0.727", "train_ntokens": "260421", "train_nsentences": "1750.04", "train_wps": "59840.9", "train_ups": "0.23", "train_wpb": "260421", "train_bsz": "1750", "train_num_updates": "21012", "train_lr": "0.000328312", "train_gnorm": "0.487", "train_loss_scale": "2", "train_train_wall": "43", "train_gb_free": "39.7", "train_wall": "20699"}
[2024-10-06 22:07:01,163][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 22:07:01,167][fairseq.trainer][INFO] - begin training epoch 439
[2024-10-06 22:07:01,167][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 22:09:22,598][train_inner][INFO] - {"epoch": 438, "update": 437.771, "loss": "0.727", "ntokens": "260544", "nsentences": "1768.33", "wps": "43852.8", "ups": "0.17", "wpb": "260544", "bsz": "1768.3", "num_updates": "21000", "lr": "0.000328125", "gnorm": "0.495", "loss_scale": "1", "train_wall": "234", "gb_free": "39.7", "wall": "20837"}
[2024-10-06 22:09:46,170][fairseq_cli.train][INFO] - end of epoch 438 (average epoch stats below)
[2024-10-06 22:09:46,180][train][INFO] - {"epoch": 438, "train_loss": "0.727", "train_ntokens": "260421", "train_nsentences": "1750.04", "train_wps": "50335.8", "train_ups": "0.19", "train_wpb": "260421", "train_bsz": "1750", "train_num_updates": "21011", "train_lr": "0.000328297", "train_gnorm": "0.467", "train_loss_scale": "1", "train_train_wall": "52", "train_gb_free": "39.7", "train_wall": "20861"}
[2024-10-06 22:09:46,287][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 22:09:46,295][fairseq.trainer][INFO] - begin training epoch 439
[2024-10-06 22:09:46,296][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 22:10:50,210][fairseq_cli.train][INFO] - end of epoch 439 (average epoch stats below)
[2024-10-06 22:10:50,216][train][INFO] - {"epoch": 439, "train_loss": "0.73", "train_ntokens": "261162", "train_nsentences": "1750.04", "train_wps": "54686.3", "train_ups": "0.21", "train_wpb": "261162", "train_bsz": "1750", "train_num_updates": "21060", "train_lr": "0.000329062", "train_gnorm": "0.494", "train_loss_scale": "2", "train_train_wall": "49", "train_gb_free": "39.3", "train_wall": "20928"}
[2024-10-06 22:10:50,305][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 22:10:50,308][fairseq.trainer][INFO] - begin training epoch 440
[2024-10-06 22:10:50,309][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 22:13:00,612][fairseq_cli.train][INFO] - end of epoch 439 (average epoch stats below)
[2024-10-06 22:13:00,623][train][INFO] - {"epoch": 439, "train_loss": "0.731", "train_ntokens": "261162", "train_nsentences": "1750.04", "train_wps": "64472.9", "train_ups": "0.25", "train_wpb": "261162", "train_bsz": "1750", "train_num_updates": "21059", "train_lr": "0.000329047", "train_gnorm": "0.574", "train_loss_scale": "1", "train_train_wall": "65", "train_gb_free": "39.3", "train_wall": "21055"}
[2024-10-06 22:13:00,798][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 22:13:00,801][fairseq.trainer][INFO] - begin training epoch 440
[2024-10-06 22:13:00,802][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 22:14:23,355][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 440 @ 21108 updates
[2024-10-06 22:14:23,357][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-06 22:14:27,078][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-06 22:14:27,081][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 440 @ 21108 updates, score None) (writing took 3.7263182075694203 seconds)
[2024-10-06 22:14:27,082][fairseq_cli.train][INFO] - end of epoch 440 (average epoch stats below)
[2024-10-06 22:14:27,085][train][INFO] - {"epoch": 440, "train_loss": "0.73", "train_ntokens": "260951", "train_nsentences": "1750.04", "train_wps": "57758.1", "train_ups": "0.22", "train_wpb": "260951", "train_bsz": "1750", "train_num_updates": "21108", "train_lr": "0.000329813", "train_gnorm": "0.564", "train_loss_scale": "2", "train_train_wall": "73", "train_gb_free": "39.8", "train_wall": "21145"}
[2024-10-06 22:14:27,153][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 22:14:27,158][fairseq.trainer][INFO] - begin training epoch 441
[2024-10-06 22:14:27,159][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 22:16:18,771][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 440 @ 21107 updates
[2024-10-06 22:16:18,773][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-06 22:16:23,784][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-06 22:16:23,788][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 440 @ 21107 updates, score None) (writing took 5.016913526691496 seconds)
[2024-10-06 22:16:23,789][fairseq_cli.train][INFO] - end of epoch 440 (average epoch stats below)
[2024-10-06 22:16:23,798][train][INFO] - {"epoch": 440, "train_loss": "0.729", "train_ntokens": "260951", "train_nsentences": "1750.04", "train_wps": "61652.7", "train_ups": "0.24", "train_wpb": "260951", "train_bsz": "1750", "train_num_updates": "21107", "train_lr": "0.000329797", "train_gnorm": "0.48", "train_loss_scale": "1", "train_train_wall": "71", "train_gb_free": "39.8", "train_wall": "21259"}
[2024-10-06 22:16:24,050][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 22:16:24,058][fairseq.trainer][INFO] - begin training epoch 441
[2024-10-06 22:16:24,059][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 22:17:38,481][fairseq_cli.train][INFO] - end of epoch 441 (average epoch stats below)
[2024-10-06 22:17:38,484][train][INFO] - {"epoch": 441, "train_loss": "0.722", "train_ntokens": "260309", "train_nsentences": "1750.04", "train_wps": "65282.8", "train_ups": "0.25", "train_wpb": "260309", "train_bsz": "1750", "train_num_updates": "21156", "train_lr": "0.000330562", "train_gnorm": "0.429", "train_loss_scale": "2", "train_train_wall": "60", "train_gb_free": "41.1", "train_wall": "21336"}
[2024-10-06 22:17:38,612][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 22:17:38,615][fairseq.trainer][INFO] - begin training epoch 442
[2024-10-06 22:17:38,616][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 22:19:53,119][fairseq_cli.train][INFO] - end of epoch 441 (average epoch stats below)
[2024-10-06 22:19:53,152][train][INFO] - {"epoch": 441, "train_loss": "0.724", "train_ntokens": "260309", "train_nsentences": "1750.04", "train_wps": "59690.6", "train_ups": "0.23", "train_wpb": "260309", "train_bsz": "1750", "train_num_updates": "21155", "train_lr": "0.000330547", "train_gnorm": "0.554", "train_loss_scale": "1", "train_train_wall": "68", "train_gb_free": "41.1", "train_wall": "21468"}
[2024-10-06 22:19:53,468][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 22:19:53,476][fairseq.trainer][INFO] - begin training epoch 442
[2024-10-06 22:19:53,483][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 22:21:21,557][train_inner][INFO] - {"epoch": 442, "update": 441.917, "loss": "0.727", "ntokens": "260620", "nsentences": "1752.74", "wps": "59648.8", "ups": "0.23", "wpb": "260620", "bsz": "1752.7", "num_updates": "21200", "lr": "0.00033125", "gnorm": "0.485", "loss_scale": "2", "train_wall": "283", "gb_free": "40.7", "wall": "21559"}
[2024-10-06 22:21:23,585][fairseq_cli.train][INFO] - end of epoch 442 (average epoch stats below)
[2024-10-06 22:21:23,675][train][INFO] - {"epoch": 442, "train_loss": "0.722", "train_ntokens": "260478", "train_nsentences": "1750.04", "train_wps": "55544.4", "train_ups": "0.21", "train_wpb": "260478", "train_bsz": "1750", "train_num_updates": "21204", "train_lr": "0.000331313", "train_gnorm": "0.484", "train_loss_scale": "2", "train_train_wall": "90", "train_gb_free": "40.3", "train_wall": "21562"}
[2024-10-06 22:21:24,219][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 22:21:24,223][fairseq.trainer][INFO] - begin training epoch 443
[2024-10-06 22:21:24,223][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 22:23:14,835][train_inner][INFO] - {"epoch": 442, "update": 441.938, "loss": "0.727", "ntokens": "260642", "nsentences": "1749.97", "wps": "62637.2", "ups": "0.24", "wpb": "260642", "bsz": "1750", "num_updates": "21200", "lr": "0.00033125", "gnorm": "0.555", "loss_scale": "1", "train_wall": "314", "gb_free": "40.1", "wall": "21670"}
[2024-10-06 22:23:16,556][fairseq_cli.train][INFO] - end of epoch 442 (average epoch stats below)
[2024-10-06 22:23:16,561][train][INFO] - {"epoch": 442, "train_loss": "0.724", "train_ntokens": "260478", "train_nsentences": "1750.04", "train_wps": "61468.6", "train_ups": "0.24", "train_wpb": "260478", "train_bsz": "1750", "train_num_updates": "21203", "train_lr": "0.000331297", "train_gnorm": "0.634", "train_loss_scale": "1", "train_train_wall": "89", "train_gb_free": "40.3", "train_wall": "21671"}
[2024-10-06 22:23:16,729][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 22:23:16,732][fairseq.trainer][INFO] - begin training epoch 443
[2024-10-06 22:23:16,732][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 22:25:08,565][fairseq_cli.train][INFO] - end of epoch 443 (average epoch stats below)
[2024-10-06 22:25:08,568][train][INFO] - {"epoch": 443, "train_loss": "0.725", "train_ntokens": "260824", "train_nsentences": "1750.04", "train_wps": "55670.4", "train_ups": "0.21", "train_wpb": "260824", "train_bsz": "1750", "train_num_updates": "21252", "train_lr": "0.000332063", "train_gnorm": "0.493", "train_loss_scale": "2", "train_train_wall": "62", "train_gb_free": "40.1", "train_wall": "21787"}
[2024-10-06 22:25:08,749][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 22:25:08,752][fairseq.trainer][INFO] - begin training epoch 444
[2024-10-06 22:25:08,753][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 22:27:00,843][fairseq_cli.train][INFO] - end of epoch 443 (average epoch stats below)
[2024-10-06 22:27:00,848][train][INFO] - {"epoch": 443, "train_loss": "0.723", "train_ntokens": "260824", "train_nsentences": "1750.04", "train_wps": "55819.9", "train_ups": "0.21", "train_wpb": "260824", "train_bsz": "1750", "train_num_updates": "21251", "train_lr": "0.000332047", "train_gnorm": "0.434", "train_loss_scale": "1", "train_train_wall": "59", "train_gb_free": "40.1", "train_wall": "21896"}
[2024-10-06 22:27:01,218][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 22:27:01,222][fairseq.trainer][INFO] - begin training epoch 444
[2024-10-06 22:27:01,222][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 22:29:02,096][fairseq_cli.train][INFO] - end of epoch 444 (average epoch stats below)
[2024-10-06 22:29:02,100][train][INFO] - {"epoch": 444, "train_loss": "0.721", "train_ntokens": "260740", "train_nsentences": "1750.04", "train_wps": "53593.4", "train_ups": "0.21", "train_wpb": "260740", "train_bsz": "1750", "train_num_updates": "21300", "train_lr": "0.000332812", "train_gnorm": "0.507", "train_loss_scale": "2", "train_train_wall": "59", "train_gb_free": "40.3", "train_wall": "22020"}
[2024-10-06 22:29:02,274][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 22:29:02,297][fairseq.trainer][INFO] - begin training epoch 445
[2024-10-06 22:29:02,297][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 22:31:16,011][fairseq_cli.train][INFO] - end of epoch 444 (average epoch stats below)
[2024-10-06 22:31:16,016][train][INFO] - {"epoch": 444, "train_loss": "0.72", "train_ntokens": "260740", "train_nsentences": "1750.04", "train_wps": "49048.7", "train_ups": "0.19", "train_wpb": "260740", "train_bsz": "1750", "train_num_updates": "21299", "train_lr": "0.000332797", "train_gnorm": "0.473", "train_loss_scale": "1", "train_train_wall": "57", "train_gb_free": "40.3", "train_wall": "22151"}
[2024-10-06 22:31:16,182][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 22:31:16,188][fairseq.trainer][INFO] - begin training epoch 445
[2024-10-06 22:31:16,189][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 22:33:19,193][fairseq_cli.train][INFO] - end of epoch 445 (average epoch stats below)
[2024-10-06 22:33:19,199][train][INFO] - {"epoch": 445, "train_loss": "0.727", "train_ntokens": "260772", "train_nsentences": "1750.04", "train_wps": "48686.7", "train_ups": "0.19", "train_wpb": "260772", "train_bsz": "1750", "train_num_updates": "21348", "train_lr": "0.000333562", "train_gnorm": "0.523", "train_loss_scale": "2", "train_train_wall": "66", "train_gb_free": "40.5", "train_wall": "22277"}
[2024-10-06 22:33:19,359][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 22:33:19,362][fairseq.trainer][INFO] - begin training epoch 446
[2024-10-06 22:33:19,363][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 22:35:23,559][fairseq_cli.train][INFO] - end of epoch 445 (average epoch stats below)
[2024-10-06 22:35:23,575][train][INFO] - {"epoch": 445, "train_loss": "0.726", "train_ntokens": "260772", "train_nsentences": "1750.04", "train_wps": "50564.2", "train_ups": "0.19", "train_wpb": "260772", "train_bsz": "1750", "train_num_updates": "21347", "train_lr": "0.000333547", "train_gnorm": "0.468", "train_loss_scale": "1", "train_train_wall": "71", "train_gb_free": "40.5", "train_wall": "22398"}
[2024-10-06 22:35:23,776][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 22:35:23,783][fairseq.trainer][INFO] - begin training epoch 446
[2024-10-06 22:35:23,783][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 22:36:20,930][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2024-10-06 22:36:37,408][fairseq_cli.train][INFO] - end of epoch 446 (average epoch stats below)
[2024-10-06 22:36:37,422][train][INFO] - {"epoch": 446, "train_loss": "0.722", "train_ntokens": "261002", "train_nsentences": "1750.79", "train_wps": "61892", "train_ups": "0.24", "train_wpb": "261002", "train_bsz": "1750.8", "train_num_updates": "21395", "train_lr": "0.000334297", "train_gnorm": "0.462", "train_loss_scale": "2", "train_train_wall": "58", "train_gb_free": "39.8", "train_wall": "22475"}
[2024-10-06 22:36:37,586][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 22:36:37,592][fairseq.trainer][INFO] - begin training epoch 447
[2024-10-06 22:36:37,592][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 22:38:38,489][fairseq_cli.train][INFO] - end of epoch 446 (average epoch stats below)
[2024-10-06 22:38:38,494][train][INFO] - {"epoch": 446, "train_loss": "0.723", "train_ntokens": "261043", "train_nsentences": "1750.04", "train_wps": "64284.3", "train_ups": "0.25", "train_wpb": "261043", "train_bsz": "1750", "train_num_updates": "21395", "train_lr": "0.000334297", "train_gnorm": "0.489", "train_loss_scale": "1", "train_train_wall": "80", "train_gb_free": "39.8", "train_wall": "22593"}
[2024-10-06 22:38:39,069][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 22:38:39,072][fairseq.trainer][INFO] - begin training epoch 447
[2024-10-06 22:38:39,073][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 22:39:03,735][train_inner][INFO] - {"epoch": 447, "update": 446.104, "loss": "0.724", "ntokens": "260904", "nsentences": "1745.23", "wps": "49127.5", "ups": "0.19", "wpb": "260904", "bsz": "1745.2", "num_updates": "21400", "lr": "0.000334375", "gnorm": "0.499", "loss_scale": "2", "train_wall": "261", "gb_free": "40.3", "wall": "22622"}
[2024-10-06 22:39:54,377][fairseq_cli.train][INFO] - end of epoch 447 (average epoch stats below)
[2024-10-06 22:39:54,380][train][INFO] - {"epoch": 447, "train_loss": "0.726", "train_ntokens": "261351", "train_nsentences": "1750.04", "train_wps": "63694", "train_ups": "0.24", "train_wpb": "261351", "train_bsz": "1750", "train_num_updates": "21443", "train_lr": "0.000335047", "train_gnorm": "0.585", "train_loss_scale": "2", "train_train_wall": "49", "train_gb_free": "40.1", "train_wall": "22672"}
[2024-10-06 22:39:54,542][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 22:39:54,550][fairseq.trainer][INFO] - begin training epoch 448
[2024-10-06 22:39:54,550][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 22:41:01,837][train_inner][INFO] - {"epoch": 447, "update": 446.104, "loss": "0.723", "ntokens": "260902", "nsentences": "1746.29", "wps": "48904.2", "ups": "0.19", "wpb": "260902", "bsz": "1746.3", "num_updates": "21400", "lr": "0.000334375", "gnorm": "0.468", "loss_scale": "1", "train_wall": "286", "gb_free": "40.3", "wall": "22737"}
[2024-10-06 22:41:49,219][fairseq_cli.train][INFO] - end of epoch 447 (average epoch stats below)
[2024-10-06 22:41:49,221][train][INFO] - {"epoch": 447, "train_loss": "0.726", "train_ntokens": "261351", "train_nsentences": "1750.04", "train_wps": "65774.8", "train_ups": "0.25", "train_wpb": "261351", "train_bsz": "1750", "train_num_updates": "21443", "train_lr": "0.000335047", "train_gnorm": "0.567", "train_loss_scale": "1", "train_train_wall": "50", "train_gb_free": "40.1", "train_wall": "22784"}
[2024-10-06 22:41:49,395][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 22:41:49,414][fairseq.trainer][INFO] - begin training epoch 448
[2024-10-06 22:41:49,414][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 22:43:20,233][fairseq_cli.train][INFO] - end of epoch 448 (average epoch stats below)
[2024-10-06 22:43:20,240][train][INFO] - {"epoch": 448, "train_loss": "0.718", "train_ntokens": "260383", "train_nsentences": "1750.04", "train_wps": "60714.1", "train_ups": "0.23", "train_wpb": "260383", "train_bsz": "1750", "train_num_updates": "21491", "train_lr": "0.000335797", "train_gnorm": "0.503", "train_loss_scale": "2", "train_train_wall": "46", "train_gb_free": "39.6", "train_wall": "22878"}
[2024-10-06 22:43:31,296][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 22:43:31,304][fairseq.trainer][INFO] - begin training epoch 449
[2024-10-06 22:43:31,305][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 22:45:57,071][fairseq_cli.train][INFO] - end of epoch 448 (average epoch stats below)
[2024-10-06 22:45:57,082][train][INFO] - {"epoch": 448, "train_loss": "0.718", "train_ntokens": "260383", "train_nsentences": "1750.04", "train_wps": "50425.5", "train_ups": "0.19", "train_wpb": "260383", "train_bsz": "1750", "train_num_updates": "21491", "train_lr": "0.000335797", "train_gnorm": "0.473", "train_loss_scale": "1", "train_train_wall": "43", "train_gb_free": "39.6", "train_wall": "23032"}
[2024-10-06 22:45:57,356][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 22:45:57,375][fairseq.trainer][INFO] - begin training epoch 449
[2024-10-06 22:45:57,375][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 22:47:24,164][fairseq_cli.train][INFO] - end of epoch 449 (average epoch stats below)
[2024-10-06 22:47:24,175][train][INFO] - {"epoch": 449, "train_loss": "0.717", "train_ntokens": "260677", "train_nsentences": "1750.04", "train_wps": "51295.1", "train_ups": "0.2", "train_wpb": "260677", "train_bsz": "1750", "train_num_updates": "21539", "train_lr": "0.000336547", "train_gnorm": "0.538", "train_loss_scale": "2", "train_train_wall": "66", "train_gb_free": "39.7", "train_wall": "23122"}
[2024-10-06 22:47:24,321][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 22:47:24,324][fairseq.trainer][INFO] - begin training epoch 450
[2024-10-06 22:47:24,325][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 22:49:15,703][fairseq_cli.train][INFO] - end of epoch 449 (average epoch stats below)
[2024-10-06 22:49:15,711][train][INFO] - {"epoch": 449, "train_loss": "0.717", "train_ntokens": "260677", "train_nsentences": "1750.04", "train_wps": "62997.1", "train_ups": "0.24", "train_wpb": "260677", "train_bsz": "1750", "train_num_updates": "21539", "train_lr": "0.000336547", "train_gnorm": "0.576", "train_loss_scale": "1", "train_train_wall": "68", "train_gb_free": "39.7", "train_wall": "23231"}
[2024-10-06 22:49:15,816][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 22:49:15,830][fairseq.trainer][INFO] - begin training epoch 450
[2024-10-06 22:49:15,830][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 22:50:46,737][fairseq_cli.train][INFO] - end of epoch 450 (average epoch stats below)
[2024-10-06 22:50:46,755][train][INFO] - {"epoch": 450, "train_loss": "0.716", "train_ntokens": "260821", "train_nsentences": "1750.04", "train_wps": "61801.2", "train_ups": "0.24", "train_wpb": "260820", "train_bsz": "1750", "train_num_updates": "21587", "train_lr": "0.000337297", "train_gnorm": "0.514", "train_loss_scale": "2", "train_train_wall": "62", "train_gb_free": "40.2", "train_wall": "23325"}
[2024-10-06 22:51:00,711][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 22:51:00,722][fairseq.trainer][INFO] - begin training epoch 451
[2024-10-06 22:51:00,723][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 22:52:55,221][fairseq_cli.train][INFO] - end of epoch 450 (average epoch stats below)
[2024-10-06 22:52:55,227][train][INFO] - {"epoch": 450, "train_loss": "0.715", "train_ntokens": "260821", "train_nsentences": "1750.04", "train_wps": "57032.4", "train_ups": "0.22", "train_wpb": "260820", "train_bsz": "1750", "train_num_updates": "21587", "train_lr": "0.000337297", "train_gnorm": "0.455", "train_loss_scale": "1", "train_train_wall": "42", "train_gb_free": "40.2", "train_wall": "23450"}
[2024-10-06 22:52:55,308][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 22:52:55,311][fairseq.trainer][INFO] - begin training epoch 451
[2024-10-06 22:52:55,312][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 22:53:48,877][train_inner][INFO] - {"epoch": 451, "update": 450.271, "loss": "0.718", "ntokens": "260649", "nsentences": "1748.96", "wps": "58894.8", "ups": "0.23", "wpb": "260649", "bsz": "1749", "num_updates": "21600", "lr": "0.0003375", "gnorm": "0.531", "loss_scale": "2", "train_wall": "243", "gb_free": "39.3", "wall": "23507"}
[2024-10-06 22:54:30,912][fairseq_cli.train][INFO] - end of epoch 451 (average epoch stats below)
[2024-10-06 22:54:30,918][train][INFO] - {"epoch": 451, "train_loss": "0.717", "train_ntokens": "260401", "train_nsentences": "1750.04", "train_wps": "55761.1", "train_ups": "0.21", "train_wpb": "260401", "train_bsz": "1750", "train_num_updates": "21635", "train_lr": "0.000338047", "train_gnorm": "0.529", "train_loss_scale": "2", "train_train_wall": "74", "train_gb_free": "39.2", "train_wall": "23549"}
[2024-10-06 22:54:30,992][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 22:54:30,995][fairseq.trainer][INFO] - begin training epoch 452
[2024-10-06 22:54:30,996][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 22:55:38,989][train_inner][INFO] - {"epoch": 451, "update": 450.271, "loss": "0.718", "ntokens": "260649", "nsentences": "1748.96", "wps": "59431.6", "ups": "0.23", "wpb": "260649", "bsz": "1749", "num_updates": "21600", "lr": "0.0003375", "gnorm": "0.519", "loss_scale": "1", "train_wall": "225", "gb_free": "39.3", "wall": "23614"}
[2024-10-06 22:56:16,856][fairseq_cli.train][INFO] - end of epoch 451 (average epoch stats below)
[2024-10-06 22:56:16,858][train][INFO] - {"epoch": 451, "train_loss": "0.717", "train_ntokens": "260401", "train_nsentences": "1750.04", "train_wps": "61991.6", "train_ups": "0.24", "train_wpb": "260401", "train_bsz": "1750", "train_num_updates": "21635", "train_lr": "0.000338047", "train_gnorm": "0.51", "train_loss_scale": "1", "train_train_wall": "77", "train_gb_free": "39.2", "train_wall": "23652"}
[2024-10-06 22:56:17,149][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 22:56:17,166][fairseq.trainer][INFO] - begin training epoch 452
[2024-10-06 22:56:17,166][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 22:57:54,430][fairseq_cli.train][INFO] - end of epoch 452 (average epoch stats below)
[2024-10-06 22:57:54,441][train][INFO] - {"epoch": 452, "train_loss": "0.719", "train_ntokens": "260490", "train_nsentences": "1750.04", "train_wps": "61436.6", "train_ups": "0.24", "train_wpb": "260490", "train_bsz": "1750", "train_num_updates": "21683", "train_lr": "0.000338797", "train_gnorm": "0.451", "train_loss_scale": "2", "train_train_wall": "78", "train_gb_free": "40.6", "train_wall": "23752"}
[2024-10-06 22:57:54,608][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 22:57:54,613][fairseq.trainer][INFO] - begin training epoch 453
[2024-10-06 22:57:54,614][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 22:59:32,739][fairseq_cli.train][INFO] - end of epoch 452 (average epoch stats below)
[2024-10-06 22:59:32,775][train][INFO] - {"epoch": 452, "train_loss": "0.72", "train_ntokens": "260490", "train_nsentences": "1750.04", "train_wps": "63823.6", "train_ups": "0.25", "train_wpb": "260490", "train_bsz": "1750", "train_num_updates": "21683", "train_lr": "0.000338797", "train_gnorm": "0.476", "train_loss_scale": "1", "train_train_wall": "54", "train_gb_free": "40.6", "train_wall": "23848"}
[2024-10-06 22:59:32,899][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 22:59:32,904][fairseq.trainer][INFO] - begin training epoch 453
[2024-10-06 22:59:32,908][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 23:01:12,367][fairseq_cli.train][INFO] - end of epoch 453 (average epoch stats below)
[2024-10-06 23:01:12,370][train][INFO] - {"epoch": 453, "train_loss": "0.713", "train_ntokens": "260694", "train_nsentences": "1750.04", "train_wps": "63227.9", "train_ups": "0.24", "train_wpb": "260694", "train_bsz": "1750", "train_num_updates": "21731", "train_lr": "0.000339547", "train_gnorm": "0.463", "train_loss_scale": "2", "train_train_wall": "47", "train_gb_free": "39.3", "train_wall": "23950"}
[2024-10-06 23:01:15,799][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 23:01:15,805][fairseq.trainer][INFO] - begin training epoch 454
[2024-10-06 23:01:15,805][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 23:03:56,658][fairseq_cli.train][INFO] - end of epoch 453 (average epoch stats below)
[2024-10-06 23:03:56,665][train][INFO] - {"epoch": 453, "train_loss": "0.713", "train_ntokens": "260694", "train_nsentences": "1750.04", "train_wps": "47419.9", "train_ups": "0.18", "train_wpb": "260694", "train_bsz": "1750", "train_num_updates": "21731", "train_lr": "0.000339547", "train_gnorm": "0.454", "train_loss_scale": "1", "train_train_wall": "57", "train_gb_free": "39.3", "train_wall": "24111"}
[2024-10-06 23:03:56,855][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 23:03:56,860][fairseq.trainer][INFO] - begin training epoch 454
[2024-10-06 23:03:56,861][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 23:05:34,835][fairseq_cli.train][INFO] - end of epoch 454 (average epoch stats below)
[2024-10-06 23:05:34,842][train][INFO] - {"epoch": 454, "train_loss": "0.719", "train_ntokens": "260490", "train_nsentences": "1750.04", "train_wps": "47638.6", "train_ups": "0.18", "train_wpb": "260490", "train_bsz": "1750", "train_num_updates": "21779", "train_lr": "0.000340297", "train_gnorm": "0.503", "train_loss_scale": "2", "train_train_wall": "65", "train_gb_free": "39.3", "train_wall": "24213"}
[2024-10-06 23:05:34,949][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 23:05:34,952][fairseq.trainer][INFO] - begin training epoch 455
[2024-10-06 23:05:34,953][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 23:07:05,431][fairseq_cli.train][INFO] - end of epoch 454 (average epoch stats below)
[2024-10-06 23:07:05,449][train][INFO] - {"epoch": 454, "train_loss": "0.719", "train_ntokens": "260490", "train_nsentences": "1750.04", "train_wps": "66237.4", "train_ups": "0.25", "train_wpb": "260490", "train_bsz": "1750", "train_num_updates": "21779", "train_lr": "0.000340297", "train_gnorm": "0.479", "train_loss_scale": "1", "train_train_wall": "58", "train_gb_free": "39.3", "train_wall": "24300"}
[2024-10-06 23:07:05,572][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 23:07:05,575][fairseq.trainer][INFO] - begin training epoch 455
[2024-10-06 23:07:05,576][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 23:08:32,581][train_inner][INFO] - {"epoch": 455, "update": 454.438, "loss": "0.718", "ntokens": "260531", "nsentences": "1762.18", "wps": "58963.7", "ups": "0.23", "wpb": "260531", "bsz": "1762.2", "num_updates": "21800", "lr": "0.000340625", "gnorm": "0.485", "loss_scale": "2", "train_wall": "266", "gb_free": "40.3", "wall": "24391"}
[2024-10-06 23:09:01,066][fairseq_cli.train][INFO] - end of epoch 455 (average epoch stats below)
[2024-10-06 23:09:01,069][train][INFO] - {"epoch": 455, "train_loss": "0.722", "train_ntokens": "260412", "train_nsentences": "1750.04", "train_wps": "60613.5", "train_ups": "0.23", "train_wpb": "260412", "train_bsz": "1750", "train_num_updates": "21827", "train_lr": "0.000341047", "train_gnorm": "0.539", "train_loss_scale": "2", "train_train_wall": "63", "train_gb_free": "39.3", "train_wall": "24419"}
[2024-10-06 23:09:01,153][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 23:09:01,156][fairseq.trainer][INFO] - begin training epoch 456
[2024-10-06 23:09:01,156][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 23:10:02,166][train_inner][INFO] - {"epoch": 455, "update": 454.438, "loss": "0.718", "ntokens": "260531", "nsentences": "1762.18", "wps": "60366.1", "ups": "0.23", "wpb": "260531", "bsz": "1762.2", "num_updates": "21800", "lr": "0.000340625", "gnorm": "0.485", "loss_scale": "1", "train_wall": "241", "gb_free": "40.3", "wall": "24477"}
[2024-10-06 23:10:33,743][fairseq_cli.train][INFO] - end of epoch 455 (average epoch stats below)
[2024-10-06 23:10:33,745][train][INFO] - {"epoch": 455, "train_loss": "0.723", "train_ntokens": "260412", "train_nsentences": "1750.04", "train_wps": "60011.2", "train_ups": "0.23", "train_wpb": "260412", "train_bsz": "1750", "train_num_updates": "21827", "train_lr": "0.000341047", "train_gnorm": "0.6", "train_loss_scale": "1", "train_train_wall": "66", "train_gb_free": "39.3", "train_wall": "24509"}
[2024-10-06 23:10:33,882][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 23:10:33,886][fairseq.trainer][INFO] - begin training epoch 456
[2024-10-06 23:10:33,889][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 23:12:19,652][fairseq_cli.train][INFO] - end of epoch 456 (average epoch stats below)
[2024-10-06 23:12:19,656][train][INFO] - {"epoch": 456, "train_loss": "0.714", "train_ntokens": "260809", "train_nsentences": "1750.04", "train_wps": "63040.5", "train_ups": "0.24", "train_wpb": "260809", "train_bsz": "1750", "train_num_updates": "21875", "train_lr": "0.000341797", "train_gnorm": "0.399", "train_loss_scale": "2", "train_train_wall": "76", "train_gb_free": "39.6", "train_wall": "24618"}
[2024-10-06 23:12:19,752][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 23:12:19,756][fairseq.trainer][INFO] - begin training epoch 457
[2024-10-06 23:12:19,757][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 23:14:01,676][fairseq_cli.train][INFO] - end of epoch 456 (average epoch stats below)
[2024-10-06 23:14:01,679][train][INFO] - {"epoch": 456, "train_loss": "0.714", "train_ntokens": "260809", "train_nsentences": "1750.04", "train_wps": "60206.7", "train_ups": "0.23", "train_wpb": "260809", "train_bsz": "1750", "train_num_updates": "21875", "train_lr": "0.000341797", "train_gnorm": "0.513", "train_loss_scale": "1", "train_train_wall": "78", "train_gb_free": "39.6", "train_wall": "24716"}
[2024-10-06 23:14:01,766][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 23:14:01,772][fairseq.trainer][INFO] - begin training epoch 457
[2024-10-06 23:14:01,772][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 23:15:37,845][fairseq_cli.train][INFO] - end of epoch 457 (average epoch stats below)
[2024-10-06 23:15:37,849][train][INFO] - {"epoch": 457, "train_loss": "0.717", "train_ntokens": "260874", "train_nsentences": "1750.04", "train_wps": "63181.9", "train_ups": "0.24", "train_wpb": "260874", "train_bsz": "1750", "train_num_updates": "21923", "train_lr": "0.000342547", "train_gnorm": "0.466", "train_loss_scale": "2", "train_train_wall": "64", "train_gb_free": "39.2", "train_wall": "24816"}
[2024-10-06 23:15:38,060][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 23:15:38,072][fairseq.trainer][INFO] - begin training epoch 458
[2024-10-06 23:15:38,073][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 23:17:25,100][fairseq_cli.train][INFO] - end of epoch 457 (average epoch stats below)
[2024-10-06 23:17:25,107][train][INFO] - {"epoch": 457, "train_loss": "0.717", "train_ntokens": "260874", "train_nsentences": "1750.04", "train_wps": "61556", "train_ups": "0.24", "train_wpb": "260874", "train_bsz": "1750", "train_num_updates": "21923", "train_lr": "0.000342547", "train_gnorm": "0.493", "train_loss_scale": "1", "train_train_wall": "64", "train_gb_free": "39.2", "train_wall": "24920"}
[2024-10-06 23:17:25,267][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 23:17:25,270][fairseq.trainer][INFO] - begin training epoch 458
[2024-10-06 23:17:25,270][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 23:19:05,596][fairseq_cli.train][INFO] - end of epoch 458 (average epoch stats below)
[2024-10-06 23:19:05,603][train][INFO] - {"epoch": 458, "train_loss": "0.713", "train_ntokens": "260804", "train_nsentences": "1750.04", "train_wps": "60257.6", "train_ups": "0.23", "train_wpb": "260804", "train_bsz": "1750", "train_num_updates": "21971", "train_lr": "0.000343297", "train_gnorm": "0.592", "train_loss_scale": "2", "train_train_wall": "51", "train_gb_free": "40.2", "train_wall": "25024"}
[2024-10-06 23:19:05,680][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 23:19:05,683][fairseq.trainer][INFO] - begin training epoch 459
[2024-10-06 23:19:05,684][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 23:21:18,195][fairseq_cli.train][INFO] - end of epoch 458 (average epoch stats below)
[2024-10-06 23:21:18,209][train][INFO] - {"epoch": 458, "train_loss": "0.712", "train_ntokens": "260804", "train_nsentences": "1750.04", "train_wps": "53706.4", "train_ups": "0.21", "train_wpb": "260804", "train_bsz": "1750", "train_num_updates": "21971", "train_lr": "0.000343297", "train_gnorm": "0.509", "train_loss_scale": "1", "train_train_wall": "74", "train_gb_free": "40.2", "train_wall": "25153"}
[2024-10-06 23:21:18,444][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 23:21:18,462][fairseq.trainer][INFO] - begin training epoch 459
[2024-10-06 23:21:18,462][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 23:22:10,835][train_inner][INFO] - {"epoch": 459, "update": 458.604, "loss": "0.716", "ntokens": "260888", "nsentences": "1742.96", "wps": "63767.6", "ups": "0.24", "wpb": "260888", "bsz": "1743", "num_updates": "22000", "lr": "0.00034375", "gnorm": "0.496", "loss_scale": "2", "train_wall": "265", "gb_free": "39.3", "wall": "25209"}
[2024-10-06 23:22:33,249][fairseq_cli.train][INFO] - end of epoch 459 (average epoch stats below)
[2024-10-06 23:22:33,253][train][INFO] - {"epoch": 459, "train_loss": "0.715", "train_ntokens": "260898", "train_nsentences": "1750.04", "train_wps": "60309.9", "train_ups": "0.23", "train_wpb": "260898", "train_bsz": "1750", "train_num_updates": "22019", "train_lr": "0.000344047", "train_gnorm": "0.426", "train_loss_scale": "2", "train_train_wall": "68", "train_gb_free": "39.3", "train_wall": "25231"}
[2024-10-06 23:22:33,332][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 23:22:33,335][fairseq.trainer][INFO] - begin training epoch 460
[2024-10-06 23:22:33,336][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 23:24:14,740][train_inner][INFO] - {"epoch": 459, "update": 458.604, "loss": "0.716", "ntokens": "260888", "nsentences": "1742.96", "wps": "61200.6", "ups": "0.23", "wpb": "260888", "bsz": "1743", "num_updates": "22000", "lr": "0.00034375", "gnorm": "0.532", "loss_scale": "1", "train_wall": "298", "gb_free": "39.3", "wall": "25330"}
[2024-10-06 23:24:55,478][fairseq_cli.train][INFO] - end of epoch 459 (average epoch stats below)
[2024-10-06 23:24:55,483][train][INFO] - {"epoch": 459, "train_loss": "0.716", "train_ntokens": "260898", "train_nsentences": "1750.04", "train_wps": "57638.9", "train_ups": "0.22", "train_wpb": "260898", "train_bsz": "1750", "train_num_updates": "22019", "train_lr": "0.000344047", "train_gnorm": "0.516", "train_loss_scale": "1", "train_train_wall": "91", "train_gb_free": "39.3", "train_wall": "25370"}
[2024-10-06 23:24:55,629][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 23:24:55,635][fairseq.trainer][INFO] - begin training epoch 460
[2024-10-06 23:24:55,636][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 23:25:48,033][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 460 @ 22067 updates
[2024-10-06 23:25:48,034][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-06 23:25:51,688][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-06 23:25:51,692][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 460 @ 22067 updates, score None) (writing took 3.6587535431608558 seconds)
[2024-10-06 23:25:51,692][fairseq_cli.train][INFO] - end of epoch 460 (average epoch stats below)
[2024-10-06 23:25:51,699][train][INFO] - {"epoch": 460, "train_loss": "0.714", "train_ntokens": "260512", "train_nsentences": "1750.04", "train_wps": "63015.1", "train_ups": "0.24", "train_wpb": "260512", "train_bsz": "1750", "train_num_updates": "22067", "train_lr": "0.000344797", "train_gnorm": "0.477", "train_loss_scale": "2", "train_train_wall": "65", "train_gb_free": "39.2", "train_wall": "25430"}
[2024-10-06 23:25:51,758][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 23:25:51,762][fairseq.trainer][INFO] - begin training epoch 461
[2024-10-06 23:25:51,762][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 23:28:01,440][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 460 @ 22067 updates
[2024-10-06 23:28:01,442][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-06 23:28:06,505][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt
[2024-10-06 23:28:06,507][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_random_400_10percent/ckpt/checkpoint_last.pt (epoch 460 @ 22067 updates, score None) (writing took 5.067624108865857 seconds)
[2024-10-06 23:28:06,508][fairseq_cli.train][INFO] - end of epoch 460 (average epoch stats below)
[2024-10-06 23:28:06,523][train][INFO] - {"epoch": 460, "train_loss": "0.714", "train_ntokens": "260512", "train_nsentences": "1750.04", "train_wps": "65460.9", "train_ups": "0.25", "train_wpb": "260512", "train_bsz": "1750", "train_num_updates": "22067", "train_lr": "0.000344797", "train_gnorm": "0.452", "train_loss_scale": "1", "train_train_wall": "85", "train_gb_free": "39.2", "train_wall": "25561"}
[2024-10-06 23:28:06,640][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 23:28:06,647][fairseq.trainer][INFO] - begin training epoch 461
[2024-10-06 23:28:06,647][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 23:29:11,188][fairseq_cli.train][INFO] - end of epoch 461 (average epoch stats below)
[2024-10-06 23:29:11,196][train][INFO] - {"epoch": 461, "train_loss": "0.714", "train_ntokens": "260851", "train_nsentences": "1750.04", "train_wps": "62764.6", "train_ups": "0.24", "train_wpb": "260851", "train_bsz": "1750", "train_num_updates": "22115", "train_lr": "0.000345547", "train_gnorm": "0.461", "train_loss_scale": "2", "train_train_wall": "67", "train_gb_free": "39.6", "train_wall": "25629"}
[2024-10-06 23:29:11,379][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 23:29:11,396][fairseq.trainer][INFO] - begin training epoch 462
[2024-10-06 23:29:11,397][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 23:31:21,129][fairseq_cli.train][INFO] - end of epoch 461 (average epoch stats below)
[2024-10-06 23:31:21,149][train][INFO] - {"epoch": 461, "train_loss": "0.714", "train_ntokens": "260851", "train_nsentences": "1750.04", "train_wps": "64336.7", "train_ups": "0.25", "train_wpb": "260851", "train_bsz": "1750", "train_num_updates": "22115", "train_lr": "0.000345547", "train_gnorm": "0.431", "train_loss_scale": "1", "train_train_wall": "83", "train_gb_free": "39.6", "train_wall": "25756"}
[2024-10-06 23:31:21,286][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 23:31:21,290][fairseq.trainer][INFO] - begin training epoch 462
[2024-10-06 23:31:21,292][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 23:32:29,813][fairseq_cli.train][INFO] - end of epoch 462 (average epoch stats below)
[2024-10-06 23:32:29,818][train][INFO] - {"epoch": 462, "train_loss": "0.711", "train_ntokens": "260605", "train_nsentences": "1750.04", "train_wps": "62981", "train_ups": "0.24", "train_wpb": "260605", "train_bsz": "1750", "train_num_updates": "22163", "train_lr": "0.000346297", "train_gnorm": "0.592", "train_loss_scale": "2", "train_train_wall": "70", "train_gb_free": "39.3", "train_wall": "25828"}
[2024-10-06 23:32:29,885][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 23:32:29,888][fairseq.trainer][INFO] - begin training epoch 463
[2024-10-06 23:32:29,889][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 23:34:56,538][fairseq_cli.train][INFO] - end of epoch 462 (average epoch stats below)
[2024-10-06 23:34:56,548][train][INFO] - {"epoch": 462, "train_loss": "0.71", "train_ntokens": "260605", "train_nsentences": "1750.04", "train_wps": "58074.5", "train_ups": "0.22", "train_wpb": "260605", "train_bsz": "1750", "train_num_updates": "22163", "train_lr": "0.000346297", "train_gnorm": "0.526", "train_loss_scale": "1", "train_train_wall": "94", "train_gb_free": "39.3", "train_wall": "25971"}
[2024-10-06 23:34:56,714][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 23:34:56,724][fairseq.trainer][INFO] - begin training epoch 463
[2024-10-06 23:34:56,725][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 23:35:34,705][train_inner][INFO] - {"epoch": 463, "update": 462.771, "loss": "0.712", "ntokens": "260511", "nsentences": "1752.35", "wps": "64814.8", "ups": "0.25", "wpb": "260511", "bsz": "1752.3", "num_updates": "22200", "lr": "0.000346875", "gnorm": "0.496", "loss_scale": "2", "train_wall": "260", "gb_free": "39.6", "wall": "26013"}
[2024-10-06 23:35:49,387][fairseq_cli.train][INFO] - end of epoch 463 (average epoch stats below)
[2024-10-06 23:35:49,390][train][INFO] - {"epoch": 463, "train_loss": "0.709", "train_ntokens": "260403", "train_nsentences": "1750.04", "train_wps": "62632.1", "train_ups": "0.24", "train_wpb": "260403", "train_bsz": "1750", "train_num_updates": "22211", "train_lr": "0.000347047", "train_gnorm": "0.473", "train_loss_scale": "2", "train_train_wall": "50", "train_gb_free": "39.6", "train_wall": "26027"}
[2024-10-06 23:35:49,528][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 23:35:49,532][fairseq.trainer][INFO] - begin training epoch 464
[2024-10-06 23:35:49,532][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 23:37:43,381][train_inner][INFO] - {"epoch": 463, "update": 462.771, "loss": "0.712", "ntokens": "260511", "nsentences": "1752.35", "wps": "64433.6", "ups": "0.25", "wpb": "260511", "bsz": "1752.3", "num_updates": "22200", "lr": "0.000346875", "gnorm": "0.484", "loss_scale": "1", "train_wall": "338", "gb_free": "39.6", "wall": "26138"}
[2024-10-06 23:38:05,451][fairseq_cli.train][INFO] - end of epoch 463 (average epoch stats below)
[2024-10-06 23:38:05,467][train][INFO] - {"epoch": 463, "train_loss": "0.71", "train_ntokens": "260403", "train_nsentences": "1750.04", "train_wps": "66168.7", "train_ups": "0.25", "train_wpb": "260403", "train_bsz": "1750", "train_num_updates": "22211", "train_lr": "0.000347047", "train_gnorm": "0.532", "train_loss_scale": "1", "train_train_wall": "56", "train_gb_free": "39.6", "train_wall": "26160"}
[2024-10-06 23:38:05,737][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 23:38:05,748][fairseq.trainer][INFO] - begin training epoch 464
[2024-10-06 23:38:05,749][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 23:39:08,843][fairseq_cli.train][INFO] - end of epoch 464 (average epoch stats below)
[2024-10-06 23:39:08,864][train][INFO] - {"epoch": 464, "train_loss": "0.709", "train_ntokens": "260361", "train_nsentences": "1750.04", "train_wps": "62657.4", "train_ups": "0.24", "train_wpb": "260361", "train_bsz": "1750", "train_num_updates": "22259", "train_lr": "0.000347797", "train_gnorm": "0.463", "train_loss_scale": "2", "train_train_wall": "80", "train_gb_free": "40.2", "train_wall": "26227"}
[2024-10-06 23:39:09,005][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 23:39:09,014][fairseq.trainer][INFO] - begin training epoch 465
[2024-10-06 23:39:09,014][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 23:41:19,249][fairseq_cli.train][INFO] - end of epoch 464 (average epoch stats below)
[2024-10-06 23:41:19,260][train][INFO] - {"epoch": 464, "train_loss": "0.709", "train_ntokens": "260361", "train_nsentences": "1750.04", "train_wps": "64491.4", "train_ups": "0.25", "train_wpb": "260361", "train_bsz": "1750", "train_num_updates": "22259", "train_lr": "0.000347797", "train_gnorm": "0.501", "train_loss_scale": "1", "train_train_wall": "88", "train_gb_free": "40.2", "train_wall": "26354"}
[2024-10-06 23:41:19,541][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 23:41:19,548][fairseq.trainer][INFO] - begin training epoch 465
[2024-10-06 23:41:19,549][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 23:42:17,223][fairseq_cli.train][INFO] - end of epoch 465 (average epoch stats below)
[2024-10-06 23:42:17,229][train][INFO] - {"epoch": 465, "train_loss": "0.713", "train_ntokens": "260863", "train_nsentences": "1750.04", "train_wps": "66476.9", "train_ups": "0.25", "train_wpb": "260863", "train_bsz": "1750", "train_num_updates": "22307", "train_lr": "0.000348547", "train_gnorm": "0.464", "train_loss_scale": "2", "train_train_wall": "52", "train_gb_free": "40.1", "train_wall": "26415"}
[2024-10-06 23:42:17,487][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 23:42:17,497][fairseq.trainer][INFO] - begin training epoch 466
[2024-10-06 23:42:17,498][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 23:44:47,409][fairseq_cli.train][INFO] - end of epoch 465 (average epoch stats below)
[2024-10-06 23:44:47,413][train][INFO] - {"epoch": 465, "train_loss": "0.712", "train_ntokens": "260863", "train_nsentences": "1750.04", "train_wps": "60156.2", "train_ups": "0.23", "train_wpb": "260863", "train_bsz": "1750", "train_num_updates": "22307", "train_lr": "0.000348547", "train_gnorm": "0.488", "train_loss_scale": "1", "train_train_wall": "100", "train_gb_free": "40.1", "train_wall": "26562"}
[2024-10-06 23:44:47,516][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 23:44:47,522][fairseq.trainer][INFO] - begin training epoch 466
[2024-10-06 23:44:47,523][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 23:45:36,347][fairseq_cli.train][INFO] - end of epoch 466 (average epoch stats below)
[2024-10-06 23:45:36,351][train][INFO] - {"epoch": 466, "train_loss": "0.704", "train_ntokens": "260730", "train_nsentences": "1750.04", "train_wps": "62855.3", "train_ups": "0.24", "train_wpb": "260730", "train_bsz": "1750", "train_num_updates": "22355", "train_lr": "0.000349297", "train_gnorm": "0.538", "train_loss_scale": "2", "train_train_wall": "80", "train_gb_free": "39.6", "train_wall": "26614"}
[2024-10-06 23:45:36,411][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 23:45:36,416][fairseq.trainer][INFO] - begin training epoch 467
[2024-10-06 23:45:36,417][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 23:47:52,274][fairseq_cli.train][INFO] - end of epoch 466 (average epoch stats below)
[2024-10-06 23:47:52,303][train][INFO] - {"epoch": 466, "train_loss": "0.704", "train_ntokens": "260730", "train_nsentences": "1750.04", "train_wps": "67696.8", "train_ups": "0.26", "train_wpb": "260730", "train_bsz": "1750", "train_num_updates": "22355", "train_lr": "0.000349297", "train_gnorm": "0.494", "train_loss_scale": "1", "train_train_wall": "83", "train_gb_free": "39.6", "train_wall": "26747"}
[2024-10-06 23:47:52,433][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 23:47:52,436][fairseq.trainer][INFO] - begin training epoch 467
[2024-10-06 23:47:52,437][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 23:48:45,003][train_inner][INFO] - {"epoch": 467, "update": 466.938, "loss": "0.708", "ntokens": "260780", "nsentences": "1747.38", "wps": "66007.6", "ups": "0.25", "wpb": "260780", "bsz": "1747.4", "num_updates": "22400", "lr": "0.00035", "gnorm": "0.493", "loss_scale": "2", "train_wall": "293", "gb_free": "40", "wall": "26803"}
[2024-10-06 23:48:46,088][fairseq_cli.train][INFO] - end of epoch 467 (average epoch stats below)
[2024-10-06 23:48:46,107][train][INFO] - {"epoch": 467, "train_loss": "0.704", "train_ntokens": "261100", "train_nsentences": "1750.04", "train_wps": "66053.2", "train_ups": "0.25", "train_wpb": "261100", "train_bsz": "1750", "train_num_updates": "22403", "train_lr": "0.000350047", "train_gnorm": "0.525", "train_loss_scale": "2", "train_train_wall": "68", "train_gb_free": "40.1", "train_wall": "26804"}
[2024-10-06 23:48:46,198][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 23:48:46,210][fairseq.trainer][INFO] - begin training epoch 468
[2024-10-06 23:48:46,211][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 23:51:01,899][train_inner][INFO] - {"epoch": 467, "update": 466.938, "loss": "0.708", "ntokens": "260780", "nsentences": "1747.38", "wps": "65318.2", "ups": "0.25", "wpb": "260780", "bsz": "1747.4", "num_updates": "22400", "lr": "0.00035", "gnorm": "0.481", "loss_scale": "1", "train_wall": "358", "gb_free": "40", "wall": "26937"}
[2024-10-06 23:51:03,693][fairseq_cli.train][INFO] - end of epoch 467 (average epoch stats below)
[2024-10-06 23:51:03,708][train][INFO] - {"epoch": 467, "train_loss": "0.705", "train_ntokens": "261100", "train_nsentences": "1750.04", "train_wps": "65484.2", "train_ups": "0.25", "train_wpb": "261100", "train_bsz": "1750", "train_num_updates": "22403", "train_lr": "0.000350047", "train_gnorm": "0.456", "train_loss_scale": "1", "train_train_wall": "68", "train_gb_free": "40.1", "train_wall": "26939"}
[2024-10-06 23:51:03,851][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 23:51:03,854][fairseq.trainer][INFO] - begin training epoch 468
[2024-10-06 23:51:03,855][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 23:52:01,654][fairseq_cli.train][INFO] - end of epoch 468 (average epoch stats below)
[2024-10-06 23:52:01,659][train][INFO] - {"epoch": 468, "train_loss": "0.707", "train_ntokens": "260589", "train_nsentences": "1750.04", "train_wps": "63965.2", "train_ups": "0.25", "train_wpb": "260589", "train_bsz": "1750", "train_num_updates": "22451", "train_lr": "0.000350797", "train_gnorm": "0.422", "train_loss_scale": "2", "train_train_wall": "67", "train_gb_free": "39.6", "train_wall": "27000"}
[2024-10-06 23:52:01,782][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 23:52:01,787][fairseq.trainer][INFO] - begin training epoch 469
[2024-10-06 23:52:01,787][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 23:54:30,866][fairseq_cli.train][INFO] - end of epoch 468 (average epoch stats below)
[2024-10-06 23:54:30,876][train][INFO] - {"epoch": 468, "train_loss": "0.707", "train_ntokens": "260589", "train_nsentences": "1750.04", "train_wps": "60379.3", "train_ups": "0.23", "train_wpb": "260589", "train_bsz": "1750", "train_num_updates": "22451", "train_lr": "0.000350797", "train_gnorm": "0.465", "train_loss_scale": "2", "train_train_wall": "89", "train_gb_free": "39.6", "train_wall": "27146"}
[2024-10-06 23:54:31,005][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 23:54:31,011][fairseq.trainer][INFO] - begin training epoch 469
[2024-10-06 23:54:31,011][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 23:55:22,775][fairseq_cli.train][INFO] - end of epoch 469 (average epoch stats below)
[2024-10-06 23:55:22,778][train][INFO] - {"epoch": 469, "train_loss": "0.708", "train_ntokens": "260907", "train_nsentences": "1750.04", "train_wps": "62272.5", "train_ups": "0.24", "train_wpb": "260907", "train_bsz": "1750", "train_num_updates": "22499", "train_lr": "0.000351547", "train_gnorm": "0.521", "train_loss_scale": "2", "train_train_wall": "54", "train_gb_free": "39.8", "train_wall": "27201"}
[2024-10-06 23:55:22,891][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 23:55:22,894][fairseq.trainer][INFO] - begin training epoch 470
[2024-10-06 23:55:22,895][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 23:57:40,554][fairseq_cli.train][INFO] - end of epoch 469 (average epoch stats below)
[2024-10-06 23:57:40,578][train][INFO] - {"epoch": 469, "train_loss": "0.708", "train_ntokens": "260907", "train_nsentences": "1750.04", "train_wps": "66025.5", "train_ups": "0.25", "train_wpb": "260907", "train_bsz": "1750", "train_num_updates": "22499", "train_lr": "0.000351547", "train_gnorm": "0.453", "train_loss_scale": "2", "train_train_wall": "65", "train_gb_free": "39.8", "train_wall": "27335"}
[2024-10-06 23:57:40,821][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 23:57:40,830][fairseq.trainer][INFO] - begin training epoch 470
[2024-10-06 23:57:40,831][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 23:58:35,184][fairseq_cli.train][INFO] - end of epoch 470 (average epoch stats below)
[2024-10-06 23:58:35,187][train][INFO] - {"epoch": 470, "train_loss": "0.707", "train_ntokens": "260517", "train_nsentences": "1750.04", "train_wps": "64991.9", "train_ups": "0.25", "train_wpb": "260517", "train_bsz": "1750", "train_num_updates": "22547", "train_lr": "0.000352297", "train_gnorm": "0.465", "train_loss_scale": "2", "train_train_wall": "68", "train_gb_free": "39.9", "train_wall": "27393"}
[2024-10-06 23:58:35,266][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-06 23:58:35,272][fairseq.trainer][INFO] - begin training epoch 471
[2024-10-06 23:58:35,273][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 00:00:58,669][fairseq_cli.train][INFO] - end of epoch 470 (average epoch stats below)
[2024-10-07 00:00:58,700][train][INFO] - {"epoch": 470, "train_loss": "0.708", "train_ntokens": "260517", "train_nsentences": "1750.04", "train_wps": "63122.9", "train_ups": "0.24", "train_wpb": "260517", "train_bsz": "1750", "train_num_updates": "22547", "train_lr": "0.000352297", "train_gnorm": "0.536", "train_loss_scale": "2", "train_train_wall": "84", "train_gb_free": "39.9", "train_wall": "27534"}
[2024-10-07 00:00:59,121][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 00:00:59,128][fairseq.trainer][INFO] - begin training epoch 471
[2024-10-07 00:00:59,135][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 00:01:50,340][fairseq_cli.train][INFO] - end of epoch 471 (average epoch stats below)
[2024-10-07 00:01:50,348][train][INFO] - {"epoch": 471, "train_loss": "0.704", "train_ntokens": "260992", "train_nsentences": "1750.04", "train_wps": "64194.3", "train_ups": "0.25", "train_wpb": "260992", "train_bsz": "1750", "train_num_updates": "22595", "train_lr": "0.000353047", "train_gnorm": "0.502", "train_loss_scale": "2", "train_train_wall": "53", "train_gb_free": "39.6", "train_wall": "27588"}
[2024-10-07 00:01:50,537][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 00:01:50,556][fairseq.trainer][INFO] - begin training epoch 472
[2024-10-07 00:01:50,563][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 00:04:09,237][fairseq_cli.train][INFO] - end of epoch 471 (average epoch stats below)
[2024-10-07 00:04:09,258][train][INFO] - {"epoch": 471, "train_loss": "0.703", "train_ntokens": "260992", "train_nsentences": "1750.04", "train_wps": "65747.3", "train_ups": "0.25", "train_wpb": "260992", "train_bsz": "1750", "train_num_updates": "22595", "train_lr": "0.000353047", "train_gnorm": "0.534", "train_loss_scale": "2", "train_train_wall": "72", "train_gb_free": "39.6", "train_wall": "27724"}
[2024-10-07 00:04:09,417][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 00:04:09,420][fairseq.trainer][INFO] - begin training epoch 472
[2024-10-07 00:04:09,421][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 00:04:12,727][train_inner][INFO] - {"epoch": 472, "update": 471.104, "loss": "0.707", "ntokens": "260752", "nsentences": "1750.73", "wps": "56213.5", "ups": "0.22", "wpb": "260752", "bsz": "1750.7", "num_updates": "22600", "lr": "0.000353125", "gnorm": "0.482", "loss_scale": "2", "train_wall": "271", "gb_free": "39.7", "wall": "27731"}
[2024-10-07 00:05:03,845][fairseq_cli.train][INFO] - end of epoch 472 (average epoch stats below)
[2024-10-07 00:05:03,850][train][INFO] - {"epoch": 472, "train_loss": "0.709", "train_ntokens": "260799", "train_nsentences": "1750.04", "train_wps": "64696.7", "train_ups": "0.25", "train_wpb": "260800", "train_bsz": "1750", "train_num_updates": "22643", "train_lr": "0.000353797", "train_gnorm": "0.573", "train_loss_scale": "2", "train_train_wall": "78", "train_gb_free": "40.7", "train_wall": "27782"}
[2024-10-07 00:05:04,093][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 00:05:04,097][fairseq.trainer][INFO] - begin training epoch 473
[2024-10-07 00:05:04,097][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 00:06:15,266][train_inner][INFO] - {"epoch": 472, "update": 471.104, "loss": "0.707", "ntokens": "260752", "nsentences": "1750.73", "wps": "57097.5", "ups": "0.22", "wpb": "260752", "bsz": "1750.7", "num_updates": "22600", "lr": "0.000353125", "gnorm": "0.497", "loss_scale": "2", "train_wall": "335", "gb_free": "39.7", "wall": "27850"}
[2024-10-07 00:07:25,696][fairseq_cli.train][INFO] - end of epoch 472 (average epoch stats below)
[2024-10-07 00:07:25,708][train][INFO] - {"epoch": 472, "train_loss": "0.707", "train_ntokens": "260799", "train_nsentences": "1750.04", "train_wps": "63726.7", "train_ups": "0.24", "train_wpb": "260800", "train_bsz": "1750", "train_num_updates": "22643", "train_lr": "0.000353797", "train_gnorm": "0.519", "train_loss_scale": "2", "train_train_wall": "93", "train_gb_free": "40.7", "train_wall": "27921"}
[2024-10-07 00:07:25,797][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 00:07:25,812][fairseq.trainer][INFO] - begin training epoch 473
[2024-10-07 00:07:25,812][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 00:08:22,585][fairseq_cli.train][INFO] - end of epoch 473 (average epoch stats below)
[2024-10-07 00:08:22,595][train][INFO] - {"epoch": 473, "train_loss": "0.709", "train_ntokens": "260856", "train_nsentences": "1750.04", "train_wps": "63007.9", "train_ups": "0.24", "train_wpb": "260856", "train_bsz": "1750", "train_num_updates": "22691", "train_lr": "0.000354547", "train_gnorm": "0.419", "train_loss_scale": "2", "train_train_wall": "78", "train_gb_free": "39.2", "train_wall": "27981"}
[2024-10-07 00:08:22,718][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 00:08:22,723][fairseq.trainer][INFO] - begin training epoch 474
[2024-10-07 00:08:22,729][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 00:10:35,395][fairseq_cli.train][INFO] - end of epoch 473 (average epoch stats below)
[2024-10-07 00:10:35,407][train][INFO] - {"epoch": 473, "train_loss": "0.71", "train_ntokens": "260856", "train_nsentences": "1750.04", "train_wps": "66009", "train_ups": "0.25", "train_wpb": "260856", "train_bsz": "1750", "train_num_updates": "22691", "train_lr": "0.000354547", "train_gnorm": "0.442", "train_loss_scale": "2", "train_train_wall": "87", "train_gb_free": "39.2", "train_wall": "28110"}
[2024-10-07 00:10:35,636][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 00:10:35,644][fairseq.trainer][INFO] - begin training epoch 474
[2024-10-07 00:10:35,645][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 00:11:36,687][fairseq_cli.train][INFO] - end of epoch 474 (average epoch stats below)
[2024-10-07 00:11:36,690][train][INFO] - {"epoch": 474, "train_loss": "0.71", "train_ntokens": "260751", "train_nsentences": "1750.04", "train_wps": "64485.4", "train_ups": "0.25", "train_wpb": "260751", "train_bsz": "1750", "train_num_updates": "22739", "train_lr": "0.000355297", "train_gnorm": "0.523", "train_loss_scale": "2", "train_train_wall": "72", "train_gb_free": "39.3", "train_wall": "28175"}
[2024-10-07 00:11:36,770][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 00:11:36,774][fairseq.trainer][INFO] - begin training epoch 475
[2024-10-07 00:11:36,774][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 00:13:47,366][fairseq_cli.train][INFO] - end of epoch 474 (average epoch stats below)
[2024-10-07 00:13:47,395][train][INFO] - {"epoch": 474, "train_loss": "0.711", "train_ntokens": "260751", "train_nsentences": "1750.04", "train_wps": "65202.6", "train_ups": "0.25", "train_wpb": "260751", "train_bsz": "1750", "train_num_updates": "22739", "train_lr": "0.000355297", "train_gnorm": "0.536", "train_loss_scale": "2", "train_train_wall": "79", "train_gb_free": "39.3", "train_wall": "28302"}
[2024-10-07 00:13:47,479][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 00:13:47,484][fairseq.trainer][INFO] - begin training epoch 475
[2024-10-07 00:13:47,485][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 00:14:53,603][fairseq_cli.train][INFO] - end of epoch 475 (average epoch stats below)
[2024-10-07 00:14:53,612][train][INFO] - {"epoch": 475, "train_loss": "0.709", "train_ntokens": "260974", "train_nsentences": "1750.04", "train_wps": "63614.8", "train_ups": "0.24", "train_wpb": "260974", "train_bsz": "1750", "train_num_updates": "22787", "train_lr": "0.000356047", "train_gnorm": "0.465", "train_loss_scale": "2", "train_train_wall": "77", "train_gb_free": "39.6", "train_wall": "28372"}
[2024-10-07 00:14:53,685][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 00:14:53,689][fairseq.trainer][INFO] - begin training epoch 476
[2024-10-07 00:14:53,690][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 00:17:03,194][fairseq_cli.train][INFO] - end of epoch 475 (average epoch stats below)
[2024-10-07 00:17:03,220][train][INFO] - {"epoch": 475, "train_loss": "0.709", "train_ntokens": "260974", "train_nsentences": "1750.04", "train_wps": "63973.5", "train_ups": "0.25", "train_wpb": "260974", "train_bsz": "1750", "train_num_updates": "22787", "train_lr": "0.000356047", "train_gnorm": "0.517", "train_loss_scale": "2", "train_train_wall": "91", "train_gb_free": "39.6", "train_wall": "28498"}
[2024-10-07 00:17:03,397][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 00:17:03,400][fairseq.trainer][INFO] - begin training epoch 476
[2024-10-07 00:17:03,402][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 00:17:21,418][train_inner][INFO] - {"epoch": 476, "update": 475.271, "loss": "0.708", "ntokens": "260919", "nsentences": "1742.6", "wps": "66165.4", "ups": "0.25", "wpb": "260919", "bsz": "1742.6", "num_updates": "22800", "lr": "0.00035625", "gnorm": "0.494", "loss_scale": "2", "train_wall": "301", "gb_free": "40.3", "wall": "28519"}
[2024-10-07 00:18:09,837][fairseq_cli.train][INFO] - end of epoch 476 (average epoch stats below)
[2024-10-07 00:18:09,840][train][INFO] - {"epoch": 476, "train_loss": "0.707", "train_ntokens": "260355", "train_nsentences": "1750.04", "train_wps": "63687.6", "train_ups": "0.24", "train_wpb": "260355", "train_bsz": "1750", "train_num_updates": "22835", "train_lr": "0.000356797", "train_gnorm": "0.541", "train_loss_scale": "2", "train_train_wall": "72", "train_gb_free": "40.1", "train_wall": "28568"}
[2024-10-07 00:18:09,908][fairseq.data.iterators][INFO] - grouped total_num_itrs = 48
[2024-10-07 00:18:09,911][fairseq.trainer][INFO] - begin training epoch 477
[2024-10-07 00:18:09,912][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-07 00:19:19,471][train_inner][INFO] - {"epoch": 476, "update": 475.271, "loss": "0.708", "ntokens": "260919", "nsentences": "1742.6", "wps": "66544.8", "ups": "0.26", "wpb": "260919", "bsz": "1742.6", "num_updates": "22800", "lr": "0.00035625", "gnorm": "0.498", "loss_scale": "2", "train_wall": "352", "gb_free": "40.3", "wall": "28634"}
