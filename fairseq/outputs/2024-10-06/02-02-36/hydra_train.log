[2024-10-06 02:03:06,706][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:13724', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'gloss'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_800k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-06 02:03:07,482][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:11781', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'gloss'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_800k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-06 02:03:07,618][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:15718', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'gloss'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_800k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-06 02:03:09,535][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-06 02:03:09,536][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-06 02:03:09,536][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-06 02:03:09,537][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-06 02:03:09,537][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-06 02:03:09,538][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-06 02:03:11,524][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-06 02:03:11,525][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-06 02:03:11,525][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-06 02:03:11,525][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-06 02:03:11,526][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-06 02:03:11,528][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-06 02:03:12,376][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:11808', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'gloss'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_800k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-06 02:03:13,315][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-06 02:03:13,746][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:14339', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'gloss'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_800k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-06 02:03:14,997][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:19344', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'gloss'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_800k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-06 02:03:15,602][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:15080', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'gloss'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_800k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-06 02:03:15,762][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/log.txt', 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/tb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/share/data/pals/shester/sign_dinosr/fairseq/examples/dinosr', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:15743', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 16, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 100, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 50000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': True, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'signhubert', 'extractor_mode': layer_norm, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 3, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 32, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'discrete': True, 'codebook_size': 256, 'max_update': '${optimization.max_update}', 'channels_embed_dim': 384, 'channels_pose_embed_dim': 14, 'intermediate_dim': 1024, 'mask_strategy': 'gloss'}, 'task': {'_name': 'audio_pretraining', 'data': '/share/data/pals/shester/sign_dinosr_logs/dataset/train_ssl_ysl25_ase_800k_share.csv', 'labels': None, 'binarized_dataset': False, 'sample_rate': 1, 'normalize': True, 'enable_padding': False, 'max_sample_size': 1000, 'min_sample_size': 1, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'kmeans_label_paths': {'face': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/face/face_256.km', 'left_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/lhand/lhand_256.km', 'right_hand': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/rhand/rhand_256.km', 'body_posture': '/share/data/pals/shester/sign_dinosr_logs/km_labels/ysl25/ase800/body/body_256.km'}, 'tpu': False, 'text_compression_level': none}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': []}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-10-06 02:03:16,339][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-06 02:03:16,340][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-06 02:03:16,340][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-06 02:03:16,340][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-06 02:03:16,341][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-06 02:03:16,341][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-06 02:03:16,748][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-06 02:03:16,752][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-06 02:03:16,752][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-06 02:03:16,752][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-06 02:03:16,753][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-06 02:03:16,753][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-06 02:03:17,894][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-06 02:03:18,715][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-06 02:03:18,717][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-06 02:03:18,717][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-06 02:03:18,717][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-06 02:03:18,717][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-06 02:03:18,718][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-06 02:03:21,658][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-06 02:03:21,660][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-06 02:03:21,660][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-06 02:03:21,660][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-06 02:03:21,661][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-06 02:03:21,661][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-06 02:03:23,293][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-06 02:03:23,384][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-06 02:03:23,384][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-06 02:03:23,384][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-06 02:03:23,385][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-06 02:03:23,431][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-06 02:03:24,761][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-06 02:03:26,210][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-06 02:03:28,725][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-06 02:03:29,904][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-06 02:03:31,380][fairseq_cli.train][INFO] - SignHubertModel(
  (post_extract_proj): Linear(in_features=1024, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(32,), stride=(1,), padding=(16,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (layer_norm_face): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_lhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_rhand): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (layer_norm_body): LayerNorm((14,), eps=1e-05, elementwise_affine=True)
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=768, out_features=256, bias=True)
  )
  (face_proj): Linear(in_features=384, out_features=256, bias=True)
  (left_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (right_hand_proj): Linear(in_features=384, out_features=256, bias=True)
  (body_posture_proj): Linear(in_features=14, out_features=256, bias=True)
)
[2024-10-06 02:03:31,391][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2024-10-06 02:03:31,391][fairseq_cli.train][INFO] - model: SignHubertModel
[2024-10-06 02:03:31,391][fairseq_cli.train][INFO] - criterion: ModelCriterion
[2024-10-06 02:03:31,392][fairseq_cli.train][INFO] - num. shared model params: 88,116,284 (num. trained: 88,116,284)
[2024-10-06 02:03:31,392][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-10-06 02:03:59,659][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-06 02:04:05,792][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-06 02:05:47,073][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 02:05:47,078][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 02:05:47,078][fairseq.utils][INFO] - rank   1: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 02:05:47,078][fairseq.utils][INFO] - rank   2: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 02:05:47,078][fairseq.utils][INFO] - rank   3: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 02:05:47,078][fairseq.utils][INFO] - rank   4: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 02:05:47,078][fairseq.utils][INFO] - rank   5: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 02:05:47,078][fairseq.utils][INFO] - rank   6: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 02:05:47,078][fairseq.utils][INFO] - rank   7: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 02:05:47,078][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 02:05:47,079][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-06 02:05:47,084][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-06 02:05:47,085][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-06 02:05:56,915][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 02:05:56,916][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 02:05:56,916][fairseq.utils][INFO] - rank   1: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 02:05:56,916][fairseq.utils][INFO] - rank   2: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 02:05:56,916][fairseq.utils][INFO] - rank   3: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 02:05:56,916][fairseq.utils][INFO] - rank   4: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 02:05:56,916][fairseq.utils][INFO] - rank   5: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 02:05:56,916][fairseq.utils][INFO] - rank   6: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 02:05:56,916][fairseq.utils][INFO] - rank   7: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 02:05:56,916][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 02:05:56,917][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-06 02:05:56,917][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-06 02:05:56,917][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-06 02:06:25,535][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 89 @ 42131 updates)
[2024-10-06 02:06:25,536][fairseq.trainer][INFO] - loading train data for epoch 89
[2024-10-06 02:06:32,458][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-06 02:06:32,918][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 89 @ 42131 updates)
[2024-10-06 02:06:32,920][fairseq.trainer][INFO] - loading train data for epoch 89
[2024-10-06 02:06:35,187][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-06 02:07:09,811][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 02:07:09,818][fairseq.trainer][INFO] - begin training epoch 89
[2024-10-06 02:07:09,819][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 02:07:10,849][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 02:07:10,859][fairseq.trainer][INFO] - begin training epoch 89
[2024-10-06 02:07:10,860][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 02:08:58,361][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 02:08:58,384][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 02:08:58,385][fairseq.utils][INFO] - rank   1: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 02:08:58,385][fairseq.utils][INFO] - rank   2: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 02:08:58,385][fairseq.utils][INFO] - rank   3: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 02:08:58,385][fairseq.utils][INFO] - rank   4: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 02:08:58,385][fairseq.utils][INFO] - rank   5: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 02:08:58,385][fairseq.utils][INFO] - rank   6: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 02:08:58,385][fairseq.utils][INFO] - rank   7: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 02:08:58,385][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 02:08:58,571][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-06 02:08:58,578][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-06 02:08:58,583][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-06 02:09:34,931][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 02:09:34,932][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 02:09:34,932][fairseq.utils][INFO] - rank   1: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 02:09:34,932][fairseq.utils][INFO] - rank   2: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 02:09:34,932][fairseq.utils][INFO] - rank   3: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 02:09:34,938][fairseq.utils][INFO] - rank   4: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 02:09:34,938][fairseq.utils][INFO] - rank   5: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 02:09:34,938][fairseq.utils][INFO] - rank   6: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 02:09:34,938][fairseq.utils][INFO] - rank   7: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 02:09:34,938][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 02:09:34,939][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-06 02:09:34,939][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-06 02:09:34,940][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-06 02:09:54,584][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 89 @ 42131 updates)
[2024-10-06 02:09:54,592][fairseq.trainer][INFO] - loading train data for epoch 89
[2024-10-06 02:09:59,420][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 89 @ 42131 updates)
[2024-10-06 02:09:59,422][fairseq.trainer][INFO] - loading train data for epoch 89
[2024-10-06 02:10:01,720][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-06 02:10:04,644][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-06 02:11:06,751][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 02:11:06,768][fairseq.trainer][INFO] - begin training epoch 89
[2024-10-06 02:11:06,768][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 02:12:09,679][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 02:12:09,683][fairseq.trainer][INFO] - begin training epoch 89
[2024-10-06 02:12:09,683][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 02:12:41,368][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 02:12:41,370][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 02:12:41,370][fairseq.utils][INFO] - rank   1: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 02:12:41,371][fairseq.utils][INFO] - rank   2: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 02:12:41,371][fairseq.utils][INFO] - rank   3: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 02:12:41,371][fairseq.utils][INFO] - rank   4: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 02:12:41,371][fairseq.utils][INFO] - rank   5: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 02:12:41,372][fairseq.utils][INFO] - rank   6: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 02:12:41,372][fairseq.utils][INFO] - rank   7: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 02:12:41,372][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 02:12:41,372][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-06 02:12:41,373][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-06 02:12:41,374][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-06 02:13:09,688][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 89 @ 42131 updates)
[2024-10-06 02:13:09,695][fairseq.trainer][INFO] - loading train data for epoch 89
[2024-10-06 02:13:14,516][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-06 02:14:26,191][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 02:14:26,192][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 02:14:26,192][fairseq.utils][INFO] - rank   1: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 02:14:26,192][fairseq.utils][INFO] - rank   2: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 02:14:26,192][fairseq.utils][INFO] - rank   3: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 02:14:26,192][fairseq.utils][INFO] - rank   4: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 02:14:26,192][fairseq.utils][INFO] - rank   5: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 02:14:26,192][fairseq.utils][INFO] - rank   6: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 02:14:26,192][fairseq.utils][INFO] - rank   7: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 02:14:26,192][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 02:14:26,192][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-06 02:14:26,192][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-06 02:14:26,193][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-06 02:14:34,433][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 02:14:34,447][fairseq.trainer][INFO] - begin training epoch 89
[2024-10-06 02:14:34,448][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 02:15:08,218][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 89 @ 42131 updates)
[2024-10-06 02:15:08,407][fairseq.trainer][INFO] - loading train data for epoch 89
[2024-10-06 02:15:11,213][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-06 02:15:32,483][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 02:15:32,484][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 02:15:32,484][fairseq.utils][INFO] - rank   1: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 02:15:32,484][fairseq.utils][INFO] - rank   2: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 02:15:32,484][fairseq.utils][INFO] - rank   3: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 02:15:32,484][fairseq.utils][INFO] - rank   4: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 02:15:32,484][fairseq.utils][INFO] - rank   5: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 02:15:32,484][fairseq.utils][INFO] - rank   6: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 02:15:32,484][fairseq.utils][INFO] - rank   7: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 02:15:32,484][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 02:15:32,484][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-06 02:15:32,485][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-06 02:15:32,498][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-06 02:15:46,272][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 02:15:46,273][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 02:15:46,273][fairseq.utils][INFO] - rank   1: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 02:15:46,273][fairseq.utils][INFO] - rank   2: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 02:15:46,273][fairseq.utils][INFO] - rank   3: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 02:15:46,274][fairseq.utils][INFO] - rank   4: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 02:15:46,274][fairseq.utils][INFO] - rank   5: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 02:15:46,274][fairseq.utils][INFO] - rank   6: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 02:15:46,274][fairseq.utils][INFO] - rank   7: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
[2024-10-06 02:15:46,274][fairseq.utils][INFO] - ***********************CUDA enviroments for all 8 workers***********************
[2024-10-06 02:15:46,274][fairseq_cli.train][INFO] - training on 8 devices (GPUs/TPUs)
[2024-10-06 02:15:46,274][fairseq_cli.train][INFO] - max tokens per device = 15000 and max sentences per device = None
[2024-10-06 02:15:46,275][fairseq.trainer][INFO] - Preparing to load checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-06 02:16:07,777][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 02:16:07,783][fairseq.trainer][INFO] - begin training epoch 89
[2024-10-06 02:16:07,784][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 02:16:08,164][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 89 @ 42131 updates)
[2024-10-06 02:16:08,282][fairseq.trainer][INFO] - loading train data for epoch 89
[2024-10-06 02:16:12,261][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-06 02:16:15,787][fairseq.trainer][INFO] - Loaded checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 89 @ 42131 updates)
[2024-10-06 02:16:15,968][fairseq.trainer][INFO] - loading train data for epoch 89
[2024-10-06 02:16:21,925][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 840029, skipped 0 samples
[2024-10-06 02:17:29,586][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 02:17:29,597][fairseq.trainer][INFO] - begin training epoch 89
[2024-10-06 02:17:29,598][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 02:17:38,823][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 02:17:38,836][fairseq.trainer][INFO] - begin training epoch 89
[2024-10-06 02:17:38,838][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 02:45:13,962][train_inner][INFO] - {"epoch": 89, "update": 88.144, "loss": "1.08", "ntokens": "240121", "nsentences": "1724.01", "wps": "29494.2", "ups": "0.12", "wpb": "240121", "bsz": "1724", "num_updates": "42200", "lr": "0.000486141", "gnorm": "0.318", "loss_scale": "4", "train_wall": "635", "gb_free": "40.1", "wall": "2139"}
[2024-10-06 02:45:13,963][train_inner][INFO] - {"epoch": 89, "update": 88.144, "loss": "1.08", "ntokens": "240121", "nsentences": "1724.01", "wps": "54043", "ups": "0.23", "wpb": "240121", "bsz": "1724", "num_updates": "42200", "lr": "0.000486141", "gnorm": "0.318", "loss_scale": "4", "train_wall": "635", "gb_free": "40.1", "wall": "1781"}
[2024-10-06 02:45:14,081][train_inner][INFO] - {"epoch": 89, "update": 88.144, "loss": "1.08", "ntokens": "240121", "nsentences": "1724.01", "wps": "38804.5", "ups": "0.16", "wpb": "240121", "bsz": "1724", "num_updates": "42200", "lr": "0.000486141", "gnorm": "0.317", "loss_scale": "4", "train_wall": "426", "gb_free": "40.1", "wall": "1952"}
[2024-10-06 03:08:25,889][train_inner][INFO] - {"epoch": 89, "update": 88.562, "loss": "1.077", "ntokens": "240055", "nsentences": "1724.93", "wps": "34494.7", "ups": "0.14", "wpb": "240055", "bsz": "1724.9", "num_updates": "42400", "lr": "0.00048587", "gnorm": "0.292", "loss_scale": "4", "train_wall": "967", "gb_free": "39.6", "wall": "3173"}
[2024-10-06 03:08:25,892][train_inner][INFO] - {"epoch": 89, "update": 88.562, "loss": "1.077", "ntokens": "240055", "nsentences": "1724.93", "wps": "34493.9", "ups": "0.14", "wpb": "240055", "bsz": "1724.9", "num_updates": "42400", "lr": "0.00048587", "gnorm": "0.292", "loss_scale": "4", "train_wall": "957", "gb_free": "39.6", "wall": "3531"}
[2024-10-06 03:27:32,939][train_inner][INFO] - {"epoch": 89, "update": 88.979, "loss": "1.079", "ntokens": "238758", "nsentences": "1805.17", "wps": "41633", "ups": "0.17", "wpb": "238758", "bsz": "1805.2", "num_updates": "42600", "lr": "0.000485598", "gnorm": "0.288", "loss_scale": "4", "train_wall": "1042", "gb_free": "39.8", "wall": "4678"}
[2024-10-06 03:27:33,379][train_inner][INFO] - {"epoch": 89, "update": 88.979, "loss": "1.079", "ntokens": "238758", "nsentences": "1805.17", "wps": "41623.4", "ups": "0.17", "wpb": "238758", "bsz": "1805.2", "num_updates": "42600", "lr": "0.000485598", "gnorm": "0.288", "loss_scale": "4", "train_wall": "1036", "gb_free": "39.8", "wall": "4321"}
[2024-10-06 03:28:32,881][fairseq_cli.train][INFO] - end of epoch 89 (average epoch stats below)
[2024-10-06 03:28:32,958][train][INFO] - {"epoch": 89, "train_loss": "1.078", "train_ntokens": "239145", "train_nsentences": "1753.71", "train_wps": "39398.3", "train_ups": "0.16", "train_wpb": "239145", "train_bsz": "1753.7", "train_num_updates": "42610", "train_lr": "0.000485584", "train_gnorm": "0.294", "train_loss_scale": "4", "train_train_wall": "2692", "train_gb_free": "40.1", "train_wall": "4380"}
[2024-10-06 03:28:35,865][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 03:28:35,906][fairseq.trainer][INFO] - begin training epoch 90
[2024-10-06 03:28:35,911][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 03:28:38,099][fairseq_cli.train][INFO] - end of epoch 89 (average epoch stats below)
[2024-10-06 03:28:38,175][train][INFO] - {"epoch": 89, "train_loss": "1.078", "train_ntokens": "239145", "train_nsentences": "1753.71", "train_wps": "36196.2", "train_ups": "0.15", "train_wpb": "239145", "train_bsz": "1753.7", "train_num_updates": "42610", "train_lr": "0.000485584", "train_gnorm": "0.294", "train_loss_scale": "4", "train_train_wall": "2698", "train_gb_free": "40.1", "train_wall": "4743"}
[2024-10-06 03:28:38,397][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 03:28:38,408][fairseq.trainer][INFO] - begin training epoch 90
[2024-10-06 03:28:38,414][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 03:45:32,105][train_inner][INFO] - {"epoch": 90, "update": 89.397, "loss": "1.075", "ntokens": "238198", "nsentences": "1798.18", "wps": "44146", "ups": "0.19", "wpb": "238198", "bsz": "1798.2", "num_updates": "42800", "lr": "0.000485326", "gnorm": "0.305", "loss_scale": "4", "train_wall": "637", "gb_free": "39.8", "wall": "5757"}
[2024-10-06 03:45:32,766][train_inner][INFO] - {"epoch": 90, "update": 89.397, "loss": "1.075", "ntokens": "238198", "nsentences": "1798.18", "wps": "44136.7", "ups": "0.19", "wpb": "238198", "bsz": "1798.2", "num_updates": "42800", "lr": "0.000485326", "gnorm": "0.304", "loss_scale": "4", "train_wall": "627", "gb_free": "39.8", "wall": "5400"}
[2024-10-06 03:53:10,582][train_inner][INFO] - {"epoch": 90, "update": 89.814, "loss": "1.078", "ntokens": "239023", "nsentences": "1736.83", "wps": "104425", "ups": "0.44", "wpb": "239023", "bsz": "1736.8", "num_updates": "43000", "lr": "0.000485054", "gnorm": "0.298", "loss_scale": "4", "train_wall": "444", "gb_free": "39.6", "wall": "5858"}
[2024-10-06 03:53:12,756][train_inner][INFO] - {"epoch": 90, "update": 89.814, "loss": "1.078", "ntokens": "239023", "nsentences": "1736.83", "wps": "103782", "ups": "0.43", "wpb": "239023", "bsz": "1736.8", "num_updates": "43000", "lr": "0.000485054", "gnorm": "0.293", "loss_scale": "4", "train_wall": "453", "gb_free": "39.6", "wall": "6218"}
[2024-10-06 03:56:45,690][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 90 @ 43089 updates
[2024-10-06 03:56:45,695][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-06 03:56:52,160][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 90 @ 43089 updates
[2024-10-06 03:56:52,161][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-06 03:56:52,317][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-06 03:56:52,368][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 90 @ 43089 updates, score None) (writing took 6.678347384557128 seconds)
[2024-10-06 03:56:52,369][fairseq_cli.train][INFO] - end of epoch 90 (average epoch stats below)
[2024-10-06 03:56:52,393][train][INFO] - {"epoch": 90, "train_loss": "1.076", "train_ntokens": "238945", "train_nsentences": "1753.71", "train_wps": "67349.7", "train_ups": "0.28", "train_wpb": "238945", "train_bsz": "1753.7", "train_num_updates": "43089", "train_lr": "0.000484933", "train_gnorm": "0.303", "train_loss_scale": "4", "train_train_wall": "1221", "train_gb_free": "39.8", "train_wall": "6080"}
[2024-10-06 03:56:52,629][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 03:56:52,668][fairseq.trainer][INFO] - begin training epoch 91
[2024-10-06 03:56:52,669][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 03:56:55,795][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-06 03:56:55,811][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 90 @ 43089 updates, score None) (writing took 3.6505089523270726 seconds)
[2024-10-06 03:56:55,811][fairseq_cli.train][INFO] - end of epoch 90 (average epoch stats below)
[2024-10-06 03:56:55,831][train][INFO] - {"epoch": 90, "train_loss": "1.076", "train_ntokens": "238945", "train_nsentences": "1753.71", "train_wps": "67420.1", "train_ups": "0.28", "train_wpb": "238945", "train_bsz": "1753.7", "train_num_updates": "43089", "train_lr": "0.000484933", "train_gnorm": "0.301", "train_loss_scale": "4", "train_train_wall": "1243", "train_gb_free": "39.8", "train_wall": "6441"}
[2024-10-06 03:56:56,052][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 03:56:56,068][fairseq.trainer][INFO] - begin training epoch 91
[2024-10-06 03:56:56,068][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 04:06:13,056][train_inner][INFO] - {"epoch": 91, "update": 90.232, "loss": "1.075", "ntokens": "239117", "nsentences": "1716.3", "wps": "61291.2", "ups": "0.26", "wpb": "239117", "bsz": "1716.3", "num_updates": "43200", "lr": "0.000484783", "gnorm": "0.312", "loss_scale": "4", "train_wall": "432", "gb_free": "39.8", "wall": "6998"}
[2024-10-06 04:06:16,671][train_inner][INFO] - {"epoch": 91, "update": 90.232, "loss": "1.075", "ntokens": "239117", "nsentences": "1716.3", "wps": "60842", "ups": "0.25", "wpb": "239117", "bsz": "1716.3", "num_updates": "43200", "lr": "0.000484783", "gnorm": "0.312", "loss_scale": "4", "train_wall": "323", "gb_free": "39.8", "wall": "6644"}
[2024-10-06 04:12:21,051][train_inner][INFO] - {"epoch": 91, "update": 90.649, "loss": "1.074", "ntokens": "239736", "nsentences": "1743.93", "wps": "131592", "ups": "0.55", "wpb": "239736", "bsz": "1743.9", "num_updates": "43400", "lr": "0.000484511", "gnorm": "0.288", "loss_scale": "4", "train_wall": "247", "gb_free": "39.3", "wall": "7009"}
[2024-10-06 04:12:54,552][train_inner][INFO] - {"epoch": 91, "update": 90.649, "loss": "1.074", "ntokens": "239736", "nsentences": "1743.93", "wps": "119427", "ups": "0.5", "wpb": "239736", "bsz": "1743.9", "num_updates": "43400", "lr": "0.000484511", "gnorm": "0.292", "loss_scale": "4", "train_wall": "396", "gb_free": "39.3", "wall": "7400"}
[2024-10-06 04:14:34,529][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2024-10-06 04:17:29,379][fairseq_cli.train][INFO] - end of epoch 91 (average epoch stats below)
[2024-10-06 04:17:29,431][train][INFO] - {"epoch": 91, "train_loss": "1.075", "train_ntokens": "239231", "train_nsentences": "1753.71", "train_wps": "92636.3", "train_ups": "0.39", "train_wpb": "239231", "train_bsz": "1753.7", "train_num_updates": "43568", "train_lr": "0.000484283", "train_gnorm": "0.299", "train_loss_scale": "4", "train_train_wall": "581", "train_gb_free": "40.1", "train_wall": "7317"}
[2024-10-06 04:17:29,735][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 04:17:29,776][fairseq.trainer][INFO] - begin training epoch 92
[2024-10-06 04:17:29,776][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 04:18:32,100][fairseq_cli.train][INFO] - end of epoch 91 (average epoch stats below)
[2024-10-06 04:18:32,132][train][INFO] - {"epoch": 91, "train_loss": "1.075", "train_ntokens": "239224", "train_nsentences": "1754.81", "train_wps": "88213.2", "train_ups": "0.37", "train_wpb": "239224", "train_bsz": "1754.8", "train_num_updates": "43567", "train_lr": "0.000484284", "train_gnorm": "0.3", "train_loss_scale": "2", "train_train_wall": "944", "train_gb_free": "40.1", "train_wall": "7737"}
[2024-10-06 04:18:32,257][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 04:18:32,263][fairseq.trainer][INFO] - begin training epoch 92
[2024-10-06 04:18:32,263][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 04:25:05,932][train_inner][INFO] - {"epoch": 92, "update": 91.067, "loss": "1.077", "ntokens": "238535", "nsentences": "1759.73", "wps": "62377.7", "ups": "0.26", "wpb": "238535", "bsz": "1759.7", "num_updates": "43600", "lr": "0.000484239", "gnorm": "0.302", "loss_scale": "4", "train_wall": "281", "gb_free": "40.3", "wall": "7773"}
[2024-10-06 04:26:38,316][train_inner][INFO] - {"epoch": 92, "update": 91.069, "loss": "1.077", "ntokens": "238497", "nsentences": "1762.17", "wps": "57905.2", "ups": "0.24", "wpb": "238497", "bsz": "1762.2", "num_updates": "43600", "lr": "0.000484239", "gnorm": "0.303", "loss_scale": "2", "train_wall": "460", "gb_free": "39.1", "wall": "8223"}
[2024-10-06 04:30:53,562][train_inner][INFO] - {"epoch": 92, "update": 91.484, "loss": "1.074", "ntokens": "239616", "nsentences": "1765.44", "wps": "137866", "ups": "0.58", "wpb": "239616", "bsz": "1765.4", "num_updates": "43800", "lr": "0.000483967", "gnorm": "0.288", "loss_scale": "4", "train_wall": "342", "gb_free": "40.1", "wall": "8121"}
[2024-10-06 04:32:55,205][train_inner][INFO] - {"epoch": 92, "update": 91.486, "loss": "1.074", "ntokens": "239631", "nsentences": "1766.27", "wps": "127166", "ups": "0.53", "wpb": "239631", "bsz": "1766.3", "num_updates": "43800", "lr": "0.000483967", "gnorm": "0.284", "loss_scale": "2", "train_wall": "371", "gb_free": "39.6", "wall": "8600"}
[2024-10-06 04:36:01,492][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2024-10-06 04:37:04,420][train_inner][INFO] - {"epoch": 92, "update": 91.904, "loss": "1.076", "ntokens": "240039", "nsentences": "1733.68", "wps": "129461", "ups": "0.54", "wpb": "240039", "bsz": "1733.7", "num_updates": "44000", "lr": "0.000483696", "gnorm": "0.302", "loss_scale": "2", "train_wall": "349", "gb_free": "39.1", "wall": "8492"}
[2024-10-06 04:39:26,897][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 92 @ 44046 updates
[2024-10-06 04:39:26,903][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-06 04:39:31,318][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-06 04:39:31,327][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 92 @ 44046 updates, score None) (writing took 4.430139943026006 seconds)
[2024-10-06 04:39:31,328][fairseq_cli.train][INFO] - end of epoch 92 (average epoch stats below)
[2024-10-06 04:39:31,390][train][INFO] - {"epoch": 92, "train_loss": "1.075", "train_ntokens": "239254", "train_nsentences": "1754.01", "train_wps": "86514.7", "train_ups": "0.36", "train_wpb": "239254", "train_bsz": "1754", "train_num_updates": "44046", "train_lr": "0.000483633", "train_gnorm": "0.296", "train_loss_scale": "2", "train_train_wall": "897", "train_gb_free": "39.3", "train_wall": "8639"}
[2024-10-06 04:39:31,534][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 04:39:31,540][fairseq.trainer][INFO] - begin training epoch 93
[2024-10-06 04:39:31,541][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 04:39:38,721][train_inner][INFO] - {"epoch": 92, "update": 91.904, "loss": "1.076", "ntokens": "240041", "nsentences": "1732.32", "wps": "118982", "ups": "0.5", "wpb": "240041", "bsz": "1732.3", "num_updates": "44000", "lr": "0.000483696", "gnorm": "0.294", "loss_scale": "2", "train_wall": "398", "gb_free": "39.1", "wall": "9004"}
[2024-10-06 04:40:42,473][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 92 @ 44046 updates
[2024-10-06 04:40:42,481][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-06 04:40:50,170][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-06 04:40:50,243][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 92 @ 44046 updates, score None) (writing took 7.770167015492916 seconds)
[2024-10-06 04:40:50,267][fairseq_cli.train][INFO] - end of epoch 92 (average epoch stats below)
[2024-10-06 04:40:50,272][train][INFO] - {"epoch": 92, "train_loss": "1.075", "train_ntokens": "239252", "train_nsentences": "1753.71", "train_wps": "85642.6", "train_ups": "0.36", "train_wpb": "239252", "train_bsz": "1753.7", "train_num_updates": "44046", "train_lr": "0.000483633", "train_gnorm": "0.295", "train_loss_scale": "2", "train_train_wall": "958", "train_gb_free": "39.3", "train_wall": "9075"}
[2024-10-06 04:40:50,839][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 04:40:50,936][fairseq.trainer][INFO] - begin training epoch 93
[2024-10-06 04:40:50,947][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 04:50:30,762][train_inner][INFO] - {"epoch": 93, "update": 92.322, "loss": "1.074", "ntokens": "238511", "nsentences": "1743.68", "wps": "59161.1", "ups": "0.25", "wpb": "238511", "bsz": "1743.7", "num_updates": "44200", "lr": "0.000483424", "gnorm": "0.296", "loss_scale": "2", "train_wall": "419", "gb_free": "40.3", "wall": "9298"}
[2024-10-06 04:52:23,496][train_inner][INFO] - {"epoch": 93, "update": 92.322, "loss": "1.074", "ntokens": "238511", "nsentences": "1743.68", "wps": "62374.9", "ups": "0.26", "wpb": "238511", "bsz": "1743.7", "num_updates": "44200", "lr": "0.000483424", "gnorm": "0.302", "loss_scale": "2", "train_wall": "371", "gb_free": "40.3", "wall": "9769"}
[2024-10-06 04:56:28,750][train_inner][INFO] - {"epoch": 93, "update": 92.739, "loss": "1.071", "ntokens": "240056", "nsentences": "1767.46", "wps": "134126", "ups": "0.56", "wpb": "240056", "bsz": "1767.5", "num_updates": "44400", "lr": "0.000483152", "gnorm": "0.285", "loss_scale": "2", "train_wall": "351", "gb_free": "40.1", "wall": "9656"}
[2024-10-06 04:58:45,448][train_inner][INFO] - {"epoch": 93, "update": 92.739, "loss": "1.071", "ntokens": "240056", "nsentences": "1767.46", "wps": "125707", "ups": "0.52", "wpb": "240056", "bsz": "1767.5", "num_updates": "44400", "lr": "0.000483152", "gnorm": "0.287", "loss_scale": "2", "train_wall": "376", "gb_free": "40.1", "wall": "10151"}
[2024-10-06 05:00:11,847][fairseq_cli.train][INFO] - end of epoch 93 (average epoch stats below)
[2024-10-06 05:00:11,932][train][INFO] - {"epoch": 93, "train_loss": "1.073", "train_ntokens": "239283", "train_nsentences": "1753.71", "train_wps": "92396.7", "train_ups": "0.39", "train_wpb": "239283", "train_bsz": "1753.7", "train_num_updates": "44525", "train_lr": "0.000482982", "train_gnorm": "0.293", "train_loss_scale": "2", "train_train_wall": "848", "train_gb_free": "39.6", "train_wall": "9879"}
[2024-10-06 05:00:12,257][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 05:00:12,274][fairseq.trainer][INFO] - begin training epoch 94
[2024-10-06 05:00:12,274][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 05:02:28,823][fairseq_cli.train][INFO] - end of epoch 93 (average epoch stats below)
[2024-10-06 05:02:28,855][train][INFO] - {"epoch": 93, "train_loss": "1.073", "train_ntokens": "239283", "train_nsentences": "1753.71", "train_wps": "88264.2", "train_ups": "0.37", "train_wpb": "239283", "train_bsz": "1753.7", "train_num_updates": "44525", "train_lr": "0.000482982", "train_gnorm": "0.294", "train_loss_scale": "2", "train_train_wall": "903", "train_gb_free": "39.6", "train_wall": "10374"}
[2024-10-06 05:02:29,030][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 05:02:29,049][fairseq.trainer][INFO] - begin training epoch 94
[2024-10-06 05:02:29,051][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 05:09:39,562][train_inner][INFO] - {"epoch": 94, "update": 93.157, "loss": "1.075", "ntokens": "238421", "nsentences": "1763.71", "wps": "60298.6", "ups": "0.25", "wpb": "238421", "bsz": "1763.7", "num_updates": "44600", "lr": "0.00048288", "gnorm": "0.298", "loss_scale": "2", "train_wall": "317", "gb_free": "40.1", "wall": "10447"}
[2024-10-06 05:11:42,667][train_inner][INFO] - {"epoch": 94, "update": 93.157, "loss": "1.075", "ntokens": "238421", "nsentences": "1763.71", "wps": "61353.1", "ups": "0.26", "wpb": "238421", "bsz": "1763.7", "num_updates": "44600", "lr": "0.00048288", "gnorm": "0.296", "loss_scale": "2", "train_wall": "401", "gb_free": "40.1", "wall": "10928"}
[2024-10-06 05:15:15,078][train_inner][INFO] - {"epoch": 94, "update": 93.574, "loss": "1.072", "ntokens": "240022", "nsentences": "1749.81", "wps": "143095", "ups": "0.6", "wpb": "240022", "bsz": "1749.8", "num_updates": "44800", "lr": "0.000482609", "gnorm": "0.288", "loss_scale": "2", "train_wall": "330", "gb_free": "40.2", "wall": "10783"}
[2024-10-06 05:16:58,123][train_inner][INFO] - {"epoch": 94, "update": 93.574, "loss": "1.072", "ntokens": "240022", "nsentences": "1749.81", "wps": "152195", "ups": "0.63", "wpb": "240022", "bsz": "1749.8", "num_updates": "44800", "lr": "0.000482609", "gnorm": "0.278", "loss_scale": "2", "train_wall": "310", "gb_free": "40.2", "wall": "11243"}
[2024-10-06 05:20:13,572][train_inner][INFO] - {"epoch": 94, "update": 93.992, "loss": "1.074", "ntokens": "239477", "nsentences": "1751.03", "wps": "160527", "ups": "0.67", "wpb": "239477", "bsz": "1751", "num_updates": "45000", "lr": "0.000482337", "gnorm": "0.294", "loss_scale": "2", "train_wall": "263", "gb_free": "40.5", "wall": "11081"}
[2024-10-06 05:20:27,253][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 94 @ 45004 updates
[2024-10-06 05:20:27,253][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-06 05:20:48,306][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-06 05:20:48,779][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 94 @ 45004 updates, score None) (writing took 21.517341832630336 seconds)
[2024-10-06 05:20:48,780][fairseq_cli.train][INFO] - end of epoch 94 (average epoch stats below)
[2024-10-06 05:20:48,815][train][INFO] - {"epoch": 94, "train_loss": "1.073", "train_ntokens": "239323", "train_nsentences": "1753.71", "train_wps": "92683.6", "train_ups": "0.39", "train_wpb": "239323", "train_bsz": "1753.7", "train_num_updates": "45004", "train_lr": "0.000482332", "train_gnorm": "0.292", "train_loss_scale": "2", "train_train_wall": "699", "train_gb_free": "39.2", "train_wall": "11116"}
[2024-10-06 05:20:49,134][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 05:20:49,150][fairseq.trainer][INFO] - begin training epoch 95
[2024-10-06 05:20:49,151][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 05:22:43,139][train_inner][INFO] - {"epoch": 94, "update": 93.992, "loss": "1.074", "ntokens": "239477", "nsentences": "1751.03", "wps": "138829", "ups": "0.58", "wpb": "239477", "bsz": "1751", "num_updates": "45000", "lr": "0.000482337", "gnorm": "0.295", "loss_scale": "2", "train_wall": "339", "gb_free": "40.5", "wall": "11588"}
[2024-10-06 05:22:48,696][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 94 @ 45004 updates
[2024-10-06 05:22:48,697][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-06 05:22:59,708][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-06 05:22:59,752][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 94 @ 45004 updates, score None) (writing took 11.055548826232553 seconds)
[2024-10-06 05:22:59,752][fairseq_cli.train][INFO] - end of epoch 94 (average epoch stats below)
[2024-10-06 05:22:59,754][train][INFO] - {"epoch": 94, "train_loss": "1.073", "train_ntokens": "239323", "train_nsentences": "1753.71", "train_wps": "93131.9", "train_ups": "0.39", "train_wpb": "239323", "train_bsz": "1753.7", "train_num_updates": "45004", "train_lr": "0.000482332", "train_gnorm": "0.287", "train_loss_scale": "2", "train_train_wall": "836", "train_gb_free": "39.2", "train_wall": "11605"}
[2024-10-06 05:22:59,982][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 05:23:00,041][fairseq.trainer][INFO] - begin training epoch 95
[2024-10-06 05:23:00,041][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 05:32:55,023][train_inner][INFO] - {"epoch": 95, "update": 94.409, "loss": "1.069", "ntokens": "238087", "nsentences": "1768.47", "wps": "62543.8", "ups": "0.26", "wpb": "238087", "bsz": "1768.5", "num_updates": "45200", "lr": "0.000482065", "gnorm": "0.297", "loss_scale": "2", "train_wall": "349", "gb_free": "40.1", "wall": "11843"}
[2024-10-06 05:35:54,255][train_inner][INFO] - {"epoch": 95, "update": 94.409, "loss": "1.069", "ntokens": "238087", "nsentences": "1768.47", "wps": "60193.1", "ups": "0.25", "wpb": "238087", "bsz": "1768.5", "num_updates": "45200", "lr": "0.000482065", "gnorm": "0.289", "loss_scale": "2", "train_wall": "360", "gb_free": "40.1", "wall": "12379"}
[2024-10-06 05:39:18,166][train_inner][INFO] - {"epoch": 95, "update": 94.827, "loss": "1.07", "ntokens": "239520", "nsentences": "1791.7", "wps": "125034", "ups": "0.52", "wpb": "239520", "bsz": "1791.7", "num_updates": "45400", "lr": "0.000481793", "gnorm": "0.297", "loss_scale": "2", "train_wall": "379", "gb_free": "39.8", "wall": "12226"}
[2024-10-06 05:42:02,185][train_inner][INFO] - {"epoch": 95, "update": 94.827, "loss": "1.07", "ntokens": "239520", "nsentences": "1791.7", "wps": "130206", "ups": "0.54", "wpb": "239520", "bsz": "1791.7", "num_updates": "45400", "lr": "0.000481793", "gnorm": "0.286", "loss_scale": "2", "train_wall": "363", "gb_free": "39.8", "wall": "12747"}
[2024-10-06 05:42:38,799][fairseq_cli.train][INFO] - end of epoch 95 (average epoch stats below)
[2024-10-06 05:42:38,922][train][INFO] - {"epoch": 95, "train_loss": "1.07", "train_ntokens": "239208", "train_nsentences": "1753.71", "train_wps": "87465.5", "train_ups": "0.37", "train_wpb": "239208", "train_bsz": "1753.7", "train_num_updates": "45483", "train_lr": "0.000481681", "train_gnorm": "0.293", "train_loss_scale": "2", "train_train_wall": "917", "train_gb_free": "39.3", "train_wall": "12426"}
[2024-10-06 05:42:39,904][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 05:42:39,956][fairseq.trainer][INFO] - begin training epoch 96
[2024-10-06 05:42:39,959][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 05:44:38,983][fairseq_cli.train][INFO] - end of epoch 95 (average epoch stats below)
[2024-10-06 05:44:38,989][train][INFO] - {"epoch": 95, "train_loss": "1.07", "train_ntokens": "239208", "train_nsentences": "1753.71", "train_wps": "88191.3", "train_ups": "0.37", "train_wpb": "239208", "train_bsz": "1753.7", "train_num_updates": "45483", "train_lr": "0.000481681", "train_gnorm": "0.285", "train_loss_scale": "2", "train_train_wall": "870", "train_gb_free": "39.3", "train_wall": "12904"}
[2024-10-06 05:44:39,132][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 05:44:39,148][fairseq.trainer][INFO] - begin training epoch 96
[2024-10-06 05:44:39,149][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 05:52:22,802][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2024-10-06 05:52:54,664][train_inner][INFO] - {"epoch": 96, "update": 95.244, "loss": "1.069", "ntokens": "239192", "nsentences": "1691.68", "wps": "58595", "ups": "0.24", "wpb": "239192", "bsz": "1691.7", "num_updates": "45600", "lr": "0.000481522", "gnorm": "0.284", "loss_scale": "2", "train_wall": "354", "gb_free": "39.7", "wall": "13042"}
[2024-10-06 05:54:54,319][train_inner][INFO] - {"epoch": 96, "update": 95.246, "loss": "1.069", "ntokens": "239153", "nsentences": "1691.01", "wps": "61947.6", "ups": "0.26", "wpb": "239153", "bsz": "1691", "num_updates": "45600", "lr": "0.000481522", "gnorm": "0.309", "loss_scale": "2", "train_wall": "381", "gb_free": "39.6", "wall": "13519"}
[2024-10-06 05:58:59,002][train_inner][INFO] - {"epoch": 96, "update": 95.662, "loss": "1.067", "ntokens": "239355", "nsentences": "1765.65", "wps": "131402", "ups": "0.55", "wpb": "239355", "bsz": "1765.7", "num_updates": "45800", "lr": "0.00048125", "gnorm": "0.297", "loss_scale": "2", "train_wall": "257", "gb_free": "39.6", "wall": "13407"}
[2024-10-06 06:01:10,111][train_inner][INFO] - {"epoch": 96, "update": 95.664, "loss": "1.067", "ntokens": "239412", "nsentences": "1764.3", "wps": "127424", "ups": "0.53", "wpb": "239412", "bsz": "1764.3", "num_updates": "45800", "lr": "0.00048125", "gnorm": "0.307", "loss_scale": "2", "train_wall": "369", "gb_free": "39.5", "wall": "13895"}
[2024-10-06 06:03:53,315][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 96 @ 45962 updates
[2024-10-06 06:03:53,331][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-06 06:04:09,172][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-06 06:04:09,292][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 96 @ 45962 updates, score None) (writing took 15.976249103434384 seconds)
[2024-10-06 06:04:09,301][fairseq_cli.train][INFO] - end of epoch 96 (average epoch stats below)
[2024-10-06 06:04:09,319][train][INFO] - {"epoch": 96, "train_loss": "1.069", "train_ntokens": "239110", "train_nsentences": "1753.71", "train_wps": "88760.6", "train_ups": "0.37", "train_wpb": "239110", "train_bsz": "1753.7", "train_num_updates": "45962", "train_lr": "0.00048103", "train_gnorm": "0.296", "train_loss_scale": "2", "train_train_wall": "610", "train_gb_free": "39.3", "train_wall": "13717"}
[2024-10-06 06:04:09,918][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 06:04:09,932][fairseq.trainer][INFO] - begin training epoch 97
[2024-10-06 06:04:09,933][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 06:05:17,122][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 96 @ 45961 updates
[2024-10-06 06:05:17,132][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-06 06:05:22,125][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-06 06:05:22,133][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 96 @ 45961 updates, score None) (writing took 5.011365626938641 seconds)
[2024-10-06 06:05:22,139][fairseq_cli.train][INFO] - end of epoch 96 (average epoch stats below)
[2024-10-06 06:05:22,145][train][INFO] - {"epoch": 96, "train_loss": "1.069", "train_ntokens": "239092", "train_nsentences": "1754.03", "train_wps": "91932.4", "train_ups": "0.38", "train_wpb": "239092", "train_bsz": "1754", "train_num_updates": "45961", "train_lr": "0.000481031", "train_gnorm": "0.31", "train_loss_scale": "2", "train_train_wall": "840", "train_gb_free": "39.3", "train_wall": "14147"}
[2024-10-06 06:05:22,261][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 06:05:22,319][fairseq.trainer][INFO] - begin training epoch 97
[2024-10-06 06:05:22,319][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 06:12:22,126][train_inner][INFO] - {"epoch": 97, "update": 96.079, "loss": "1.072", "ntokens": "238651", "nsentences": "1751.6", "wps": "59436.9", "ups": "0.25", "wpb": "238651", "bsz": "1751.6", "num_updates": "46000", "lr": "0.000480978", "gnorm": "0.296", "loss_scale": "4", "train_wall": "328", "gb_free": "39.9", "wall": "14210"}
[2024-10-06 06:13:14,454][train_inner][INFO] - {"epoch": 97, "update": 96.081, "loss": "1.072", "ntokens": "238606", "nsentences": "1755.58", "wps": "65882.8", "ups": "0.28", "wpb": "238606", "bsz": "1755.6", "num_updates": "46000", "lr": "0.000480978", "gnorm": "0.294", "loss_scale": "2", "train_wall": "305", "gb_free": "39.1", "wall": "14620"}
[2024-10-06 06:17:59,038][train_inner][INFO] - {"epoch": 97, "update": 96.497, "loss": "1.066", "ntokens": "239950", "nsentences": "1745.06", "wps": "142477", "ups": "0.59", "wpb": "239950", "bsz": "1745.1", "num_updates": "46200", "lr": "0.000480707", "gnorm": "0.284", "loss_scale": "4", "train_wall": "331", "gb_free": "39.6", "wall": "14547"}
[2024-10-06 06:18:35,451][train_inner][INFO] - {"epoch": 97, "update": 96.499, "loss": "1.066", "ntokens": "239988", "nsentences": "1739.94", "wps": "149536", "ups": "0.62", "wpb": "239988", "bsz": "1739.9", "num_updates": "46200", "lr": "0.000480707", "gnorm": "0.277", "loss_scale": "2", "train_wall": "314", "gb_free": "39.6", "wall": "14941"}
[2024-10-06 06:24:18,939][train_inner][INFO] - {"epoch": 97, "update": 96.914, "loss": "1.072", "ntokens": "239653", "nsentences": "1762.45", "wps": "126173", "ups": "0.53", "wpb": "239654", "bsz": "1762.5", "num_updates": "46400", "lr": "0.000480435", "gnorm": "0.292", "loss_scale": "4", "train_wall": "374", "gb_free": "39.7", "wall": "14926"}
[2024-10-06 06:24:46,286][train_inner][INFO] - {"epoch": 97, "update": 96.916, "loss": "1.072", "ntokens": "239645", "nsentences": "1764.58", "wps": "129253", "ups": "0.54", "wpb": "239645", "bsz": "1764.6", "num_updates": "46400", "lr": "0.000480435", "gnorm": "0.284", "loss_scale": "2", "train_wall": "364", "gb_free": "40.1", "wall": "15311"}
[2024-10-06 06:25:04,968][fairseq_cli.train][INFO] - end of epoch 97 (average epoch stats below)
[2024-10-06 06:25:04,991][train][INFO] - {"epoch": 97, "train_loss": "1.068", "train_ntokens": "239399", "train_nsentences": "1753.71", "train_wps": "91325", "train_ups": "0.38", "train_wpb": "239399", "train_bsz": "1753.7", "train_num_updates": "46441", "train_lr": "0.000480379", "train_gnorm": "0.287", "train_loss_scale": "4", "train_train_wall": "880", "train_gb_free": "40.5", "train_wall": "14972"}
[2024-10-06 06:25:05,207][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 06:25:05,245][fairseq.trainer][INFO] - begin training epoch 98
[2024-10-06 06:25:05,246][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 06:26:06,938][fairseq_cli.train][INFO] - end of epoch 97 (average epoch stats below)
[2024-10-06 06:26:06,960][train][INFO] - {"epoch": 97, "train_loss": "1.069", "train_ntokens": "239399", "train_nsentences": "1753.71", "train_wps": "92120.7", "train_ups": "0.38", "train_wpb": "239399", "train_bsz": "1753.7", "train_num_updates": "46440", "train_lr": "0.00048038", "train_gnorm": "0.284", "train_loss_scale": "2", "train_train_wall": "820", "train_gb_free": "40.5", "train_wall": "15392"}
[2024-10-06 06:26:07,115][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 06:26:07,119][fairseq.trainer][INFO] - begin training epoch 98
[2024-10-06 06:26:07,119][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 06:37:10,799][train_inner][INFO] - {"epoch": 98, "update": 97.332, "loss": "1.063", "ntokens": "238691", "nsentences": "1772.3", "wps": "61850.2", "ups": "0.26", "wpb": "238691", "bsz": "1772.3", "num_updates": "46600", "lr": "0.000480163", "gnorm": "0.303", "loss_scale": "4", "train_wall": "325", "gb_free": "40.3", "wall": "15698"}
[2024-10-06 06:37:13,698][train_inner][INFO] - {"epoch": 98, "update": 97.334, "loss": "1.063", "ntokens": "238657", "nsentences": "1773.68", "wps": "63864.8", "ups": "0.27", "wpb": "238657", "bsz": "1773.7", "num_updates": "46600", "lr": "0.000480163", "gnorm": "0.301", "loss_scale": "2", "train_wall": "380", "gb_free": "39.3", "wall": "16059"}
[2024-10-06 06:42:42,915][train_inner][INFO] - {"epoch": 98, "update": 97.749, "loss": "1.07", "ntokens": "240660", "nsentences": "1708.5", "wps": "144936", "ups": "0.6", "wpb": "240660", "bsz": "1708.5", "num_updates": "46800", "lr": "0.000479891", "gnorm": "0.282", "loss_scale": "4", "train_wall": "322", "gb_free": "40.1", "wall": "16030"}
[2024-10-06 06:43:22,626][train_inner][INFO] - {"epoch": 98, "update": 97.752, "loss": "1.07", "ntokens": "240697", "nsentences": "1707.62", "wps": "130496", "ups": "0.54", "wpb": "240697", "bsz": "1707.6", "num_updates": "46800", "lr": "0.000479891", "gnorm": "0.295", "loss_scale": "2", "train_wall": "363", "gb_free": "40.1", "wall": "16428"}
[2024-10-06 06:46:16,579][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 98 @ 46920 updates
[2024-10-06 06:46:16,603][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-06 06:46:24,532][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-06 06:46:24,535][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 98 @ 46920 updates, score None) (writing took 7.956014445982873 seconds)
[2024-10-06 06:46:24,536][fairseq_cli.train][INFO] - end of epoch 98 (average epoch stats below)
[2024-10-06 06:46:24,539][train][INFO] - {"epoch": 98, "train_loss": "1.068", "train_ntokens": "239634", "train_nsentences": "1753.71", "train_wps": "89707.7", "train_ups": "0.37", "train_wpb": "239634", "train_bsz": "1753.7", "train_num_updates": "46920", "train_lr": "0.000479728", "train_gnorm": "0.291", "train_loss_scale": "4", "train_train_wall": "813", "train_gb_free": "39.6", "train_wall": "16252"}
[2024-10-06 06:46:24,841][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 06:46:24,903][fairseq.trainer][INFO] - begin training epoch 99
[2024-10-06 06:46:24,911][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 06:46:27,324][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 98 @ 46919 updates
[2024-10-06 06:46:27,331][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-06 06:46:34,410][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-06 06:46:34,606][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 98 @ 46919 updates, score None) (writing took 7.282666205428541 seconds)
[2024-10-06 06:46:34,607][fairseq_cli.train][INFO] - end of epoch 98 (average epoch stats below)
[2024-10-06 06:46:34,614][train][INFO] - {"epoch": 98, "train_loss": "1.068", "train_ntokens": "239634", "train_nsentences": "1753.71", "train_wps": "93500", "train_ups": "0.39", "train_wpb": "239634", "train_bsz": "1753.7", "train_num_updates": "46919", "train_lr": "0.00047973", "train_gnorm": "0.299", "train_loss_scale": "2", "train_train_wall": "846", "train_gb_free": "39.6", "train_wall": "16620"}
[2024-10-06 06:46:34,683][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 06:46:34,707][fairseq.trainer][INFO] - begin training epoch 99
[2024-10-06 06:46:34,707][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 06:55:50,655][train_inner][INFO] - {"epoch": 99, "update": 98.169, "loss": "1.068", "ntokens": "238813", "nsentences": "1767.93", "wps": "63853", "ups": "0.27", "wpb": "238813", "bsz": "1767.9", "num_updates": "47000", "lr": "0.00047962", "gnorm": "0.303", "loss_scale": "2", "train_wall": "281", "gb_free": "39.3", "wall": "17176"}
[2024-10-06 06:55:56,906][train_inner][INFO] - {"epoch": 99, "update": 98.167, "loss": "1.068", "ntokens": "238862", "nsentences": "1765.84", "wps": "60168.7", "ups": "0.25", "wpb": "238862", "bsz": "1765.8", "num_updates": "47000", "lr": "0.00047962", "gnorm": "0.292", "loss_scale": "4", "train_wall": "414", "gb_free": "39.4", "wall": "16824"}
[2024-10-06 06:56:45,886][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2024-10-06 07:01:03,213][train_inner][INFO] - {"epoch": 99, "update": 98.587, "loss": "1.063", "ntokens": "239617", "nsentences": "1725.38", "wps": "153337", "ups": "0.64", "wpb": "239617", "bsz": "1725.4", "num_updates": "47200", "lr": "0.000479348", "gnorm": "0.299", "loss_scale": "2", "train_wall": "248", "gb_free": "39.3", "wall": "17488"}
[2024-10-06 07:01:36,146][train_inner][INFO] - {"epoch": 99, "update": 98.587, "loss": "1.063", "ntokens": "239559", "nsentences": "1728.77", "wps": "141236", "ups": "0.59", "wpb": "239559", "bsz": "1728.8", "num_updates": "47200", "lr": "0.000479348", "gnorm": "0.296", "loss_scale": "2", "train_wall": "334", "gb_free": "39.3", "wall": "17164"}
[2024-10-06 07:07:02,216][fairseq_cli.train][INFO] - end of epoch 99 (average epoch stats below)
[2024-10-06 07:07:02,283][train][INFO] - {"epoch": 99, "train_loss": "1.065", "train_ntokens": "239084", "train_nsentences": "1754.8", "train_wps": "92332.8", "train_ups": "0.39", "train_wpb": "239084", "train_bsz": "1754.8", "train_num_updates": "47398", "train_lr": "0.000479079", "train_gnorm": "0.305", "train_loss_scale": "2", "train_train_wall": "858", "train_gb_free": "39.8", "train_wall": "17490"}
[2024-10-06 07:07:02,489][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 07:07:02,511][fairseq.trainer][INFO] - begin training epoch 100
[2024-10-06 07:07:02,513][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 07:07:04,953][fairseq_cli.train][INFO] - end of epoch 99 (average epoch stats below)
[2024-10-06 07:07:04,974][train][INFO] - {"epoch": 99, "train_loss": "1.065", "train_ntokens": "239107", "train_nsentences": "1753.71", "train_wps": "93089.7", "train_ups": "0.39", "train_wpb": "239107", "train_bsz": "1753.7", "train_num_updates": "47398", "train_lr": "0.000479079", "train_gnorm": "0.3", "train_loss_scale": "2", "train_train_wall": "638", "train_gb_free": "39.8", "train_wall": "17850"}
[2024-10-06 07:07:05,226][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 07:07:05,261][fairseq.trainer][INFO] - begin training epoch 100
[2024-10-06 07:07:05,264][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 07:14:10,615][train_inner][INFO] - {"epoch": 100, "update": 99.004, "loss": "1.068", "ntokens": "238118", "nsentences": "1799.4", "wps": "63123.3", "ups": "0.27", "wpb": "238118", "bsz": "1799.4", "num_updates": "47400", "lr": "0.000479076", "gnorm": "0.314", "loss_scale": "2", "train_wall": "350", "gb_free": "40.6", "wall": "17918"}
[2024-10-06 07:14:11,394][train_inner][INFO] - {"epoch": 100, "update": 99.004, "loss": "1.068", "ntokens": "238118", "nsentences": "1799.4", "wps": "60424.3", "ups": "0.25", "wpb": "238118", "bsz": "1799.4", "num_updates": "47400", "lr": "0.000479076", "gnorm": "0.305", "loss_scale": "2", "train_wall": "294", "gb_free": "40.6", "wall": "18276"}
[2024-10-06 07:19:36,442][train_inner][INFO] - {"epoch": 100, "update": 99.422, "loss": "1.06", "ntokens": "239925", "nsentences": "1734.77", "wps": "147638", "ups": "0.62", "wpb": "239925", "bsz": "1734.8", "num_updates": "47600", "lr": "0.000478804", "gnorm": "0.274", "loss_scale": "4", "train_wall": "319", "gb_free": "39.3", "wall": "18601"}
[2024-10-06 07:19:41,739][train_inner][INFO] - {"epoch": 100, "update": 99.422, "loss": "1.06", "ntokens": "239925", "nsentences": "1734.77", "wps": "144928", "ups": "0.6", "wpb": "239925", "bsz": "1734.8", "num_updates": "47600", "lr": "0.000478804", "gnorm": "0.273", "loss_scale": "2", "train_wall": "326", "gb_free": "39.3", "wall": "18249"}
[2024-10-06 07:25:57,194][train_inner][INFO] - {"epoch": 100, "update": 99.839, "loss": "1.067", "ntokens": "239131", "nsentences": "1800.96", "wps": "125622", "ups": "0.53", "wpb": "239130", "bsz": "1801", "num_updates": "47800", "lr": "0.000478533", "gnorm": "0.291", "loss_scale": "4", "train_wall": "375", "gb_free": "40.5", "wall": "18982"}
[2024-10-06 07:26:05,737][train_inner][INFO] - {"epoch": 100, "update": 99.839, "loss": "1.066", "ntokens": "239131", "nsentences": "1800.96", "wps": "124549", "ups": "0.52", "wpb": "239130", "bsz": "1801", "num_updates": "47800", "lr": "0.000478533", "gnorm": "0.292", "loss_scale": "2", "train_wall": "378", "gb_free": "40.5", "wall": "18633"}
[2024-10-06 07:28:01,416][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 100 @ 47877 updates
[2024-10-06 07:28:01,417][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-06 07:28:12,684][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-06 07:28:12,817][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 100 @ 47877 updates, score None) (writing took 11.401401773095131 seconds)
[2024-10-06 07:28:12,829][fairseq_cli.train][INFO] - end of epoch 100 (average epoch stats below)
[2024-10-06 07:28:12,862][train][INFO] - {"epoch": 100, "train_loss": "1.064", "train_ntokens": "239201", "train_nsentences": "1753.71", "train_wps": "90370.9", "train_ups": "0.38", "train_wpb": "239201", "train_bsz": "1753.7", "train_num_updates": "47877", "train_lr": "0.000478428", "train_gnorm": "0.282", "train_loss_scale": "4", "train_train_wall": "819", "train_gb_free": "39.3", "train_wall": "19118"}
[2024-10-06 07:28:13,022][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 07:28:13,085][fairseq.trainer][INFO] - begin training epoch 101
[2024-10-06 07:28:13,085][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 07:28:25,831][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 100 @ 47877 updates
[2024-10-06 07:28:25,832][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-06 07:28:37,379][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-06 07:28:37,450][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 100 @ 47877 updates, score None) (writing took 11.618605921044946 seconds)
[2024-10-06 07:28:37,451][fairseq_cli.train][INFO] - end of epoch 100 (average epoch stats below)
[2024-10-06 07:28:37,458][train][INFO] - {"epoch": 100, "train_loss": "1.064", "train_ntokens": "239201", "train_nsentences": "1753.71", "train_wps": "88465.4", "train_ups": "0.37", "train_wpb": "239201", "train_bsz": "1753.7", "train_num_updates": "47877", "train_lr": "0.000478428", "train_gnorm": "0.281", "train_loss_scale": "2", "train_train_wall": "869", "train_gb_free": "39.3", "train_wall": "18785"}
[2024-10-06 07:28:37,541][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 07:28:37,549][fairseq.trainer][INFO] - begin training epoch 101
[2024-10-06 07:28:37,549][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 07:37:33,256][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2024-10-06 07:38:52,471][train_inner][INFO] - {"epoch": 101, "update": 100.259, "loss": "1.061", "ntokens": "238440", "nsentences": "1739.47", "wps": "61516.6", "ups": "0.26", "wpb": "238440", "bsz": "1739.5", "num_updates": "48000", "lr": "0.000478261", "gnorm": "0.294", "loss_scale": "2", "train_wall": "367", "gb_free": "40.1", "wall": "19758"}
[2024-10-06 07:39:15,292][train_inner][INFO] - {"epoch": 101, "update": 100.257, "loss": "1.061", "ntokens": "238334", "nsentences": "1747.47", "wps": "60373.5", "ups": "0.25", "wpb": "238334", "bsz": "1747.5", "num_updates": "48000", "lr": "0.000478261", "gnorm": "0.293", "loss_scale": "2", "train_wall": "326", "gb_free": "39.6", "wall": "19423"}
[2024-10-06 07:51:29,352][train_inner][INFO] - {"epoch": 101, "update": 100.674, "loss": "1.063", "ntokens": "239848", "nsentences": "1731.49", "wps": "65351.2", "ups": "0.27", "wpb": "239848", "bsz": "1731.5", "num_updates": "48200", "lr": "0.000477989", "gnorm": "0.29", "loss_scale": "2", "train_wall": "619", "gb_free": "39.8", "wall": "20157"}
[2024-10-06 07:51:31,085][train_inner][INFO] - {"epoch": 101, "update": 100.676, "loss": "1.063", "ntokens": "239796", "nsentences": "1737.14", "wps": "63222.8", "ups": "0.26", "wpb": "239796", "bsz": "1737.1", "num_updates": "48200", "lr": "0.000477989", "gnorm": "0.295", "loss_scale": "2", "train_wall": "742", "gb_free": "40", "wall": "20516"}
[2024-10-06 07:55:43,677][fairseq_cli.train][INFO] - end of epoch 101 (average epoch stats below)
[2024-10-06 07:55:43,803][train][INFO] - {"epoch": 101, "train_loss": "1.064", "train_ntokens": "239208", "train_nsentences": "1750.99", "train_wps": "69261.1", "train_ups": "0.29", "train_wpb": "239208", "train_bsz": "1751", "train_num_updates": "48355", "train_lr": "0.000477779", "train_gnorm": "0.294", "train_loss_scale": "2", "train_train_wall": "1234", "train_gb_free": "39.6", "train_wall": "20769"}
[2024-10-06 07:55:44,045][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 07:55:44,077][fairseq.trainer][INFO] - begin training epoch 102
[2024-10-06 07:55:44,078][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 07:55:44,830][fairseq_cli.train][INFO] - end of epoch 101 (average epoch stats below)
[2024-10-06 07:55:44,852][train][INFO] - {"epoch": 101, "train_loss": "1.064", "train_ntokens": "239166", "train_nsentences": "1753.71", "train_wps": "70395.8", "train_ups": "0.29", "train_wpb": "239166", "train_bsz": "1753.7", "train_num_updates": "48356", "train_lr": "0.000477777", "train_gnorm": "0.291", "train_loss_scale": "2", "train_train_wall": "1059", "train_gb_free": "39.6", "train_wall": "20412"}
[2024-10-06 07:55:45,309][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 07:55:45,408][fairseq.trainer][INFO] - begin training epoch 102
[2024-10-06 07:55:45,408][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 08:04:12,646][train_inner][INFO] - {"epoch": 102, "update": 101.092, "loss": "1.068", "ntokens": "238607", "nsentences": "1765.83", "wps": "62525.5", "ups": "0.26", "wpb": "238607", "bsz": "1765.8", "num_updates": "48400", "lr": "0.000477717", "gnorm": "0.281", "loss_scale": "2", "train_wall": "346", "gb_free": "39.6", "wall": "20920"}
[2024-10-06 08:04:17,062][train_inner][INFO] - {"epoch": 102, "update": 101.094, "loss": "1.068", "ntokens": "238675", "nsentences": "1760.88", "wps": "62321.9", "ups": "0.26", "wpb": "238675", "bsz": "1760.9", "num_updates": "48400", "lr": "0.000477717", "gnorm": "0.283", "loss_scale": "2", "train_wall": "313", "gb_free": "39.3", "wall": "21282"}
[2024-10-06 08:09:37,333][train_inner][INFO] - {"epoch": 102, "update": 101.509, "loss": "1.059", "ntokens": "239673", "nsentences": "1767.14", "wps": "147649", "ups": "0.62", "wpb": "239673", "bsz": "1767.1", "num_updates": "48600", "lr": "0.000477446", "gnorm": "0.284", "loss_scale": "2", "train_wall": "310", "gb_free": "39.3", "wall": "21245"}
[2024-10-06 08:10:30,821][train_inner][INFO] - {"epoch": 102, "update": 101.511, "loss": "1.059", "ntokens": "239634", "nsentences": "1767.48", "wps": "128237", "ups": "0.54", "wpb": "239634", "bsz": "1767.5", "num_updates": "48600", "lr": "0.000477446", "gnorm": "0.283", "loss_scale": "2", "train_wall": "212", "gb_free": "39.2", "wall": "21656"}
[2024-10-06 08:15:45,434][train_inner][INFO] - {"epoch": 102, "update": 101.927, "loss": "1.065", "ntokens": "239566", "nsentences": "1756.41", "wps": "130179", "ups": "0.54", "wpb": "239566", "bsz": "1756.4", "num_updates": "48800", "lr": "0.000477174", "gnorm": "0.284", "loss_scale": "2", "train_wall": "314", "gb_free": "40.1", "wall": "21613"}
[2024-10-06 08:16:11,774][train_inner][INFO] - {"epoch": 102, "update": 101.929, "loss": "1.065", "ntokens": "239627", "nsentences": "1754.59", "wps": "140571", "ups": "0.59", "wpb": "239627", "bsz": "1754.6", "num_updates": "48800", "lr": "0.000477174", "gnorm": "0.286", "loss_scale": "2", "train_wall": "250", "gb_free": "39.6", "wall": "21997"}
[2024-10-06 08:17:12,676][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 102 @ 48834 updates
[2024-10-06 08:17:12,676][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-06 08:17:14,048][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 102 @ 48835 updates
[2024-10-06 08:17:14,049][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-06 08:17:20,263][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-06 08:17:20,272][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 102 @ 48834 updates, score None) (writing took 7.596102230250835 seconds)
[2024-10-06 08:17:20,286][fairseq_cli.train][INFO] - end of epoch 102 (average epoch stats below)
[2024-10-06 08:17:20,295][train][INFO] - {"epoch": 102, "train_loss": "1.062", "train_ntokens": "239242", "train_nsentences": "1753.71", "train_wps": "88390.7", "train_ups": "0.37", "train_wpb": "239242", "train_bsz": "1753.7", "train_num_updates": "48834", "train_lr": "0.000477128", "train_gnorm": "0.287", "train_loss_scale": "2", "train_train_wall": "583", "train_gb_free": "39.3", "train_wall": "22065"}
[2024-10-06 08:17:20,412][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 08:17:20,421][fairseq.trainer][INFO] - begin training epoch 103
[2024-10-06 08:17:20,431][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 08:24:28,792][train_inner][INFO] - {"epoch": 103, "update": 102.347, "loss": "1.059", "ntokens": "238570", "nsentences": "1729.61", "wps": "96002", "ups": "0.4", "wpb": "238570", "bsz": "1729.6", "num_updates": "49000", "lr": "0.000476902", "gnorm": "0.309", "loss_scale": "2", "train_wall": "188", "gb_free": "40.3", "wall": "22494"}
[2024-10-06 08:28:03,523][train_inner][INFO] - {"epoch": 103, "update": 102.764, "loss": "1.062", "ntokens": "239648", "nsentences": "1758.73", "wps": "223213", "ups": "0.93", "wpb": "239648", "bsz": "1758.7", "num_updates": "49200", "lr": "0.00047663", "gnorm": "0.283", "loss_scale": "2", "train_wall": "211", "gb_free": "39.2", "wall": "22709"}
[2024-10-06 08:30:24,585][fairseq_cli.train][INFO] - end of epoch 103 (average epoch stats below)
[2024-10-06 08:30:24,604][train][INFO] - {"epoch": 103, "train_loss": "1.061", "train_ntokens": "239117", "train_nsentences": "1753.71", "train_wps": "146036", "train_ups": "0.61", "train_wpb": "239117", "train_bsz": "1753.7", "train_num_updates": "49313", "train_lr": "0.000476477", "train_gnorm": "0.289", "train_loss_scale": "2", "train_train_wall": "482", "train_gb_free": "39.4", "train_wall": "22850"}
[2024-10-06 08:30:24,712][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 08:30:24,730][fairseq.trainer][INFO] - begin training epoch 104
[2024-10-06 08:30:24,730][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 08:36:20,480][train_inner][INFO] - {"epoch": 104, "update": 103.182, "loss": "1.062", "ntokens": "238506", "nsentences": "1765.42", "wps": "96020.5", "ups": "0.4", "wpb": "238506", "bsz": "1765.4", "num_updates": "49400", "lr": "0.000476359", "gnorm": "0.282", "loss_scale": "2", "train_wall": "235", "gb_free": "39.6", "wall": "23206"}
[2024-10-06 08:39:17,668][train_inner][INFO] - {"epoch": 104, "update": 103.599, "loss": "1.057", "ntokens": "239412", "nsentences": "1762.79", "wps": "270239", "ups": "1.13", "wpb": "239412", "bsz": "1762.8", "num_updates": "49600", "lr": "0.000476087", "gnorm": "0.29", "loss_scale": "2", "train_wall": "173", "gb_free": "40.1", "wall": "23383"}
[2024-10-06 08:42:06,396][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 104 @ 49792 updates
[2024-10-06 08:42:06,403][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-06 08:42:13,046][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-06 08:42:13,066][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 104 @ 49792 updates, score None) (writing took 6.669908362440765 seconds)
[2024-10-06 08:42:13,067][fairseq_cli.train][INFO] - end of epoch 104 (average epoch stats below)
[2024-10-06 08:42:13,086][train][INFO] - {"epoch": 104, "train_loss": "1.061", "train_ntokens": "239328", "train_nsentences": "1753.71", "train_wps": "161812", "train_ups": "0.68", "train_wpb": "239328", "train_bsz": "1753.7", "train_num_updates": "49792", "train_lr": "0.000475826", "train_gnorm": "0.286", "train_loss_scale": "2", "train_train_wall": "434", "train_gb_free": "39.7", "train_wall": "23558"}
[2024-10-06 08:42:13,146][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 08:42:13,158][fairseq.trainer][INFO] - begin training epoch 105
[2024-10-06 08:42:13,158][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 08:47:17,263][train_inner][INFO] - {"epoch": 105, "update": 104.017, "loss": "1.065", "ntokens": "239422", "nsentences": "1722.94", "wps": "99848.1", "ups": "0.42", "wpb": "239422", "bsz": "1722.9", "num_updates": "49800", "lr": "0.000475815", "gnorm": "0.281", "loss_scale": "2", "train_wall": "174", "gb_free": "40.1", "wall": "23862"}
[2024-10-06 08:49:57,993][train_inner][INFO] - {"epoch": 105, "update": 104.434, "loss": "1.056", "ntokens": "240127", "nsentences": "1730.13", "wps": "298812", "ups": "1.24", "wpb": "240127", "bsz": "1730.1", "num_updates": "50000", "lr": "0.000475543", "gnorm": "0.282", "loss_scale": "4", "train_wall": "128", "gb_free": "39.1", "wall": "24023"}
[2024-10-06 08:49:57,998][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 105 @ 50000 updates
[2024-10-06 08:49:57,999][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_105_50000.pt
[2024-10-06 08:50:00,941][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_105_50000.pt
[2024-10-06 08:50:05,895][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_105_50000.pt (epoch 105 @ 50000 updates, score None) (writing took 7.896722903475165 seconds)
[2024-10-06 08:53:17,581][train_inner][INFO] - {"epoch": 105, "update": 104.852, "loss": "1.06", "ntokens": "239218", "nsentences": "1791.56", "wps": "239720", "ups": "1", "wpb": "239218", "bsz": "1791.6", "num_updates": "50200", "lr": "0.000475272", "gnorm": "0.284", "loss_scale": "4", "train_wall": "154", "gb_free": "40.2", "wall": "24223"}
[2024-10-06 08:54:35,057][fairseq_cli.train][INFO] - end of epoch 105 (average epoch stats below)
[2024-10-06 08:54:35,172][train][INFO] - {"epoch": 105, "train_loss": "1.059", "train_ntokens": "239262", "train_nsentences": "1753.71", "train_wps": "154440", "train_ups": "0.65", "train_wpb": "239262", "train_bsz": "1753.7", "train_num_updates": "50271", "train_lr": "0.000475175", "train_gnorm": "0.285", "train_loss_scale": "4", "train_train_wall": "360", "train_gb_free": "39.2", "train_wall": "24300"}
[2024-10-06 08:54:36,596][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 08:54:36,710][fairseq.trainer][INFO] - begin training epoch 106
[2024-10-06 08:54:36,710][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 09:01:24,386][train_inner][INFO] - {"epoch": 106, "update": 105.269, "loss": "1.06", "ntokens": "238401", "nsentences": "1741.05", "wps": "97946.9", "ups": "0.41", "wpb": "238401", "bsz": "1741", "num_updates": "50400", "lr": "0.000475", "gnorm": "0.292", "loss_scale": "4", "train_wall": "199", "gb_free": "39.6", "wall": "24709"}
[2024-10-06 09:02:02,654][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2024-10-06 09:04:17,350][train_inner][INFO] - {"epoch": 106, "update": 105.689, "loss": "1.059", "ntokens": "239857", "nsentences": "1770.16", "wps": "277353", "ups": "1.16", "wpb": "239857", "bsz": "1770.2", "num_updates": "50600", "lr": "0.000474728", "gnorm": "0.291", "loss_scale": "2", "train_wall": "168", "gb_free": "39.2", "wall": "24882"}
[2024-10-06 09:07:02,186][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 106 @ 50749 updates
[2024-10-06 09:07:02,188][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-06 09:07:05,479][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-06 09:07:05,481][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 106 @ 50749 updates, score None) (writing took 3.295080492272973 seconds)
[2024-10-06 09:07:05,482][fairseq_cli.train][INFO] - end of epoch 106 (average epoch stats below)
[2024-10-06 09:07:05,484][train][INFO] - {"epoch": 106, "train_loss": "1.059", "train_ntokens": "239235", "train_nsentences": "1752.73", "train_wps": "152410", "train_ups": "0.64", "train_wpb": "239235", "train_bsz": "1752.7", "train_num_updates": "50749", "train_lr": "0.000474526", "train_gnorm": "0.29", "train_loss_scale": "2", "train_train_wall": "459", "train_gb_free": "39.6", "train_wall": "25051"}
[2024-10-06 09:07:05,608][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 09:07:05,617][fairseq.trainer][INFO] - begin training epoch 107
[2024-10-06 09:07:05,618][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 09:13:41,688][train_inner][INFO] - {"epoch": 107, "update": 106.106, "loss": "1.059", "ntokens": "238473", "nsentences": "1731.81", "wps": "84514.7", "ups": "0.35", "wpb": "238473", "bsz": "1731.8", "num_updates": "50800", "lr": "0.000474457", "gnorm": "0.292", "loss_scale": "2", "train_wall": "261", "gb_free": "40.1", "wall": "25447"}
[2024-10-06 09:19:08,936][train_inner][INFO] - {"epoch": 107, "update": 106.524, "loss": "1.054", "ntokens": "238943", "nsentences": "1791.91", "wps": "146073", "ups": "0.61", "wpb": "238943", "bsz": "1791.9", "num_updates": "51000", "lr": "0.000474185", "gnorm": "0.285", "loss_scale": "2", "train_wall": "325", "gb_free": "39.6", "wall": "25774"}
[2024-10-06 09:24:00,667][train_inner][INFO] - {"epoch": 107, "update": 106.942, "loss": "1.06", "ntokens": "239967", "nsentences": "1735.12", "wps": "164552", "ups": "0.69", "wpb": "239967", "bsz": "1735.1", "num_updates": "51200", "lr": "0.000473913", "gnorm": "0.288", "loss_scale": "2", "train_wall": "289", "gb_free": "39.3", "wall": "26066"}
[2024-10-06 09:24:38,601][fairseq_cli.train][INFO] - end of epoch 107 (average epoch stats below)
[2024-10-06 09:24:38,614][train][INFO] - {"epoch": 107, "train_loss": "1.057", "train_ntokens": "239032", "train_nsentences": "1753.71", "train_wps": "108722", "train_ups": "0.45", "train_wpb": "239032", "train_bsz": "1753.7", "train_num_updates": "51228", "train_lr": "0.000473875", "train_gnorm": "0.288", "train_loss_scale": "2", "train_train_wall": "750", "train_gb_free": "40.2", "train_wall": "26104"}
[2024-10-06 09:24:38,755][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 09:24:38,780][fairseq.trainer][INFO] - begin training epoch 108
[2024-10-06 09:24:38,782][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 09:33:47,118][train_inner][INFO] - {"epoch": 108, "update": 107.359, "loss": "1.058", "ntokens": "238388", "nsentences": "1792.29", "wps": "81300.2", "ups": "0.34", "wpb": "238388", "bsz": "1792.3", "num_updates": "51400", "lr": "0.000473641", "gnorm": "0.282", "loss_scale": "2", "train_wall": "275", "gb_free": "40.5", "wall": "26652"}
[2024-10-06 09:37:51,565][train_inner][INFO] - {"epoch": 108, "update": 107.777, "loss": "1.057", "ntokens": "239924", "nsentences": "1743.71", "wps": "196342", "ups": "0.82", "wpb": "239924", "bsz": "1743.7", "num_updates": "51600", "lr": "0.00047337", "gnorm": "0.271", "loss_scale": "2", "train_wall": "227", "gb_free": "39.6", "wall": "26897"}
[2024-10-06 09:40:11,716][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 108 @ 51707 updates
[2024-10-06 09:40:11,729][fairseq.trainer][INFO] - Saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-06 09:40:21,540][fairseq.trainer][INFO] - Finished saving checkpoint to /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt
[2024-10-06 09:40:21,545][fairseq.checkpoint_utils][INFO] - Saved checkpoint /share/data/pals/shester/sign_dinosr_logs/outputs/masking_gloss_400/ckpt/checkpoint_last.pt (epoch 108 @ 51707 updates, score None) (writing took 9.828397542238235 seconds)
[2024-10-06 09:40:21,545][fairseq_cli.train][INFO] - end of epoch 108 (average epoch stats below)
[2024-10-06 09:40:21,562][train][INFO] - {"epoch": 108, "train_loss": "1.057", "train_ntokens": "239334", "train_nsentences": "1753.71", "train_wps": "121580", "train_ups": "0.51", "train_wpb": "239334", "train_bsz": "1753.7", "train_num_updates": "51707", "train_lr": "0.000473224", "train_gnorm": "0.279", "train_loss_scale": "2", "train_train_wall": "582", "train_gb_free": "39.2", "train_wall": "27047"}
[2024-10-06 09:40:21,633][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 09:40:21,652][fairseq.trainer][INFO] - begin training epoch 109
[2024-10-06 09:40:21,653][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 09:46:51,609][train_inner][INFO] - {"epoch": 109, "update": 108.194, "loss": "1.055", "ntokens": "239025", "nsentences": "1712.86", "wps": "88530.9", "ups": "0.37", "wpb": "239025", "bsz": "1712.9", "num_updates": "51800", "lr": "0.000473098", "gnorm": "0.288", "loss_scale": "2", "train_wall": "225", "gb_free": "39.9", "wall": "27437"}
[2024-10-06 09:49:59,006][train_inner][INFO] - {"epoch": 109, "update": 108.612, "loss": "1.052", "ntokens": "239927", "nsentences": "1738.11", "wps": "256108", "ups": "1.07", "wpb": "239927", "bsz": "1738.1", "num_updates": "52000", "lr": "0.000472826", "gnorm": "0.283", "loss_scale": "2", "train_wall": "183", "gb_free": "39.3", "wall": "27624"}
[2024-10-06 09:52:54,621][fairseq_cli.train][INFO] - end of epoch 109 (average epoch stats below)
[2024-10-06 09:52:54,633][train][INFO] - {"epoch": 109, "train_loss": "1.055", "train_ntokens": "239109", "train_nsentences": "1753.71", "train_wps": "152089", "train_ups": "0.64", "train_wpb": "239109", "train_bsz": "1753.7", "train_num_updates": "52186", "train_lr": "0.000472573", "train_gnorm": "0.284", "train_loss_scale": "2", "train_train_wall": "462", "train_gb_free": "40.1", "train_wall": "27800"}
[2024-10-06 09:52:54,915][fairseq.data.iterators][INFO] - grouped total_num_itrs = 479
[2024-10-06 09:52:54,936][fairseq.trainer][INFO] - begin training epoch 110
[2024-10-06 09:52:54,937][fairseq_cli.train][INFO] - Start iterating over samples
[2024-10-06 09:58:06,267][train_inner][INFO] - {"epoch": 110, "update": 109.029, "loss": "1.059", "ntokens": "237759", "nsentences": "1774.72", "wps": "97591.7", "ups": "0.41", "wpb": "237759", "bsz": "1774.7", "num_updates": "52200", "lr": "0.000472554", "gnorm": "0.283", "loss_scale": "2", "train_wall": "205", "gb_free": "39.8", "wall": "28111"}
[2024-10-06 10:01:12,232][train_inner][INFO] - {"epoch": 110, "update": 109.447, "loss": "1.05", "ntokens": "239628", "nsentences": "1762.07", "wps": "257731", "ups": "1.08", "wpb": "239628", "bsz": "1762.1", "num_updates": "52400", "lr": "0.000472283", "gnorm": "0.29", "loss_scale": "2", "train_wall": "183", "gb_free": "39.6", "wall": "28297"}
